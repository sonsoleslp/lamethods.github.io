@article{Adnan_Habib_Ashraf_Mussadiq_Raza_Abid_Bashir_Khan_2021,
	title        = {Predicting at-Risk Students at Different Percentages of Course Length for Early Intervention Using Machine Learning Models},
	author       = {Adnan, Muhammad and Habib, Asad and Ashraf, Jawad and Mussadiq, Shafaq and Raza, Arsalan Ali and Abid, Muhammad and Bashir, Maryam and Khan, Sana Ullah},
	year         = 2021,
	journal      = {IEEE Access},
	volume       = 9,
	pages        = {7519–7539},
	doi          = {10.1109/ACCESS.2021.3049446},
	issn         = {2169-3536},
	url          = {http://dx.doi.org/10.1109/ACCESS.2021.3049446},
	abstractnote = {Online learning platforms such as Massive Open Online Course (MOOC), Virtual Learning Environments (VLEs), and Learning Management Systems (LMS) facilitate thousands or even millions of students to learn according to their interests without spatial and temporal constraints. Besides many advantages, online learning platforms face several challenges such as students’ lack of interest, high dropouts, low engagement, students’ self-regulated behavior, and compelling students to take responsibility for settings their own goals. In this study, we propose a predictive model that analyzes the problems faced by at-risk students, subsequently, facilitating instructors for timely intervention to persuade students to increase their study engagements and improve their study performance. The predictive model is trained and tested using various machine learning (ML) and deep learning (DL) algorithms to characterize the learning behavior of students according to their study variables. The performance of various ML algorithms is compared by using accuracy, precision, support, and f-score. The ML algorithm that gives the best result in terms of accuracy, precision, recall, support, and f-score metric is ultimately selected for creating the predictive model at different percentages of course length. The predictive model can help instructors in identifying at-risk students early in the course for timely intervention thus avoiding student dropouts. Our results showed that students’ assessment scores, engagement intensity i.e. clickstream data, and time-dependent variables are important factors in online learning. The experimental results revealed that the predictive model trained using Random Forest (RF) gives the best results with averaged precision =0.60%, 0.79%, 0.84%, 0.88%, 0.90%, 0.92%, averaged recall =0.59%, 0.79%, 0.84%, 0.88%, 0.90%, 0.91%, averaged F-score =0.59%, 0.79%, 0.84%, 0.88%, 0.90%, 0.91%, and average accuracy =0.59%, 0.79%, 0.84%, 0.88%, 0.90%, 0.91% at 0%, 20%, 40%, 60%, 80% and 100% of course length.},
	keywords     = {Predictive models;Prediction algorithms;Electronic learning;Machine learning algorithms;Support vector machines;Random forests;Predictive model;earliest possible prediction;at-risk students;machine learning;feed-forward neural network;random forest;early intervention}
}
@article{Agudo-Peregrina_Iglesias-Pradas_Conde-González_Hernández-García_2014,
	title        = {Can we predict success from log data in VLEs? Classification of interactions for learning analytics and their relation with performance in VLE-supported F2F and online learning},
	author       = {Agudo-Peregrina, Ángel F. and Iglesias-Pradas, Santiago and Conde-González, Miguel Ángel and Hernández-García, Ángel},
	year         = 2014,
	month        = {Feb},
	journal      = {Computers in human behavior},
	volume       = 31,
	number       = 1,
	pages        = {542–550},
	doi          = {10.1016/j.chb.2013.05.031},
	issn         = {0747-5632},
	url          = {http://linkinghub.elsevier.com/retrieve/pii/S074756321300188X},
	abstractnote = {Learning analytics is the analysis of electronic learning data which allows teachers, course designers and administrators of virtual learning environments to search for unobserved patterns and underlying information in learning processes. The main aim of learning analytics is to improve learning outcomes and the overall learning process in electronic learning virtual classrooms and computer-supported education. The most basic unit of learning data in virtual learning environments for learning analytics is the interaction, but there is no consensus yet on which interactions are relevant for effective learning. Drawing upon extant literature, this research defines three system-independent classifications of interactions and evaluates the relation of their components with academic performance across two different learning modalities: virtual learning environment (VLE) supported face-to-face (F2F) and online learning. In order to do so, we performed an empirical study with data from six online and two VLE-supported F2F courses. Data extraction and analysis required the development of an ad hoc tool based on the proposed interaction classification. The main finding from this research is that, for each classification, there is a relation between some type of interactions and academic performance in online courses, whereas this relation is non-significant in the case of VLE-supported F2F courses. Implications for theory and practice are discussed next. © 2013 Elsevier Ltd. All rights reserved.},
	keywords     = {Academic performance; Educational data; Interactions; Learning analytics; Virtual learning environments; e-Learning}
}
@article{Ahmad_Schneider_Griffiths_Biedermann_Schiffner_Greller_Drachsler_2022,
	title        = {Connecting the dots – A literature review on learning analytics indicators from a learning design perspective},
	author       = {Ahmad, Atezaz and Schneider, Jan and Griffiths, Dai and Biedermann, Daniel and Schiffner, Daniel and Greller, Wolfgang and Drachsler, Hendrik},
	year         = 2022,
	month        = {Jul},
	journal      = {Journal of computer assisted learning},
	publisher    = {Wiley},
	doi          = {10.1111/jcal.12716},
	issn         = {0266-4909},
	url          = {https://onlinelibrary.wiley.com/doi/10.1111/jcal.12716},
	abstractnote = {Abstract Background During the past decade, the increasingly heterogeneous field of learning analytics has been critiqued for an over-emphasis on data-driven approaches at the expense of paying attention to learning designs. Method and objective In response to this critique, we investigated the role of learning design in learning analytics through a systematic literature review. 161 learning analytics (LA) articles were examined to identify indicators that were based on learning design events and their associated metrics. Through this research, we address two objectives. First, to achieve a better alignment between learning design and learning analytics by proposing a reference framework, where we present possible connections between learning analytics and learning design. Second, to present how LA indicators and metrics have been researched and applied in the past. Results and conclusion In our review, we found that a number of learning analytics papers did indeed consider learning design activities for harvesting user data. We also found a consistent increase in the number and quality of indicators and their evolution over the years.},
	language     = {en}
}
@article{Albreiki_Zaki_Alashwal_2021,
	title        = {A Systematic Literature Review of Student’ Performance Prediction Using Machine Learning Techniques},
	author       = {Albreiki, Balqis and Zaki, Nazar and Alashwal, Hany},
	year         = 2021,
	month        = {Sep},
	journal      = {Education Sciences},
	publisher    = {Multidisciplinary Digital Publishing Institute},
	volume       = 11,
	number       = 9,
	pages        = 552,
	doi          = {10.3390/educsci11090552},
	issn         = {2227-7102},
	url          = {https://www.mdpi.com/2227-7102/11/9/552},
	abstractnote = {Educational Data Mining plays a critical role in advancing the learning environment by contributing state-of-the-art methods, techniques, and applications. The recent development provides valuable tools for understanding the student learning environment by exploring and utilizing educational data using machine learning and data mining techniques. Modern academic institutions operate in a highly competitive and complex environment. Analyzing performance, providing high-quality education, strategies for evaluating the students’ performance, and future actions are among the prevailing challenges universities face. Student intervention plans must be implemented in these universities to overcome problems experienced by the students during their studies. In this systematic review, the relevant EDM literature related to identifying student dropouts and students at risk from 2009 to 2021 is reviewed. The review results indicated that various Machine Learning (ML) techniques are used to understand and overcome the underlying challenges; predicting students at risk and students drop out prediction. Moreover, most studies use two types of datasets: data from student colleges/university databases and online learning platforms. ML methods were confirmed to play essential roles in predicting students at risk and dropout rates, thus improving the students’ performance.},
	language     = {en}
}
@article{Arnold_Pistilli_2012,
	title        = {Course signals at Purdue},
	author       = {Arnold, Kimberly E. and Pistilli, Matthew D.},
	year         = 2012,
	journal      = {Proceedings of the 2nd International Conference on Learning Analytics and Knowledge - LAK ’12},
	number       = {October},
	pages        = {267–267},
	doi          = {10.1145/2330601.2330666},
	url          = {http://dl.acm.org/citation.cfm?doid=2330601.2330666},
	abstractnote = {In this paper, an early intervention solution for collegiate faculty called Course Signals is discussed. Course Signals was developed to allow instructors the opportunity to employ the power of learner analytics to provide real-time feedback to a student. Course Signals relies not only on grades to predict students’ performance, but also demographic characteristics, past academic history, and students' effort as measured by interaction with Blackboard Vista, Purdue's learning management system. The outcome is delivered to the students via a personalized email from the faculty member to each student, as well as a specific color on a stoplight - traffic signal - to indicate how each student is doing. The system itself is explained in detail, along with retention and performance outcomes realized since its implementation. In addition, faculty and student perceptions will be shared. © 2012 ACM.},
	keywords     = {college student success; early intervention; learning analytics; retention}
}
@article{Asselman_Khaldi_Aammou_2021,
	title        = {Enhancing the prediction of student performance based on the machine learning XGBoost algorithm},
	author       = {Asselman, Amal and Khaldi, Mohamed and Aammou, Souhaib},
	year         = 2021,
	month        = {May},
	journal      = {Interactive Learning Environments},
	publisher    = {Routledge},
	pages        = {1–20},
	doi          = {10.1080/10494820.2021.1928235},
	issn         = {1049-4820},
	url          = {https://doi.org/10.1080/10494820.2021.1928235},
	abstractnote = {ABSTRACTPerformance Factors Analysis (PFA) is considered one of the most important Knowledge Tracing (KT) approaches used for constructing adaptive educational hypermedia systems. It has shown a high prediction accuracy against many other KT approaches. While, the desire to estimate more accurately the student level leads researchers to enhance PFA by inventing several advanced extensions. However, most of the proposed extensions have exclusively been improved in a pedagogical sense, as the improvements have mostly been limited to the analysis of students? behaviour during their learning process. In contrast, Machine Learning provides many powerful methods that could be efficient to enhance, in the technical sense, the prediction of student performance. Our goal is to focus on the exploitation of Ensemble Learning methods as an extremely effective Machine Learning paradigm used to create many advanced solutions in several fields. In this sense, we propose a new PFA approach based on different models (Random Forest, AdaBoost, and XGBoost) in order to increase the predictive accuracy of student performance. Our models have been evaluated on three different datasets. The experimental results show that the scalable XGBoost has outperformed the other evaluated models and substantially improved the performance prediction compared to the original PFA algorithm.}
}
@article{Badal_Sungkur_2023,
	title        = {Predictive modelling and analytics of students’ grades using machine learning algorithms},
	author       = {Badal, Yudish Teshal and Sungkur, Roopesh Kevin},
	year         = 2023,
	journal      = {Education and information technologies},
	volume       = 28,
	number       = 3,
	pages        = {3027–3057},
	doi          = {10.1007/s10639-022-11299-8},
	issn         = {1360-2357},
	url          = {http://dx.doi.org/10.1007/s10639-022-11299-8},
	abstractnote = {The outbreak of COVID-19 has caused significant disruption in all sectors and industries around the world. To tackle the spread of the novel coronavirus, the learning process and the modes of delivery had to be altered. Most courses are delivered traditionally with face-to-face or a blended approach through online learning platforms. In addition, researchers and educational specialists around the globe always had a keen interest in predicting a student’s performance based on the student's information such as previous exam results obtained and experiences. With the upsurge in using online learning platforms, predicting the student's performance by including their interactions such as discussion forums could be integrated to create a predictive model. The aims of the research are to provide a predictive model to forecast students' performance (grade/engagement) and to analyse the effect of online learning platform's features. The model created in this study made use of machine learning techniques to predict the final grade and engagement level of a learner. The quantitative approach for student's data analysis and processing proved that the Random Forest classifier outperformed the others. An accuracy of 85% and 83% were recorded for grade and engagement prediction respectively with attributes related to student profile and interaction on a learning platform.},
	keywords     = {Machine learning; Online learning platform; Predictive analysis; Random forest; Student engagement},
	language     = {en}
}
@article{Baker_Yacef_Others_2009,
	title        = {The state of educational data mining in 2009: A review and future visions},
	author       = {Baker, Ryan Sjd and Yacef, Kalina and Others},
	year         = 2009,
	journal      = {Journal of educational data mining},
	volume       = 1,
	number       = 1,
	pages        = {3–17},
	url          = {https://jedm.educationaldatamining.org/index.php/JEDM/article/view/8}
}
@article{Bañeres_Rodríguez_Guerrero-Roldán_Karadeniz_2020,
	title        = {An Early Warning System to Detect At-Risk Students in Online Higher Education},
	author       = {Bañeres, David and Rodríguez, M. Elena and Guerrero-Roldán, Ana Elena and Karadeniz, Abdulkadir},
	year         = 2020,
	month        = {Jun},
	journal      = {NATO Advanced Science Institutes series E: Applied sciences},
	publisher    = {Multidisciplinary Digital Publishing Institute},
	volume       = 10,
	number       = 13,
	pages        = 4427,
	doi          = {10.3390/app10134427},
	issn         = {0168-132X},
	url          = {https://www.mdpi.com/2076-3417/10/13/4427},
	abstractnote = {Artificial intelligence has impacted education in recent years. Datafication of education has allowed developing automated methods to detect patterns in extensive collections of educational data to estimate unknown information and behavior about the students. This research has focused on finding accurate predictive models to identify at-risk students. This challenge may reduce the students’ risk of failure or disengage by decreasing the time lag between identification and the real at-risk state. The contribution of this paper is threefold. First, an in-depth analysis of a predictive model to detect at-risk students is performed. This model has been tested using data available in an institutional data mart where curated data from six semesters are available, and a method to obtain the best classifier and training set is proposed. Second, a method to determine a threshold for evaluating the quality of the predictive model is established. Third, an early warning system has been developed and tested in a real educational setting being accurate and useful for its purpose to detect at-risk students in online higher education. The stakeholders (i.e., students and teachers) can analyze the information through different dashboards, and teachers can also send early feedback as an intervention mechanism to mitigate at-risk situations. The system has been evaluated on two undergraduate courses where results shown a high accuracy to correctly detect at-risk students.},
	language     = {en}
}
@article{Breiman_2001,
	title        = {Random Forests},
	author       = {Breiman, Leo},
	year         = 2001,
	month        = {Oct},
	journal      = {Machine learning},
	volume       = 45,
	number       = 1,
	pages        = {5–32},
	doi          = {10.1023/A:1010933404324},
	issn         = {0885-6125},
	url          = {https://doi.org/10.1023/A:1010933404324},
	abstractnote = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.}
}
@inbook{Brooks_Thompson_2017,
	title        = {Predictive Modelling in Teaching and Learning},
	author       = {Brooks, Christopher and Thompson, Craig},
	year         = 2017,
	month        = {May},
	booktitle    = {Handbook of Learning Analytics},
	publisher    = {Society for Learning Analytics Research (SoLAR)},
	pages        = {61–68},
	doi          = {10.18608/hla17.005},
	url          = {https://solaresearch.org/hla-17/hla17-chapter5},
	keywords     = {0 ihdwxuh; 3uhglfwlyh dqdo; 3uhglfwlyh prgholqj pdfklqh ohduqlqj; hgxfdwlrqdo gdwd plqlqj; model evaluation; ri whfkqltxhv xvhg; selection; to make inferences about; uncertain future events; wlfv duh d jurxs}
}
@article{Bulut_Gorgun_Yildirim-Erbasli_Wongvorachan_Daniels_Gao_Lai_Shin_2023,
	title        = {Standing on the shoulders of giants: Online formative assessments as the foundation for predictive learning analytics models},
	author       = {Bulut, Okan and Gorgun, Guher and Yildirim-Erbasli, Seyma N. and Wongvorachan, Tarid and Daniels, Lia M. and Gao, Yizhu and Lai, Ka Wing and Shin, Jinnie},
	year         = 2023,
	month        = {Jan},
	journal      = {British journal of educational technology: journal of the Council for Educational Technology},
	publisher    = {Wiley},
	volume       = 54,
	number       = 1,
	pages        = {19–39},
	doi          = {10.1111/bjet.13276},
	issn         = {0007-1013},
	url          = {https://onlinelibrary.wiley.com/doi/10.1111/bjet.13276},
	language     = {en}
}
@article{Campbell_DeBlois_Oblinger_2007,
	title        = {Academic Analytics},
	author       = {Campbell, J. P. and DeBlois, P. B. and Oblinger, D. G.},
	year         = 2007,
	journal      = {Educause Review},
	volume       = 42,
	number       = {October},
	pages        = {40–57},
	issn         = {1527-6619},
	url          = {http://net.educause.edu/ir/library/pdf/ERM0742.pdf%5Cnhttp://net.educause.edu/ir/library/pdf/erm0742.pdf%5Cnhttp://eric.ed.gov/ERICWebPortal/recordDetail?accno=EJ769402%5Cnhttp://net.educause.edu/ir/library/pdf/ERM0742.pdf%5Cnhttp://net.educause.edu/ir/li},
	abstractnote = {The importance of student success (commonly measured as degree completion) continues to rise, as does the demand for institutional accountability. Academic analytics can help institutions address student success and accountability while better fulfilling their academic missions. Academic systems generate a wide array of data that can predict retention and graduation. Academic analytics marries that data with statistical techniques and predictive modeling to help faculty and advisors determine which students may face academic difficulty, allowing interventions to help them succeed. This paper highlights what IT and institutional leaders need to understand about academic analytics, including changes it may require in data standards, systems, processes, policies, and institutional culture.}
}
@article{Caulfield_2013,
	title        = {What the course signals “kerfuffle” is about, and what it means to you},
	author       = {Caulfield, M.},
	year         = 2013,
	journal      = {EDUCAUSE. edu}
}
@article{Chen_Brinton_Cao_Mason-Singh_Lu_Chiang_2019,
	title        = {Early Detection Prediction of Learning Outcomes in Online Short-Courses via Learning Behaviors},
	author       = {Chen, Weiyu and Brinton, Christopher G. and Cao, Da and Mason-Singh, Amanda and Lu, Charlton and Chiang, Mung},
	year         = 2019,
	month        = {Jan},
	journal      = {IEEE Transactions on Learning Technologies},
	volume       = 12,
	number       = 1,
	pages        = {44–58},
	doi          = {10.1109/TLT.2018.2793193},
	issn         = {1939-1382},
	url          = {http://dx.doi.org/10.1109/TLT.2018.2793193},
	abstractnote = {We study learning outcome prediction for online courses. Whereas prior work has focused on semester-long courses with frequent student assessments, we focus on short-courses that have single outcomes assigned by instructors at the end. The lack of performance data and generally small enrollments makes the behavior of learners, captured as they interact with course content and with one another in Social Learning Networks (SLN), essential for prediction. Our method defines several (machine) learning features based on the processing of behaviors collected on the modes of (human) learning in a course, and uses them in appropriate classifiers. Through evaluation on data captured from three two-week courses hosted through our delivery platforms, we make three key observations: (i) behavioral data contains signals predictive of learning outcomes in short-courses (with classifiers achieving AUCs ≥ 0.8 after the two weeks), (ii) early detection is possible within the first week (AUCs ≥ 0.7 with the first week of data), and (iii) the content features have an “earliest” detection capability (with higher AUC in the first few days), while the SLN features become the more predictive set over time as the network matures. We also discuss how our method can generate behavioral analytics for instructors.},
	keywords     = {Databases;Programmable logic arrays;Data models;Feature extraction;Training;Predictive models;Computational modeling;Clickstream data;data mining;predictive learning analytics;learning outcome prediction;social learning networks.}
}
@article{Conijn_Snijders_Kleingeld_Matzat_2017,
	title        = {Predicting Student Performance from LMS Data: A Comparison of 17 Blended Courses Using Moodle LMS},
	author       = {Conijn, Rianne and Snijders, Chris and Kleingeld, Ad and Matzat, Uwe},
	year         = 2017,
	month        = {Jan},
	journal      = {IEEE Transactions on Learning Technologies},
	volume       = 10,
	number       = 1,
	pages        = {17–29},
	doi          = {10.1109/TLT.2016.2616312},
	issn         = {1939-1382},
	url          = {http://dx.doi.org/10.1109/TLT.2016.2616312},
	abstractnote = {With the adoption of Learning Management Systems (LMSs) in educational institutions, a lot of data has become available describing students’ online behavior. Many researchers have used these data to predict student performance. This has led to a rather diverse set of findings, possibly related to the diversity in courses and predictor variables extracted from the LMS, which makes it hard to draw general conclusions about the mechanisms underlying student performance. We first provide an overview of the theoretical arguments used in learning analytics research and the typical predictors that have been used in recent studies. We then analyze 17 blended courses with 4,989 students in a single institution using Moodle LMS, in which we predict student performance from LMS predictor variables as used in the literature and from in-between assessment grades, using both multi-level and standard regressions. Our analyses show that the results of predictive modeling, notwithstanding the fact that they are collected within a single institution, strongly vary across courses. Thus, the portability of the prediction models across courses is low. In addition, we show that for the purpose of early intervention or when in-between assessment grades are taken into account, LMS data are of little (additional) value. We outline the implications of our findings and emphasize the need to include more specific theoretical argumentation and additional data sources other than just the LMS data.},
	keywords     = {Predictive models;Analytical models;Data models;Frequency measurement;Learning management systems;Internet;Learning analytics;learning management systems;portability;predictive modeling;student performance}
}
@article{Cornog_Stoddard_1925,
	title        = {Predicting performance in chemistry},
	author       = {Cornog, Jacob and Stoddard, George D.},
	year         = 1925,
	month        = {Aug},
	journal      = {Journal of chemical education},
	publisher    = {American Chemical Society (ACS)},
	volume       = 2,
	number       = 8,
	pages        = 701,
	doi          = {10.1021/ed002p701},
	issn         = {0021-9584},
	url          = {http://dx.doi.org/10.1021/ed002p701},
	language     = {en}
}
@article{Deeva_De_Smedt_De_Weerdt_2022,
	title        = {Educational Sequence Mining for Dropout Prediction in MOOCs: Model Building, Evaluation, and Benchmarking},
	author       = {Deeva, Galina and De Smedt, Johannes and De Weerdt, Jochen},
	year         = 2022,
	month        = {Dec},
	journal      = {IEEE Transactions on Learning Technologies},
	volume       = 15,
	number       = 6,
	pages        = {720–735},
	doi          = {10.1109/TLT.2022.3215598},
	issn         = {1939-1382},
	url          = {http://dx.doi.org/10.1109/TLT.2022.3215598},
	abstractnote = {Due to the unprecedented growth in available data collected by e-learning platforms, including platforms used by massive open online course (MOOC) providers, important opportunities arise to structurally use these data for decision making and improvement of the educational offering. Student retention is a strategic task that can be supported by means of automated data-driven dropout prediction. Given the time-based nature of the collected data (user activity), these data can be viewed as sequences, and thus, sequence mining presents itself as a fitting set of techniques to automatically extract valuable insights. However, there is a lack of general guidelines for using sequence mining in specific educational settings, as well as little information on how different techniques perform in comparison to each other. We address these limitations with two main contributions. First, we propose a framework for applying sequence classification for dropout prediction in MOOCs. This framework includes two data-driven dropout definitions, the specification of data formatting and preparation tasks, and a blackprint on how to train dropout prediction models at suitable time points in the run of the course. Second, we conduct a benchmarking study of recent and well-performing sequence classification techniques, tested with different parametrizations on 47 real-life datasets from MOOCs, resulting in a comparative assessment of over 18 000 models. Our results provide insight into the performance differences between the techniques and allow us to formulate concrete recommendations toward the choice of suitable hyperparameters that have a significant influence on the predictive performance.},
	keywords     = {Data mining;Electronic learning;Predictive models;Computer aided instruction;Task analysis;Timing;Education;Benchmarking study;dropout prediction;educational data mining;massive open online course (MOOC);sequence classification;sequence mining}
}
@article{Fernandez-Delgado_Cernadas_Barro_Amorim_2014,
	title        = {Do we need hundreds of classifiers to solve real world classification problems?},
	author       = {Fernandez-Delgado, Manuel and Cernadas, Eva and Barro, Senen and Amorim, Dinani},
	year         = 2014,
	journal      = {Journal of machine learning research: JMLR},
	volume       = 15,
	number       = 1,
	pages        = {3133–3181},
	issn         = {1532-4435},
	url          = {https://www.jmlr.org/papers/volume15/delgado14a/delgado14a.pdf}
}
@article{Finnegan_Morris_Lee_2008,
	title        = {Differences by course discipline on student behavior, persistence, and achievement in online courses of undergraduate general education},
	author       = {Finnegan, Catherine and Morris, Libby V. and Lee, Kangjoo},
	year         = 2008,
	journal      = {Journal of college student retention: research, theory & practice},
	volume       = 10,
	number       = 1,
	pages        = {39–54},
	doi          = {10.2190/CS.10.1.d},
	issn         = {1521-0251},
	url          = {http://search.ebscohost.com/login.aspx?direct=true&db=ehh&AN=33048985&site=ehost-live},
	abstractnote = {This research empirically examined student behavior in online courses and its relationship to persistence and achievement across fields. Eight variables descriptive of student behaviors online were measured for 1) frequency and 2) duration of participation. Twenty-two courses were grouped into three broad fields: English and Communication; Social Sciences; and Math, Science, and Technology. The descriptive data revealed significant differences in student online participation, persistence, and achievement across the fields. Multiple regression analyses were used to evaluate how well student participation measures predicted achievement. Depending on the field, different portions of the variability in achievement were accounted for by student participation measures. © 2008, Baywood Publishing Co., Inc.},
	keywords     = {ENGLISH -- Study & teaching; MATHEMATICS -- Study & teaching; MULTIPLE regression analysis; ONLINE courses; SOCIAL sciences -- Study & teaching; STUDENTS; STUDENTS -- Attitudes}
}
@article{Gasevic2016,
	title        = {Learning analytics should not promote one size fits all: The effects of instructional conditions in predicting academic success},
	author       = {Gašević, Dragan and Dawson, Shane and Rogers, Tim and Gasevic, Danijela and Ga, Dragan and Dawson, Shane and Rogers, Tim and Gasevic, Danijela and Ga??evi??, Dragan and Dawson, Shane and Rogers, Tim and Gasevic, Danijela and Ga, Dragan and Dawson, Shane and Rogers, Tim and Gasevic, Danijela},
	year         = 2016,
	month        = {Jan},
	journal      = {Internet and Higher Education},
	volume       = 28,
	pages        = {68–84},
	doi          = {10.1016/j.iheduc.2015.10.002},
	issn         = {1096-7516},
	url          = {http://linkinghub.elsevier.com/retrieve/pii/S1096751615300038},
	abstractnote = {This study examined the extent to which instructional conditions influence the prediction of academic success in nine undergraduate courses offered in a blended learning model (n = 4134). The study illustrates the differences in predictive power and significant predictors between course-specific models and generalized predictive models. The results suggest that it is imperative for learning analytics research to account for the diverse ways technology is adopted and applied in course-specific contexts. The differences in technology use, especially those related to whether and how learners use the learning management system, require consideration before the log-data can be merged to create a generalized model for predicting academic success. A lack of attention to instructional conditions can lead to an over or under estimation of the effects of LMS features on students’ academic success. These findings have broader implications for institutions seeking generalized and portable models for identifying students at risk of academic failure.},
	keywords     = {Education & Educational Research; Instructional; Instructional conditions; Learning analytics; Learning success; Self-regulated learning; Student retention; beliefs; conditions; management-systems; online; participation; performance; students; tool-use; university},
	annote       = {From Duplicate 1 (Internet and Higher Education Learning analytics should not promote one size fi ts all : The effects of instructional conditions in predicting academic success - Ga, Dragan; Dawson, Shane; Rogers, Tim; Gasevic, Danijela; Ga??evi??, Dragan; Dawson, Shane; Rogers, Tim; Gasevic, Danijela; Ga, Dragan; Dawson, Shane; Rogers, Tim; Gasevic, Danijela) From Duplicate 2 (Learning analytics should not promote one size fits all: The effects of instructional conditions in predicting academic success - Ga??evi??, Dragan; Dawson, Shane; Rogers, Tim; Gasevic, Danijela) From Duplicate 4 (Learning analytics should not promote one size fits all: The effects of instructional conditions in predicting academic success - Ga??evi??, Dragan; Dawson, Shane; Rogers, Tim; Gasevic, Danijela; Gašević, Dragan; Dawson, Shane; Rogers, Tim; Gasevic, Danijela) From Duplicate 2 (Learning analytics should not promote one size fits all: The effects of instructional conditions in predicting academic success - Ga??evi??, Dragan; Dawson, Shane; Rogers, Tim; Gasevic, Danijela) Gasevic, Dragan Dawson, Shane Rogers, Tim Gasevic, Danijela 1873-5525 From Duplicate 5 (Learning analytics should not promote one size fits all: The effects of instructional conditions in predicting academic success - Gašević, Dragan; Dawson, Shane; Rogers, Tim; Gasevic, Danijela; Ga??evi??, Dragan; Dawson, Shane; Rogers, Tim; Gasevic, Danijela; Gašević, Dragan; Dawson, Shane; Rogers, Tim; Gasevic, Danijela) From Duplicate 1 (Learning analytics should not promote one size fits all: The effects of instructional conditions in predicting academic success - Ga??evi??, Dragan; Dawson, Shane; Rogers, Tim; Gasevic, Danijela; Gašević, Dragan; Dawson, Shane; Rogers, Tim; Gasevic, Danijela) From Duplicate 2 (Learning analytics should not promote one size fits all: The effects of instructional conditions in predicting academic success - Ga??evi??, Dragan; Dawson, Shane; Rogers, Tim; Gasevic, Danijela) Gasevic, Dragan Dawson, Shane Rogers, Tim Gasevic, Danijela 1873-5525}
}
@article{Gitinabard_Xu_Heckman_Barnes_Lynch_2019,
	title        = {How Widely Can Prediction Models Be Generalized? Performance Prediction in Blended Courses},
	author       = {Gitinabard, Niki and Xu, Yiqiao and Heckman, Sarah and Barnes, Tiffany and Lynch, Collin F.},
	year         = 2019,
	month        = {Apr},
	journal      = {IEEE Transactions on Learning Technologies},
	volume       = 12,
	number       = 2,
	pages        = {184–197},
	doi          = {10.1109/TLT.2019.2911832},
	issn         = {1939-1382},
	url          = {http://dx.doi.org/10.1109/TLT.2019.2911832},
	abstractnote = {Blended courses that mix in-person instruction with online platforms are increasingly common in secondary education. These platforms record a rich amount of data on students’ study habits and social interactions. Prior research has shown that these metrics are correlated with students performance in face-to-face classes. However, predictive models for blended courses are still limited and have not yet succeeded at early prediction or cross-class predictions, even for repeated offerings of the same course. In this paper, we use data from two offerings of two different undergraduate courses to train and evaluate predictive models of student performance based on persistent student characteristics including study habits and social interactions. We analyze the performance of these models on the same offering, on different offerings of the same course, and across courses to see how well they generalize. We also evaluate the models on different segments of the courses to determine how early reliable predictions can be made. This paper tells us in part how much data is required to make robust predictions and how cross-class data may be used, or not, to boost model performance. The results of this study will help us better understand how similar the study habits, social activities, and the teamwork styles are across semesters for students in each performance category. These trained models also provide an avenue to improve our existing support platforms to better support struggling students early in the semester with the goal of providing timely intervention.},
	keywords     = {Predictive models;Measurement;Tools;Education;Discussion forums;Videos;Analytical models;Social Network Analysis;Performance Prediction;Cross-Class Performance Prediction;Early Performance Prediction;Blended Courses.}
}
@article{Gray_Perkins_2019,
	title        = {Utilizing early engagement and machine learning to predict student outcomes},
	author       = {Gray, Cameron C. and Perkins, Dave},
	year         = 2019,
	month        = {Apr},
	journal      = {Computers & education},
	volume       = 131,
	pages        = {22–32},
	doi          = {10.1016/j.compedu.2018.12.006},
	issn         = {0360-1315},
	url          = {https://www.sciencedirect.com/science/article/pii/S0360131518303191},
	abstractnote = {Finding a solution to the problem of student retention is an often-required task across Higher Education. Most often managers and academics alike rely on intuition and experience to identify the potential risk students and factors. This paper examines the literature surrounding current methods and measures in use in Learning Analytics. We find that while tools are available, they do not focus on earliest possible identification of struggling students. Our work defines a new descriptive statistic for student attendance and applies modern machine learning tools and techniques to create a predictive model. We demonstrate how students can be identified as early as week 3 (of the Fall semester) with approximately 97% accuracy. We, furthermore, situate this result within an appropriate pedagogical context to support its use as part of a more comprehensive student support mechanism.},
	keywords     = {Machine learning; Learning analytics; Student retention}
}
@inproceedings{Ho_Jin_Shim_2018,
	title        = {Data Mining Approach to the Identification of At-Risk Students},
	author       = {Ho, Li Chin and Jin Shim, Kyong},
	year         = 2018,
	month        = {Dec},
	booktitle    = {2018 IEEE International Conference on Big Data (Big Data)},
	pages        = {5333–5335},
	doi          = {10.1109/BigData.2018.8622495},
	url          = {http://dx.doi.org/10.1109/BigData.2018.8622495},
	abstractnote = {In recent years, the use of digital tools and technologies in educational institutions are continuing to generate large amounts of digital traces of student learning behavior. This study presents a proof-of-concept analytics system that can detect at-risk students along their learning journey. Educators can benefit from the early detection of at-risk students by understanding factors which may lead to failure or drop-out. Further, educators can devise appropriate intervention measures before the students drop out of the course. Our system was built using SAS® Enterprise Miner (EM) and SAS® JMP Pro.},
	keywords     = {Predictive models;Data models;Synthetic aperture sonar;Decision trees;Field-flow fractionation;Data mining;Information systems;learning analytics;at-risk students;learning management systems;educational data mining}
}
@article{Hussain_Khan_2021,
	title        = {Student-Performulator: Predicting Students’ Academic Performance at Secondary and Intermediate Level Using Machine Learning},
	author       = {Hussain, Shah and Khan, Muhammad Qasim},
	year         = 2021,
	month        = {Jun},
	journal      = {Annals of Data Science},
	doi          = {10.1007/s40745-021-00341-0},
	issn         = {2198-5812},
	url          = {https://doi.org/10.1007/s40745-021-00341-0},
	abstractnote = {Forecasting academic performance of student has been a substantial research inquest in the Educational Data-Mining that utilizes Machine-learning (ML) procedures to probe the data of educational setups. Quantifying student academic performance is challenging because academic performance of students hinges on several factors. The in hand research work focuses on students’ grade and marks prediction utilizing supervised ML approaches. The data-set utilized in this research work has been obtained from the Board of Intermediate & Secondary Education (B.I.S.E) Peshawar, Khyber Pakhtunkhwa. There are 7 areas in BISEP i.e., Peshawar, FR-Peshawar, Charsadda, Khyber, Mohmand and Upper and Lower Chitral. This paper aims to examine the quality of education that is closely related to the aims of sustainability. The system has created an abundance of data which needs to be properly analyzed so that most useful information should be obtained for planning and future development. Grade and marks forecasting of students with their historical educational record is a renowned and valuable application in the EDM. It becomes an incredible information source that could be utilized in various ways to enhance the standard of education nationwide. Relevant research study reveals that numerous methods for academic performance forecasting are built to carryout improvements in administrative and teaching staff of academic organizations. In the put forwarded approach, the acquired data-set is pre-processed to purify the data quality, the labeled academic historical data of student (30 optimum attributes) is utilized to train regression model and DT-classifier. The regression will forecast marks, while grade will be forecasted by classification system, eventually analyzed the results obtained by the models. The results obtained show that machine learning technology is efficient and relevant for predicting students performance.}
}
@article{Ifenthaler_Yau_2020,
	title        = {Utilising learning analytics to support study success in higher education: a systematic review},
	author       = {Ifenthaler, Dirk and Yau, Jane Yin Kim},
	year         = 2020,
	journal      = {Educational technology research and development: ETR & D},
	publisher    = {Springer US},
	number       = {0123456789},
	doi          = {10.1007/s11423-020-09788-z},
	issn         = {1042-1629},
	url          = {https://doi.org/10.1007/s11423-020-09788-z},
	abstractnote = {Study success includes the successful completion of a first degree in higher education to the largest extent, and the successful completion of individual learning tasks to the smallest extent. Factors affecting study success range from individual dispositions (e.g., motivation, prior academic performance) to characteristics of the educational environment (e.g., attendance, active learning, social embeddedness). Recent developments in learning analytics, which are a socio-technical data mining and analytic practice in educational contexts, show promise in enhancing study success in higher education, through the collection and analysis of data from learners, learning processes, and learning environments in order to provide meaningful feedback and scaffolds when needed. This research reports a systematic review focusing on empirical evidence, demonstrating how learning analytics have been successful in facilitating study success in continuation and completion of students’ university courses. Using standardised steps of conducting a systematic review, an initial set of 6220 articles was identified. The final sample includes 46 key publications. The findings obtained in this systematic review suggest that there are a considerable number of learning analytics approaches which utilise effective techniques in supporting study success and students at risk of dropping out. However, rigorous, large-scale evidence of the effectiveness of learning analytics in supporting study success is still lacking. The tested variables, algorithms, and methods collected in this systematic review can be used as a guide in helping researchers and educators to further improve the design and implementation of learning analytics systems.},
	keywords     = {Attrition; Dropout; Higher education; Learning analytics; Retention; Study success}
}
@book{James_Witten_Hastie_Tibshirani_2021,
	title        = {An Introduction to Statistical Learning: with Applications in R},
	author       = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year         = 2021,
	month        = {Jul},
	publisher    = {Springer US},
	doi          = {10.1007/978-1-0716-1418-1},
	isbn         = 9781071614174,
	url          = {https://play.google.com/store/books/details?id=g5gezgEACAAJ},
	abstractnote = {An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, deep learning, survival analysis, multiple testing, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform.Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.This Second Edition features new chapters on deep learning, survival analysis, and multiple testing, as well as expanded treatments of naïve Bayes, generalized linear models, Bayesian additive regression trees, and matrix completion. R code has been updated throughout to ensure compatibility.},
	language     = {en}
}
@article{Jokhan_Sharma_Singh_2019,
	title        = {Early warning system as a predictor for student performance in higher education blended courses},
	author       = {Jokhan, Anjeela and Sharma, Bibhya and Singh, Shaveen},
	year         = 2019,
	month        = {Nov},
	journal      = {Studies in Higher Education},
	publisher    = {Routledge},
	volume       = 44,
	number       = 11,
	pages        = {1900–1911},
	doi          = {10.1080/03075079.2018.1466872},
	issn         = {0307-5079},
	url          = {https://doi.org/10.1080/03075079.2018.1466872},
	abstractnote = {ABSTRACTEarly warning systems are being used to assist students in their studies as well as understanding student behaviour and performance better. A home-grown EWS plug-in for Moodle was used to predict the student performance in a first year IT literacy course at University of the South Pacific. The alert tool was designed to capture student logins, completion of online activities and online engagement. Data were captured from Moodle and statistical modelling using the regression model was used to determine any correlation between student?s online behaviour and their performance. Student performance in this higher education course could be predicted based on their average logins per week and the average completion rates of activities. The accuracy of the model was 60.8%. Hence the EWS can be a very useful tool to measure student progression in a course as well as identifying underperforming students early in their course of allowing for early intervention.}
}
@article{Joksimović_Gašević_Kovanović_Riecke_Hatala_2015,
	title        = {Social presence in online discussions as a process predictor of academic performance},
	author       = {Joksimović, S. and Gašević, D. and Kovanović, V. and Riecke, B. E. and Hatala, M.},
	year         = 2015,
	month        = {Dec},
	journal      = {Journal of computer assisted learning},
	publisher    = {Wiley},
	volume       = 31,
	number       = 6,
	pages        = {638–654},
	doi          = {10.1111/jcal.12107},
	issn         = {0266-4909},
	url          = {https://onlinelibrary.wiley.com/doi/10.1111/jcal.12107},
	abstractnote = {With the steady development of online education and online learning environments, possibilities to support social interactions between students have advanced significantly. This study examined the relationship between indicators of social presence and academic performance. Social presence is defined as students’ ability to engage socially with an online learning community. The results of a multiple regression analysis showed that certain indicators of social presence were significant predictors of final grades in a master's level computer science online course. Moreover, the study also revealed that teaching presence moderated the association between social presence and academic performance, indicating that a course design that increased the level of meaningful interactions between students had a significant impact on the development of social presence, and thus could positively affect students' academic performance. This is especially important in situations when discussions are introduced to promote the development of learning outcomes assessed in courses. Another implication of our results is that indicators of social presence can be used for early detection of students at risk of failing a course. Findings inform research and practice in the emerging field of learning analytics by prompting the opportunities to offer actionable insights into the reasons why certain students are lagging behind.},
	language     = {en}
}
@book{Jorgensen_Ferraro_Fichten_Havel_2009,
	title        = {Predicting College Retention and Dropout: Sex and Disability},
	author       = {Jorgensen, Shirley and Ferraro, Vittoria and Fichten, Catherine and Havel, Alice},
	year         = 2009,
	publisher    = {ERIC Clearinghouse},
	url          = {https://play.google.com/store/books/details?id=cKkAvwEACAAJ},
	abstractnote = {The relationship between student dropout and high school grades, demographic factors and psychosocial variables was examined for 40 682 full-time Dawson College students. Overall, low high school grades and older age were the best predictors of dropout. Male students, who generally had lower grades than females, were more likely to drop out than female students. In particular, male students with high school averages below 80% dropped out at substantially higher rates than females. Males also scored lower than females on questionnaire measures of academic communication skills, time spent on out-of-class study, motivation, discipline, following through on commitments and obligations, and involvement in the college community. Compared to students without disabilities, students with disabilities dropped out at lower rates between the first and third semesters, but at higher rates in later semesters. This resulted in similar dropout and graduation rates at the end of ten semesters. Students with disabilities scored lower than their nondisabled peers on measures of academic self-confidence and social connection, were more likely to report feeling alone and isolated, and were more likely to drop out because of a disability or health issue. Female students with disabilities were more likely to feel that their chosen occupation did not require further study. The results provide evidence that patterns of student attrition and the factors that influence these can vary between student sub-populations. Recommendations are made based on the findings.},
	language     = {en}
}
@inproceedings{Jovanović_Dawson_Joksimović_Siemens_2020,
	title        = {Supporting actionable intelligence: reframing the analysis of observed study strategies},
	author       = {Jovanović, Jelena and Dawson, Shane and Joksimović, Srećko and Siemens, George},
	year         = 2020,
	month        = {Mar},
	booktitle    = {Proceedings of the Tenth International Conference on Learning Analytics & Knowledge},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {LAK ’20},
	pages        = {161–170},
	doi          = {10.1145/3375462.3375474},
	isbn         = 9781450377126,
	url          = {https://doi.org/10.1145/3375462.3375474},
	abstractnote = {Models and processes developed in learning analytics research are increasing in sophistication and predictive power. However, the ability to translate analytic findings to practice remains problematic. This study aims to address this issue by establishing a model of learner behaviour that is both predictive of student course performance, and easily interpreted by instructors. To achieve this aim, we analysed fine grained trace data (from 3 offerings of an undergraduate online course, N=1068) to establish a comprehensive set of behaviour indicators aligned with the course design. The identified behaviour patterns, which we refer to as observed study strategies, proved to be associated with the student course performance. By examining the observed strategies of high and low performers throughout the course, we identified prototypical pathways associated with course success and failure. The proposed model and approach offers valuable insights for the provision of process-oriented feedback early in the course, and thus can aid learners in developing their capacity to succeed online.},
	collection   = {LAK ’20},
	keywords     = {explanatory models, learning analytics, learner behaviour, learning tactics and strategies, trace data}
}
@article{Jovanovic_Mirriahi_Gašević_Dawson_Pardo_2019,
	title        = {Predictive power of regularity of pre-class activities in a flipped classroom},
	author       = {Jovanovic, Jelena and Mirriahi, Negin and Gašević, Dragan and Dawson, Shane and Pardo, Abelardo},
	year         = 2019,
	month        = {Jun},
	journal      = {Computers & education},
	volume       = 134,
	pages        = {156–168},
	doi          = {10.1016/j.compedu.2019.02.011},
	issn         = {0360-1315},
	url          = {https://www.sciencedirect.com/science/article/pii/S0360131519300405},
	abstractnote = {Flipped classroom (FC) is an active learning design requiring students to complete assigned pre-class learning activities in preparation for face-to-face sessions. Students’ timely, regular, and productive engagement with the pre-class activities is considered critical for the success of the overall FC design, as these activities serve to prepare students for effective participation in face-to-face sessions. However, there is limited empirical evidence on the strength of association between students' regularity of engagement with the pre-class activities and their learning performance in a FC course. Hence, the current study uses learning trace data from three consecutive offerings of a FC course to examine students' regularity of pre-class learning activities and its association with the students' course performance. In particular, the study derives several indicators of regularity from the trace data, including indicators related to time management and those reflecting regularity in the pattern of engagement with pre-class learning activities. The association with course performance is examined by building predictive regression models with the defined indicators as features. To examine the relevance of incorporating the specificities of the instructional design in predictive models, we designed and compared two kinds of indicators: generic (i.e. course-design-agnostic) and course-design-specific indicators. The study identified several indicators of regularity of pre-class activities as significant predictors of course performance. It also demonstrated that predictive models with only generic indicators were able to explain only a small portion of the overall variability in the students' course performance, and were significantly outperformed by models that incorporated coursespecific indicators. Finally, the study findings point to the importance of assisting students in regulating their use of learning resources during class preparation activities in a FC.},
	keywords     = {Improving classroom teaching; Post-secondary education; Teaching/learning strategies}
}
@article{Jovanović_Saqr_Joksimović_Gašević_2021,
	title        = {Students matter the most in learning analytics: The effects of internal and instructional conditions in predicting academic success},
	author       = {Jovanović, Jelena and Saqr, Mohammed and Joksimović, Srećko and Gašević, Dragan},
	year         = 2021,
	month        = {Oct},
	journal      = {Computers & education},
	volume       = 172,
	number       = {April},
	pages        = 104251,
	doi          = {10.1016/j.compedu.2021.104251},
	issn         = {0360-1315},
	url          = {https://www.sciencedirect.com/science/article/pii/S0360131521001287},
	abstractnote = {Predictive modelling of academic success and retention has been a key research theme in Learning Analytics. While the initial work on predictive modelling was focused on the development of general predictive models, portable across different learning settings, later studies demonstrated the drawbacks of not considering the specificities of course design and disciplinary context. This study builds on the methods and findings of related earlier studies to further explore factors predictive of learners’ academic success in blended learning. In doing so, it differentiates itself by (i) relying on a larger and homogeneous course sample (15 courses, 50 course offerings in total), and (ii) considering both internal and external conditions as factors affecting the learning process. We apply mixed effect linear regression models, to examine: i) to what extent indicators of students' online learning behaviour can explain the variability in the final grades, and ii) to what extent that variability is attributable to the course and students' internal conditions, not captured by the logged data. Having examined different types of behaviour indicators (e.g., indicators of the overall activity level, those indicative of regularity of study, etc), we found little difference, if any, in their predictive power. Our results further indicate that a low proportion of variance is explained by the behaviour-based indicators, while a significant portion of variability stems from the learners' internal conditions. Hence, when variability in external conditions is largely controlled for (the same institution, discipline, and nominal pedagogical model), students' internal state is the key predictor of their course performance.},
	keywords     = {data science applications in; distance education and online; education; learning}
}
@article{Kuhn_2008,
	title        = {Building Predictive Models in R Using the caret Package},
	author       = {Kuhn, Max},
	year         = 2008,
	month        = {Nov},
	journal      = {Journal of statistical software},
	volume       = 28,
	pages        = {1–26},
	doi          = {10.18637/jss.v028.i05},
	issn         = {1548-7660},
	url          = {http://www.jstatsoft.org/v28/i05/},
	abstractnote = {The caret package, short for classification and regression training, contains numerous tools for developing predictive models using the rich set of models available in R. The package focuses on simplifying model training and tuning across a wide variety of modeling techniques. It also includes methods for pre-processing training data, calculating variable importance, and model visualizations. An example from computational chemistry is used to illustrate the functionality on a real data set and to benchmark the benefits of parallel processing with several types of models.},
	language     = {en}
}
@article{Kuzilek_Hlosta_Herrmannova_Zdrahal_Vaclavek_Wolff_2015,
	title        = {OU Analyse: analysing at-risk students at The Open University},
	author       = {Kuzilek, Jakub and Hlosta, Martin and Herrmannova, Drahomira and Zdrahal, Zdenek and Vaclavek, Jonas and Wolff, Annika},
	year         = 2015,
	month        = {Mar},
	journal      = {Learning Analytics Review},
	volume       = {LAK15-1},
	pages        = {1–16},
	issn         = {2057-7494},
	url          = {http://oro.open.ac.uk/42529/},
	abstractnote = {The OU Analyse project aims at providing early prediction of “at-risk” students based on their demographic data and their interaction with Virtual Learning Environment. Four predictive models have been constructed from legacy data using machine learning methods. In Spring 2014 the approach was piloted and evaluated on two introductory university courses with about 1500 and 3000 students, respectively. Since October 2014 the predictions have been extended to include 10+ courses of different level. The OU Analyse dashboard has been implemented, for presenting predictions and providing a course overview and a view of individual students.},
	keywords     = {student data; distance learning; predictive models; machine learning;information visualisation}
}
@article{Liaw_Wiener_2002,
	title        = {Classification and Regression by randomForest},
	author       = {Liaw, Andy and Wiener, Matthew},
	year         = 2002,
	journal      = {R news},
	volume       = 2,
	number       = 3,
	pages        = {18–22},
	issn         = {1609-3631},
	url          = {https://CRAN.R-project.org/doc/Rnews/.}
}
@inbook{López-Pernas_Saqr_Conde_Del_Rio_2024,
	title        = {A broad collection of datasets for educational research training and application},
	author       = {López-Pernas, Sonsoles and Saqr, Mohammed and Conde, Javier and Del-Río-Carazo, Laura},
	year         = 2024,
	booktitle    = {Learning analytics methods and tutorials: A practical guide using R},
	publisher    = {Springer},
	pages        = {in–press},
	editor       = {Saqr, Mohammed and López-Pernas, Sonsoles}
}
@article{Lüdecke_Ben-Shachar_Patil_Waggoner_Makowski_2021,
	title        = {Performance: An R package for assessment, comparison and testing of statistical models},
	author       = {Lüdecke, Daniel and Ben-Shachar, Mattan and Patil, Indrajeet and Waggoner, Philip and Makowski, Dominique},
	year         = 2021,
	month        = {Apr},
	journal      = {Journal of open source software},
	publisher    = {The Open Journal},
	volume       = 6,
	number       = 60,
	pages        = 3139,
	doi          = {10.21105/joss.03139},
	issn         = {2475-9066},
	url          = {https://joss.theoj.org/papers/10.21105/joss.03139}
}
@inproceedings{Marras_Vignoud_Kaser_2021,
	title        = {Can feature predictive power generalize? Benchmarking early predictors of student success across flipped and online courses},
	author       = {Marras, Mirko and Vignoud, Julien Tuang Tu and Kaser, Tanja},
	year         = 2021,
	booktitle    = {Proceedings of the 14th International Conference on Educational Data Mining},
	pages        = {150–160},
	url          = {https://iris.unica.it/handle/11584/322791},
	abstractnote = {open},
	keywords     = {flipped classroom; MOOC; success prediction; early warning; clickstream; at-risk students; learning analytics},
	language     = {en}
}
@inbook{Nouri_Larsson_Saqr_2019,
	title        = {Identifying factors for master thesis completion and non-completion through learning analytics and machine learning},
	author       = {Nouri, Jalal and Larsson, Ken and Saqr, Mohammed},
	year         = 2019,
	booktitle    = {Lecture Notes in Computer Science},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	series       = {Lecture notes in computer science},
	pages        = {28–39},
	doi          = {10.1007/978-3-030-29736-7_3},
	isbn         = 9783030297350,
	issn         = {0302-9743},
	url          = {http://link.springer.com/10.1007/978-3-030-29736-7_3},
	collection   = {Lecture notes in computer science}
}
@article{Ober_Hong_Rebouças-Ju_Carter_Liu_Cheng_2021,
	title        = {Linking self-report and process data to performance as measured by different assessment types},
	author       = {Ober, Teresa M. and Hong, Maxwell R. and Rebouças-Ju, Daniella A. and Carter, Matthew F. and Liu, Cheng and Cheng, Ying},
	year         = 2021,
	month        = {Jul},
	journal      = {Computers & education},
	volume       = 167,
	pages        = 104188,
	doi          = {10.1016/j.compedu.2021.104188},
	issn         = {0360-1315},
	url          = {https://www.sciencedirect.com/science/article/pii/S0360131521000658},
	abstractnote = {This study was motivated by a need to understand the extent to which behavioral indicators of engagement from digital log data are associated with various student learning outcomes above and beyond self-reported levels of engagement, and whether the strength of these associations vary depending on the type of learning outcome. Student learning was assessed by way of four distinct learning outcomes that varied according to stakes (low-v. high-stakes) and span (one-time v. aggregated). Participants included high school students between 14 and 18 years of age enrolled in an AP Statistics course (N = 320, M age = 16.76 years, SD age = 0.85; 60.2% female) who had consented to use an online assessment system over the course of an academic year that was designed to provide personalized performance reports. While largely uncorrelated with self-report measures, certain process data variables were significantly correlated with learning outcomes. In particular, students’ frequency of score report checking, an indication of feedback-seeking behavior, while uncorrelated with self-reported student engagement, was associated with all learning outcomes. Other behaviors, such as the number of log-in sessions and the duration of sessions, were not. These findings suggest that process data from online assessment systems can help broaden and deepen our understanding of student behavior above and beyond self-report. That said, given that the volume and complexity of process data can make it challenging to mine and interpret, researchers must consider theory when identifying process data variables that are critical to the understanding of constructs of interest.},
	keywords     = {Engagement; Process data; Clickstream data; Assessment; low- and high-stakes}
}
@article{Sani_Fikri_Ali_Zakree_Nadiyah_2020,
	title        = {Drop-out prediction in higher education among B40 students},
	author       = {Sani, Nor Samsiah and Fikri, Ahmad and Ali, Zulaiha and Zakree, Mohd and Nadiyah, Khairul},
	year         = 2020,
	journal      = {International journal of advanced computer science and applications : IJACSA},
	publisher    = {The Science and Information Organization},
	volume       = 11,
	number       = 11,
	doi          = {10.14569/ijacsa.2020.0111169},
	issn         = {2158-107X},
	url          = {http://thesai.org/Publications/ViewPaper?Volume=11&Issue=11&Code=IJACSA&SerialNo=69},
	language     = {en}
}
@article{Saqr_Fors_Tedre_2017,
	title        = {How learning analytics can early predict under-achieving students in a blended medical education course},
	author       = {Saqr, Mohammed and Fors, Uno and Tedre, Matti},
	year         = 2017,
	month        = {Jul},
	journal      = {Medical teacher},
	volume       = 39,
	number       = 7,
	pages        = {757–767},
	doi          = {10.1080/0142159X.2017.1309376},
	issn         = {0142-159X},
	url          = {http://dx.doi.org/10.1080/0142159X.2017.1309376},
	abstractnote = {AIM: Learning analytics (LA) is an emerging discipline that aims at analyzing students’ online data in order to improve the learning process and optimize learning environments. It has yet un-explored potential in the field of medical education, which can be particularly helpful in the early prediction and identification of under-achieving students. The aim of this study was to identify quantitative markers collected from students' online activities that may correlate with students' final performance and to investigate the possibility of predicting the potential risk of a student failing or dropping out of a course. METHODS: This study included 133 students enrolled in a blended medical course where they were free to use the learning management system at their will. We extracted their online activity data using database queries and Moodle plugins. Data included logins, views, forums, time, formative assessment, and communications at different points of time. Five engagement indicators were also calculated which would reflect self-regulation and engagement. Students who scored below 5% over the passing mark were considered to be potentially at risk of under-achieving. RESULTS: At the end of the course, we were able to predict the final grade with 63.5% accuracy, and identify 53.9% of at-risk students. Using a binary logistic model improved prediction to 80.8%. Using data recorded until the mid-course, prediction accuracy was 42.3%. The most important predictors were factors reflecting engagement of the students and the consistency of using the online resources. CONCLUSIONS: The analysis of students' online activities in a blended medical education course by means of LA techniques can help early predict underachieving students, and can be used as an early warning sign for timely intervention.},
	language     = {en}
}
@article{Saqr_Jovanović_Viberg_Gašević_2022,
	title        = {Is there order in the mess? A single paper meta-analysis approach to identification of predictors of success in learning analytics},
	author       = {Saqr, Mohammed and Jovanović, Jelena and Viberg, Olga and Gašević, Dragan},
	year         = 2022,
	month        = {Dec},
	journal      = {Studies in Higher Education},
	publisher    = {Routledge},
	volume       = 47,
	number       = 12,
	pages        = {2370–2391},
	doi          = {10.1080/03075079.2022.2061450},
	issn         = {0307-5079},
	url          = {https://doi.org/10.1080/03075079.2022.2061450},
	abstractnote = {ABSTRACTPredictors of student academic success do not always replicate well across different learning designs, subject areas, or educational institutions. This suggests that characteristics of a particular discipline and learning design have to be carefully considered when creating predictive models in order to scale up learning analytics. This study aimed to examine if and to what extent frequently used predictors of study success are portable across a homogenous set of courses. The research was conducted in an integrated blended problem-based curriculum with trace data (n?=?2,385 students) from 50 different course offerings across four academic years. We applied the statistical method of single paper meta-analysis to combine correlations of several indicators with students? success. Total activity and the forum indicators exhibited the highest prediction intervals, where the former represented proxies of the overall engagement with online tasks, and the latter with online collaborative learning activities. Indicators of lecture reading (frequency of lecture view) showed statistically insignificant prediction intervals and, therefore, are less likely to be portable across course offerings. The findings show moderate amounts of variability both within iterations of the same course and across courses. The results suggest that the use of the meta-analytic statistical method for the examination of study success indicators across courses with similar learning design and subject area can offer valuable quantitative means for the identification of predictors that reasonably well replicate and consequently can be reliably portable in the future.}
}
@inproceedings{Saqr_López-Pernas_2024,
	title        = {Why learning and teaching learning analytics is hard: An experience from a real-life LA course using LA methods},
	author       = {Saqr, Mohammed and López-Pernas, Sonsoles},
	year         = 2024,
	booktitle    = {Proceedings of the Eleventh International Conference on Technological Ecosystems for Enhancing Multiculturality (TEEM’23)},
	publisher    = {Springer},
	pages        = {in press},
	abstractnote = {Research in learning analytics needs longitudinal studies that explore the learner’s behaviour, disposition, and learning practices across time, a gap this article aims to bridge. We present VaSSTra: an innovative method for the longitudinal analysis of educational data that can be applied at different time scales (e.g., days, weeks, or courses), and allows the study of different aspects of learning as well as the factors that explain how such aspects evolve over time. Our method combines life- events methods with sequence analysis and consists of three steps: (1) converting variables to states (where variables are grouped into homogenous states); (2) from states to sequences (where the states are used to construct sequences across time), and (3) from sequences to trajectories (where similar sequences are grouped in trajectories). VaSSTra enables us to map the longitudinal unfolding of events while taking advantage of the wealth of life-events methods to visualize, model and describe the temporal dynamics of longitudinal activities. We demonstrate the method with a practical case study example.}
}
@inproceedings{Saqr_Nouri_2020,
	title        = {High resolution temporal network analysis to understand and improve collaborative learning},
	author       = {Saqr, Mohammed and Nouri, Jalal},
	year         = 2020,
	month        = {Mar},
	booktitle    = {Proceedings of the Tenth International Conference on Learning Analytics & Knowledge},
	publisher    = {ACM},
	address      = {New York, NY, USA},
	pages        = {314–319},
	doi          = {10.1145/3375462.3375501},
	isbn         = 9781450377126,
	url          = {https://dl.acm.org/doi/10.1145/3375462.3375501},
	abstractnote = {There has been significant efforts in studying collaborative and social learning using aggregate networks. Such efforts have demonstrated the worth of the approach by providing insights about the interactions, student and teacher roles, and predictability of performance. However, using an aggregated network discounts the fine resolution of temporal interactions. By doing so, we might overlook the regularities/irregularities of students’ interactions, the process of learning regulation, and how and when different actors influence each other. Thus, compressing a complex temporal process such as learning may be oversimplifying and reductionist. Through a temporal network analysis of 54 students interactions (in total 3134 interactions) in an online medical education course, this study contributes with a methodological approach to building, visualizing and quantitatively analyzing temporal networks, that could help educational practitioners understand important temporal aspects of collaborative learning that might need attention and action. Furthermore, the analysis conducted emphasize the importance of considering the time characteristics of the data that should be used when attempting to, for instance, implement early predictions of performance and early detection of students and groups that need support and attention.},
	keywords     = {Collaborative learning; Learning analytics; Medical education; Problem-based learning; Social network analysis; Temporal networks; Temporarily; collaborative learning; learning analytics; problem-based learning; social network analysis; temporal networks; temporarily}
}
@article{Scheffel_Drachsler_de_Kraker_Kreijns_Slootmaker_Specht_2017,
	title        = {Widget, Widget on the Wall, Am I Performing Well at All?},
	author       = {Scheffel, Maren and Drachsler, Hendrik and de Kraker, Joop and Kreijns, Karel and Slootmaker, Aad and Specht, Marcus},
	year         = 2017,
	month        = {Jan},
	journal      = {IEEE Transactions on Learning Technologies},
	volume       = 10,
	number       = 1,
	pages        = {42–52},
	doi          = {10.1109/TLT.2016.2622268},
	issn         = {1939-1382},
	url          = {http://dx.doi.org/10.1109/TLT.2016.2622268},
	abstractnote = {In collaborative learning environments, students work together on assignments in virtual teams and depend on each other’s contribution to achieve their learning objectives. The online learning environment, however, may not only facilitate but also hamper group communication, coordination, and collaboration. Group awareness widgets that visualize information about the different group members based on information collected from the individuals can foster awareness and reflection processes within the group. In this paper, we present a formative data study about the predictive power of several indicators of an awareness widget based on automatically logged user data from an online learning environment. In order to test whether the information visualized by the widget is in line with the study outcomes, we instantiated the widget indicators with data from four previous runs of the European Virtual Seminar on Sustainable Development (EVS). We analyzed whether the tutor gradings in these previous years correlated with the students' scores calculated for the widget indicators. Furthermore, we tested the predictive power of the widget indicators at various points in time with respect to the final grades of the students. The results of our analysis show that the grades and widget indicator scores are significantly and positively correlated, which provides a useful empirical basis for the development of guidelines for students and tutors on how to interpret the widget's visualizations in live runs.},
	keywords     = {Data visualization;Collaborative work;Europe;Virtual groups;Seminars;Learning analytics;visualization;group awareness;correlation analysis;regression analysis}
}
@article{Sghir_Adadi_Lahmer_2022,
	title        = {Recent advances in Predictive Learning Analytics: A decade systematic review (2012-2022)},
	author       = {Sghir, Nabila and Adadi, Amina and Lahmer, Mohammed},
	year         = 2022,
	month        = {Dec},
	journal      = {Education and information technologies},
	pages        = {1–35},
	doi          = {10.1007/s10639-022-11536-0},
	issn         = {1360-2357},
	url          = {http://dx.doi.org/10.1007/s10639-022-11536-0},
	abstractnote = {The last few years have witnessed an upsurge in the number of studies using Machine and Deep learning models to predict vital academic outcomes based on different kinds and sources of student-related data, with the goal of improving the learning process from all perspectives. This has led to the emergence of predictive modelling as a core practice in Learning Analytics and Educational Data Mining. The aim of this study is to review the most recent research body related to Predictive Analytics in Higher Education. Articles published during the last decade between 2012 and 2022 were systematically reviewed following PRISMA guidelines. We identified the outcomes frequently predicted in the literature as well as the learning features employed in the prediction and investigated their relationship. We also deeply analyzed the process of predictive modelling, including data collection sources and types, data preprocessing methods, Machine Learning models and their categorization, and key performance metrics. Lastly, we discussed the relevant gaps in the current literature and the future research directions in this area. This study is expected to serve as a comprehensive and up-to-date reference for interested researchers intended to quickly grasp the current progress in the Predictive Learning Analytics field. The review results can also inform educational stakeholders and decision-makers about future prospects and potential opportunities.},
	keywords     = {Educational data mining; Higher education; Learning analytics; Machine learning; Predictive modelling},
	language     = {en}
}
@article{Shafiq_Marjani_Habeeb_Asirvatham_2022,
	title        = {Student Retention Using Educational Data Mining and Predictive Analytics: A Systematic Literature Review},
	author       = {Shafiq, Dalia Abdulkareem and Marjani, Mohsen and Habeeb, Riyaz Ahamed Ariyaluran and Asirvatham, David},
	year         = 2022,
	journal      = {IEEE Access},
	volume       = 10,
	pages        = {72480–72503},
	doi          = {10.1109/ACCESS.2022.3188767},
	issn         = {2169-3536},
	url          = {http://dx.doi.org/10.1109/ACCESS.2022.3188767},
	abstractnote = {Student retention is an essential measurement metric in education, indicated by retention rates, which are accumulated as students re-enroll from one academic year to the next. High retention rates can be obtained if institutions aim to provide appropriate support and teaching methods among the various practices to prevent students from deferring their studies. To address this pressing challenge faced by educational institutions, the underlying factors and the methodological aspects of building robust predictive models are reviewed and scrutinized. Educational Data Mining (EDM) and Learning Analytics (LA) have been widely adopted for knowledge discovery from educational data sources, improving the teaching practice, and identifying at-risk students. Various predictive techniques are applied in LA, such as Machine Learning (ML), Statistical Analysis, and Deep Learning (DL). To gain an in-depth review of these techniques, academic publications have been reviewed to highlight their potential to resolve Student Retention issues in education. Additionally, the paper presents a taxonomy of ML approaches and a comprehensive review of the success factors and the features that are not indicative of student performance in three different learning environments: Traditional Learning, Blended Learning, and Online Learning. The survey reveals that supervised ML and DL techniques are broadly applied in Student Retention. However, the application of ensemble and unsupervised learning clustering techniques supporting the heterogenous and homogenous groups of students is generally lacking. Moreover, static and traditional features are commonly used in student performance, ignoring vital factors such as educators-related, cognitive, and personal data. Furthermore, the paper highlights open challenges for future research directions.},
	keywords     = {Education;Data mining;Systematics;Predictive models;Prediction algorithms;Bibliographies;Soft sensors;Educational data mining;learning analytics;machine learning;predictive models;student retention}
}
@article{Siemens_2013,
	title        = {Learning Analytics: The Emergence of a Discipline},
	author       = {Siemens, George},
	year         = 2013,
	month        = {Oct},
	journal      = {The American behavioral scientist},
	publisher    = {SAGE Publications Inc},
	volume       = 57,
	number       = 10,
	pages        = {1380–1400},
	doi          = {10.1177/0002764213498851},
	issn         = {0002-7642},
	url          = {https://doi.org/10.1177/0002764213498851},
	abstractnote = {Recently, learning analytics (LA) has drawn the attention of academics, researchers, and administrators. This interest is motivated by the need to better understand teaching, learning, “ intelligent content, ” and personalization and adaptation. While still in the early stages of research and implementation, several organizations (Society for Learning Analytics Research and the International Educational Data Mining Society) have formed to foster a research community around the role of data analytics in education. This article considers the research fields that have contributed technologies and methodologies to the development of learning analytics, analytics models, the importance of increasing analytics capabilities in organizations, and models for deploying analytics in educational settings. The challenges facing LA as a field are also reviewed, particularly regarding the need to increase the scope of data capture so that the complexity of the learning process can be more accurately reflected in analysis. Privacy and data ownership will become increasingly important for all participants in analytics projects. The current legal system is immature in relation to privacy and ethics concerns in analytics. The article concludes by arguing that LA has sufficiently developed, through conferences, journals, summer institutes, and research labs, to be considered an emerging research field. The slightest move in the virtual landscape has to be paid for in lines of code. Siemens 1381 When P. W. Anderson stated in 1972 that “ more is different, ” he argued that the quan-tity of an entity influences how researchers engage with it. As the quantity of data has increased, the attention of researchers, academics, and businesses has turned to new methods to understand and make sense of that data. In some sectors, the relatively recent emergence of big data and analytics is now viewed as having the potential to transform economies and increase organizational productivity (Manyika et al., 2011, p. 13) and increase competitiveness (Kiron, Shockley, Kruschwitz, Finch, Haydock, 2011). Unfortunately, education systems—primary, secondary, and postsecondary— have made limited use of the available data to improve teaching, learning, and learner success. Despite the field of education lagging behind other sectors, there has been a recent explosion of interest in analytics as a solution for many current challenges, such as retention and learner support. Science is concerned with discovering or recognizing the nature of the universe, particularly in terms of how entities are connected or related to each other. New dis-coveries are added to “ the network of theory ” (Kuhn, 1996, p. 7), and when discover-ies and innovations “ align and come together ” (Morin, 2008, p. 51), scientific paradigms, models, and programs result. This new knowledge leads to revisions and questions about our prior understanding and beliefs and reflection on the connections within “ the network of theory. ” For example, connections that have been now demon-strated to be false (earth-centered universe, personality caused by the four humors) have been replaced by new connections between entities that can be validated. Over time, these connections may be further modified and revised and linked into the new nodes and areas of knowledge. The improvements in the process of discoveries can be considered as “ more important than any single discovery ” (Nielsen, 2012, p. 3). Analytics is another approach, or cognitive aid, that can be applied to assist scien-tists, researchers, and academics to make sense of the connective structures that under-pin their field of knowledge. The methods of science and the questions investigated have rapidly changed as large data sets have become available. Early attempts to man-age knowledge through classification systems (such as early attempts by Yahoo to orga-nize the web into categories) have now been replaced by the big data and algorithmically driven approach of Google. The emphasis on large quantities of data for discovery has important implications for education. Through the use of mobile devices, learning man-agement systems (LMS), and social media, a greater portion of the learning process generates digital trails. A student who logs into an LMS leaves thousands of data points, including navigation patterns, pauses, reading habits, and writing habits. These data points may be ambiguous and require additional exploration in order to understand what an extended pause of reading means (perhaps the student is distracted or engaged in other tasks, or perhaps the student is grappling with a challenging concept in the text), but for researchers, learning sciences, and education in general, data trails offer an opportunity to explore learning from new and multiple angles. As stated by Latour (2008), the “ slightest move in the virtual landscape [is] paid for in lines of code. ” The view that data and analytics offer a new mode of thinking and a new model of discovery is at least partially rooted in the artificial intelligence and machine learning fields. Halevy, Norvig, and Pereira (2009) argue for the " unreasonable effectiveness of at UNIVERSITE LAVAL on December 18, 2015 abs.sagepub.com Downloaded from},
	keywords     = {distributed learning; learning analytics; models},
	annote       = {From Duplicate 1 (Learning Analytics: The Emergence of a Discipline - Siemens, George) From Duplicate 1 (Learning Analytics: The Emergence of a Discipline - Siemens, George; Latour, —bruno) From Duplicate 2 (Learning Analytics: The Emergence of a Discipline - Siemens, George; Latour, —bruno) Siemens, George Si}
}
@article{Siemens_Long_2011,
	title        = {Penetrating the fog: Analytics in learning and education},
	author       = {Siemens, George and Long, Phil},
	year         = 2011,
	journal      = {EDUCAUSE review},
	publisher    = {ERIC},
	volume       = 46,
	number       = 5,
	pages        = 30,
	issn         = {1527-6619}
}
@article{Stadler_Hofer_Greiff_2020,
	title        = {First among equals: Log data indicates ability differences despite equal scores},
	author       = {Stadler, Matthias and Hofer, Sarah and Greiff, Samuel},
	year         = 2020,
	month        = {Oct},
	journal      = {Computers in human behavior},
	volume       = 111,
	pages        = 106442,
	doi          = {10.1016/j.chb.2020.106442},
	issn         = {0747-5632},
	url          = {https://www.sciencedirect.com/science/article/pii/S0747563220301953},
	abstractnote = {Analyzing test-taking behavior allows researchers to investigate the steps and actions resulting in the specific test outcome. The underlying assumption is that test-taking behavior is a valid indicator of the tested ability. The aim of this paper is to scrutinize this assumption in the context of complex problem solving (CPS) by analyzing individual differences in test-taking behavior and their relation to individual differences in established correlates of CPS ability. We investigated a sample of Finnish students who achieved the maximum score on five CPS tasks and worked on an additional intelligence measure. We logged the number of interactions with the CPS tasks and time-on-task. Students showed significant variance in both time-on-task (s2 = 260.09, p = .005) and the number of interactions (s2 = 0.381, p < .001) despite having no variance in their CPS test scores. Using structural equation modeling, both time-on-task (β = 0.17, p = .015) and the number of interactions (β = −0.17, p < .001) significantly predicted students’ GPA, which has frequently shown to be associated with CPS ability. When adding intelligence – another established correlate of CPS – as predictor of GPA (β = 0.55, p < .001), the relation both between time-on-task (β = −0.09, p = .053) and the number of interactions (β = 0.09, p = .100) and students' GPA was reduced to negligible effects. Taken together, our study supports the assumption of test-taking behavior actually representing differences in CPS ability – over and above test scores.},
	keywords     = {Log data; Time-on-task; Complex problem solving; Structural equation modelling; Validity}
}
@article{Tempelaar_Rienties_Nguyen_2020,
	title        = {Subjective data, objective data and the role of bias in predictive modelling: Lessons from a dispositional learning analytics application},
	author       = {Tempelaar, Dirk and Rienties, Bart and Nguyen, Quan},
	year         = 2020,
	month        = {Jun},
	journal      = {PloS one},
	volume       = 15,
	number       = 6,
	pages        = {e0233977},
	doi          = {10.1371/journal.pone.0233977},
	issn         = {1932-6203},
	url          = {http://dx.doi.org/10.1371/journal.pone.0233977},
	abstractnote = {For decades, self-report measures based on questionnaires have been widely used in educational research to study implicit and complex constructs such as motivation, emotion, cognitive and metacognitive learning strategies. However, the existence of potential biases in such self-report instruments might cast doubts on the validity of the measured constructs. The emergence of trace data from digital learning environments has sparked a controversial debate on how we measure learning. On the one hand, trace data might be perceived as “objective” measures that are independent of any biases. On the other hand, there is mixed evidence of how trace data are compatible with existing learning constructs, which have traditionally been measured with self-reports. This study investigates the strengths and weaknesses of different types of data when designing predictive models of academic performance based on computer-generated trace data and survey data. We investigate two types of bias in self-report surveys: response styles (i.e., a tendency to use the rating scale in a certain systematic way that is unrelated to the content of the items) and overconfidence (i.e., the differences in predicted performance based on surveys’ responses and a prior knowledge test). We found that the response style bias accounts for a modest to a substantial amount of variation in the outcomes of the several self-report instruments, as well as in the course performance data. It is only the trace data, notably that of process type, that stand out in being independent of these response style patterns. The effect of overconfidence bias is limited. Given that empirical models in education typically aim to explain the outcomes of learning processes or the relationships between antecedents of these learning outcomes, our analyses suggest that the bias present in surveys adds predictive power in the explanation of performance data and other questionnaire data.},
	language     = {en}
}
@book{Tomcsik_Joksimovic_Juhász_Mihályi_2014,
	title        = {Early warning systems in six European countries Desk research report on study visit countries in the frame of CROCOOS-Cross-sectoral cooperation focused solutions for the prevention of early school leaving project Interim report},
	author       = {Tomcsik, Dóra and Joksimovic, Jelena and Juhász, Judit and Mihályi, Krisztina},
	year         = 2014,
	url          = {http://ok.proa.hu/crocoos-project}
}
@article{Wang_Mousavi_2023,
	title        = {Which log variables significantly predict academic achievement? A systematic review and meta‐analysis},
	author       = {Wang, Qin and Mousavi, Amin},
	year         = 2023,
	month        = {Jan},
	journal      = {British journal of educational technology: journal of the Council for Educational Technology},
	publisher    = {Wiley},
	volume       = 54,
	number       = 1,
	pages        = {142–191},
	doi          = {10.1111/bjet.13282},
	issn         = {0007-1013},
	url          = {https://onlinelibrary.wiley.com/doi/10.1111/bjet.13282},
	abstractnote = {Abstract Technologies and teaching practices can provide a rich log data, which enables learning analytics (LA) to bring new insights into the learning process for ultimately enhancing student success. This type of data has been used to discover student online learning patterns, relationships between online learning behaviors and assessment performance. Previous studies have provided empirical evidence that not all log variables were significantly associated with student academic achievement and the relationships varied across courses. Therefore, this study employs a systematic review with meta-analysis method to provide a comprehensive review of the log variables that have an impact on student academic achievement. We searched six databases and reviewed 88 relevant empirical studies published from 2010 to 2021 for an in-depth analysis. The results show different types of log variables and the learning contexts investigated in the reviewed studies. We also included four moderating factors to do moderator analyses. A further significance test was performed to test the difference of effect size among different types of log variables. Limitations and future research expectations are provided subsequently. Practitioner notes What is already known about this topic Significant relationship between active engagement in online courses and academic achievement was identified in a number of previous studies. Researchers have reviewed the literature to examine different aspects of applying LA to gain insights for monitoring student learning in digital environments (eg, data sources, data analysis techniques). What this paper adds Presents a new perspective of the log variables, which provides a reliable quantitative conclusion of log variables in predicting student academic achievement. Conducted subgroup analysis, examined four potential moderating variables and identified their moderating effect on several log variables such as regularity of study interval, number of online sessions, time-on-task, starting late and late submission. Compared the effect of generic and course-specific, basic and elaborated log variables, and found significant difference between the basic and elaborated. Implications for practice and/or policy A depth of understanding of these log variables may enable researchers to build robust prediction models. It can guide the instructors to timely adjust teaching strategies according to their online learning behaviors.},
	language     = {en}
}
@book{Wei_Simko_2021,
	title        = {R package “corrplot”: Visualization of a Correlation Matrix},
	author       = {Wei, Taiyun and Simko, Viliam},
	year         = 2021,
	url          = {https://github.com/taiyun/corrplot},
	annote       = {(Version 0.92)}
}
@inproceedings{Wu_Zhao_Wang_2021,
	title        = {Analysis of Students’ Learning Behavior under Network Learning Environment},
	author       = {Wu, Zeping and Zhao, Bo and Wang, Yong},
	year         = 2021,
	month        = {Jun},
	booktitle    = {2021 IEEE 3rd International Conference on Computer Science and Educational Informatization (CSEI)},
	pages        = {46–50},
	doi          = {10.1109/CSEI51395.2021.9477755},
	url          = {http://dx.doi.org/10.1109/CSEI51395.2021.9477755},
	abstractnote = {The emergence of new technologies makes online learning platforms flourish. Although learners can break through the limitations of time and space and choose any resource to learn according to their own pace, there is a problem of low learning pass. This paper attempts to take the Computer Basic online course offered by the university in Yunnan Province on the platform of Superstar as an example, and uses the statistical methods to mine the learners’ behavior under the network environment, so as to help stakeholders to better implement teaching and learning, and solve the problem of poor learning effect. This study first adopts Spearman correlation analysis on the learners' online learning behavior, and then uses multiple linear regression analysis method to explore which factors affect comprehensive score, and finally makes clustering analysis on six variables representing learners' behavior to obtain behavior characteristics with similar learners. The research shows that: (1) Learners can be divided into four different groups according to the behavior indicators; (2) Course video progress and chapter learning times have a positive and significant impact on comprehensive score. Through data mining and learning behavior analysis, teachers can identify the characteristics of learners, and then carry out personalized teaching.},
	keywords     = {Analytical models;Correlation;Conferences;Education;Linear regression;Data models;Data mining;Learning behavior analysis;clustering analysis;correlation analysis;multiple linear regression analysis}
}
@article{You_2016,
	title        = {Identifying significant indicators using LMS data to predict course achievement in online learning},
	author       = {You, Ji Won},
	year         = 2016,
	month        = {Apr},
	journal      = {The Internet and Higher Education},
	volume       = 29,
	pages        = {23–30},
	doi          = {10.1016/j.iheduc.2015.11.003},
	issn         = {1096-7516},
	url          = {https://www.sciencedirect.com/science/article/pii/S1096751615300063},
	abstractnote = {This study sought to identify significant behavioral indicators of learning using learning management system (LMS) data regarding online course achievement. Because self-regulated learning is critical to success in online learning, measures reflecting self-regulated learning were included to examine the relationship between LMS data measures and course achievement. Data were collected from 530 college students who took an online course. The results demonstrated that students’ regular study, late submissions of assignments, number of sessions (the frequency of course logins), and proof of reading the course information packets significantly predicted their course achievement. These findings verify the importance of self-regulated learning and reveal the advantages of using measures related to meaningful learning behaviors rather than simple frequency measures. Furthermore, the measures collected in the middle of the course significantly predicted course achievement, and the findings support the potential for early prediction using learning performance data. Several implications of these findings are discussed.},
	keywords     = {LMS data; Self-regulated learning; Course achievement; Learning analytics; Online learning}
}
@article{Zarrabi_Bozorgian_2020,
	title        = {EFL Students’ Cognitive Performance during Argumentative Essay Writing: A log-file data analysis},
	author       = {Zarrabi, Forooq and Bozorgian, Hossein},
	year         = 2020,
	month        = {Mar},
	journal      = {Computers and Composition},
	volume       = 55,
	pages        = 102546,
	doi          = {10.1016/j.compcom.2020.102546},
	issn         = {8755-4615},
	url          = {https://www.sciencedirect.com/science/article/pii/S8755461520300074},
	abstractnote = {The continuing growth of technology has led to a growing interest in online writing tools to gain an in-depth insight into students’ cognitive performance. Using Inputlog 8.0, the present work explored undergraduate male and female students' (N = 72, mean age = 19.7 years) Online Argumentative Writing Performance (OAWP). Participants were provided argumentative writing tasks in two temporal phases: a) composing and b) revision. The log-file data were analyzed using structural equation modeling (SEM) to explore any linear or quadratic relations among manifest (i.e., revision frequency, text length, mean R-bursts, total pause time, total active writing time, pause frequency, P-burst frequency, mean typed in P-burst and geometric mean of within-word, between-word, sentence, paragraph pauses) and latent (OAWP) variables simultaneously. The result showed significant linear relations between all variables and OAWP but no quadric relations were observed. Overall, findings suggest that students who employed between-word or between-sentence pause strategy (PS), active writing plus planned pause showed better OAWP, while high frequency of revision and pause at between word boundaries indicated much less successful OAWP. The result indicates that scaffolded revision, worked for L2 students' advantage in making strategic use of time for planning and improvement of written argumentation.},
	keywords     = {Online Argumentative Writing Performance (OAWP); Log-file data; Time on task; Pause behavior; Revision behavior; Pause strategy (PS); Keystroke Logging (KL)}
}
