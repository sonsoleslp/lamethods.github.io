{
  "hash": "f4904c5e66d020e8ebdfc3b5d27268bc",
  "result": {
    "markdown": "---\ntitle: \"Comparative Analysis of Regularization Methods for Predicting Student Certification\"\nwarning: FALSE\ncode-overflow: wrap\nformat: html\nexecute:\n  message: FALSE\nauthor: \n   - name: \"Tian Li\"\n   - name: \"Feifei Han\"\n   - name: \"Jiesi Guo\"\n   - name: \"Jinran Wu\"\ncrossref:\n  fig-title: '**Figure**'\n  fig-labels: arabic\n  title-delim: \"**.**\"\nabstract-title: \"Abstract\"\nabstract: \"This chapter uses predicting student certification in online courses hosted on edX to demonstrate how to apply three powerful regularization methods, namely Least Absolute Shrinkage and Selection Operator (LASSO), Smoothly Clipped Absolute Deviation (SCAD), and Minimax Concave Penalty (MCP) in constructing a predictive model. These advanced statistical methods are especially valuable in scenarios where we have a multitude of potential predictors and need to select appropriate variables to establish viable predictive models.\"\nkeywords: \"Regularization Methods, Comparison, Prediction, Online courses in edX\"\ndpi: 900\nbibliography: refs.bib\nextract-media: \"img\"\nfig-align: center\n---\n\n\n## Introduction {#sec:1}\n\nIn the past decade, the educational landscape has undergone a seismic\nshift with the advent and proliferation of online learning\nplatforms  [@papadakis2023moocs]. Among these, Massive Open Online\nCourses (MOOCs) have emerged as a transformative force, democratizing\naccess to high-quality education on a global scale  [@deng2019progress].\nFirst emerged in 2008, Massive Open Online Courses (MOOCs) provide\nanyone with an internet connection and low-cost access to educational\nmaterials and activities delivered by experts. Like other forms of adult\neducation, the establishment of MOOCs aims to help learners realize\ntheir potential and apply newly acquired knowledge and skills to improve\ntheir lives  [@knowles1980modern]. Students enroll in MOOCs for various\nreasons, including personal interest, career advancement, academic\nsupport, or social connections  [@brooker2018tale]. Unlike traditional\nonline or in-person adult education courses, MOOCs allow students to\naccess courses anywhere and time. Despite their potential, MOOCs have\nfaced many challenges, such as accreditation and certification\n(Sanchez-Gordon), instructional design  [@spector2014editorial], types of\nlearners  [@bingol2020factors], as well as dropout\nrates  [@xiong2015examining].\n\nAt the forefront of this revolution stands edX Online Courses &\nPrograms, a pioneering platform founded in May 2012 under the\nadministration of the Massachusetts Institute of Technology (MIT). As a\nleader in the MOOC space, the courses on the edX are, offered by\nworld-renowned institutions, schools, non-profit organizations, and\ncorporations across a diverse array of subjects, from subjects in\nscience, technology, engineering, and math (STEM) to social sciences and\nhumanities. edX has opened the doors of elite education to millions of\nlearners worldwide through its commitment to accessibility and rigorous\nacademic standards, making it a beacon of innovation in online\neducation.\n\nIn the traditional classrooms where the teaching staff interact with\nstudents face-to-face  [@han2023relations], this allows them to have\ndirect perceptions of students' levels of engagement, hence, targeted\nstrategies can be implemented in advance to increase student engagement\nand to prevent students from dropping out.\n\nIn the online context, with the advancement of learning analytics to\ncapture large volumes of digital trace data, which are combined with\nvarious demographic data, researchers and educators can predict\nstudents' tendency to drop out earlier in the course by applying\nadvanced data mining and statistical techniques  [@han2022descriptive].\nThis also aids in understanding and improving the quality of student\nlearning experiences  [@siemens2012guest].\n\nOne of the important indicators of student completion of online courses\nhosted on edX is to obtain an official certification of course. Hence,\nunderstanding factors impacting students obtaining course certification\nmay provide course developers valuable information on strategies to\nimprove the design of the course, online pedagogies, and methods of\ndelivery to foster student online engagement and improve retention rate.\n\nIt shall be noted that many references like Jovanovic et\nal.  [@jovanovic2024predictive] and Gray and Perkins  [@gray2019utilizing]\nstudied machine learning methods, such as random forests, to predict\nstudent course performance. However, our method differs as we focus on\nhigh-dimensional predictors and apply regularization techniques to\nhighlight key predictors associated with learning outcome predictions.\nSpecifically, we aim to create an elegant predictive model by using\nregularization to eliminate redundant predictors. Moreover, this chapter\nuses predicting student certification in online courses hosted on edX to\ndemonstrate how to apply three powerful regularization methods, namely\nLeast Absolute Shrinkage and Selection Operator\n(LASSO)  [@tibshirani1996regression], Smoothly Clipped Absolute Deviation\n(SCAD)  [@fan2001variable], and Minimax Concave Penalty\n(MCP)  [@zhang2010nearly] in constructing a predictive model. These\nadvanced statistical methods are especially valuable in scenarios where\nwe have a multitude of potential predictors and need to select\nappropriate variables to establish viable predictive models. Therefore,\nwe aim to achieve the following objectives:\n\n1.  Illustrate the strengths of LASSO, SCAD, and MCP in constructing\n    predictive models using predicting obtaining course certification in\n    edX courses as an example. The three advanced regularization methods\n    each have unique strengths in the process of establishing predictive\n    models. Through applying them to a specific context -- success in\n    obtaining course certification in edX courses, it is easy to\n    understand the relative effectiveness of these methods, providing\n    researchers, educators, and course designers hands-on knowledge so\n    that they can select appropriate methods by considering the unique\n    characteristics of each method and make similar predictions of their\n    online courses;\n\n2.  Uncover the key factors contributing to student certification in edX\n    courses: We hope to identify the most key predictors of student\n    success by analyzing a wide range of variables. This knowledge can\n    inform both learners and educators, helping to create more effective\n    learning strategies and support systems;\n\n3.  Identify the most important predictors of student success: Through\n    variable selection and model interpretation, we aim to pinpoint the\n    factors that strongly influence certification outcomes. This\n    information can be invaluable for course designers, instructors, and\n    platform developers in optimizing the online learning experience;\n\n4.  Provide actionable insights to inform course design and student\n    support strategies: By understanding the key drivers of student\n    success, we can offer evidence-based recommendations for improving\n    course structure, content delivery, and support mechanisms on edX\n    and similar platforms.\n\n## Methodologies\n\nIn this section, we will describe three regularization methods (i.e.,\nLASSO, SCAD, and MCP) and the cross-validation method for hyperparameter\ntuning. In addition, we also will introduce some popular classification\nevaluation metrics.\n\n### Regularization Methods for High-Dimensional Data\n\nIn the context of high-dimensional data, where the number of predictors\n($p$) is large relative to the number of observations ($n$), traditional\nstatistical methods often fail due to overfitting, multicollinearity,\nand computational issues. Regularization methods have emerged as\npowerful tools to address these challenges by introducing a penalty term\nto the loss function, effectively shrinking some coefficient estimates\ntowards zero and, in some cases, setting them exactly to\nzero  [@hastie2015statistical].\n\nThese methods perform simultaneous variable selection and coefficient\nestimation, making them particularly useful in scenarios where\nidentifying the most important predictors is as crucial as making\naccurate predictions. The general form of a penalized regression problem\n(with regression coefficients $\\beta$) can be written\nas  [@fu2021robust; @wang2023new]:\n$$\\min_{\\beta}\\{L(\\beta) + \\lambda P(\\beta)\\}$$ where $L(\\beta)$ is the\nloss function (e.g., negative log-likelihood for logistic regression),\n$P(\\beta)$ is the penalty function, and $\\lambda$ is the tuning\nparameter controlling the strength of regularization. Moreover, we will\ndelve deeper into each of the three regularization methods (LASSO, SCAD,\nand MCP) we implemented in the following contents.\n\n::: remark\nIn this study, we focus on a classification task using regularization\nmethods, specifically through three types of regularized logistic\nregression models. We need to adjust the first component for continuous\nresponse variables, replacing the logistic loss function with\nalternatives like least squares or robust losses such as\nHuber  [@huber1973robust] or exponential loss  [@jiang2019robust].\nMulticollinearity is not a concern in our regularization approach.\nRegularization methods inherently address multicollinearity by applying\npenalties that shrink the coefficients of correlated predictors,\neffectively reducing redundancy and enhancing model stability.\n:::\n\n#### LASSO\n\nLASSO, introduced by Tibshirani  [@tibshirani1996regression], adds an L1\npenalty to the loss function:\n$$P(\\beta) = ||\\beta||_{1} =\\Sigma |\\beta_j|.$$ The L1 penalty has the\nproperty of shrinking some coefficients exactly to zero, effectively\nperforming variable selection. This is due to the geometry of the L1\nball, which intersects with the contours of the loss function at the\naxes, leading to sparse solutions  [@hastie2015statistical]. Key\nproperties of LASSO are listed: a) Performs both variable selection and\nregularization; b) Produces sparse models, which are often easier to\ninterpret; c) Can handle high-dimensional data ($p > n$); and d) Tends\nto select one variable from a group of correlated predictors. However,\nLASSO has some limitations: a) In the $p > n$ case, it selects most $n$\nvariables before saturation; b) It can be biased for large coefficients;\nand c) It does not have the oracle property (consistent variable\nselection).\n\n#### SCAD\n\nSCAD, proposed by Fan and Li  [@fan2001variable], is a non-convex penalty\nthat aims to overcome some of the limitations of LASSO. The SCAD penalty\nis defined as: $$P(\\beta) = \n\\begin{cases}\n\\lambda|\\beta| & \\text{if } |\\beta| \\leq \\lambda \\\\\n-\\frac{\\beta^2 - 2a\\lambda|\\beta| + \\lambda^2}{2(a-1)} & \\text{if } \\lambda < |\\beta| \\leq a\\lambda \\\\\n\\frac{(a+1)\\lambda^2}{2} & \\text{if } |\\beta| > a\\lambda\n\\end{cases},$$ where $a > 2$ is a tuning parameter, often set to $3.7$\nas suggested by Fan and Li  [@fan2001variable]. Moreover, key properties\nof SCAD are given as a) Overcomes the bias issue of LASSO for large\ncoefficients; b) Possesses the oracle property (asymptotically performs\nas well as if the true model were known in advance); c) Produces sparse\nsolutions; d) Continuous in $\\beta$, which leads to more stable\nsolutions. However, SCAD also has some challenges: a) Non-convex\noptimization problem, which can be computationally more demanding; and\nb) Requires tuning of two parameters ($\\lambda$ and $a$).\n\n#### MCP\n\nMCP introduced by Zhang  [@zhang2010nearly], is another non-convex\npenalty designed to address the limitations of LASSO. The MCP is defined\nas: $$P(\\beta) = \n\\begin{cases} \n\\lambda |\\beta| - \\frac{\\beta^2}{2\\gamma} & \\text{if } |\\beta| \\leq \\gamma \\lambda \\\\\n\\frac{\\gamma \\lambda^2}{2} & \\text{if } |\\beta| > \\gamma \\lambda\n\\end{cases},$$ where $\\gamma > 1$ is a tuning parameter controlling the\nconcavity of the penalty. It shall be noted that some key properties of\nMCP are listed as a) Like SCAD, it overcomes the bias issue of LASSO for\nlarge coefficients; b) Possesses the oracle property; c) Produces sparse\nsolutions; d) Provides a smooth transition between the region of\npenalization and the constant region. However, there are still\nchallenges of MCP: a) Non-convex optimization problem; and b) Requires\ntuning of two parameters ($\\lambda$ and $\\gamma$).\n\nIt shall be noted that both SCAD and MCP are part of a broader class of\nconcave penalty functions that aim to combine the variable selection\nproperties of L1 penalties with the unbiasedness of L2 penalties for\nlarge coefficients  [@zou2008one]. In practice, the choice between these\nmethods often depends on the specific characteristics of the data and\nthe goals of the analysis. LASSO is often preferred for its simplicity\nand convexity, while SCAD and MCP can provide better theoretical\nproperties and potentially more accurate coefficient estimates,\nespecially for large effects.\n\n### K-Fold Cross-Validation for Hyperparameter Tuning\n\nK-Fold Cross-Validation (CV) is a widely used method for hyperparameter\ntuning in statistical learning, providing a robust estimate of model\nperformance across different data subsets  [@hastie2009elements]. The\nprocess begins by setting aside an independent test set and dividing the\nremaining data into $K$ equally sized folds. Hyperparameter tuning is\nthen performed by systematically exploring a predefined hyperparameter\nspace. For each hyperparameter combination, the model is trained on\n$K-1$ folds and validated on the remaining fold, rotating through all\n$K$ folds. The average performance across these $K$ iterations is used\nto assess each hyperparameter set. This approach helps select\nhyperparameters that generalize well to unseen data, reducing\noverfitting risk.\n\nThe K-Fold CV process for hyperparameter tuning typically involves\nseveral steps: defining a hyperparameter grid or search space,\nperforming the cross-validation loop for each hyperparameter\ncombination, selecting the best-performing hyperparameters, and finally\ntraining and evaluating the model on the entire dataset and held-out\ntest set, respectively. The hyperparameter search can be conducted using\nvarious methods, including grid search, random\nsearch  [@bergstra2012random], or more advanced techniques like Bayesian\noptimization  [@snoek2012practical]. While this method offers advantages\nsuch as robust performance estimation and efficient use of limited data,\nit also presents challenges. These include computational cost,\nespecially for large datasets or complex models, and the potential\ninstability of results depending on the specific folds used.\n\n### Classification Evaluation Metrics\n\nIn classification problems, especially binary classification, several\nmetrics are commonly used to evaluate model performance. These metrics\nprovide different perspectives on the model's effectiveness and are\nderived from the confusion matrix. Understanding these metrics is\ncrucial for properly assessing and comparing classification models.\nBefore diving into the metrics, it is important to understand the\nconfusion matrix in Table  [1](#confusion_matrix){reference-type=\"ref\"\nreference=\"confusion_matrix\"}, which is the foundation for these\nmetrics.\n\n::: {#confusion_matrix}\n                     Predicted Positive    Predicted Negative\n  ----------------- --------------------- ---------------------\n   Actual Positive   True Positive (TP)    False Negative (FN)\n   Actual Negative   False Positive (FP)   True Negative (TN)\n\n  : The illustration of the confusion matrix\n:::\n\n[]{#confusion_matrix label=\"confusion_matrix\"}\n\nBased on the confusion matrix, five popular classification measurement\nmetrics (Accuracy, Precision, Recall, F1 score, and AUC) are derived as\nbelow  [@hand2012assessing]:\n\n1.  **Accuracy** is the proportion of correct predictions (both true\n    positives and true negatives) among the total number of cases\n    examined as:\n    $$\\text{Accuracy} =\\frac{\\text{TP} + \\text{TN}}{\\text{TP} +\\text{ TN}+ \\text{FP} + \\text{FN}}.$$\n\n2.  **Precision** is the proportion of true positive predictions among\n    all positive predictions and is formulated as:\n    $$\\text{Precision} =\\frac{\\text{TP}}{\\text{TP}+\\text{FP}}.$$\n\n3.  **Recall** is the proportion of actual positive instances that were\n    correctly identified and is formulated as:\n    $$\\text{Recall} =\\frac{\\text{TP}}{\\text{TP}+\\text{FN}}.$$\n\n4.  **F1 score** is the harmonic mean of precision and recall, providing\n    a single score that balances both metrics and the mathematical\n    formula is:\n    $$\\text{F1} =\\frac{2 \\times ( \\text{Precision}\\times \\text{Recall}) }{\\text{Precision} +\\text{Recall}}.$$\n\n5.  **AUC** (Area Under the receiver operating characteristic curve) is\n    a performance metric that represents the probability that a randomly\n    chosen positive instance is ranked higher than a randomly chosen\n    negative instance. It is calculated as the area under the Receiver\n    Operating Characteristic curve. A higher AUC value indicates better\n    model performance in distinguishing between the classes.\n\nIt shall be noted that for imbalanced datasets, in cases where classes\nare imbalanced, accuracy can be misleading. For example, in a dataset\nwith 95% negative cases and 5% positive cases, a model that always\npredicts negative would have 95% accuracy but would be useless for\nidentifying positive cases. Therefore, an F1 score (or AUC) that\ncombines Precision and Recall is suggested to measure the classification\nperformance for imbalanced datasets.\n\n## Data Description and Exploratory Analysis\n\nThis section will present the dataset chosen for our illustration and\nconduct a preliminary exploration using basic visualization techniques.\n\n### Dataset Overview\n\nOur dataset comes from [an online learning\nplatform](https://github.com/lamethods/data2/blob/main/mooc/HXPC13_DI_v3_11-13-2019.RDS)\nand contains information about students enrolled in various courses. The\nkey variables in our analysis include:\n\n-   `certified`: Whether the student received certification (our target\n    variable);\n\n-   `course_id`: Identifier for different courses;\n\n-   `explored`: Whether the student explored the course content;\n\n-   `gender`: Student's gender;\n\n-   `nevents`: Number of events (interactions) by the student;\n\n-   `ndays_act`: Number of days the student was active;\n\n-   `nplay_video`: Number of video plays;\n\n-   `nchapters`: Number of chapters accessed;\n\n-   `nforum_posts`: Number of forum posts made by the student;\n\n-   `final_cc_cname_DI`: Student's country name.\n\nIt is important to note that the response variable in our study is\n\"certified\", with the other variables serving as predictors, as applied\nin Yuan et al. [@yuan2024integrating].\n\n### Data Preparation and Exploratory Data Analysis\n\nWe begin our analysis by loading the necessary libraries and preparing\nour data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(ncvreg)\nlibrary(pROC)\n\nrepo <- \"https://github.com/lamethods/data2/raw/refs/heads/main/\"\ndataset <- \"mooc/HXPC13_DI_v3_11-13-2019.RDS\"\n\nmydata <- readRDS(url(paste0(repo, dataset)))\ndata <- mydata[,c(\"certified\",\"course_id\",\"explored\",\"gender\",\n                  \"nevents\",\"ndays_act\",\"nplay_video\",\"nchapters\",\n                  \"nforum_posts\",\"final_cc_cname_DI\")]\ndata[data == \"\"] <- NA\ndata <- na.omit(data)\n```\n:::\n\n\nThe following code snippet loads our dataset, selects the relevant columns,\nreplaces empty strings with NA values, and removes any rows with missing\ndata. Here, we shall notice our target of this chapter is to illustrate\nhow to employ high-dimensionality reduction approaches for identifying\nkey predictors (factors) relevant to the response (whether certified or\nnot), thus we remove all the samples with missing data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(data$certified, data$course_id)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   \n    HarvardX/PH207x/2012_Fall HarvardX/PH278x/2013_Spring\n  0                     17671                       11006\n  1                      1724                         616\n```\n:::\n\n```{.r .cell-code}\ntable(data$certified, data$gender)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   \n        f     m\n  0 13077 15600\n  1  1078  1262\n```\n:::\n\n```{.r .cell-code}\ntable(data$certified, data$explored)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   \n        0     1\n  0 26027  2650\n  1    60  2280\n```\n:::\n\n```{.r .cell-code}\n# Ensure 'course_id' is a factor\ndata$course_id <- as.factor(data$course_id)\n# Rename specific course names\nlevels(data$course_id)[levels(data$course_id) == \n                         \"HarvardX/PH207x/2012_Fall\"] <- \"c_id1\"\nlevels(data$course_id)[levels(data$course_id) == \n                         \"HarvardX/PH278x/2013_Spring\"] <- \"c_id2\"\n```\n:::\n\n\nBefore diving into our predictive modeling, it is crucial to understand\nthe distribution and relationships within our data. We use visualization\ntechniques with two R packages \"ggplot2\"  [@wickham2011ggplot2] and\n\"patchwork\"  [@pedersen2019package] to gain these insights.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Certification rates by course\nplot1 <- ggplot(data, aes(x = course_id, fill = factor(certified))) +\n  geom_bar(position = \"fill\") +\n  labs(y = \"proportion\", title = \"course_id\", fill = \"certified\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(hjust = 0.5))\n# Certification rates by gender\nplot2 <- ggplot(data, aes(x = gender, fill = factor(certified))) +\n  geom_bar(position = \"fill\") +\n  labs(y = \"proportion\", title = \"gender\", fill = \"Certified\") +\n  theme(plot.title = element_text(hjust = 0.5))\n# Certification rates by exploration status\nplot3 <- ggplot(data, aes(x = explored, fill = factor(certified))) +\n  geom_bar(position = \"fill\") +\n  labs(y = \"proportion\", title = \"explored\", fill = \"Certified\") +\n  theme(plot.title = element_text(hjust = 0.5))\n# Combine the plots\ncombined_plot <- plot1 | plot2 | plot3\ncombined_plot + plot_layout(guides = 'collect')\n```\n\n::: {.cell-output-display}\n![The visualization of the certification rates by course, gender, and exploration status](ch05-regularization_files/figure-html/fig-cert-rates-1.png){#fig-cert-rates width=960}\n:::\n:::\n\n\nThis code creates three bar plots reported in @fig-cert-rates showing the\ncertification rates by course (categorical variable), gender (binary\nvariable), and exploration status (binary variable). These\nvisualizations help us understand how certification rates vary across\ndifferent subgroups in our dataset.\n\n\nMoreover, we can obtain the detailed numerical results of the\ncertification rates by course, gender, and exploration status with the\nfollowing codes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# course_id\ntable(data$certified,data$course_id)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   \n    c_id1 c_id2\n  0 17671 11006\n  1  1724   616\n```\n:::\n\n```{.r .cell-code}\n# gender\ntable(data$certified,data$gender)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   \n        f     m\n  0 13077 15600\n  1  1078  1262\n```\n:::\n\n```{.r .cell-code}\n# explored\ntable(data$certified,data$explored)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   \n        0     1\n  0 26027  2650\n  1    60  2280\n```\n:::\n:::\n\n\n \n\nIn addition, different from the former visualization for binary and\ncategorical variables, here, we also create boxplots to visualize the\nrelationship between certification and various numeric variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_long <- data.frame(\n  certified = rep(data$certified, 5),\n  metric = factor(rep(c(\"nevents\", \"ndays_act\", \"nchapters\", \"nplay_video\",\n                        \"nforum_posts\"), each = nrow(data))),\n  value = c(data$nevents, data$ndays_act, data$nchapters, data$nplay_video, data$nforum_posts)\n)\n# Create a data frame manually for faceting\nfaceted_plot <- ggplot(data_long, aes(x = factor(certified), y = value, fill = factor(certified))) +\n  geom_boxplot() +\n  facet_wrap(~ metric, scales = \"free_y\", ncol = 2) +\n  labs(title = \"Metrics by Certification Status\", x = \"Certified\", y = \"Value\", fill = \"Certified\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        strip.text = element_text(size = 12),\n        legend.position = \"bottom\",\n        aspect.ratio = 0.45) # Adjust the aspect ratio to make plots taller or wider\n\nprint(faceted_plot)\n```\n\n::: {.cell-output-display}\n![The visualization of the certification outcomes by `nevents`, `ndays_act`, `nchapters`, `nplay_video`, and `nforum_posts`](ch05-regularization_files/figure-html/fig-boxplot-1.png){#fig-boxplot width=480}\n:::\n:::\n\n\n \n\nThese boxplots in @fig-boxplot reveal the differences in various engagement metrics between certified\nand non-certified students, highlighting that certified users generally\nshow higher levels of engagement across multiple dimensions. In each\nboxplot, variables such as `nevents`, `ndays_act`, and `nchapters`\ndisplay distinct patterns between certified and non-certified\nindividuals, with certified students showing higher medians and wider\ndistributions in these metrics. This pattern suggests that active and\nsustained participation---through frequent events, consistent activity\ndays, and more completed chapters---is associated with a greater\nlikelihood of certification. However, the patterns for `nplay_video` and\n`nforum_posts` are less clear, as both certified and non-certified\nstudents show similar distributions with low levels of forum\nparticipation overall. This lack of differentiation implies that while\ndirect interaction with course material is crucial, forum activity and\nvideo play frequency may not significantly impact certification\noutcomes. Therefore, we will further analyze the visualization results\nwith advanced regression approaches to better understand these\nrelationships and quantify the extent to which each metric influences\nthe likelihood of certification.\n\n## Data Modelling and Numerical Results\n\n### Data Preprocessing\n\nIn regularized regression for high-dimensional predictors, the\noptimization objective combines two components: the negative\nlog-likelihood (for logistic regression) and the penalty function, with\na hyperparameter lambda balancing them. In practice, predictors can have\ndifferent scales, which may complicate the hyperparameter tuning process\nby impacting the search for an optimal lambda within a specified range.\nTherefore, it is necessary to scale the numeric variables to ensure they\nare on a consistent scale, improving the stability and efficiency of\ntuning. Additionally, converting categorical variables to factors allows\nthe model to correctly interpret them as categorical, which is essential\nfor meaningful analysis and accurate results. Therefore, before applying\nour regularization methods, we need to preprocess our data. We scale the\nnumeric variables and convert categorical variables to factors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata2 <- as.data.frame(scale(data[,c(\"nevents\", \"ndays_act\", \"nplay_video\", \n                                     \"nchapters\", \"nforum_posts\")]))\ndata2$certified <- as.factor(data$certified)\ndata2$course_id <- as.factor(data$course_id)\ndata2$explored <- as.factor(data$explored)\ndata2$gender <- as.factor(data$gender)\ndata2$final_cc_cname_DI <- as.factor(data$final_cc_cname_DI)\n```\n:::\n\n\n### Model Preparation\n\nWe split our data into training (80%) and test (20%) sets:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\ntrain_indices <- createDataPartition(data2$certified, p = 0.8, list = FALSE)\ntrain_data <- data2[train_indices, ]\ntest_data <- data2[-train_indices, ]\n```\n:::\n\n\nConsidering the potential impacts of interactions between predictors, we\nconstruct many interaction terms in our statistical modeling. With such\nsome interaction terms, we can clearly illustrate the effectiveness of\nthe regularization approaches for handling high-dimensional predictors\nin regression modeling. We prepare our design matrices with interaction\nterms and response vectors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_train <- train_data$certified\nX_train <- model.matrix(certified ~ course_id + gender + final_cc_cname_DI + \n                          nevents + ndays_act +nplay_video + explored + nchapters +\n                          nforum_posts + final_cc_cname_DI + \n                          (course_id + nevents + ndays_act + nplay_video + \n                             explored + nchapters + nforum_posts) * gender + \n                          (nevents + ndays_act + nplay_video + explored + \n                             nchapters + nforum_posts) * course_id, \n                        data = train_data)[,-1]\n\ny_test <- test_data$certified\nX_test <- model.matrix(certified ~ course_id + gender + final_cc_cname_DI + \n                         nevents + ndays_act + nplay_video + explored + nchapters +\n                         nforum_posts + final_cc_cname_DI + \n                         (course_id + nevents + ndays_act + nplay_video + \n                            explored + nchapters + nforum_posts) * gender +\n                         (nevents + ndays_act + nplay_video + explored + \n                            nchapters + nforum_posts) * course_id, \n                       data = test_data)[,-1]\n```\n:::\n\n\n### Cross-validation and Model Fitting\n\nWe implement three regularization methods: LASSO, SCAD, and MCP. These\nmethods are particularly useful for high-dimensional data where we have\na large number of predictors relative to the number of observations.\nThey perform variable selection by shrinking some coefficients to\nexactly zero, effectively removing those predictors from the model. We\nimplement these methods using the \"ncvreg\" package  [@breheny2024package]\nin R.\n\nWe use k-fold cross-validation to select the optimal regularization\nparameter (lambda) for each method with the folder number $5$. Our\nimplementation uses the Area Under the ROC Curve (AUC) as the\nperformance metric:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to perform cross-validation using AUC and fit the model\ncv_and_fit_auc <- function(X, y, penalty, nfolds = 5) {\n  set.seed(123)  # for reproducibility\n  # Fit the model using the entire path\n  fit <- ncvreg(X, y, family = \"binomial\", penalty = penalty, standardize = FALSE)\n  # Perform k-fold cross-validation\n  folds <- sample(rep(1:nfolds, length.out = length(y))) \n  aucs <- matrix(0, nrow = nfolds, ncol = length(fit$lambda))\n\n  for (i in 1:nfolds) {\n    test_indices <- which(folds == i)\n    X_train_cv <- X[-test_indices, ]\n    y_train_cv <- y[-test_indices]\n    X_test_cv <- X[test_indices, ]\n    y_test_cv <- y[test_indices]\n    # Fit model on training data\n    cv_fit <- ncvreg(X_train_cv, y_train_cv, family = \"binomial\", \n                     penalty = penalty, lambda = fit$lambda,standardize = FALSE)\n    # Calculate AUC for each lambda\n    for (j in 1:length(cv_fit$lambda)) {\n      beta <- coef(cv_fit)[, j]\n      # Ensure beta is a numeric vector\n      beta <- as.numeric(beta)\n      # Initialize vector to store probabilities\n      probs <- numeric(nrow(X_test_cv))\n      # Loop over each row in X_test_cv\n      for (k in 1:nrow(X_test_cv)) {\n        # Extract the predictor vector for the current row\n        x_row <- X_test_cv[k, , drop = FALSE]\n        # Calculate the linear predictor (including intercept)\n        linear_predictor <- cbind(1, x_row) %*% beta\n        # Calculate the predicted probability\n        probs[k] <- 1 / (1 + exp(-linear_predictor))\n      }\n      # Calculate AUC for the current lambda\n      roc_curve <- roc(y_test_cv, probs, quiet = TRUE)\n      aucs[i, j] <- auc(roc_curve)\n    }\n  }\n  # Calculate mean AUCs across folds\n  mean_aucs <- colMeans(aucs)\n  # Determine the optimal lambda\n  optimal_lambda_index <- which.max(mean_aucs)\n  optimal_lambda <- fit$lambda[optimal_lambda_index]\n  # Extract coefficients at the optimal lambda\n  coef <- coef(fit)[, optimal_lambda_index]\n  return(list(fit = fit, coef = coef, optimal_lambda = optimal_lambda, \n              mean_aucs = mean_aucs))\n}\n```\n:::\n\n\nThe cv_and_fit_auc() function performs cross-validation and model\nfitting to select an optimal regularization parameter (lambda) using AUC\nas the performance metric. We first fit a model across a range of lambda\nvalues using the ncvreg function, which applies a penalized regression\ntechnique. The data is split into nfolds subsets for cross-validation,\nwherein each fold, the function trains a model on the training data and\nevaluates it on the test data. The AUC for each value of lambda is\ncalculated by looping through the test set and computing predicted\nprobabilities, which are then used to generate the ROC curve. After\ncalculating the mean AUC across all folds for each lambda, the optimal\nlambda is selected based on the highest average AUC. The function\nreturns the fitted model, the coefficients at the optimal lambda, the\nselected lambda, and the mean AUCs across folds, providing an effective\nmethod for model tuning and evaluation.\n\nWe then fit our models with cv_and_fit_auc() function:\n\n\n::: {.cell hash='ch05-regularization_cache/html/unnamed-chunk-10_6cd1ece59d59ff9b289d1a2a7bad3a87'}\n\n```{.r .cell-code}\nlasso_results <- cv_and_fit_auc(X_train, y_train, \"lasso\")\nscad_results <- cv_and_fit_auc(X_train, y_train, \"SCAD\")\nmcp_results <- cv_and_fit_auc(X_train, y_train, \"MCP\")\n```\n:::\n\n\nIt shall be noted that the computational efficiency of the model fitting\ndepends on the sample size and the dimensionality of the predictors. To\nspeed up the fitting process, some parallel computing techniques are\nsuggested.\n\n### Results\n\nWe evaluate our models on the test set using various performance\nmetrics. The function `calculate_test_performance()` evaluates a binary\nclassification model's performance by taking model coefficients (`coef`),\ntest predictors `X_test`, and true labels (`y_test`) as inputs. First, we\nensure the data formats and dimensions are correct. Then, we calculate\nlinear predictor values and use the logistic function to compute\npredicted probabilities for each test instance. These probabilities are\nconverted to binary predictions using a threshold of 0.5. Next, the\nfunction computes various performance metrics, including accuracy,\nprecision, recall, and F1 score using a confusion matrix, as well as the\nAUC using the ROC curve to measure the model's ability to distinguish\nbetween classes. Finally, all calculated metrics are returned in a list\nfor easy interpretation, providing a comprehensive evaluation of the\nmodel's predictive performance.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to calculate test performance metrics\ncalculate_test_performance <- function(coef, X_test, y_test) {\n  # Ensure X_test is a matrix\n  X_test <- as.matrix(X_test)\n  # Ensure coef is a numeric vector\n  coef <- as.numeric(coef)\n  # Check that coef length matches the number of columns in X_test + 1 \n  # (for the intercept)\n  if (length(coef) != (ncol(X_test) + 1)) {\n    stop(\"Length of coef must be equal to the number of predictors \n         plus one for the intercept.\")\n  }\n  # Initialize vectors to store linear predictors and probabilities\n  linear_predictors <- numeric(nrow(X_test))\n  probs <- numeric(nrow(X_test))\n  # Loop over each row in X_test\n  for (i in 1:nrow(X_test)) {\n    # Extract the predictor vector\n    x_row <- X_test[i, , drop = FALSE]\n    # Calculate the linear predictor (including intercept)\n    linear_predictors[i] <- cbind(1, x_row) %*% coef\n    # Calculate the predicted probability\n    probs[i] <- 1 / (1 + exp(-linear_predictors[i]))\n  }\n  # Convert probabilities to binary predictions\n  predictions <- ifelse(probs > 0.5, 1, 0)\n  # Calculate performance metrics\n  confusion <- confusionMatrix(factor(predictions), factor(y_test))\n  accuracy <- confusion$overall['Accuracy']\n  precision <- confusion$byClass['Pos Pred Value']\n  recall <- confusion$byClass['Sensitivity']\n  f1_score <- 2 * (precision * recall) / (precision + recall)\n  # Calculate AUC\n  roc_curve <- roc(y_test, probs, quiet = TRUE)\n  auc <- auc(roc_curve)\n  return(list(accuracy = accuracy, precision = precision, recall = recall, \n              f1_score = f1_score, auc = auc))\n}\n```\n:::\n\n\nNow, we employ the function `calculate_test_performance()` with parameter\nestimates from three different regularization methods.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate test performance for each method\nlasso_performance <- calculate_test_performance(lasso_results$coef, X_test, y_test)\nscad_performance <- calculate_test_performance(scad_results$coef, X_test, y_test)\nmcp_performance <- calculate_test_performance(mcp_results$coef, X_test, y_test)\n```\n:::\n\n\nLet's compare the performance of our three models on the test set:\n\n``` {style=\"RstyleR\"}\nTest Performance Metrics:\n--------------------------------------------------------\nLASSO:\nAccuracy:  0.9234694\nPrecision: 0.8973384\nRecall:    0.7744361\nF1 Score:  0.8314607\nAUC:       0.9508162\n\nSCAD:\nAccuracy:  0.9210204\nPrecision: 0.8888889\nRecall:    0.7744361\nF1 Score:  0.8278146\nAUC:       0.9502024\n\nMCP:\nAccuracy:  0.9210204\nPrecision: 0.8888889\nRecall:    0.7744361\nF1 Score:  0.8278146\nAUC:       0.9502024\n```\n\nAll three models perform well, with accuracy above 92% and AUC above\n0.95. LASSO slightly outperforms SCAD and MCP in terms of accuracy and\nAUC, while all three models have the same recall. The high AUC values\nindicate that all models are good at distinguishing between certified\nand non-certified students. Furthermore, one of the key advantages of\nthese regularization methods is their ability to perform variable\nselection. Let's compare which variables were selected by each method:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare selected variables\ncompare_selection <- function(lasso, scad, mcp) {\n  all_vars <- unique(c(names(lasso), names(scad), names(mcp)))\n  selection <- data.frame(Variable = all_vars,\n          LASSO = ifelse(all_vars %in% names(lasso)[lasso != 0], \"Selected\", \"\"),\n          SCAD = ifelse(all_vars %in% names(scad)[scad != 0], \"Selected\", \"\"),\n          MCP = ifelse(all_vars %in% names(mcp)[mcp != 0], \"Selected\", \"\"))\n  return(selection)\n}\n\nvariable_selection <- compare_selection(lasso_results$coef, \n                                        scad_results$coef, \n                                        mcp_results$coef)\n# Print the first 5 rows of the variable_selection data frame\nprint(head(variable_selection, 5))\n```\n:::\n\n\nHere, we output the first 5 rows of the variable selection by using\nthese three regularization methods where empty means unselected:\n \n\nThis comparison reveals which predictors are considered important by\neach method, and the whole results are reported in\n@tbl-2.\n \n\n       Variable                                                  LASSO      SCAD       MCP\n  ---- --------------------------------------------------------- ---------- ---------- ----------\n     1 (Intercept)                                               Selected   Selected   Selected\n     2 course_idc_id2                                            Selected   Selected   \n     3 genderm                                                                         \n     4 final_cc_cname_DIBangladesh                                                     \n     5 final_cc_cname_DIBrazil                                   Selected   Selected   \n     6 final_cc_cname_DICanada                                   Selected              \n     7 final_cc_cname_DIChina                                    Selected              \n     8 final_cc_cname_DIColombia                                 Selected              \n     9 final_cc_cname_DIEgypt                                    Selected              \n    10 final_cc_cname_DIFrance                                   Selected              \n    11 final_cc_cname_DIGermany                                  Selected              \n    12 final_cc_cname_DIGreece                                   Selected              \n    13 final_cc_cname_DIIndia                                    Selected   Selected   Selected\n    14 final_cc_cname_DIIndonesia                                                      \n    15 final_cc_cname_DIJapan                                                          \n    16 final_cc_cname_DIMexico                                                         \n    17 final_cc_cname_DIMorocco                                                        \n    18 final_cc_cname_DINigeria                                  Selected   Selected   Selected\n    19 final_cc_cname_DIOther Africa                                                   \n    20 final_cc_cname_DIOther East Asia                                                \n    21 final_cc_cname_DIOther Europe                             Selected              \n    22 final_cc_cname_DIOther Middle East/Central Asia                                 \n    23 final_cc_cname_DIOther North & Central Amer., Caribbean   Selected              \n    24 final_cc_cname_DIOther Oceania                            Selected              \n    25 final_cc_cname_DIOther South America                                            \n    26 final_cc_cname_DIOther South Asia                                               \n    27 final_cc_cname_DIPakistan                                 Selected   Selected   \n    28 final_cc_cname_DIPhilippines                              Selected              \n    29 final_cc_cname_DIPoland                                                         \n    30 final_cc_cname_DIPortugal                                 Selected   Selected   \n    31 final_cc_cname_DIRussian Federation                       Selected              \n    32 final_cc_cname_DISpain                                    Selected   Selected   Selected\n    33 final_cc_cname_DIUkraine                                  Selected              \n    34 final_cc_cname_DIUnited Kingdom                                                 \n    35 final_cc_cname_DIUnited States                            Selected              \n    36 final_cc_cname_DIUnknown/Other                            Selected              \n    37 nevents                                                   Selected   Selected   Selected\n    38 ndays_act                                                 Selected   Selected   Selected\n    39 nplay_video                                               Selected   Selected   Selected\n    40 explored1                                                 Selected   Selected   \n    41 nchapters                                                 Selected   Selected   Selected\n    42 nforum_posts                                              Selected              \n    43 course_idc_id2:genderm                                                          \n    44 genderm:nevents                                                                 \n    45 genderm:ndays_act                                         Selected              \n    46 genderm:nplay_video                                                             \n    47 genderm:explored1                                                               \n    48 genderm:nchapters                                         Selected              \n    49 genderm:nforum_posts                                                            \n    50 course_idc_id2:nevents                                    Selected   Selected   Selected\n    51 course_idc_id2:ndays_act                                  Selected              \n    52 course_idc_id2:nplay_video                                Selected   Selected   Selected\n    53 course_idc_id2:explored1                                  Selected              Selected\n    54 course_idc_id2:nchapters                                  Selected              \n    55 course_idc_id2:nforum_posts                               Selected              \n\n: Results of variable selection with three different methods {#tbl-2}\n\n \n\nMoreover, we can obtain the number of selected variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnum_vars <- c(sum(lasso_results$coef != 0) - 1,\n              sum(scad_results$coef != 0) - 1,\n              sum(mcp_results$coef != 0) - 1)\ncat(\"\\nNumber of selected variables:\\n\")\ncat(\"LASSO:\", num_vars[1], \"\\n\")\ncat(\"SCAD:\", num_vars[2], \"\\n\")\ncat(\"MCP:\", num_vars[3], \"\\n\")\n```\n:::\n\n\nThis comparison shows how aggressive each method is in terms of variable\nselection. Typically, LASSO tends to select more predictors than SCAD\nand MCP, which are often more aggressive in their variable selection.\n\n \n\nBased on the provided results, the MCP method demonstrates superior\nperformance in variable selection while maintaining competitive\npredictive accuracy. MCP selected only 10 variables, significantly fewer\nthan LASSO (36) and SCAD (14), indicating its effectiveness in achieving\na more parsimonious model. Despite using fewer predictors, MCP matched\nSCAD's performance across all metrics (Accuracy: 0.9210, Precision:\n0.8889, Recall: 0.7744, F1 Score: 0.8278, AUC: 0.9502) and was only\nmarginally behind LASSO in accuracy and AUC. This suggests that MCP\neffectively identified the most influential predictors, resulting in a\nsimpler model with comparable predictive power. The ability to achieve\nsuch performance with fewer variables highlights MCP's strength in\nbalancing model complexity and predictive accuracy, making it the\npreferable choice for this dataset.\n\n::: remark\nIt shall be noted that although the original dataset contained only 10\nvariables, we constructed an additional 45 interaction terms for our\nmodeling, which increased the dimensionality. As shown in Table 2, all\nthree regularization methods effectively shrank redundant predictors to\nzero, with the MCP method being particularly efficient, retaining only\n10 predictors. This demonstrates the effectiveness of regularization for\nvariable selection in educational data modeling. It is also important to\nnote that regularization methods tend to be even more effective when the\nnumber of predictors significantly exceeds the sample size, especially\nin extremely high-dimensional settings.\n:::\n\n### Implications for Online Learning\n\nThe Minimax Concave Penalty (MCP) method identified 10 key predictors\nfor online learning outcomes, offering a concise yet comprehensive\nmodel. These predictors reflected the critical role of student\nengagement, including metrics such as total events, active days, video\nplays, and chapters accessed. The model also highlighted specific\ncountry effects of India, Nigeria, and Spain, suggesting that\ngeographical or cultural backgrounds had an impact on learning outcomes.\nNotably, the interaction terms between course ID and engagement metrics\nwere selected, indicating that the impact of student engagement varies\nacross different courses. This parsimonious selection of variables\nprovides valuable insights into the most significant factors affecting\nonline learning success.\n\nThese findings have important implications for online education\nstakeholders. The results underscore the necessity of promoting active\nparticipation and regular interaction with course content, particularly\nvideo lectures. The model suggests that tailored approaches may be\nbeneficial for students from specific countries, addressing unique\nchallenges or leveraging strengths associated with these geographical\ncontexts. The variation in engagement effects across courses indicates\nthat customized strategies for course design and student support could\nbe more effective than a uniform approach. Interestingly, the absence of\ndemographic variables like gender in the final model implies that\nengagement metrics are more predictive of success than these demographic\nvariables in this context. These insights can guide educators, course\ndesigners, and platform developers to embed desirable elements (e.g.,\ngamification or animation) into video lectures to improve students'\nonline engagement  [@chans2021gamification; @looyestyn2017does], which\nmay, in turn, help students from diverse geographical regions to\ncomplete the course successfully.\n\n## Conclusion\n\nThis chapter has made significant progress in understanding the dynamics\nof student success in online education, particularly within the edX\nplatform. Through our quantitative analysis employing LASSO, SCAD, and\nMCP regularization methods, we have successfully identified key factors\ncontributing to obtaining certification in edX courses. The comparative\nevaluation of these advanced regularization techniques has highlighted\ntheir respective strengths in this context, which researchers can easily\nadapt to construct predictive models for similar contexts in online\neducation.\n\nOur findings have identified the most crucial predictors of student\nsuccess, emphasizing key aspects of student engagement, course design,\nand geographical factors. These insights provide practical guidance for\ncourse design and student support. By applying these evidence-based\nresults, course designers, instructors, and platform developers can\nenhance the online learning experience and potentially improve student\noutcomes and certification rates. This study lays a strong foundation\nfor optimizing digital learning platforms and supporting diverse\nlearners worldwide as online education evolves.\n\nFurthermore, in the field of education, regularization methods can help\nresearchers identify the most influential factors impacting various\ncognitive (e.g., academic achievement, development of generic skills)\nand affective learning outcomes (e.g., learning engagement, course\nsatisfaction, motivation in learning), even when dealing with numerous\npotential predictors. In particular, regularization methods are useful\nin understanding learners' experiences in online/blended learning via\nselecting key factors from the large volume of observational data, which\nrecords detailed and nuanced learner behaviors in a time-stamped manner.\n\n\n\n::: {#refs}\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}