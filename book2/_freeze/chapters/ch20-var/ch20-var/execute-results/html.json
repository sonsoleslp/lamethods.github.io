{
  "hash": "defc1c8e1063dcb1d193d3227056281d",
  "result": {
    "markdown": "---\ntitle: \"Idiographic Networks: A tutorial on Graphical Vector Autoregression and Unified Structural Equation Modeling\"\nwarning: FALSE\nformat: html\nexecute:\n  message: FALSE\n  cache: true\nauthor: \n   - name: \"Mohammed Saqr\"\n   - name: \"Daryn Dever\"\n   - name: \"Sonsoles López-Pernas\"\ncrossref:\n  fig-title: '**Figure**'\n  fig-labels: arabic\n  title-delim: \"**.**\"\nabstract-title: \"Abstract\"\nabstract: \"This tutorial introduces the application of advanced network analysis methods, specifically Graphical Vector Autoregression (`graphicalVAR`) and Unified Structural Equation Modeling (`uSEM`), to model learning processes as complex, dynamic systems. These approaches allow exploring both temporal and contemporaneous relationships among variables within individual learners over time. The chapter begins by conceptualizing learning as a networked system and reviewing relevant literature, discussing the advantages of probabilistic network models in education. Then, a step-by-step tutorial in the R programming language is presented so readers can learn to estimate idiographic models, visualize dynamic relationships, and interpret. As such, this tutorial aims to provide researchers with tools to analyze multivariate time-series data, which is a necessary step for truly personalized interventions in educational research\"\nkeywords: \"learning analytics, network analysis, idiographic analysis, complex systems, graphical vector autoregression, unified structural equation modeling\"\ndpi: 900\nbibliography: references.bib\nextract-media: \"img\"\nfig-align: center\nfig-width: 5\nfig-height: 5\n---\n\n\n## Introduction\n\nNetwork analysis provides a suite of methods for examining both static and dynamic relationships within systems. These methods reveal insights into collaborations between students or student-teacher interactions, knowledge construction on a topic, social dynamics between groups, and system-wide properties, including how these connections evolve over time [@Saqr2022-lr; @Saqr2024-ce; @cela2014]. Applications of network analysis span diverse contexts, from understanding how groups of students learn on massive online platforms [@Fan2022-ue] to identifying how a single learner evolves over time [@saqr2021]. Representing elements of learning processes—such as cognitive and social aspects—within a network has become a well-established research method, encompassing various analytical approaches, including social network analysis (SNA), epistemic network analysis (ENA), ordered network analysis (ONA), and temporal network analysis (TNA) [@elmoazen2022; @Saqr2024; @cela2014]. More recently, transition network analysis (TNA) has also emerged[@Saqr2024].\n\nWhile much of the prior work in network analysis focuses on group-level phenomena, this chapter takes an **idiographic perspective**, where the primary unit of analysis is the individual learner [@LABOOK2_Chapter_19]. The idiographic approach diverges from traditional nomothetic methods by emphasizing the unique patterns, processes, and dynamics that characterize a single person, thereby offering a more granular understanding of learning as a personal and evolving process [@LABOOK2_Chapter_18; @LABOOK2_Chapter_12]. This focus aligns with a growing recognition of the importance of personalized education, where interventions and analyses are tailored to the distinctive needs and experiences of individuals [@LABOOK2_Chapter_18; @LABOOK2_Chapter_12].\n\nThis chapter focuses on a specific type of network: probabilistic networks. These networks represent variables as nodes and the statistical relationships between them (e.g., correlations or regression) as edges. Probabilistic networks, discussed in detail in the previous book [@saqr2024], have been increasingly employed to model processes as they emerge, progress, or unfold, with their elements represented as nodes [@malmberg2022; @malmberg2022a]. These networks allow researchers to understand interactions and dynamics within a process.\n\nHere, we adopt an **idiographic** perspective, treating the student as the central unit of analysis. This approach examines data from a single student, continuing our exploration of individualized processes, as detailed in the chapter on idiographic machine learning [@LABOOK2_Chapter_22]. In this chapter, we demonstrate how to use Graphical Vector Autoregression (GVAR) and Unified Structural Equation Modeling (uSEM) to analyze an individual's process. We begin with a background section that defines key terms and concepts and provides a concise literature review. Following this, we provide a step-by-step tutorial using R to implement the discussed techniques.\n\n## Background\n\n### The cognitive process as a networked system\n\nRepresenting elements of the cognitive and social processes as a network is an established research method. Such representation has afforded researchers a way to visualize the structure of these processes and to measure the magnitude of association between their elements, and to devise statistical indices that allow a precise interpretation of the resulting graphs [@Epskamp2018-rj]. Recent advances in network science have seen a surge in probabilistic network models, also referred to as GGMs, which are designed to examine the relationships of multivariate data or elements of a cogintive or psycholgocial process [@Epskamp2018-rj. GGMs are applied within the literature to map and visualize the dynamic relationships between processes and their elements, considering them as a complex system [@Epskamp2018-rj; @Hevey2018-nm; @Artner2022-ze; @Hamilton2020-zp; @Borsboom2017-om]. GGMs commonly estimate a partial correlation between variables while controlling for the influence of all other variables and their relationships within the system. Within these networks, two nodes are connected if the covariance of the edge cannot be explained by any other variable in the network, eliminating the possibility of confounding variables [@Epskamp2018-rj; @Hevey2018-nm]. Resulting outputs of these models show only significant relationships between nodes, being the weight of the significant edges proportional to the strength of the relationships, as well as whether the relationship is positive or negative, and mediation pathways (i.e., indirect relationships between two variables transmitted through one or more intermediary variables within the same network) [@Epskamp2018-rj].\n\nVector autoregression (VAR)is an extension of GGM methods has enabled the capability for modeling temporal processes, i.e., how a variable predicts another in the next time window. VAR estimates a directed network (in contrast to undirected in GGM); the nodes are variables (e.g., motivation, behavior or attitude) and the link between them are temporal relationships (a variable predicts another in the next time window) [@Epskamp2018-rj].\n\nThe abundance of intensive time-stamped data (e.g., time-series data) has led to the existence of enough observations from individual subjects across time using e.g., data collected with experience sampling methods, observational data and physiological data, which enabled the study of an individual as a unique case (N=1) [@Epskamp2018-rj; @Molenaar2004-np]. The representation and estimation of the temporal network allows researchers to study the individual phenomena, the progression of behavior, predict future behavior, as well as create relevant intervention. In other words, the network allows the *idiographic* assessment of the person dispositions and their temporal dynamics. The network is commonly represented by drawing an arrow from the node that represents the variable (e.g., motivation) to the variable that it predicts in the next time window of measurement (e.g., engagement) [@saqr2024a].\n\nTo explain it, an example is presented in @fig-temp-net. We created a simulated dataset about working and achievement within an individual on a daily basis. The graph shows that motivation predicts work, as well as feeling of achievement of goals within the next day. Similarly, working predicts feeling of goal-achievement. However, engaging with work predicts slight stress the next day, and having stress negatively predicts feeling of achievement. Helping that individual could be done though offering stress management advice. Another type of network is the *contemporaneous network (associative*), a partial correlation network that maps the correlation between the elements of the studied phenomena within the same time window. For instance, when the subject is having comfort, he/she also eats snacks at the same time. In our study this is used to study the co-temporal association between daily events, for instance, how motivation predicts working on the task within the same day.\n\n![A fictional temporal network of four constructs. The circles are variables. Blue lines are positive partial correlations. The thickness of the line is proportional to the magnitude of the correlation. The direction of the arrow points to the direction of the temporal correlation. Extracted from [@Malmberg2022-xm]](temporalnet.png){#fig-temp-net fig-align=\"center\" width=\"50%\"}\n\n## Review of Network Analysis Applications in Education Literature\n\nProbabilistic network models, such as GGMs, have become an essential tool for examining how complex learning processes unfold and how the interconnectedness of psychological constructs contributes to enhanced learning outcomes. For instance, Malmberg et al. [@Malmberg2022-xm] explored how students monitor events and demonstrated that this monitoring facilitates functional regulatory behaviors during collaborative physics tasks. Their findings indicated that cycles of regulation during collaborative learning do not always coexist, revealing inherent limitations in group-level regulation. Moreover, they highlighted the temporal interplay between motivation and task monitoring, showing that motivation persists across different regulatory phases and significantly predicts how tasks are enacted.\n\nSimilarly, Zhou and Kang [@Zhou2023-ri] employed network analysis to investigate collaborative problem-solving behaviors in teams during an astronomy simulation. Using multivariate autoregression, they identified temporal patterns in problem-solving behaviors, revealing how individual exploration within the simulation catalyzed group discussions and joint engagement. Their findings underscored the importance of individual reflections in driving group coordination and the construction of shared knowledge.\n\nCloser to the methods discussed in this tutorial, vector autoregression (VAR) networks have been applied to examine students' self-regulation. Saqr [@saqr2024a] demonstrated that self-regulation profiles differ significantly across individuals, with each student exhibiting a unique regulatory pattern. Importantly, none of the students conformed to the aggregate patterns observed when data were analyzed at the group level. This underscores the critical need for idiographic approaches to capture the distinctive dynamics of each learner, challenging traditional nomothetic methodologies [@saqr2024b].\n\nAdditional research has used network analysis to uncover how students regulate cognitive and emotional processes, both individually and in groups, across diverse learning contexts, such as language acquisition and physics education [@De-Neve2023-nx; @Malmberg2022-il; @Saqr2024-su; @Saqr2021-lj; @Qin2020-mg; @Jovanovic2024; @saqr2021; @saqr2024c]. These studies establish network analysis as a powerful and reliable framework for identifying how learners’ regulatory strategies influence motivation, engagement, and ultimately, learning outcomes.\n\n### Loading data and pre-processing\n\nBefore proceeding to the actual analysis, we will need to load the packages, import the data, and process it to prepare for the analysis. Below are the necessary packages:\n\n-   `tidyverse`: A renowned R package for data manipulation, wrangling and visualization [@tidy].\n\n-   `skimr`: A package to perform summary statistics and help us get an overview of the dataset and in particular the variable distributions, any missing data, and basic descriptive statistics [@skimr].\n\n-   `graphicalVAR`: An R package which will be used for modeling and visualizing the `graphicalVAR` models as described before [@gvar].\n\n-   `qgraph`: An R package for network visualization [@qgr].\n\n-   `pompom`: An R package that offers an implementation of unified structural equation modeling [@pompom].\n\n-   `caret`: Short for Classification And REgression Training, `caret` is a set of functions that attempt to simplify the creation and evaluation of predictive models [@caret].\n\n-   `rio`: A package for importing data in several formats [@rio].\n\nIn addition, we will import an additional R file with helper functions that we will need throughout the chapter (`aux.R`).\n\n\n\n\n\nOnce we have loaded the necessary R packages and dependencies, we import the dataset `synthetic_Data_share.RDS` (step 1). This dataset contains ESM measurements of 36 students that reported how they regulate their own learning activities including aspects like planning, monitoring their progress, seeking help, managing their environment, and reflecting on feedback. We then create a vector for the variables of interest (`Vars`), which includes nine SRL processes that will be the target of our the analysis (step 2).\n\nIn the next step (step 3), we create a `Day` variable for each student (by grouping by the `name` variable) using `seq_along()` to create sequential numbers for each time-point in the data for each individual. Next (step 4), given that we would like to analyze a single student, we will filter the data of a student, in our case, we chose Grace for analysis. In this step we also select only the specific SRL-related variables that we would like to analyze, defined by `Vars`, along with the new **Day** variable, preparing the filtered data for further analysis specific to Grace's behaviors.\n\n\n::: {.cell hash='ch20-var_cache/html/unnamed-chunk-1_5745d1985b866365c54b84821c71330c'}\n\n```{.r .cell-code}\n# Step 1: Load the dataset\ndf <- import(\"https://github.com/lamethods/data2/raw/main/srl/srl.RDS\")\n\n# Step 2: Define variables for the graphical VAR model\nVars <- c(\"planning\", \"monitoring\", \"effort\", \"control\", \n          \"help\", \"social\", \"organizing\", \"feedback\", \"evaluating\")\n\n# Step 3: Create a \"Day\" variable for each entry within a person\ndf <- df |>\n  group_by(name) |>\n  mutate(Day = seq_along(name)) |>\n  ungroup()\n\n# Step 4: Filter the data for a specific individual named 'Grace'\nGrace_Data <- df |>\n  filter(name == \"Grace\") |>\n  dplyr::select(all_of(Vars), Day)\n```\n:::\n\n\n#### Exploring the data\n\nIt is always a good practice to explore the data to judge the distribution of variables, missing data or any unexpected surprises in the data. We do so using the `skimr` package, which provides a detailed overview of the SRL dataset, including measures like mean, median, spread, and the presence of missing values. To do so, we simply use `skim(Grace_Data)`.\n\n\n::: {.cell hash='ch20-var_cache/html/unnamed-chunk-2_07077c0ed1b0fee4cfd1abd833d08a36'}\n\n```{.r .cell-code}\nGrace_descriptives <- skim(Grace_Data)\nprint(Grace_descriptives)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n── Data Summary ────────────────────────\n                           Values    \nName                       Grace_Data\nNumber of rows             156       \nNumber of columns          10        \n_______________________              \nColumn type frequency:               \n  numeric                  10        \n________________________             \nGroup variables            None      \n\n── Variable type: numeric ──────────────────────────────────────────────────────\n   skim_variable n_missing complete_rate mean   sd p0  p25  p50   p75 p100 hist \n 1 planning              0             1 61.7 20.4  0 54.8 64.5  74.2  100 ▂▁▃▇▂\n 2 monitoring            0             1 63.7 17.7  0 50.7 68.7  76.1  100 ▁▁▆▇▃\n 3 effort                0             1 50.6 21.9  0 36   52    68    100 ▃▆▇▇▂\n 4 control               0             1 43.6 26.0  0 22.6 38.7  61.3  100 ▆▇▆▃▅\n 5 help                  0             1 75.8 16.4 25 70   80    87.5  100 ▁▁▃▇▆\n 6 social                0             1 45.4 18.3  0 31   48    56    100 ▂▅▇▃▁\n 7 organizing            0             1 62.7 16.5  0 48.1 66.7  74.1  100 ▁▁▅▇▁\n 8 feedback              0             1 60.8 24.4  0 42.1 62.9  80.7  100 ▁▆▆▇▇\n 9 evaluating            0             1 46.5 18.6  0 39.1 45.7  52.7  100 ▁▂▇▁▁\n10 Day                   0             1 78.5 45.2  1 39.8 78.5 117.   156 ▇▇▇▇▇\n```\n:::\n:::\n\n\n\n\nThe next chunk of code visualizes the data and, since it is a longitudinal repeated measure dataset, we plot the data across time. For that purpose, we reshape it into a long format using `pivot_longer` to organize it so that the SRL variables (e.g., planning, monitoring) are represented in a single column (`variable`), and their corresponding values in another column (`value`). We then use `ggplot2`, where each SRL variable is plotted on the y-axis against time (`Day`) is plotted on the x-axis. We apply faceting so that each SRL construct is visualized in its own subplot (@fig-loess).\n\n\n::: {.cell hash='ch20-var_cache/html/fig-loess_5bf03a56814b7595613ea08251c5dbd2'}\n\n```{.r .cell-code}\n# Reshape the data to long format\nGrace_data_long <- Grace_Data |>\n  pivot_longer(cols = Vars, # Assuming 'id' and 'interaction' are the relevant columns\n               names_to = \"variable\", \n               values_to = \"value\")\n\n# Plot with faceting by variable\nggplot(Grace_data_long, aes(x = Day, y = value)) +\n  geom_line(color = \"blue\") +\n  geom_point() +\n  geom_smooth(method = \"loess\", color = \"red\", se = FALSE) + # Smoothing (LOESS)\n  facet_wrap(~variable, scales = \"free_y\", ncol = 3) +\n  theme_minimal() \n```\n\n::: {.cell-output-display}\n![Time Series of Variables with Smoothing](ch20-var_files/figure-html/fig-loess-1.png){#fig-loess width=864}\n:::\n:::\n\n\n#### Stationarity\n\nStationarity is an important pre-requisite for `graphicalVAR` models which means that the data statistical properties, ---e.g., mean and variance--- do not change over time. Non-stationary data, which exhibits trends or other systematic temporal patterns may lead to spurious results and misleading interpretations. Therefore, we need to examine the presence of trends, and if present, these trends need to be removed (de-trended) with appropriate techniques. De-trending improves the model's performance and the validity of its inferences and ensures that the relationships among variables are not artificially inflated or deflated by overarching trends. This is particularly important in `graphicalVAR` models where we focus is on understanding the network of dynamic relationships among variables. The next function `detrender` (defined in the file `aux.R` imported earlier) examines the presence of trends, and prints the output. If there are trends in the data, the functions performs detrending. As we can see the data has no trends and so we will proceed with the data without further modifications.\n\n\n::: {.cell hash='ch20-var_cache/html/unnamed-chunk-4_1272fdf4eb297cdd6997924d4b1759f3'}\n\n```{.r .cell-code}\ndetrender(Grace_Data, vars = Vars, timevar = \"Day\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNo significant trend for planning - p-value: 0.30521859863304\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nNo significant trend for monitoring - p-value: 0.0805897468195874\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nNo significant trend for effort - p-value: 0.429465303760845\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nNo significant trend for control - p-value: 0.972736537512728\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nNo significant trend for help - p-value: 0.452982605134614\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nNo significant trend for social - p-value: 0.925685121385736\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nNo significant trend for organizing - p-value: 0.460175358168101\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nNo significant trend for feedback - p-value: 0.456419063913471\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nNo significant trend for evaluating - p-value: 0.993005264585157\n```\n:::\n:::\n\n\n### Estimation of the `graphicalVAR` Model\n\nFitting the `graphicalVAR` model is straightforward, for that we use the `graphicalVAR()` function with the data argument set to the dataset of Grace `data = Grace_Data`. The `beepvar` argument specifies the time variable in the data (the temporal sequence of our observation). In our case, we use `beepvar = \"Day\"`, which refers to the day-to-day sequential measurements of Grace’s behaviors. We set the `lambda_beta` argument to 0.1 to control the regularization. A value of 0.1 is rather moderate given that we have a relatively large dataset with 156 measurements. Regularization as we mentioned before would shrink small edges, help avoid over-fitting and makes the model more interpretable. Other optional arguments, like `verbose`, can be set to `TRUE` if you want to display more detailed output during the fitting process, but it defaults to `FALSE` to suppress extra information.\n\nThe output of the `summary()` function of the model of Grace provides some useful information about the data and generated networks. As the results show, the model include 9 nodes corresponding to the SRL variables (e.g., planning, monitoring, effort, etc.). The Extended Bayesian Information Criterion (EBIC) hyperparameter was at the default value of 0.5 to control the model complexity and help avoid over-fitting. The optimal EBIC score achieved was 1329.994, indicating the best trade-off between fit and complexity. The Partial Contemporaneous Correlations (PCC) network had 11 non-zero correlations which means the network was relatively sparse where about 69% of the possible edges being zero as indicated by the PCC sparsity of 0.694. The algorithm tested 50 different values for the regularization parameter to find the optimal level. The Partial Directed Correlations (PDC) or the temporal network had 9 non-zero directed edges meaning that the network was rather sparse where PDC sparsity was 0.75, meaning 75% of potential temporal connections between variables were zero. The summary tells us that both the contemporaneous and temporal networks are stored within the object inside `$PCC` and `$PDC`, objects respectively.\n\n\n::: {.cell hash='ch20-var_cache/html/unnamed-chunk-5_edd641622579e2c7cd803f44542a1302'}\n\n```{.r .cell-code}\nset.seed(265) # for replicability\n\n# Fit the `graphicalVAR` model to the data from 'Grace' with 'lambda_beta' \n# to 0.1 to apply moderate regularization \nGraphVAR_Grace <- graphicalVAR(Grace_Data, beepvar = \"Day\", lambda_beta = 0.1)\n\n# Output a summary of the fitted model to review the model's structure, \n# including estimated parameters and fit indices.\nsummary(GraphVAR_Grace)\n```\n:::\n\n::: {.cell hash='ch20-var_cache/html/unnamed-chunk-6_1999d69b5f8cee616715bf66ee71600f'}\n::: {.cell-output .cell-output-stdout}\n```\n=== graphicalVAR results ===\nNumber of nodes: 9 \nNumber of tuning parameters tested: 50 \nEBIC hyperparameter: 0.5 \nOptimal EBIC: 1329.994 \n\nNumber of non-zero Partial Contemporaneous Correlations (PCC): 11 \nPCC Sparsity: 0.6944444 \nNumber of PCC tuning parameters tested: 50 \nPCC network stored in object$PCC \n\nNumber of non-zero Partial Directed Correlations (PDC): 9 \nPDC Sparsity: 0.75 \nNumber of PDC tuning parameters tested: 1 \nPDC network stored in object$PDC \n\nUse plot(object) to plot the estimated networks.\n```\n:::\n:::\n\n\n### Visualization and interpretation of the `graphicalVAR` Model\n\nTo visualize the results of Grace, we can use the built-in `plot` function with the required network argument. In that, we use the argument `include = \"PCC\"` , or \"`PCC\"`, to get the Partial Contemporaneous Correlations network or simply the contemporaneous network. We also use the argument `include=\"PDC\"` argument to retrieve the Partial Directed Correlations network, or simply, the temporal network.\n\nIn **graphicalVAR** contemporaneous plots, and, in fact, in most of such networks, the color of the edges reflects the direction of the correlation where blue (sometimes green) indicates a positive correlation between the two nodes and red indicates a negative correlation. The thickness of the edge indicates the strength of the relationship, with thicker edges indicating stronger associations. In this PCC network, in @fig-pcc-pdc-1, we see a strong positive correlation between monitoring and help suggesting that when Grace is actively monitoring her learning, she tends to seek more help. We also see a negative correlation between social and help indicating that when Grace engages in a social activity, she tends do less monitoring of her learning. There is also a moderate association between monitoring and effort and feedback and help. These are all indicative of how Grace regulates her learning. Furthermore, in partial correlation networks, the absence of associations is also interpretable, in a way that these processes are independent of each other, e.g., we see no association between effort and control indicating that when Grace is investing good effort in her learning, she may not control her environment and distractions.\n\nThe visualization of the temporal network or the Partial Directed Correlations (PDC) network in technical terms show the temporal (lagged) relationships between self-regulated learning (SRL) behaviors. The arrows between the nodes represent how one behavior at time t predicts another at time t+1. The color and thickness of the arrows provide information on the direction and strength of these lagged relationships in the same way as the contemporaneous network. As we can see in the figure, there is a positive relationship between planning and effort, meaning that when Grace engages in planning at one day, she is more likely to put in more effort in the next day. In the same way, we also see a strong link between organizing and evaluating indicating that time management (organizing) leads to evaluation of progress in the next day. Interestingly, we see that social is followed usually by time management (organizing) which makes sense, that when Grace socializes, she tends to catch up and organizes her duties next day. Negative links in temporal networks mean that behaviors are less likely to happen after each other. We see for instance, that evaluating is less likely to be followed by seeking help and monitoring is less likely to be followed by evaluating.\n\nOverall, the network is informative and shows show that Grace regulates her learning. She puts her plans into effect by investing effort as well as her time management skills are a driver for evaluation and implementing feedback.\n\n\n::: {#fig-pcc-pdc .cell layout=\"[1,1]\" hash='ch20-var_cache/html/fig-pcc-pdc_f3418186f704abea174bd2d60eb1fd91'}\n\n```{.r .cell-code}\n# Visualize the Partial Contemporaneous Correlations (PCC) within the model \n# using a circular layout.\nplot(GraphVAR_Grace, include = \"PCC\", layout = \"circle\", \n     theme = \"colorblind\", labels = Vars, titles = F)\n\n# Similarly, visualize the Partial Directed Correlations (PDC), highlighting \n# temporal directed influences between variables.\nplot(GraphVAR_Grace, include = \"PDC\", layout = \"circle\", \n     theme = \"colorblind\", labels = Vars, titles = F)\n```\n\n::: {.cell-output-display}\n![Partial Contemporaneous Correlations (PCC)](ch20-var_files/figure-html/fig-pcc-pdc-1.png){#fig-pcc-pdc-1 width=768}\n:::\n\n::: {.cell-output-display}\n![Partial Directed Correlations (PDC)](ch20-var_files/figure-html/fig-pcc-pdc-2.png){#fig-pcc-pdc-2 width=768}\n:::\n\nGraphical VAR plots\n:::\n\n\nGiven that the built-in plotting function is very basic, we can augment the plot with the `plotting` function as below. The `plotting` function creates a nicer visualization and shows the edge weights to allow judging the actual edge weights.\n\n\n::: {#fig-pcc-pdc-nice .cell layout=\"[1,1]\" hash='ch20-var_cache/html/fig-pcc-pdc-nice_0bf0a557d6be062133d6cbf64e0bd785'}\n\n```{.r .cell-code}\n plotting <- function(Network, title = \"\", ...){\n      qgraph(Network,\n       layout = \"circle\",          # Circular node arrangement\n       title = title,              # Setting a title\n       loop = 0.7,                 # Curvature for self-loops\n       node.width = 2,             # Node width\n       repulsion = 0.8,            # Spacing between nodes\n       label.font = 1,             # Label font styling\n       label.fill.vertical = 1,    \n       label.fill.horizontal = 1,  \n       esize = 7,                  # Edge thickness\n       vsize = 6,                  # Node size\n       color = \"#ffeece\",          # Light pink node color\n       edge.label.cex = 1.5,       # Edge label size\n       edge.width = 1,             # Edge width\n       border.width = 2,           # Border width for nodes\n       edge.labels = TRUE,         # Show edge labels\n       asize = 3,                  # Arrow size\n       labels = Vars,              # Variable labels,\n       negDashed = TRUE,           # Negative edges dashed\n       theme = \"colorblind\", ...   )}\n\nplotting(GraphVAR_Grace$PCC)\nplotting(GraphVAR_Grace$PDC)\n```\n\n::: {.cell-output-display}\n![Partial Contemporaneous Correlations (PCC)](ch20-var_files/figure-html/fig-pcc-pdc-nice-1.png){#fig-pcc-pdc-nice-1 width=768}\n:::\n\n::: {.cell-output-display}\n![Partial Directed Correlations (PDC)](ch20-var_files/figure-html/fig-pcc-pdc-nice-2.png){#fig-pcc-pdc-nice-2 width=768}\n:::\n\nPlotting the networks nicely\n:::\n\n\n### Estimation of multiple idiographic models n\\>1\n\nBesides estimating idiographic models, we can use `mlGraphicalVAR` to estimate multiple idiographic models for multiple people. Besides idiographic models, `mlGraphicalVAR` offers an aggregate or an average within-person and between person networks which are kind of approximation of the general group-level picture. As such, the results of `mlGraphicalVAR` contain several components: a between-person network (`betweenNet`), which captures the average stationary relationships between variables across different subjects. The `betweenNet` network is created from the mean values of each person in the data and then creates a partial correlation between the means of each person. A network of correlations between the means represents the stable average dynamics. *Please note that we did not have this `betweenNet` before, given that we had only one student.* Also, `mlGraphicalVAR` produces a fixed-effect partial directed network, `fixedPDC`, which captures the temporal relationships on average across the whole sample. Similarly, it produces a fixed effects partial contemporaneous network `fixedPCC` which captures the average contemporaneous correlations across all individuals. Think of these networks as a summary of all the networks in the data across the whole sample.\n\nMost importantly, the function creates an analysis for each individual (like the one we did for Grace) in the data by setting the `subjectNetworks` argument to `TRUE`. In other words, it produces a specific analysis for Bob, Chen, Eve, etc and every other person in the data. To get a specific subject results, we can query the object for that person. For instance, to get get the summary of the model of the first person we can use `GraphVAR_all$subjecResults[[1]]`, to get the contemporaneous network `GraphVAR_all$subjectPCC[[i]]` and to get the `GraphVAR_all$subjectPDC[[i]]`. In that way, we can automate the analysis of a large sample and get an a view of the average picture as well as detailed individual analysis of each person.\n\nTo estimate the `mlGraphicalVAR` model for multiple people, we use the `mlGraphicalVAR` function and specify the data `df` containing the multiple individuals' responses. Here, we need to specify the `idvar = \"name\"` and `subjectNetworks = TRUE` to create the individual analysis.\n\nAs we said before, the output of the `mlGraphicalVAR` model includes several components: The `fixedPCC`, `fixedPDC`, the `betweenNet` as well as the `subjectPCC` (the subject contemporaneous network) and `subjectPDC` (the subject temporal network) for each of the 36 subjects in the dataset.\n\nThe code below applies the de-trending function to assess any trends in the data for each person and in case they are present, the functions de-trends these variables. Then, `mlGraphicalVAR` is used to estimate the models as explained above, one for each person.\n\n\n\n\n\n#### Plotting the `mlGraphicalVAR` results\n\n`mlGraphicalVAR` produces two types of networks: the general group-level networks, and subject specific networks. Below, we plot the group level networks with the function we created (`plotting`). The interpretation of these networks shows the SRL dynamics on average across our sample or the expected common behavior.\n\nFor instance, the between-person network (@fig-mlvar-1) shows that students who use feedback always seek help (0.49). Similarly, we see a strong association between effort and organizing (0.33) as well as control and organizing (0.26) and feedback and evaluating (0.26). There is moderate association between organizing and planning (0.22), and effort and social (0.22), as well as help and effort (0.21), and social and planning (0.21). The rest of associations can be interpreted in the same ways.\n\nIn the contemporaneous network (@fig-mlvar-2), we see a snapshot of how self-regulated learning constructs are associated with each other within the same day. We see a strong relationship between planning and effort (0.24), indicating that students who actively plan their work tend to regulate their effort in the same time. Another moderate association exists between effort and control (0.16) suggesting that students who manage their effort tend to exert control over their environment in the same time. We can also see other associations, like feedback and evaluating (0.17), feedback and organizing (0.13), and organizing and control (0.15). These dynamics are relatively different from the between-person given the different time-scale.\n\nIn the temporal network (@fig-mlvar-3), the arrows represent the direction of influence between self-regulated learning (SRL) constructs over time i.e., from a day to the next. The strongest positive influence is between evaluating and control (0.02), suggesting that social interactions is followed by effort next day which could be a boost or simply that student catch up next day of socializing. There is a feedback look between planning and social (each 0.01), indicating socially-shared regulation. There is a negative influence of organizing on monitoring (-0.02), where these two activities occur in that order.\n\n\n::: {#fig-mlvar .cell layout=\"[1,1,1]\" hash='ch20-var_cache/html/fig-mlvar_a0f3a76c677af8b03e58c26194bc5cc0'}\n\n```{.r .cell-code}\nplotting(GraphVAR_all$betweenNet, minimum = 0.05)\nplotting(GraphVAR_all$fixedPCC, minimum = 0.05)\nplotting(GraphVAR_all$fixedPDC, minimum = 0.01)\n```\n\n::: {.cell-output-display}\n![Between person](ch20-var_files/figure-html/fig-mlvar-1.png){#fig-mlvar-1 width=768}\n:::\n\n::: {.cell-output-display}\n![Contemporaneous network](ch20-var_files/figure-html/fig-mlvar-2.png){#fig-mlvar-2 width=768}\n:::\n\n::: {.cell-output-display}\n![Temporal network](ch20-var_files/figure-html/fig-mlvar-3.png){#fig-mlvar-3 width=768}\n:::\n\nPlotting the `mlGraphicalVAR` results\n:::\n\n\n#### Plotting and interpreting subject level networks\n\nTo plot each individual student network, we can use `GraphVAR_all$subjectPCC[[i]]` and `GraphVAR_all$subjectPCC[[i]]` to get the contemporaneous and temporal networks respectively, we can also get their names by using `GraphVAR_all$ids[[i]]` and use the plotting function to plot them. In the code below, we choose randomly five students numbered from 20 to 24 and plot their networks (@fig-loop). Of course, you can plot the whole dataset by setting the loop to iterate from 1:36 (the number of unique students in the data). The interpretation is similar, but in this case, Judy's network represents only Judy and cannot be generalized to anyone else. So is the case for Karin, Lars, Layla, and Li. Please note also the vast differences between these students. Each and every network is rather different.\n\n\n::: {.cell hash='ch20-var_cache/html/unnamed-chunk-11_fdb5afb92557a5fc966aeaee2e7cd94e'}\n\n```{.r .cell-code}\nfor (i in 20:24) {\n  # Apply plotting to subjectPCC and subjectPDC\n  plotting(GraphVAR_all$subjectPCC[[i]], minimum = 0.05, \n           title = paste(\"Contemporaneous network for \", GraphVAR_all$ids[[i]]))\n  plotting(GraphVAR_all$subjectPDC[[i]], minimum = 0.05, \n           title = paste(\"Temporal network for \", GraphVAR_all$ids[[i]]))\n}\n```\n:::\n\n::: {.cell hash='ch20-var_cache/html/fig-loop_5d7c6676b6e68f14e99d8bac0f53f9f4'}\n::: {.cell-output-display}\n![Plotting the PCC and PDC individual student networks](ch20-var_files/figure-html/fig-loop-1.png){#fig-loop width=9000}\n:::\n:::\n\n\n### Unified Structural Equation Modeling (uSEM)\n\nUnified Structural Equation Modeling (uSEM) is another method that has the capability to offer an idiographic analysis of the individual's behavior as a complex dynamic system [@kim2006; @beltz2013; @gates2011; @gates2012]. As the name implies, uSEM is built around structure equation modelling (described in [@jongerling2024]). The term \"unified\" means that the estimation method of uSEM combines both temporal (lagged relationships or t-1) and contemporaneous relationships (same time or t) simultaneously (compared to sequential estimation of the temporal and contemporaneous networks in `GraphivalVar`).\n\nWhile uSEM share some similarities with `graphivalVar`, they have differences. In uSEM, the temporal relationships are estimated through a multivariate autoregressive model (MAR) and the contemporaneous relations are estimated through conventional SEM. Another difference is that uSEM estimation processes result in a single matrix that contains both the temporal and the contemporaneous relations (not two separate matrices or networks like Graphical VAR). Last, whereas graphical VAR uses regularization to prune the model, uSEM uses an iterative process where it begins with an empty model and applies Lagrange multiplier tests to assess which parameter, if added, would best improve the model’s fit. In doing so, the model is refined in a step wise process, with pathways added one at a time if they enhance overall fit. While some evidence suggests that Graphical VAR may offer better test-retest consistency in short time series, recent improvements in uSEM, such as integrating regularization may help solve these issues [@beltz2017; @beck2021a].\n\n#### uSEM analysis\n\nFor uSEM analysis we will use the `pompom` R package which is designed for person-specific (idiographic) modeling analysis of multivariate time series data. In particular, `pompom` takes a hybrid approach that combines intraindividual variability with network analysis to model the individuals as complex dynamic systems. Also, `pompom` offers impulse response analysis metrics (iRAM) which helps quantify how variables interact dynamically, such as how one node's perturbation affects others over time (this will not be discussed here given it is beyond the scope of our chapter). In particular, `pompom` R package uSEM estimates a person-specific model, meaning each individual's data is analyzed separately [@pompom]. If we want to analyze multiple individuals, we need to do so individually, possibly using a loop (see the section below).\n\nuSEM requires the data to be standardized so that it captures within-person fluctuations around each person’s own mean. When the data is standardized, the data can be treated as stationary, which allows us to focus on the dynamic, short-term changes in SRL without the influence of between-person differences or long-term trends. The code below standardize the data for Grace by subtracting the mean and dividing by standard deviation.\n\n\n::: {.cell hash='ch20-var_cache/html/unnamed-chunk-13_b2f9d207af61fa11cd6dba1250b5e598'}\n\n```{.r .cell-code}\nGrace_data_centered <- Grace_Data |>\n  mutate(across(everything(), ~ scale(.x, center = TRUE, scale = TRUE))) |> \n  select(-Day)\n```\n:::\n\n\nWe then proceed to estimating the uSEM model for Grace using the same data we used before, the nine SRL variables after standardization. The estimation function`uSEM()` takes four parameters: **`data = Grace_data_centered`**: The dataset which we have standardized to remove scale differences and convert the data to stationary and the **`var.number = 9`** which tells `uSEM` the number of variables. The **`lag.order = 1`** specifies a time lag of 1, meaning we are analyzing how variables affect each other from one time point to the next.The argument **`verbose = FALSE`** tells the model to run quietly. Finally, **`trim = TRUE`** tells the model to trim insignificant relationships between variables to simplify the output show only the important interactions.\n\n\n::: {.cell hash='ch20-var_cache/html/unnamed-chunk-14_5ebd3806ba82ea7a75020b4ec3f13f76'}\n\n```{.r .cell-code}\nUsim_Grace <- uSEM(var.number = 9, \n               data = Grace_data_centered, \n               lag.order = 1, \n               verbose = FALSE,\n               trim = TRUE)\n```\n:::\n\n\nFor the uSEM model to be reliable, we need to evaluate fitness criteria. uSEM computes four criteria: Comparative Fit Index (CFI), Tucker-Lewis Index (TLI), Root Mean Square Error of Approximation (RMSEA), and Standardized Root Mean Square Residual (SRMR). Ideally, the model should pass at least three of the evaluation criteria. The **CFI** and **TLI** assess how well the model fits the data compared to a baseline model where values closer to 1 indicates a better fit. The **RMSEA** estimates the error of approximation in the model where lower values (ideally below 0.08) indicates a good fit. The **SRMR** assesses the difference between observed and predicted correlations, with values below 0.08 being ideal.\n\nIn the code below, we use the `model_summary` function to evaluate the fitness of the model. We need to provide the model, tell the function that our data has nine variables, and set the time-lag order 1. The function store the evaluation criteria in `Usim_Grace_fit.` We can now retrieve the values of TLI, RMSEA, and SRMR from the model fit object `Usim_Grace_fit` and calculate the CFI by summing its values.\n\nThe code below retrieves the four criteria, and programmatically evaluates them. We see that the CFI value of 1 indicates a perfect fit, so it is evaluated as \"Passed\". Similarly, a TLI value higher than 0.95, an RMSEA of 0 (indicating no error in approximation), and an SRMR below 0.08 also indicate a good fit. Based on these criteria, our model passes four evaluation criteria and is therefore reliable.\n\n\n::: {.cell hash='ch20-var_cache/html/unnamed-chunk-15_cb1be7188e85349aba939d3bbc344a77'}\n\n```{.r .cell-code}\nUsim_Grace_fit <- model_summary(model.fit = Usim_Grace,\n                          var.number = 9, \n                          lag.order = 1)\n\ncfi_val <- sum(Usim_Grace_fit$cfi)\ntli_val <- Usim_Grace_fit$tli\nrmsea_val <- Usim_Grace_fit$rmsea\nsrmr_val <- Usim_Grace_fit$srmr\n\n# Print the values of each fit index\ncat(\"CFI:\", cfi_val, \"->\", ifelse(cfi_val > 0.95, \"Passed\", \"Failed\"), \"\\n\")\ncat(\"TLI:\", tli_val, \"->\", ifelse(tli_val > 0.95, \"Passed\", \"Failed\"), \"\\n\")\ncat(\"RMSEA:\", rmsea_val, \"->\", ifelse(rmsea_val < 0.08, \"Passed\", \"Failed\"), \"\\n\")\ncat(\"SRMR:\", srmr_val, \"->\", ifelse(srmr_val < 0.08, \"Passed\", \"Failed\"), \"\\n\")\n```\n:::\n\n::: {.cell hash='ch20-var_cache/html/unnamed-chunk-16_c6fbc9c773f9b6b423a60c68903a5233'}\n::: {.cell-output .cell-output-stdout}\n```\nCFI: 1 -> Passed \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTLI: 1.001069 -> Passed \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nRMSEA: 0 -> Passed \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nSRMR: 0.04020178 -> Passed \n```\n:::\n:::\n\n\nThe uSEM networks are a bit different because we get both temporal and contemporaneous relationships in the same plot. Here, the red edges represent negative relationships, the green edges represent positive relationships. Dashed edges indicate lag-1 temporal relationships. Solid edges indicate contemporaneous relationships. Edge width is proportional to the the strength of the relationship where thicker edges indicate stronger positive or negative relationships. As we can see in the plot, Grace has a strong temporal relationship (dashed green line) between planning and effort, suggesting that when planning increases her effort follows. We also see a negative contemporaneous relationship between monitoring and social. Compare this to the previous Graphical Var models. The default plotting function is restrictive and does not show the labels (@fig-png-1), so, we have created a slightly modified function `usemplot` to improve the plot and show the labels (@fig-png-2).\n\n\n::: {#fig-png .cell layout=\"[1,1]\" hash='ch20-var_cache/html/fig-png_56a4bfc0e4eca59318b09b2f2642633c'}\n\n```{.r .cell-code}\nplot_network_graph (Usim_Grace_fit$beta, var.number = 9)\n\nusemplot(Usim_Grace_fit$beta, var.number = 9, \n          labels = colnames(Grace_data_centered))\n```\n\n::: {.cell-output-display}\n![Defalt plotting function](ch20-var_files/figure-html/fig-png-1.png){#fig-png-1 width=480}\n:::\n\n::: {.cell-output-display}\n![Custom plotting function](ch20-var_files/figure-html/fig-png-2.png){#fig-png-2 width=480}\n:::\n\nPlotting the uSEM networks\n:::\n\n\n#### Analysis of multiple individuals with uSEM\n\nGiven that `pompom` does not have a function that automates the analysis across multiple individuals, we need to iterate through each individual in our dataset and repeat the analysis. However, before doing so, we need to check the data if it has any variables that are completely missing or have no variance e.g., has the same value. We do so by the function `nearZeroVar` from the `caret` package.\n\n\n::: {.cell hash='ch20-var_cache/html/unnamed-chunk-18_0e6391058dce1681af4b722aa981a85a'}\n\n```{.r .cell-code}\ndf |> \n  group_by(name) |> \n  reframe(across(all_of(Vars), ~ nearZeroVar(as.data.frame(.))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 0 × 10\n# ℹ 10 variables: name <chr>, planning <int>, monitoring <int>, effort <int>,\n#   control <int>, help <int>, social <int>, organizing <int>, feedback <int>,\n#   evaluating <int>\n```\n:::\n:::\n\n\nSince none of the data entries present any problems, we can proceed with the analysis. The next code simply iterates through each individual in our dataset (`df`), applies a uSEM, and fit the data. First, we filter the dataset to isolate data for each individual, then define variables of interest i.e., `Vars`, and centers/scales the data. A uSEM model is run with the nine variables, and the model’s fit is the estimated.\n\n\n::: {.cell hash='ch20-var_cache/html/unnamed-chunk-19_1c904c5d1c0f5bc6b796568f643de71f'}\n\n```{.r .cell-code}\n# Initialize a list to store the results\nresults_list <- list()\n\n# Define a vector of individual names (replace with actual names in your data)\nindividual_names <- unique(df$name)\n\n# Loop through each individual dataset\nfor (individual in individual_names) {\n  \n  # Filter the dataset for the current individual\n  individual_data <- df |> \n    filter(name == individual) |>\n    select(all_of(Vars))\n  \n  # Center and scale the data\n  centered_data <- individual_data |>\n    mutate(across(everything(), ~ scale(.x, center = TRUE, scale = TRUE)))\n  \n  # Perform uSEM operations\n  uSEM_result <- uSEM(var.number = 9, \n                      data = centered_data, \n                      lag.order = 1, \n                      verbose = FALSE,\n                      trim = TRUE)\n  \n  # Get the model summary\n  model_fit <- model_summary(model.fit = uSEM_result, \n                             var.number = 9, \n                             lag.order = 1)\n  \n  \n  # Store the results in the list\n  results_list[[individual]] <- list(\n    uSEM_result = uSEM_result,\n    model_fit = model_fit\n  )\n}\n```\n:::\n\n\nWe then plot each model by iterating through the result list. We show five as an example in @fig-png-usem.\n\n\n::: {.cell hash='ch20-var_cache/html/unnamed-chunk-20_e9493a0dea9c23af3e0375d13f47c6a9'}\n\n```{.r .cell-code}\nfor (i in 20:24) {\n  usemplot(results_list[[i]]$model_fit$beta, var.number = 9, labels = Vars,\n           title = individual_names[i])\n}\n```\n:::\n\n::: {.cell hash='ch20-var_cache/html/fig-png-usem_67af3a244f2349bbc5a2aeb436e8dacf'}\n::: {.cell-output-display}\n![Plotting multiple uSEM networks](ch20-var_files/figure-html/fig-png-usem-1.png){#fig-png-usem width=960}\n:::\n:::\n\n\n## Conclusion\n\nStudying individual processes and the complexity of behavior is essential for understanding human dynamics, particularly in educational and psychological contexts. Behavior is inherently multifaceted, driven by the interplay of cognitive, emotional, and social processes that unfold uniquely for each person. Idiographic approaches, which focus on individual patterns rather than group averages, are crucial for capturing these complexities. They allow researchers to uncover how specific factors influence behavior over time and reveal the dynamic interactions between processes like planning, monitoring, and evaluating in self-regulated learning. This perspective respects the uniqueness of individual trajectories, providing actionable insights for personalized interventions that align with each person’s needs and context. By embracing the complexity of behavior, idiographic methods offer a deeper understanding of human experiences and support more effective, tailored strategies for fostering growth and learning [@LABOOK2_Chapter_12].\n\nThis chapter introduced the use of probabilistic networks, specifically graphical Vector Autoregression (graphicalVAR) and Unified Structural Equation Modeling (uSEM), to study individual learning processes. The idiographic approach provides a personalized, dynamic view of how cognitive, social, and motivational factors interrelate over time and provides insights at N=1. This approach highlights the unique profiles of students.This approach also supports more accurate personalized learning interventions. Given that the modeling captures the unique dynamics of each student, we can tailor strategies that address their specific needs. Identifying key factors that influence performance allows for targeted interventions to improve learning outcomes.\n\nFurthermore, the integration of both temporal and contemporaneous networks enhances our ability to model the real-time interactions between variables. Temporal networks, which map how one variable predicts another in subsequent time windows, offer insight into the cause-and-effect relationships that govern student behavior over time. Contemporaneous networks, on the other hand, allow us to examine the relationships between variables within the same time window, offering a snapshot of the dynamic interdependencies at a given moment. These two complementary approaches give us a fuller picture of the cognitive, emotional, and behavioral processes that underpin learning. For instance, in our example dataset, motivation was shown to predict work behavior and achievement, while stress, in turn, negatively impacted achievement, underscoring the importance of understanding these processes as interconnected rather than isolated.\n\n\n\n::: {#refs}\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}