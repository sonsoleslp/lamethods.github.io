{
  "hash": "0300fc920225855481981d5dd8a40268",
  "result": {
    "markdown": "---\ntitle: \"Explainable Artificial Intelligence in Education: A Tutorial for Identifying the Variables that Matter\"\nformat: html\nexecute:\n  message: FALSE\n  cache: true\nauthor: \n   - name: \"Mohammed Saqr\"\n   - name: \"Sonsoles López-Pernas\"\ncrossref:\n  fig-title: '**Figure**'\n  fig-labels: arabic\n  title-delim: \"**.**\"\nabstract-title: \"Abstract\"\nabstract: \"Despite the potential of integrating machine learning (ML) and artificial intelligence capabilities into educational settings, there are several challenges that hamper their widespread use and adoption. Among these challenges is that these technologies often function as an \\\"opaque-box\\\" models. This lack of transparency can undermine trust, fairness, and accountability. To address this, explainability methods are essential for understanding how models, e.g., predict at-risk students, grade essays, or identify plagiarism. This chapter demonstrates the use of several techniques to explain ML models in educational contexts through a tutorial covering both regression (predicting student grades) and classification (identifying high versus low achievers). We describe how variable-importance measures, partial dependence plots, and accumulated local effects may help educators interpret the outcomes of predictive models, increasing transparency and trust.\"\nkeywords: \"learning analytics, explainable artificial intelligence, machine learning, DALEX\"\ndpi: 900\nbibliography: ../references.bib\nextract-media: \"img\"\n# mainfont: \"DejaVu Sans\"\n---\n\n\n## Introduction\n\nMachine learning (ML) models and artificial intelligence (AI) applications are increasingly improving their performance and efficiency, often at the expense of becoming more complex [@LABOOK2_Chapter_2]. A common drawback of complex models is that they often operate as \"opaque boxes,\" making it difficult to understand the grounds behind their decisions or predictions[@LABOOK2_Chapter_2; @Saqr2024-ie]. This lack of transparency can be problematic, especially in critical applications where trust, fairness, and accountability are indispensable. In the context of education, we must understand how models predict student performance, recommend learning resources, or profile students according to their behavior [@sghir2023recent; @na2017systematic; @Saqr2024-ie]. Otherwise, without model transparency, there is a risk of reinforcing biases, misinterpreting student actions and abilities, or making unfair decisions about placements and interventions [@LABOOK2_Chapter_2]. Explainability ensures that researchers, educators and administrators can trust AI outcomes and make informed decisions [@Saqr2024-ie; @saqr2024].\n\nVariable-importance measures hold significant value in ML explainability. They can transform models from opaque to white-box ones where the inner-workings are understandable, i.e., we can understand how variables explain the model results [@Khosravi2022]. In our case that entails knowing which variables are more associated with students' performance; in other words, what were the learning activities that students did so that they may score higher grades. Further, if we identify the variables that have little or no influence on model predictions, we can simplify our models by excluding irrelevant features. Moreover, model explainability allows us to assess the validity of a model against a theory or hypothesis or discover the factors that may be involved in affecting our outcome beyond our theoretical model. In doing so, explainability helps us to get actionable insights from the data, build informed decisions and expand our theoretical knowledge [@Khosravi2022; @alyahyan2020predicting; @Biecek2021-rf].\n\nIn this chapter, we provide a tutorial on ML explainability in the context of education using two examples. First, we describe a case study on regression using a well-known example: predicting students' grades. Secondly, we describe the same procedure for classification. It is assumed that the reader is familiar with the R programming language. If that is not the case, it is recommended to refer to previous tutorials on the basics of R [@Tikka2024-ph], data cleaning [@Kopra2024-fx], basic statistics [@Tikka2024-wl], visualization [@Lopez-Pernas2024-ge], and prediction [@jovanovic2024predictive; @LABOOK2_Chapter_3, @LABOOK2_Chapter_4].\n\n## Explainable AI in education\n\nThe use of explainable AI in educational research is not widespread but its is becoming increasingly prevalent. Studies often employ inherently interpretable algorithms, such as decision trees, to provide global explanations [@Khosravi2022]. An example is the work by [@LópezPernas_Kleimola_Väisänen_Hirsto_2022], who used logistic regression to predict factors that would allow to early predict dropout. Since this is not possible for less interpretable algorithms —which often yield higher accuracy— other studies rely on explainable AI to interpret the results of predictive models. For instance, Pellagatti et al. [@Pellagatti2021] used random forests to predict student dropout. The authors used variable importance to identify which features contributed the most to the predictions, and partial dependence plots to assess how variations in these features influenced the results. Saqr & López-Pernas [@Saqr2024-ie] predicted students’ grades using several algorithms and examined the explainability of each model, the most important variables (variable importance) and how they contributed to the predicted grade (partial dependence plot). The authors also used Shapley Additive explanations to visualize the distribution of contribution for each feature to the predictions across students. In a similar study, used Tiukhova et al. [@Tiukhova_2024] Shapley Additive explanations to explore the stability of the most important features that predict student success in a Naïve Bayes predictive model. Please, refer to Chapter 2 in this book for an in-depth explanation of xAI [@LABOOK2_Chapter_2].\n\n\n## A tutorial on global xAI explainability using `DALEX`\n\nIn this section, we provide a tutorial on ML explainability in the context of education using two examples. First, we describe a case study on regression using a well-known example: predicting students' grades. Secondly, we describe the same procedure for classification. The general workflow followed in each of the examples is the usual ML pipeline, depicted in @fig-workflow, enhanced with the xAI techniques.\n\n![xAI workflow implemented in the tutorial](workflowxai.png){#fig-workflow}\n\n\n\n\n### `DALEX` for student grade prediction \n\nFor our first example, we use a opaque box model and we show how it can be explained (i.e., white-boxed) using the `DALEX` (Descriptive mAchine Learning EXplanations) approach [@DALEX]. We demonstrate the process of building and evaluating a Support Vector Machine (SVM) regression model using the `kernlab` engine and analyzing its performance. The estimation follows what we have learned in the previous chapters e.g., [@LABOOK2_Chapter_3; @LABOOK2_Chapter_4] to create a predictive model. Briefly, we start by loading the necessary libraries and importing the student data as we mentioned before, the data is based on the study [@saqr2022] and have been briefly explained in [@LABOOK2_Chapter_3] and represent students' engagement indicators e.g., reading lectures, time spent online etc. This corresponds to the exploratory data analysis (EDA) described in @fig-workflow–A.\n\nIn the following code, we performed the necessary steps for data preparation (@fig-workflow–B). We beging by standardizing the numeric columns to ensure all features are on the same scale. Then, we split the data into training and testing sets for model evaluation (@fig-workflow–C). Then, define the formula that specifies the relationship between the target variable (`Final_Grade`) and the predictor variables (engagement indicators). Finally, we fit the SVM model using the radial kernel on the training data (@fig-workflow–D).\n\n\n::: {.cell hash='ch06-xai-global_cache/html/setup_c4b917f766ec7a41f096892161204f44'}\n\n```{.r .cell-code}\nset.seed(50)\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(rsample)\nlibrary(e1071) \nlibrary(DALEX)\nlibrary(rio)\n\n# Import the data\nstudent_data <- import(\"https://github.com/lamethods/data2/raw/main/lms/lms.csv\")\n\n# Standardize the numeric columns\nstudent_data_standardized <- student_data |>\n  mutate(across(where(is.numeric), ~scale(.) |> as.vector()))\n\n# Split the data into training and testing sets\ndata_split <- initial_split(student_data_standardized, prop = 0.8)\ntrain_data <- training(data_split)\ntest_data <- testing(data_split)\n\n# Define the formula to specify the relationship between the target variable \n# and the predictor variables\nformula <- Final_Grade ~ Freq_Course_View + Freq_Lecture_View + \n  Freq_Forum_Consume + Freq_Forum_Contribute + Regularity_Course_View +\n  Regularity_Lecture_View + Regularity_Forum_Consume + \n  Regularity_Forum_Contribute + Session_Count + Total_Duration + Active_Days\n\n# Fit the SVM model\nsvm_fit <- svm(formula,\n               data = train_data,\n               kernel = \"radial\")\n```\n:::\n\n\nIn this time, we will not proceed to evaluate the model (@fig-workflow–E) using the traditional methods discussed in the previous chapters [@LABOOK2_Chapter_3; @LABOOK2_Chapter_4], because `DALEX` has its own evaluation and explanation approach that makes this process easy and straightforward. Instead, we will create an explainer. An `explainer` is the first step in evaluating and explaining the model. Given that models create different output formats, an `explainer` standardizes model outputs making them uniform across different algorithms. In other words, the `explainer` object combines all the necessary components for evaluating a ML model (i.e., the model, the data, and the target variable) into a single uniform object. Whether comparing multiple models or analyzing a single model over different datasets, the `explainer` provides a standardized way to approach these tasks. Further, when we create the `explainer` object, it gives us access to the full potential of the `DALEX` capabilities and access to an extensive suite of diagnostic tools. These tools include variable importance measures, partial dependence plots, accumulated local effects plots, break down plots, SHAP values, *ceteris* *paribus* profiles, and more.\n\nCreating an `explainer` is a straightforward process. We use the `explain` function with a few arguments. The first argument is the model, where we pass the `svm_fit` object representing the trained SVM regression model. The `data` argument specifies the holdout dataset which will be used for evaluating the model (`test_data`) excluding the target variable `Final_Grade`. The `y` argument tells `DALEX` what is the target variable that we are predicting and we specify that by passing the target variable (`test_data$Final_Grade`). The next arguments are optional, the `label` argument assigns a readable label to the `explainer` object which will be helpful in identifying the model in plots and summaries, especially when working with multiple models. The `verbose` argument specifies the level of details of the function's output. When `verbose = FALSE`, the function suppresses output messages.\n\n\n::: {.cell hash='ch06-xai-global_cache/html/unnamed-chunk-1_5a8a1be0bd2c5e6d38a9168cd25e9b6d'}\n\n```{.r .cell-code}\nset.seed(50)\n\nlibrary(DALEX)\n# Create an explainer with DALEX\nexplainer_svm <- explain(\n  svm_fit,\n  data = test_data |> dplyr::select(-Final_Grade),\n  y = test_data$Final_Grade,\n  label = \"SVM\",\n  verbose = FALSE\n)\nprint(explainer_svm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel label:  SVM \nModel class:  svm.formula,svm \nData head  :\n  Freq_Course_View Freq_Forum_Consume Freq_Forum_Contribute Freq_Lecture_View\n1       -1.4795970         -1.6037982             -1.476917        -0.1035753\n2        0.3237727          0.9709293              2.284040         0.9374399\n  Regularity_Course_View Regularity_Lecture_View Regularity_Forum_Consume\n1            -1.04549094               0.0204884               -0.7421165\n2             0.08799424              -0.2511021                0.8384102\n  Regularity_Forum_Contribute Session_Count Total_Duration Active_Days\n1                  -1.3787562    -1.4299170     -1.8650048  -0.7372311\n2                   0.7258669     0.7089812      0.8587155   0.2254762\n```\n:::\n:::\n\n\nAlternatively (in the alternative code chunk below), given that `DALEX` works also with `tidymodels`. The same result can be also achieved using this package, where we can specify the model, create a workflow and then, fit the model as we have seen in the previous chapters [@LABOOK2_Chapter_3] and then create an `explainer`.\n\n\n::: {.cell hash='ch06-xai-global_cache/html/unnamed-chunk-2_740ca75fa6dfaeede2eecaaa66447c25'}\n\n```{.r .cell-code}\n# Alternative code chunk\nset.seed(50)\n\n#Alternative model estimation for SVM\n\nlibrary(tidymodels)\n# Define SVM model specification with a radial basis function kernel\nsvm_specification <- svm_rbf() |>\n  set_mode(\"regression\") |>\n  set_engine(\"kernlab\")\n\n# Create the workflow. Combine the model specification with the formula\nsvm_workflow <- workflow() |>\n  add_model(svm_specification) |>\n  add_formula(formula)\n\n# Fit the SVM model\nsvm_fit_tidy <- svm_workflow |>\n  parsnip::fit(data = train_data)\n\n# Create an explainer with DALEX\nexplainer_svm_tidy <- explain(\n  svm_fit_tidy,\n  data = test_data |> dplyr::select(-Final_Grade),\n  y = test_data$Final_Grade,\n  label = \"SVM\",\n  verbose = FALSE)\n\n# print(explainer_svm_tidy) # Uncomment to see the output\n```\n:::\n\n\nHaving created the `explainer`, we can use it with `DALEX` for a range of uses and, most importantly, for model explaining.\n\n#### Model evaluation\n\n`DALEX` offers an easy interface for calculating model performance. To evaluate a model, we simply call the function `model_performance()` which takes one argument, the `explainer` object (`explainer_svm`) which we created earlier. The `model_performance()` function calculates the performance metrics for the regression task: Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared (R²), and Mean Absolute Error (MAE). The result of this function call is assigned to a new object called `model_performance_svm` which we print to display the calculated performance metrics.\n\nAs the results show, The model's performance is moderate, with an R² of about 0.41 meaning it captures some of the patterns in the data, but there is still a significant amount of unexplained variance. The RMSE of about 0.71 gives us an idea of the typical prediction errors (remember that the grades were scaled). The distribution of residuals suggests that the model's errors are fairly balanced between over-predictions and under-predictions, which is good. However, there are some larger errors at the extremes (as seen in the 0% and 100% percentiles of the residuals).\n\n\n::: {.cell hash='ch06-xai-global_cache/html/unnamed-chunk-3_d52b31c865e66873d3347437f7a6d79f'}\n\n```{.r .cell-code}\nset.seed(50)\n\n# Calculate and print model performance metrics for the SVM model\nmodel_performance_svm <- model_performance(explainer_svm)\nprint(model_performance_svm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMeasures for:  regression\nmse        : 0.5556946 \nrmse       : 0.7454493 \nr2         : 0.4063128 \nmad        : 0.4442718\n\nResiduals:\n         0%         10%         20%         30%         40%         50% \n-1.64093297 -0.58085583 -0.41026001 -0.07698939  0.07855870  0.28133263 \n        60%         70%         80%         90%        100% \n 0.34303022  0.51622444  0.76114088  1.06787159  1.81745294 \n```\n:::\n:::\n\n\nTo visualize these metrics, we use `DALEX` to create several types of performance plots (@fig-perf).\n\nThe **Lift Curve** demonstrates how well the model ranks predictions compared to random guessing; a higher curve indicates better performance. The curve starts with a brief dip near zero positive rate, indicating a small initial under-performance then rises sharply to a peak lift of about 4.5 at around 10% positive rate. This suggests the model is particularly effective at identifying the top 10% of most likely positive instances. From this peak, the curve gradually declines as the positive rate increases, maintaining a lift above 1 (random selection) for most of its range (i.e., continues to outperform random selection until very high positive rates.). The curve's shape, starting high and slowly descending, is characteristic of a well-performing predictive model. This lift curve suggests that the SVM model has good discriminating power and could be particularly useful in scenarios where identifying the most likely positive cases is important.\n\nThe **Residuals Boxplot** visualizes the spread and outliers of residuals, ideally showing minimal outliers and a symmetric distribution around zero. The red dot represents the RMSE, a measure of the model's average prediction error. Its position around 0.7-0.8, suggesting moderate prediction accuracy. Overall, this boxplot indicates that while the SVM model's residuals are somewhat symmetrically distributed around zero, there is a slight tendency for the model to underpredict (positive residuals) more often than overpredict.\n\nThe **Residuals Histogram** provides a frequency distribution of residuals, ideally normal around zero. The residuals histogram reveals that most residuals are not very centered around zero. The histogram shows an asymmetrical distribution with more residuals on the positive side, suggesting a potential bias in the model where it tends to underpredict more frequently than overpredicting. The concentration is noticeable most at around 1, suggests a tendency for the model to underpredict grades.\n\nThe **Cumulative Gain Chart** shows the cumulative gain achieved by the model across percentiles, with curves above the diagonal suggesting better predictive power. Finally, the **Empirical Cumulative Distribution Function (ECDF)** plot illustrates the cumulative probability of residuals, helping to understand their distribution across values.\n\nWhile visualizing model performance using plots can be beneficial, especially for comparing multiple models, interpreting single-model plots might be challenging. However, performance plots may not provide actionable insights to help tune the model or adjust data.\n\n\n::: {#fig-perf .cell layout=\"[[1,1],[1,1],[1,1]]\" hash='ch06-xai-global_cache/html/fig-perf_f570e5375496bb052391405ada4cd78f'}\n\n```{.r .cell-code}\nset.seed(50)\n\n# Generate various model performance plots\nplot(model_performance_svm, geom = \"lift\") \nplot(model_performance_svm, geom = \"gain\") \nplot(model_performance_svm, geom = \"boxplot\") \nplot(model_performance_svm, geom = \"histogram\")\nplot(model_performance_svm, geom = \"ecdf\")\n```\n\n::: {.cell-output-display}\n![Lift Curve - SVM Model](ch06-xai-global_files/figure-html/fig-perf-1.png){#fig-perf-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Cumulative Gain Chart - SVM Model](ch06-xai-global_files/figure-html/fig-perf-2.png){#fig-perf-2 width=672}\n:::\n\n::: {.cell-output-display}\n![Residuals Boxplot - SVM Model](ch06-xai-global_files/figure-html/fig-perf-3.png){#fig-perf-3 width=672}\n:::\n\n::: {.cell-output-display}\n![Residuals Histogram - SVM Model](ch06-xai-global_files/figure-html/fig-perf-4.png){#fig-perf-4 width=672}\n:::\n\n::: {.cell-output-display}\n![Empirical Cumulative Distribution Function - SVM Model](ch06-xai-global_files/figure-html/fig-perf-5.png){#fig-perf-5 width=672}\n:::\n\nPerformance plots\n:::\n\n\n#### Model explainability\n\nThe `DALEX` way of explaining models is built on Leo Breiman's variable-importance measure for random forests [@Breiman2001-ut]. The method uses permutation to assess the significance of explanatory variables. The algorithm changes a variable's values several times and assesses the resulting impact on model performance. The greater the degradation in performance, the more important the variable is to the prediction task. These methods —while straightforward— offers an effective and robust method for model-agnostic explanations. In doing so, it can be applied to explain any model regardless of the model explainability.\n\nTo compute and visualize the importance of our SVM model, we use the `model_parts` function. `model_parts` is the main function for generating global (i.e., model level) model explanations (contrast those to local explanation in @LABOOK2_Chapter_7 ). The function accepts several arguments, but only one is required: the explainer object we previously created (`explainer_svm`). In its simplest form, you can call `model_parts(explainer_svm)`, with all other options set to their default values. For instance, `loss_function`, is set by default to `loss_root_mean_square` indicating that the drop in RMSE will be used to measure the model's performance.\n\nWe store the results of the `model_parts` in the `vi_svm` object which contains the calculated variable importance and then pass it to the `plot` function. Other arguments may need to be specified, for instance, the `B` argument (defaults to 10) controls the number of permutations used in the calculation, higher numbers (e.g., 500) are always recommended to increase the robustness of the results but may be computationally demanding. Also, the `type` argument can be specified to change the type of feature importance. By default (`\"raw\"`), provides raw drop loss values or `\"ratio\"` to return the ratio of drop loss to the full loss in model, while `\"difference\"` returns drop loss in the full model. As the results show (@fig-global-exp), the most important variables are *contributing to forums*, *regularity in course view*, and in *contributing to forums* as well as *session count and duration*. These variables are consistent across all methods and they represent cognitive engagement (contributing to problem solving through forums) and time investment in the course, all are consistent with the hypothesis that cognitive engagement is more likely to result in higher achievement.\n\n\n::: {#fig-global-exp .cell layout=\"[[1,1],[1,1]]\" hash='ch06-xai-global_cache/html/fig-global-exp_e80e4c70d6df94befd7078674bee977d'}\n\n```{.r .cell-code}\nset.seed(50)\n\n# Default model\nvi_svm <- model_parts(explainer_svm)\nplot(vi_svm)\n\n# Same as before with all arguments specified \nvi_svm <- model_parts(explainer_svm, loss_function = loss_root_mean_square, \n                      B = 100)\nplot(vi_svm)  \n\n# other variations: difference and ratio \nvi_svm_difference <- model_parts(explainer_svm, loss_function = loss_root_mean_square, \n                                 B = 100, type = \"difference\")\nplot(vi_svm_difference)  \n\nvi_svm_ratio <- model_parts(explainer_svm, loss_function = loss_root_mean_square, \n                            B = 100, type = \"ratio\")\nplot(vi_svm_ratio)\n```\n\n::: {.cell-output-display}\n![Default model](ch06-xai-global_files/figure-html/fig-global-exp-1.png){#fig-global-exp-1 width=672}\n:::\n\n::: {.cell-output-display}\n![All arguments specified](ch06-xai-global_files/figure-html/fig-global-exp-2.png){#fig-global-exp-2 width=672}\n:::\n\n::: {.cell-output-display}\n![Difference](ch06-xai-global_files/figure-html/fig-global-exp-3.png){#fig-global-exp-3 width=672}\n:::\n\n::: {.cell-output-display}\n![Ratio](ch06-xai-global_files/figure-html/fig-global-exp-4.png){#fig-global-exp-4 width=672}\n:::\n\nGlobal explanations\n:::\n\n\n##### Creating Custom Loss Functions for DALEX\n\nWhereas the default is loss function in `DALEX` is based on RMSE, we can also use different metrics to assess the model loss. In the next code chunk, we show how to build two custom functions. These custom functions are used to compute variable importance for the SVM model. In particular, we build two custom loss functions: one for MAE, which calculates the mean of absolute differences between observed and predicted values, and another for MAD, which computes the median of these absolute differences. The results are visualized by creating plots for the MAE-based and MAD-based variable importance (@fig-loss-custom). The choice of loss function can significantly impact the interpretation of variable importance, so researchers need to select a function that aligns with the data and analysis goals.\n\n\n::: {#fig-loss-custom .cell layout=\"[1,1]\" hash='ch06-xai-global_cache/html/fig-loss-custom_e220c2644c65b89ee68e6ddbbd6adbf9'}\n\n```{.r .cell-code}\nset.seed(50)\n\n# Load necessary libraries\nlibrary(DALEX)\nlibrary(ggplot2)\n\n# Define the custom MAE loss function\nloss_mae <- function(observed, predicted) {\n  # Compute the Mean Absolute Error\n  mean(abs(observed - predicted))\n}\n\n# Define the custom MAD loss function\nloss_mad <- function(observed, predicted) {\n  # Compute the Median Absolute Deviation\n  median(abs(observed - predicted))\n  #mean(abs(predicted - median(predicted))) another version\n}\n\n# Compute variable importance using the custom MAE loss function\nvi_svm_mae <- model_parts(explainer_svm, loss_function = loss_mae)\n\n# Compute variable importance using the custom MAD loss function\nvi_svm_mad <- model_parts(explainer_svm, loss_function = loss_mad)\n\n# Plot the results for MAE\nplot(vi_svm_mae) \n\n# Plot the results for MAD\nplot(vi_svm_mad)\n```\n\n::: {.cell-output-display}\n![MAE](ch06-xai-global_files/figure-html/fig-loss-custom-1.png){#fig-loss-custom-1 width=672}\n:::\n\n::: {.cell-output-display}\n![MAD](ch06-xai-global_files/figure-html/fig-loss-custom-2.png){#fig-loss-custom-2 width=672}\n:::\n\nVariable Importance with Custom loss functions\n:::\n\n\n#### Model selection: creating and evaluating several ML models\n\nWe have previously explored how to estimate, evaluate, and explain a single ML model. However, explainability techniques allow us to make an informed decision about the most suitable model beyond simple performance metrics. To showcase this potential, we will estimate several ML models, evaluate their performance, and then select and explain the best-performing model. Since we have already covered the detailed process of estimating multiple ML models in the previous chapters [@LABOOK2_Chapter_3; @LABOOK2_Chapter_4], we will not revisit that process in depth. The following code estimates seven models:\n\n-   **Linear Regression:** Fits a linear model to our data using the `lm` function available in base R.\n-   **Decision Tree:** Implements a decision tree model suitable for regression tasks with the `rpart` package [@rpart].\n-   **Random Forest:** Builds a forest model using the `randomForest` package [@randomForest].\n-   **Support Vector Machine:** Estimates a support vector machine model using the `svm` function of the `e1071` package [@svm].\n-   **K-Nearest Neighbors:** Estimates a k-nearest neighbors model via the `kNN` function [@kknn].\n-   **Gradient Boosting:** Introduces gradient boosting with the `gbm` package [@gbm].\n-   **XGBoost:** Trains an efficient and scalable version of gradient boosting with the `xgboost` package [@xgboost].\n\nFor the XGBoost model, a special approach is required. First, we convert our data into matrix format using `model.matrix`, excluding the first column to avoid the dummy variable. We then create `train_matrix` and `test_matrix` for our training and test sets and extract the target variable into `train_label`. Finally, the `xgboost` function is used to train the model.\n\n\n::: {.cell hash='ch06-xai-global_cache/html/unnamed-chunk-7_101db7c501cd25499363152f1ac11759'}\n\n```{.r .cell-code}\nset.seed(50)\n\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(rpart)\nlibrary(randomForest)\nlibrary(e1071)\nlibrary(kknn)\nlibrary(gbm)\nlibrary(xgboost)\nlibrary(DALEX)\n\n# Linear Regression\nlinear_model <- lm(formula, data = train_data)\n\n# Decision Tree\ndecision_tree_model <- rpart(formula, data = train_data, method = \"anova\")\n\n# Random Forest\nrandom_forest_model <- randomForest(formula, data = train_data, ntree = 100)\n\n# Support Vector Machine\nsvm_model <- svm(formula, data = train_data)\n\n# k-Nearest Neighbors\nknn_model <- train(formula, data = train_data, method = \"kknn\", tuneLength = 5)\n\n# Gradient Boosting Machine\ngbm_model <- gbm(formula, data = train_data, distribution = \"gaussian\", \n                 n.trees = 100, interaction.depth = 3, shrinkage = 0.1, cv.folds = 5)\n\n# XGBoost\ntrain_matrix <- model.matrix(formula, data = train_data)[,c(-1)]\ntest_matrix <- model.matrix(formula, data = test_data)[,c(-1)]\n\ntrain_label <- train_data$Final_Grade\nxgboost_model <- xgboost(data = train_matrix, label = train_label, nrounds = 100, \n                         objective = \"reg:squarederror\", verbose = 0)\n```\n:::\n\n\n#### Multiple models\n##### Creating multiple explainers\n\nIn our previous step, we constructed several ML models. To understand how each model makes predictions, we need to create an explainer for each model. We do so using the `explain` function to generate these explainers in the same way we did before. We begin by specifying the model, the test data (excluding the target variable `Final_Grade`), and a descriptive label to identify the model when multiple models are visualized or compared.\n\nThe below code creates an `explainer_linear` for the linear regression model using the test data without the `Final_Grade` column, and set a label indicating `\"Linear Regression\"`. We then replicate this process for the decision tree, random forest, SVM, k-nearest neighbors, gradient boosting, and XGBoost models, creating respective explainers for each. For XGBoost, we use the pre-created test data matrix, the XGBoost model, test data matrix. Finally, we use the print function to provide summary information about the explainer and allow us to verify its correct configuration.\n\n\n::: {.cell hash='ch06-xai-global_cache/html/unnamed-chunk-8_19c7f67e84302ab5fce01a2a0bf9b941'}\n\n```{.r .cell-code}\nset.seed(50)\n\n# Create `DALEX`explainers\nexplainer_linear <- explain(\n  model = linear_model, y = test_data$Final_Grade,\n  data = dplyr::select(test_data, -Final_Grade),\n  label = \"Linear Regression\", verbose = FALSE\n)\n\nexplainer_decision_tree <- explain(\n  model = decision_tree_model, y = test_data$Final_Grade,\n  data = dplyr::select(test_data, -Final_Grade),\n  label = \"Decision Tree\", verbose = FALSE\n)\n\nexplainer_random_forest <- explain(\n  model = random_forest_model, y = test_data$Final_Grade,\n  data = dplyr::select(test_data, -Final_Grade),\n  label = \"Random Forest\", verbose = FALSE\n)\n\nexplainer_svm <- explain(\n  model = svm_model, y = test_data$Final_Grade,\n  data = dplyr::select(test_data, -Final_Grade),\n  label = \"SVM\", verbose = FALSE\n)\n\nexplainer_knn <- explain(\n  model = knn_model$finalModel, y = test_data$Final_Grade,\n  data = dplyr::select(test_data, -Final_Grade),\n  label = \"k-NN\", verbose = FALSE\n)\n\nexplainer_gbm <- explain(\n  model = gbm_model, y = test_data$Final_Grade,\n  data = dplyr::select(test_data, -Final_Grade),\n  label = \"GBM\", verbose = FALSE\n)\n\nexplainer_xgboost <- explain(\n  model = xgboost_model, y = test_data$Final_Grade,\n  data = test_matrix, label = \"XGBoost\", verbose = FALSE\n)\n```\n:::\n\n\n##### Evaluation of multiple models\n\nIn the next step, we evaluate the performance of the multiple ML models (@fig-workflow–E). For each model of our models (linear regression, decision tree, random forest, support vector machine, k-nearest neighbors, gradient boosting machine, and XGBoost), the code calculates performance metrics using the `model_performance` function. The process is simply a replication of what we have seen for the single model. The `model_performance` function then generates the performance metrics for each model. The performance metrics are then stored in separate variables for each model, allowing for easy comparison and analysis of how well each model performs on the given dataset. We extract and combine these metrics in a table for comparison (@tbl-perfmet).\n\n\n::: {.cell hash='ch06-xai-global_cache/html/unnamed-chunk-9_ac36f10a169ea821e22f378beeec7b0f'}\n\n```{.r .cell-code}\nset.seed(50)\n\n# Calculate performance metrics for each model\nperformance_linear <- model_performance(explainer_linear)\nperformance_decision_tree <- model_performance(explainer_decision_tree)\nperformance_random_forest <- model_performance(explainer_random_forest)\nperformance_svm <- model_performance(explainer_svm)\nperformance_knn <- model_performance(explainer_knn)\nperformance_gbm <- model_performance(explainer_gbm)\nperformance_xgboost <- model_performance(explainer_xgboost)\n\n# Combine performance metrics into a single table\nperformance_metrics <- rbind(\n  `Linear Regression` = as.array(performance_linear$measure),\n  `Decision Tree` = performance_decision_tree$measures, \n  `Random Forest` = performance_random_forest$measures,\n  `SVM` = performance_svm$measures$r2, \n  `KNN` = performance_knn$measures,\n  `GBM` = performance_gbm$measures,\n  `XGBoost` = performance_xgboost$measures)  \n\n# Print the combined dataframe\nprint(performance_metrics)\n```\n:::\n\n::: {#tbl-perfmet .cell tbl-cap='Performance metrics of each model' hash='ch06-xai-global_cache/html/tbl-perfmet_16dfa903f668bb31428c664b7a00a803'}\n::: {.cell-output-display}\n|Model             |  mse| rmse|   r2|  mad|\n|:-----------------|----:|----:|----:|----:|\n|Linear Regression | 0.56| 0.75| 0.41| 0.48|\n|Decision Tree     | 0.63| 0.79| 0.33| 0.48|\n|Random Forest     | 0.51| 0.71| 0.46| 0.42|\n|SVM               | 0.41| 0.41| 0.41| 0.41|\n|KNN               | 0.57| 0.75| 0.39| 0.54|\n|GBM               | 0.49| 0.70| 0.47| 0.42|\n|XGBoost           | 0.50| 0.71| 0.46| 0.39|\n:::\n:::\n\n\nThe performance metrics for these models reveal interesting insights into their predictive capabilities. XGBoost emerges as the top performer, closely followed by Gradient Boosting Machine (GBM) and Random Forest. This pattern suggests that ensemble methods are good for this dataset, likely due to their ability to capture complex relationships and reduce overfitting. Interestingly, Linear Regression model performs rather well compared to the more complex algorithms like SVM and K-Nearest Neighbors (KNN). This suggests that while there are certainly non-linear aspects to the data, there are also significant linear relationships. The KNN model's performance is somewhat moderate, with a high Mean Absolute Deviation (MAD). This could indicate that the model is sensitive to outliers although we performed feature scaling. It is clear that no single model dominates across all metrics. Moreover, the respectable performance of Linear Regression reminds us that simpler models shouldn't be discounted, especially when interpretability is a concern. The plots of the model's performance confirm these findings (@fig-allmodels).\n\n\n::: {.cell hash='ch06-xai-global_cache/html/unnamed-chunk-11_43162d5410225e7d51aed655305edde9'}\n\n:::\n\n::: {#fig-allmodels .cell layout=\"[[1,1],[1,1],[2,0]]\" hash='ch06-xai-global_cache/html/fig-allmodels_5d71e2dac89623940d956bb522c91e32'}\n\n```{.r .cell-code}\nset.seed(50)\n\n# Create combined plots\n# Lift plot\nplot(performance_linear, performance_decision_tree, performance_random_forest,\n     performance_svm, performance_knn, performance_gbm, performance_xgboost,\n    geom = \"lift\") \n\n# Gain plot\nplot(performance_linear, performance_decision_tree, performance_random_forest,\n     performance_svm, performance_knn, performance_gbm, performance_xgboost,\n     geom = \"gain\") \n\n# Residuals plot\nplot(performance_linear, performance_decision_tree, performance_random_forest,\n     performance_svm, performance_knn, performance_gbm, performance_xgboost,\n     geom = \"boxplot\"\n)\n\n# ecdf curve\nplot(performance_linear, performance_decision_tree, performance_random_forest,\n     performance_svm, performance_knn, performance_gbm, performance_xgboost,\n     geom = \"ecdf\"\n)\n\n# histogram \nplot(performance_linear, performance_decision_tree, performance_random_forest,\n     performance_svm, performance_knn, performance_gbm, performance_xgboost,\n     geom = \"histogram\" \n) \n```\n\n::: {.cell-output-display}\n![Lift Curves](ch06-xai-global_files/figure-html/fig-allmodels-1.png){#fig-allmodels-1 width=768}\n:::\n\n::: {.cell-output-display}\n![Cumulative Gain Charts](ch06-xai-global_files/figure-html/fig-allmodels-2.png){#fig-allmodels-2 width=768}\n:::\n\n::: {.cell-output-display}\n![Residuals](ch06-xai-global_files/figure-html/fig-allmodels-3.png){#fig-allmodels-3 width=768}\n:::\n\n::: {.cell-output-display}\n![Reverse cumulative distribution](ch06-xai-global_files/figure-html/fig-allmodels-4.png){#fig-allmodels-4 width=768}\n:::\n\n::: {.cell-output-display}\n![Histogram plot](ch06-xai-global_files/figure-html/fig-allmodels-5.png){#fig-allmodels-5 width=768}\n:::\n\nAll models\n:::\n\n\nThe lift plot shows a lot of overlap, same with the cumulative gain chart. Of course, we see some models perform well across some ranges but nothing consistent. The box plot of residuals show that XGBoost, GBM, and Random Forest have the best performance, with the smallest and most compact boxes, indicating lower and more consistent residuals. Their median lines (the vertical line inside each box) are closest to zero, suggesting more accurate predictions overall. The red dots represent the root mean square of residuals for each model, providing a single metric for comparison. These dots align well with the overall box plot distributions, confirming the relative performance of the models. Overall, this visualization reinforces the strengths of ensemble methods like XGBoost and GBM for this particular dataset. The reverse cumulative distribution plot offers a visual comparison of residual performance across the ML models. The graph's leftward shift indicates better model performance, with XGBoost, GBM, and Random Forest as top performers, aligning with earlier metric analyses. Linear Regression and SVM show similar, middling performance, while k-NN and Decision Tree lag behind. The y-axis represents the percentage of predictions with residuals exceeding the x-axis value which gives an idea about the error distribution.\n\nTo conclude, it seems that XGBoost is the best performing model, and so we will demonstrate how to explain it using the same methods we learnt before.\n\n##### Explaining the XGBoost model\n\nThe next step replicates what we have already done for the SVM model. The code computes and visualizes the important variables using the default and the custom loss functions. The variable importance is calculated three times: for RMSE (the default function), MAD, and MAE (the custom loss functions). We use the `model_parts` function with the `explainer_xgboost` to generate variable importance scores. The parameter `B=500` indicates that 500 permutations are performed to assess importance. The results are stored in `vi_xgboost`, `vi_xgboost_mad`, and `vi_xgboost_mae`, corresponding to the different loss functions. Finally, these results are plotted with titles indicating the loss function used (@fig-xgboost-maemad).\n\n\n::: {#fig-xgboost-maemad .cell layout=\"[1,1,1]\" hash='ch06-xai-global_cache/html/fig-xgboost-maemad_3409bc0e58c90fd22ff9e00ba53b1a20'}\n\n```{.r .cell-code}\nset.seed(50)\n\n# Compute and plot variable importance using the custom RMSE loss function\nvi_xgboost <- model_parts(explainer_xgboost, loss_function = loss_root_mean_square,\n                          B=500, type = \"ratio\")\nplot(vi_xgboost)  \n\n# Compute and plot variable importance using the custom MAD loss function\nvi_xgboost_mad <- model_parts(explainer_xgboost, loss_function = loss_mad,  B=500)\nplot(vi_xgboost_mad)\n\n# Compute and plot variable importance using the custom MAE loss function\nvi_xgboost_mae <- model_parts(explainer_xgboost, loss_function = loss_mae,B=500)\nplot(vi_xgboost_mae) \n```\n\n::: {.cell-output-display}\n![RMSE](ch06-xai-global_files/figure-html/fig-xgboost-maemad-1.png){#fig-xgboost-maemad-1 width=672}\n:::\n\n::: {.cell-output-display}\n![MAD](ch06-xai-global_files/figure-html/fig-xgboost-maemad-2.png){#fig-xgboost-maemad-2 width=672}\n:::\n\n::: {.cell-output-display}\n![MAE](ch06-xai-global_files/figure-html/fig-xgboost-maemad-3.png){#fig-xgboost-maemad-3 width=672}\n:::\n\nXGBoost Variable importance using custom loss function\n:::\n\n\n#### Partial-dependence Profiles\n\nTwo types of plots can help explain impact of different engagement variables on students' grades (@fig-workflow–F): Partial Dependence Plots (PDPs) and Accumulated Local Effects (ALE) plots. Both plots aim to illustrate how the predicted outcome (grades) changes in response to variations in specific predictor variables (such as engagement indicators), but they do so in slightly different ways, each with its own strengths.\n\n##### Partial Dependence Plots (PDPs)\n\nPDPs are designed to show the average effect of a single feature on the predicted outcome. This is useful for understanding the overall trend or relationship between a specific feature (e.g., frequency of forum contributions) and the outcome (grades). For instance, in the generated PDPs, you might observe that increasing the frequency of forum contributions has a positive impact on the predicted grades, indicating that students who participate more in forums tend to achieve higher grades. Conversely, a flat line in a PDP, such as the one you might see for the \"active days\" variable, suggests that changes in this feature do not significantly affect the outcome. In general, the regularity of activity often seems more impactful than raw frequency or duration. For instance, regularity of course and lecture viewing show positive associations with grade predictions. The PDPs can be generated by the code using `model_profile` and `plot(pdp_xgboost)` with the test data excluding the outcome variable.\n\n##### Accumulated Local Effects (ALE) Plots\n\nALE plots offer a more refined alternative to PDPs, particularly when dealing with correlated features (@fig-ale-plots). Unlike PDPs, which show the average effect of a feature, ALE plots focus on the local changes in predictions as the feature varies. This approach accounts for the potential interactions between features and provides a more accurate interpretation when features are correlated. ALE can be created in the same way, but with the option `type = \"accumulated\"`. This makes ALE plots particularly useful in complex educational datasets where many engagement variables might be interrelated.\n\n\n::: {#fig-ale-plots .cell layout=\"[1,1]\" hash='ch06-xai-global_cache/html/fig-ale-plots_3a8b025439743ec8239cb2e40e8dc785'}\n\n```{.r .cell-code}\n# Partial Dependence Plots\npdp_xgboost <- model_profile(explainer_xgboost, \n                variables = colnames(test_data)[colnames(test_data) != \"Final_Grade\"])\nplot(pdp_xgboost) + labs(subtitle = \"\")\n\n# Accumulated Local Effects (ALE) Plots\nale_xgboost <- model_profile(explainer_xgboost, type = \"accumulated\", \n                variables = colnames(test_data)[colnames(test_data) != \"Final_Grade\"])\nplot(ale_xgboost) + labs(subtitle = \"\")\n```\n\n::: {.cell-output-display}\n![Partial Dependence Plots](ch06-xai-global_files/figure-html/fig-ale-plots-1.png){#fig-ale-plots-1 width=768}\n:::\n\n::: {.cell-output-display}\n![Accumulated Local Effects (ALE) Plots](ch06-xai-global_files/figure-html/fig-ale-plots-2.png){#fig-ale-plots-2 width=768}\n:::\n\nPDPs vs. ALE plots\n:::\n\n\nThe ALE plots might show that, even after accounting for other correlated behaviors, frequent contributions to forums still have a significant positive effect on grades, reinforcing the importance of this type of engagement. PDPs and ALEs can also be plotted while clustering the data (@fig-ale-plots3). For instance the `k=3` argument in the code specifies that the feature values should be divided into three clusters for calculating Accumulated Local Effects (ALE). This means that the ALE plots will show how the predicted outcome changes as the feature varies within each of these three clusters, providing a view of the heterogeneity of predictor outcome effect.\n\n\n::: {.cell hash='ch06-xai-global_cache/html/fig-ale-plots3_603b0c9fc5c76212d668d1314637ad0b'}\n\n```{.r .cell-code}\n# Accumulated Local Effects (ALE) Plots\nale_xgboost <- model_profile(explainer_xgboost, type = \"accumulated\", \n                variables = colnames(test_data)[colnames(test_data) != \"Final_Grade\"], \n                k = 3)\nplot(ale_xgboost) + labs(subtitle = \"\")\n```\n\n::: {.cell-output-display}\n![ALE plot with 3 clusters](ch06-xai-global_files/figure-html/fig-ale-plots3-1.png){#fig-ale-plots3 width=1152}\n:::\n:::\n\n\n### Explaining a classification model\n\nExplaining a classification model goes almost the same way as regression with the necessary modifications, e.g., specifying the correct target. The next code estimates a Gradient Boost Model (GBM) in the same way we explained before in the classification chapter. The code begins by loading the necessary libraries, setting a random seed is set to ensure consistent results. Data is imported from a CSV file, and a new binary classification variable, `Achievement`, is created based on whether a student's `Final_Grade` is above or below the median (top 50% of achievement level). The `Achievement` variable is then converted to numeric format to be suitable for model training (@fig-workflow–B). The dataset is split into training and testing sets (@fig-workflow–C), with 80% allocated for training while ensuring class distribution is preserved. Finally, a GBM model is fitted using the engagement predictors to estimate the binary outcome (@fig-workflow–D). The model is configured with a Bernoulli distribution for binary classification, 1000 trees, a tree depth of 3, a learning rate of 0.01, and a bagging fraction of 0.7. These values are chosen for demonstration.\n\n\n::: {.cell hash='ch06-xai-global_cache/html/unnamed-chunk-16_0643fb9fbe1e209220c2e4f3133e716b'}\n\n```{.r .cell-code}\n# Load necessary libraries\nlibrary(gbm)\nlibrary(rio)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(DALEX)\nlibrary(rsample)\n\n# Set seed for reproducibility\nset.seed(50)\n\n# Load data\nstudent_data <- import(\"../student_data.csv\")\n\n# Create binary classification target variable\nstudent_data <- student_data |>\n  mutate(Achievement = ifelse(Final_Grade > median(Final_Grade), \n                              \"High_Achievers\", \"Low_Achievers\")) |>\n  mutate(Achievement = factor(Achievement, \n                              levels = c(\"High_Achievers\", \"Low_Achievers\")))\n\n# Convert target to numeric for model fitting\nstudent_data$Achievement_numeric <- as.numeric(student_data$Achievement) - 1\n\n# Split data into training and testing sets\ndata_split <- initial_split(student_data, prop = 0.8, strata = Achievement_numeric)\ntrain_data <- training(data_split)\ntest_data <- testing(data_split)\n\n# Fit GBM model\ngbm_model <- gbm(\n  Achievement_numeric ~  Freq_Course_View + Freq_Lecture_View + Freq_Forum_Consume + \n    Freq_Forum_Contribute + Regularity_Course_View + Regularity_Lecture_View + \n    Regularity_Forum_Consume + Regularity_Forum_Contribute +\n    Session_Count + Total_Duration + Active_Days,\n  data = train_data,\n  distribution = \"bernoulli\",  # For binary classification\n  n.trees = 1000,\n  interaction.depth = 3,\n  shrinkage = 0.01,\n  bag.fraction = 0.7\n)\n```\n:::\n\n\nSimilar to what we did before, we create an explainer for the fitted Gradient Boosting Model (GBM). The `explain` function takes the estimated `gbm_model` and the test data (excluding the target variable and the final grade). The explainer is labeled as \"GBM Model\" for identification. Subsequently, the model's performance is assessed using the `model_performance` function, which computes the classification performance metrics (@fig-workflow–E) based on the explainer object. The results are then printed out to provide insights into how well the GBM model performs on the test data.\n\n\n::: {.cell hash='ch06-xai-global_cache/html/unnamed-chunk-17_4859508652522d137a499672b1223040'}\n\n```{.r .cell-code}\n# Create `DALEX` explainer\nexplainer_gbm <- explain(\n  model = gbm_model,\n  data = test_data[, 1:11],\n  y = test_data$Achievement_numeric,\n  label = \"GBM Model\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPreparation of a new explainer is initiated\n  -> model label       :  GBM Model \n  -> data              :  58  rows  11  cols \n  -> target variable   :  58  values \n  -> predict function  :  yhat.gbm  will be used (  default  )\n  -> predicted values  :  No value for predict function target column. (  default  )\n  -> model_info        :  package gbm , ver. 2.2.2 , task classification (  default  ) \n  -> predicted values  :  numerical, min =  0.01791613 , mean =  0.5135735 , max =  0.9899749  \n  -> residual function :  difference between y and yhat (  default  )\n  -> residuals         :  numerical, min =  -0.9362661 , mean =  -0.01357354 , max =  0.9794141  \n  A new explainer has been created!  \n```\n:::\n\n```{.r .cell-code}\n# Model performance\nmodel_performance_gbm <- model_performance(explainer_gbm)\nprint(model_performance_gbm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMeasures for:  classification\nrecall     : 0.7931034 \nprecision  : 0.7666667 \nf1         : 0.779661 \naccuracy   : 0.7758621 \nauc        : 0.8133175\n\nResiduals:\n          0%          10%          20%          30%          40%          50% \n-0.936266132 -0.537579796 -0.400499445 -0.149711621 -0.054921553 -0.003945525 \n         60%          70%          80%          90%         100% \n 0.051416919  0.101556426  0.251119244  0.505825858  0.979414109 \n```\n:::\n:::\n\n\n-   The *recall* of 79.31% means that the model correctly identifies 79.31% of the low achievers, showing strong sensitivity in detecting students who might struggle.\n-   The *precision* of 76.67% shows that, out of all the students predicted to be low achievers, 76.67% were correctly classified, highlighting the model's reliability.\n-   The *F1-score* of 77.97%, which balances both recall and precision, suggests the model performs well in identifying low achievers overall.\n-   The *accuracy* of 77.59% indicates that the model correctly classifies 77.59% of all students, whether they are high or low achievers.\n-   The *AUC* of 81.33% further confirms the model's ability to effectively distinguish between low and high achievers, with a score over 80% generally indicating strong performance.\n\nWe can also visualize the results by using the `plot` function with the `model_performance_gbm` object and specifying the desired geometry (`geom`). The resulting plots (@fig-gbm-plots) show that the ROC curve is well above the diagonal (the dashed line), indicating good model performance with high true positive rates even at low false positive rates, suggesting strong discriminating power. The residuals boxplot is fairly symmetrical around the zero line, which is favorable. The lift curve starts high (around 2) and gradually decreases, remaining above 1 throughout, meaning the model consistently outperforms random selection in identifying positive cases. The cumulative gain chart rises steeply at first and then flattens, significantly outperforming the diagonal baseline, indicating that the model effectively identifies a large proportion of positive cases early in its ranked predictions. The precision-recall curve shows high precision at low recall values, followed by a gradual decline as recall increases, suggesting a trade-off between precision and recall. The residuals histogram distribution is roughly symmetrical and centered near zero, which is desirable, with some outliers. Lastly, the empirical cumulative distribution curve indicates that a large proportion of residuals are concentrated around zero, with relatively few extreme values.\n\n\n::: {#fig-gbm-plots .cell layout=\"[[1,1],[1,1],[1,1],[1,1]]\" hash='ch06-xai-global_cache/html/fig-gbm-plots_39f74fa95a61a83de9f7876976c9d0fd'}\n\n```{.r .cell-code}\n# Create individual plots\nplot(model_performance_gbm, geom = \"roc\")\nplot(model_performance_gbm, geom = \"boxplot\")\nplot(model_performance_gbm, geom = \"lift\")\nplot(model_performance_gbm, geom = \"gain\")\nplot(model_performance_gbm, geom = \"prc\")\nplot(model_performance_gbm, geom = \"histogram\")\nplot(model_performance_gbm, geom = \"ecdf\")\n```\n\n::: {.cell-output-display}\n![ROC Curve - GBM Model](ch06-xai-global_files/figure-html/fig-gbm-plots-1.png){#fig-gbm-plots-1 width=480}\n:::\n\n::: {.cell-output-display}\n![Residuals Boxplot - GBM Model](ch06-xai-global_files/figure-html/fig-gbm-plots-2.png){#fig-gbm-plots-2 width=480}\n:::\n\n::: {.cell-output-display}\n![Lift Curve - GBM Model](ch06-xai-global_files/figure-html/fig-gbm-plots-3.png){#fig-gbm-plots-3 width=480}\n:::\n\n::: {.cell-output-display}\n![Cumulative Gain Chart - GBM Model](ch06-xai-global_files/figure-html/fig-gbm-plots-4.png){#fig-gbm-plots-4 width=480}\n:::\n\n::: {.cell-output-display}\n![Precision-Recall Curve - GBM Model](ch06-xai-global_files/figure-html/fig-gbm-plots-5.png){#fig-gbm-plots-5 width=480}\n:::\n\n::: {.cell-output-display}\n![Residuals Histogram - GBM Model](ch06-xai-global_files/figure-html/fig-gbm-plots-6.png){#fig-gbm-plots-6 width=480}\n:::\n\n::: {.cell-output-display}\n![Empirical Cumulative Distribution Function - GBM Model](ch06-xai-global_files/figure-html/fig-gbm-plots-7.png){#fig-gbm-plots-7 width=480}\n:::\n\nGBM plots\n:::\n\n\nExplaining the model (@fig-workflow–F) is performed in the same way we did before, we use the `model_parts` function with the explainer object `explainer_gbm`. The only parameter we pass is the number of permutations `B = 500` to make the results more robust. The default method computes variable importance using the AUC loss function which measures how much the AUC would decrease if that variable were removed from the model. We see that the forum contribute variables (frequency and regularity) as well as session count are the variables with most importance (@fig-gbm-varimp-auc).\n\n\n::: {.cell hash='ch06-xai-global_cache/html/fig-gbm-varimp-auc_f9af4230779e8cbee476b88991f27173'}\n\n```{.r .cell-code}\n# Variable importance with AUC loss\nvi_gbm <- model_parts(explainer_gbm, B = 500)\nplot(vi_gbm)\n```\n\n::: {.cell-output-display}\n![Variable importance with AUC loss](ch06-xai-global_files/figure-html/fig-gbm-varimp-auc-1.png){#fig-gbm-varimp-auc width=672}\n:::\n:::\n\n\nWhile standard metrics like AUC are useful, sometimes you may want to evaluate the model using custom loss functions that better align with your specific goals (@fig-gbm-customloss). Here, we define two custom loss functions: `loss_logloss`, which computes the logarithmic loss (or logloss), and `loss_f1_score`, which is based on the F1 score.\n\n-   **Logarithmic Loss (Logloss)**: This function measures how well the predicted probabilities match the actual outcomes. Lower values indicate better performance.\n\n-   **F1 Score**: The F1 score balances precision and recall, especially important when dealing with imbalanced datasets. Here, we invert the F1 score so that lower values represent better performance, aligning with the convention used by `model_parts`.\n\n\n::: {#fig-gbm-customloss .cell layout=\"[1,1,1]\" hash='ch06-xai-global_cache/html/fig-gbm-customloss_642e88f30e3ac8c5c9d752b94295bdf0'}\n\n```{.r .cell-code}\n# Custom loss functions (same as in the previous code)}\nloss_logloss <- function(observed, predicted) {\n  -mean(observed * log(predicted) + (1 - observed) * log(1 - predicted))\n}\n\nloss_f1_score <- function(observed, predicted) {\n  predicted_class <- ifelse(predicted > 0.5, 1, 0)\n  TP <- sum(observed == 1 & predicted_class == 1)\n  FP <- sum(observed == 0 & predicted_class == 1)\n  FN <- sum(observed == 1 & predicted_class == 0)\n  F1 <- 2 * TP / (2 * TP + FP + FN)\n  return(1 - F1)  # return 1 - F1 to keep lower values better\n}\n\n# Variable importance with custom loss functions\nvi_gbm_logloss <- model_parts(explainer_gbm, loss_function = loss_logloss, \n                              B = 500, type = \"ratio\")\nplot(vi_gbm_logloss)\n\nvi_gbm_f1 <- model_parts(explainer_gbm, loss_function = loss_f1_score, B = 500)\nplot(vi_gbm_f1)\n\nvi_gbm_default <- model_parts(explainer_gbm, B = 500)\nplot(vi_gbm_default)\n```\n\n::: {.cell-output-display}\n![Custom Logloss Loss](ch06-xai-global_files/figure-html/fig-gbm-customloss-1.png){#fig-gbm-customloss-1 width=672}\n:::\n\n::: {.cell-output-display}\n![F1 Score Loss](ch06-xai-global_files/figure-html/fig-gbm-customloss-2.png){#fig-gbm-customloss-2 width=672}\n:::\n\n::: {.cell-output-display}\n![Default Loss (Logloss)](ch06-xai-global_files/figure-html/fig-gbm-customloss-3.png){#fig-gbm-customloss-3 width=672}\n:::\n\nGBM Variable Importance with Custom Loss functions\n:::\n\n\nWe can also generate the PDP and ALE plots for the GBM model (@fig-gbm-ale-pd)\n\n\n::: {#fig-gbm-ale-pd .cell layout=\"[1,1]\" hash='ch06-xai-global_cache/html/fig-gbm-ale-pd_14bd3af81a250124d394d78f2b9ab895'}\n\n```{.r .cell-code}\n# Partial Dependence Plots (PDP)\npdp_gbm <- model_profile(\n  explainer_gbm,\n  variables = colnames(test_data[, 1:11])\n)\nplot(pdp_gbm) + labs(subtitle = \"\")\n\n\n# Accumulated Local Effects (ALE) Plots\nale_gbm <- model_profile(\n  explainer_gbm,\n  type = \"accumulated\",\n  variables = colnames(test_data[, 1:11])\n)\nplot(ale_gbm) + labs(subtitle = \"\")\n```\n\n::: {.cell-output-display}\n![PDP](ch06-xai-global_files/figure-html/fig-gbm-ale-pd-1.png){#fig-gbm-ale-pd-1 width=960}\n:::\n\n::: {.cell-output-display}\n![ALE Plot](ch06-xai-global_files/figure-html/fig-gbm-ale-pd-2.png){#fig-gbm-ale-pd-2 width=960}\n:::\n\nGBM Partial dependence and ALE plots\n:::\n\n\n## Discussion and conclusions\n\nIn this chapter, we went through a tutorial of xAI. In particular, we demonstrated various techniques for making the inner workings of machine learning models clear and transparent. In doing so, we can understand and reveal the variables that significantly influence student outcomes, whether predicting grades or identifying at-risk learners. We demonstrated several variable importance measures, partial dependence plots, and accumulated local effects. This transparency is very important for ensuring fairness, accountability, and the effective implementation of AI in educational settings, empowering stakeholders to make informed decisions and build a deeper understanding of the underlying learning processes. Most importantly, with transparency comes trust and reliance. If the stake holders understand the inner workings of an algorithm, they will be fast to implement it and adopt such methods in their institutions [@mustafa2024].\n\nHowever, while xAI provides a powerful lens for understanding model behavior, it is essential to acknowledge its inherent limitations and the important role of human interpretation [@saqr2024; @Ribeiro2016-go; @Saqr2024-ie]. As our results suggest, even detailed explanations of individual algorithmic decisions might fall short when it comes to achieving genuine personalization or providing truly individualized support. Furthermore, instances where algorithms rely on seemingly \"wrong predictors\" to generate explanations highlight the potential pitfalls of a purely data-driven approach [@Saqr2024-ie]. This underscores that while AI can offer valuable insights and identify patterns, it cannot and must not operate in isolation.\n\nAs such we can say that the effectiveness of AI in education hinges on a collaborative approach where human expertise and contextual understanding complement the analytical power of algorithms [@Saqr2024-ie]. The ability to interpret explanations, identify potential biases or limitations, and integrate domain knowledge remains paramount, for a more detailed discussion please refer to the second chapter of this book[@LABOOK2_Chapter_2].\n\n\n::: {#refs}\n:::\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}