@article{Afzaal_Nouri_Zia_Papapetrou_Fors_Wu_Li_Weegar_2021,
	title        = {Explainable AI for Data-Driven Feedback and Intelligent Action Recommendations to Support Students Self-Regulation},
	author       = {Afzaal, Muhammad and Nouri, Jalal and Zia, Aayesha and Papapetrou, Panagiotis and Fors, Uno and Wu, Yongchao and Li, Xiu and Weegar, Rebecka},
	year         = 2021,
	month        = {Nov},
	journal      = {Frontiers in artificial intelligence},
	volume       = 4,
	pages        = 723447,
	doi          = {10.3389/frai.2021.723447},
	issn         = {2624-8212},
	url          = {http://dx.doi.org/10.3389/frai.2021.723447},
	abstractnote = {Formative feedback has long been recognised as an effective tool for student learning, and researchers have investigated the subject for decades. However, the actual implementation of formative feedback practices is associated with significant challenges because it is highly time-consuming for teachers to analyse students’ behaviours and to formulate and deliver effective feedback and action recommendations to support students’ regulation of learning. This paper proposes a novel approach that employs learning analytics techniques combined with explainable machine learning to provide automatic and intelligent feedback and action recommendations that support student’s self-regulation in a data-driven manner, aiming to improve their performance in courses. Prior studies within the field of learning analytics have predicted students’ performance and have used the prediction status as feedback without explaining the reasons behind the prediction. Our proposed method, which has been developed based on LMS data from a university course, extends this approach by explaining the root causes of the predictions and by automatically providing data-driven intelligent recommendations for action. Based on the proposed explainable machine learning-based approach, a dashboard that provides data-driven feedback and intelligent course action recommendations to students is developed, tested and evaluated. Based on such an evaluation, we identify and discuss the utility and limitations of the developed dashboard. According to the findings of the conducted evaluation, the dashboard improved students’ learning outcomes, assisted them in self-regulation and had a positive effect on their motivation.},
	keywords     = {AI; automatic data-driven feedback; dashboard; explainable machine learning-based approach; learning analytics; recommender system; self-regulated learning},
	language     = {en}
}
@article{Ala-Mutka_2005,
	title        = {A survey of automated assessment approaches for programming assignments},
	author       = {Ala-Mutka, Kirsti M.},
	year         = 2005,
	month        = {Jun},
	journal      = {Computer science education},
	publisher    = {Informa UK Limited},
	volume       = 15,
	number       = 2,
	pages        = {83–102},
	doi          = {10.1080/08993400500150747},
	issn         = {0899-3408},
	url          = {https://www.tandfonline.com/doi/abs/10.1080/08993400500150747},
	abstractnote = {Practical programming is one of the basic skills pursued in computer science education. On programming courses, the coursework consists of programming assignments that need to be assessed from different points of view. Since the submitted assignments are executable programs with a formal structure, some features can be assessed automatically. The basic requirement for automated assessment is the numerical measurability of assessment targets, but semiautomatic approaches can overcome this restriction. Recognizing automatically assessable features can help teachers to create educational models, where automatic tools let teachers concentrate their work on the learning issues that need student-teacher interaction the most. Several automatic tools for both static and dynamic assessment of computer programs have been reported in the literature. This article promotes these issues by surveying several automatic approaches for assessing programming assignments. Not all the existing tools will be covered, simply because of the vast number of them. The article concentrates on bringing forward different assessment techniques and approaches to give an interested reader starting points for finding further information in the area. Automatic assessment tools can be used to help teachers in grading tasks as well as to support students’ working process with automatic feedback. Common advantages of automation are the speed, availability, consistency and objectivity of assessment. However, automatic tools emphasize the need for careful pedagogical design of the assignment and assessment settings. To effectively share the knowledge and good assessment solutions already developed, better interoperability and portability of the tools is needed.},
	language     = {en}
}
@article{Ali_Murray_Momin_Dwivedi_Malik_2024,
	title        = {The effects of artificial intelligence applications in educational settings: Challenges and strategies},
	author       = {Ali, Omar and Murray, Peter A. and Momin, Mujtaba and Dwivedi, Yogesh K. and Malik, Tegwen},
	year         = 2024,
	month        = {Feb},
	journal      = {Technological forecasting and social change},
	publisher    = {Elsevier BV},
	volume       = 199,
	number       = 123076,
	pages        = 123076,
	doi          = {10.1016/j.techfore.2023.123076},
	issn         = {0040-1625},
	url          = {http://dx.doi.org/10.1016/j.techfore.2023.123076},
	abstractnote = {With the continuous intervention of AI tools in the education sector, new research is required to evaluate the viability and feasibility of extant AI …},
	language     = {en}
}
@article{Ali_Abuhmed_El-Sappagh_Muhammad_Alonso-Moral_Confalonieri_Guidotti_Del_Ser_Díaz-Rodríguez_Herrera_2023,
	title        = {Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence},
	author       = {Ali, Sajid and Abuhmed, Tamer and El-Sappagh, Shaker and Muhammad, Khan and Alonso-Moral, Jose M. and Confalonieri, Roberto and Guidotti, Riccardo and Del Ser, Javier and Díaz-Rodríguez, Natalia and Herrera, Francisco},
	year         = 2023,
	month        = {Nov},
	journal      = {An international journal on information fusion},
	publisher    = {Elsevier BV},
	volume       = 99,
	number       = 101805,
	pages        = 101805,
	doi          = {10.1016/j.inffus.2023.101805},
	issn         = {1566-2535},
	url          = {https://www.sciencedirect.com/science/article/pii/S1566253523001148},
	language     = {en}
}
@article{Andrejevic_Selwyn_2020,
	title        = {Facial recognition technology in schools: critical questions and concerns},
	author       = {Andrejevic, Mark and Selwyn, Neil},
	year         = 2020,
	month        = {Apr},
	journal      = {Learning, media and technology},
	publisher    = {Informa UK Limited},
	volume       = 45,
	number       = 2,
	pages        = {115–128},
	doi          = {10.1080/17439884.2020.1686014},
	issn         = {1743-9884},
	url          = {https://www.tandfonline.com/doi/abs/10.1080/17439884.2020.1686014},
	language     = {en}
}
@article{Baker_Hawn_2022,
	title        = {Algorithmic bias in education},
	author       = {Baker, Ryan S. and Hawn, Aaron},
	year         = 2022,
	month        = {Dec},
	journal      = {International journal of artificial intelligence in education},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 32,
	number       = 4,
	pages        = {1052–1092},
	doi          = {10.1007/s40593-021-00285-9},
	issn         = {1560-4292},
	url          = {https://link.springer.com/article/10.1007/s40593-021-00285-9},
	language     = {en}
}
@article{Bearman_Tai_Dawson_Boud_Ajjawi_2024,
	title        = {Developing evaluative judgement for a time of generative artificial intelligence},
	author       = {Bearman, Margaret and Tai, Joanna and Dawson, Phillip and Boud, David and Ajjawi, Rola},
	year         = 2024,
	month        = {Aug},
	journal      = {Assessment and evaluation in higher education},
	publisher    = {Informa UK Limited},
	volume       = 49,
	number       = 6,
	pages        = {893–905},
	doi          = {10.1080/02602938.2024.2335321},
	issn         = {0260-2938},
	url          = {https://www.tandfonline.com/doi/abs/10.1080/02602938.2024.2335321},
	abstractnote = {As generative artificial intelligence (AI) rapidly integrates into society, it becomes pressingly important for every user to be able to recognise the quality of its outputs. Generative AI is notew...},
	language     = {en}
}
@article{Bedué_Fritzsche_2022,
	title        = {Can we trust AI? An empirical investigation of trust requirements and guide to successful AI adoption},
	author       = {Bedué, Patrick and Fritzsche, Albrecht},
	year         = 2022,
	month        = {Mar},
	journal      = {Journal of enterprise information management},
	publisher    = {Emerald},
	volume       = 35,
	number       = 2,
	pages        = {530–549},
	doi          = {10.1108/jeim-06-2020-0233},
	issn         = {1741-0398},
	url          = {https://www.emerald.com/insight/content/doi/10.1108/JEIM-06-2020-0233/full/html},
	abstractnote = {Purpose Artificial intelligence (AI) fosters economic growth and opens up new directions for innovation. However, the diffusion of AI proceeds very slowly and falls behind, especially in comparison to other technologies. An important path leading to better adoption rates identified is trust-building. Particular requirements for trust and their relevance for AI adoption are currently insufficiently addressed.Design/methodology/approachTo close this gap, the authors follow a qualitative approach, drawing on the extended valence framework by assessing semi-structured interviews with experts from various companies.FindingsThe authors contribute to research by finding several subcategories for the three main trust dimensions ability, integrity and benevolence, thereby revealing fundamental differences for building trust in AI compared to more traditional technologies. In particular, the authors find access to knowledge, transparency, explainability, certification, as well as self-imposed standards and guidelines to be important factors that increase overall trust in AI.Originality/valueThe results show how the valence framework needs to be elaborated to become applicable to the AI context and provide further structural orientation to better understand AI adoption intentions. This may help decision-makers to identify further requirements or strategies to increase overall trust in their AI products, creating competitive and operational advantage.},
	language     = {en}
}
@article{Bogina_Hartman_Kuflik_Shulner-Tal_2022,
	title        = {Educating software and AI stakeholders about algorithmic fairness, accountability, transparency and ethics},
	author       = {Bogina, Veronika and Hartman, Alan and Kuflik, Tsvi and Shulner-Tal, Avital},
	year         = 2022,
	month        = {Sep},
	journal      = {International journal of artificial intelligence in education},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 32,
	number       = 3,
	pages        = {808–833},
	doi          = {10.1007/s40593-021-00248-0},
	issn         = {1560-4292},
	url          = {https://link.springer.com/article/10.1007/s40593-021-00248-0},
	language     = {en}
}
@article{Bond_Khosravi_De_Laat_Bergdahl_Negrea_Oxley_Pham_Chong_Siemens_2024,
	title        = {A meta systematic review of artificial intelligence in higher education: a call for increased ethics, collaboration, and rigour},
	author       = {Bond, Melissa and Khosravi, Hassan and De Laat, Maarten and Bergdahl, Nina and Negrea, Violeta and Oxley, Emily and Pham, Phuong and Chong, Sin Wang and Siemens, George},
	year         = 2024,
	month        = {Jan},
	journal      = {International journal of educational technology in higher education},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 21,
	number       = 1,
	doi          = {10.1186/s41239-023-00436-z},
	issn         = {2365-9440},
	url          = {http://dx.doi.org/10.1186/s41239-023-00436-z},
	abstractnote = {AbstractAlthough the field of Artificial Intelligence in Education (AIEd) has a substantial history as a research domain, never before has the rapid evolution of AI applications in education sparked such prominent public discourse. Given the already rapidly growing AIEd literature base in higher education, now is the time to ensure that the field has a solid research and conceptual grounding. This review of reviews is the first comprehensive meta review to explore the scope and nature of AIEd in higher education (AIHEd) research, by synthesising secondary research (e.g., systematic reviews), indexed in the Web of Science, Scopus, ERIC, EBSCOHost, IEEE Xplore, ScienceDirect and ACM Digital Library, or captured through snowballing in OpenAlex, ResearchGate and Google Scholar. Reviews were included if they synthesised applications of AI solely in formal higher or continuing education, were published in English between 2018 and July 2023, were journal articles or full conference papers, and if they had a method section 66 publications were included for data extraction and synthesis in EPPI Reviewer, which were predominantly systematic reviews (66.7%), published by authors from North America (27.3%), conducted in teams (89.4%) in mostly domestic-only collaborations (71.2%). Findings show that these reviews mostly focused on AIHEd generally (47.0%) or Profiling and Prediction (28.8%) as thematic foci, however key findings indicated a predominance of the use of Adaptive Systems and Personalisation in higher education. Research gaps identified suggest a need for greater ethical, methodological, and contextual considerations within future research, alongside interdisciplinary approaches to AIHEd application. Suggestions are provided to guide future primary and secondary research.},
	language     = {en}
}
@article{Chaudhry_Kazim_2022,
	title        = {Artificial Intelligence in Education (AIEd): a high-level academic and industry note 2021},
	author       = {Chaudhry, Muhammad Ali and Kazim, Emre},
	year         = 2022,
	journal      = {AI and ethics},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 2,
	number       = 1,
	pages        = {157–165},
	doi          = {10.1007/s43681-021-00074-z},
	issn         = {2730-5953},
	url          = {https://pmc.ncbi.nlm.nih.gov/articles/PMC8261391/},
	abstractnote = {In the past few decades, technology has completely transformed the world around us. Indeed, experts believe that the next big digital transformation in how we live, communicate, work, trade and learn will be driven by Artificial Intelligence (AI) \[83]. This paper presents a high-level industrial and academic overview of AI in Education (AIEd). It presents the focus of latest research in AIEd on reducing teachers’ workload, contextualized learning for students, revolutionizing assessments and developments in intelligent tutoring systems. It also discusses the ethical dimension of AIEd and the potential impact of the Covid-19 pandemic on the future of AIEd’s research and practice. The intended readership of this article is policy makers and institutional leaders who are looking for an introductory state of play in AIEd.},
	keywords     = {Artificial Intelligence; Artificial Intelligence in Education (AIEd); Education; Ethical AI; Fairness; Intelligent Tutoring Systems (ITS); Learning science; Machine learning},
	language     = {en}
}
@article{Chen_Xiang_Zhou_Jia_Shang_Li_Gašević_Fan_2025,
	title        = {Unpacking help-seeking process through multimodal learning analytics: A comparative study of ChatGPT vs Human expert},
	author       = {Chen, Angxuan and Xiang, Mengtong and Zhou, Junyi and Jia, Jiyou and Shang, Junjie and Li, Xinyu and Gašević, Dragan and Fan, Yizhou},
	year         = 2025,
	month        = {Mar},
	journal      = {Computers & education},
	publisher    = {Elsevier BV},
	volume       = 226,
	number       = 105198,
	pages        = 105198,
	doi          = {10.1016/j.compedu.2024.105198},
	issn         = {0360-1315},
	url          = {http://dx.doi.org/10.1016/j.compedu.2024.105198},
	abstractnote = {Help-seeking is an active learning strategy tied to self-regulated learning (SRL), where learners seek assistance when facing challenges. They may see…},
	language     = {en}
}
@article{Cope_Kalantzis_2023,
	title        = {A little history of e-learning: finding new ways to learn in the PLATO computer education system, 1959–1976},
	author       = {Cope, Bill and Kalantzis, Mary},
	year         = 2023,
	month        = {Jan},
	journal      = {History of education},
	publisher    = {Informa UK Limited},
	pages        = {1–32},
	doi          = {10.1080/0046760x.2022.2141353},
	issn         = {0046-760X},
	url          = {https://www.tandfonline.com/doi/abs/10.1080/0046760X.2022.2141353},
	abstractnote = {This article looks at pedagogical issues arising from computer-mediated learning by tracing the history of the PLATO computer-based learning system. Developed at the University of Illinois between ...},
	language     = {en}
}
@inproceedings{Crow_Luxton-Reilly_Wuensche_2018,
	title        = {Intelligent tutoring systems for programming education: a systematic review},
	author       = {Crow, Tyne and Luxton-Reilly, Andrew and Wuensche, Burkhard},
	year         = 2018,
	month        = {Jan},
	booktitle    = {Proceedings of the 20th Australasian Computing Education Conference},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ACE ’18},
	pages        = {53–62},
	doi          = {10.1145/3160489.3160492},
	isbn         = 9781450363402,
	url          = {https://doi.org/10.1145/3160489.3160492},
	abstractnote = {A variety of intelligent tutoring systems have been created for the purpose of teaching computer programming. Most published literature focuses on systems that have been developed to teach programming within tertiary courses. A majority of systems have been developed to teach introductory programming concepts; other systems tutor more specific aspects of programming like scope or recursion. Literature reports that these systems address many of the difficulties associated with teaching programming to novices; however, individual systems vary greatly, and there is a large range of supplementary features developed in these systems. Most intelligent programming tutors involve some form of interactive programming exercises, but the use of supplementary features like plans, quizzes and worked solutions vary greatly between different systems. This systematic review reports key information about existing systems and the prevalence of different features within them. An overview of how supplementary features are integrated into these systems is given, along with implications for how intelligent programming tutors could be improved by supporting a wider range of supplementary features.},
	collection   = {ACE ’18},
	keywords     = {programming education, novice programming, intelligent tutoring, computer science education}
}
@article{Cukurova_2024,
	title        = {The interplay of learning, analytics and artificial intelligence in education: A vision for hybrid intelligence},
	author       = {Cukurova, Mutlu},
	year         = 2024,
	month        = {Aug},
	journal      = {British journal of educational technology: journal of the Council for Educational Technology},
	publisher    = {Wiley},
	doi          = {10.1111/bjet.13514},
	issn         = {0007-1013},
	url          = {http://dx.doi.org/10.1111/bjet.13514},
	abstractnote = {AbstractThis paper presents a multidimensional view of AI’s role in education, emphasising the intricate interplay among AI, analytics and human learning processes. Here, I challenge the prevalent narrow conceptualisation of AI as tools in Education, exemplified in generative AI tools, and argue for the importance of alternative conceptualisations of AI for achieving human–AI hybrid intelligence. I highlight the differences between human intelligence and artificial information processing, the importance of hybrid human–AI systems to extend human cognition and posit that AI can also serve as an instrument for understanding human learning. Early learning sciences and AI in Education Research (AIED), which saw AI as an analogy for human intelligence, have diverged from this perspective, prompting a need to rekindle this connection. The paper presents three unique conceptualisations of AI: the externalisation of human cognition, the internalisation of AI models to influence human mental models and the extension of human cognition via tightly coupled human–AI hybrid intelligence systems. Examples from current research and practice are examined as instances of the three conceptualisations in education, highlighting the potential value and limitations of each conceptualisation for human competence development, as well as the perils of overemphasis on approaches that replace human learning opportunities with AI tools. The paper concludes with advocacy for a broader approach to AIED that goes beyond considerations on the design and development of AI and includes educating people about AI and innovating educational systems to remain relevant in an AI ubiquitous world.},
	language     = {en}
}
@article{Dörrenbächer_Perels_2016,
	title        = {Self-regulated learning profiles in college students: Their relationship to achievement, personality, and the effectiveness of an intervention to foster self-regulated learning},
	author       = {Dörrenbächer, Laura and Perels, Franziska},
	year         = 2016,
	month        = {Oct},
	journal      = {Learning and individual differences},
	publisher    = {Elsevier BV},
	volume       = 51,
	pages        = {229–241},
	doi          = {10.1016/j.lindif.2016.09.015},
	issn         = {1041-6080},
	url          = {http://dx.doi.org/10.1016/j.lindif.2016.09.015},
	abstractnote = {Self-regulated learning (SRL) is highly relevant for postsecondary academic success. Nevertheless, individual differences in SRL are found and can influence SRL training results. Conducting latent profile analyses with n = 337 college students, we found four SRL profiles that differed quantitatively and with regard to motivational subcomponents. Achievement was significantly higher for students with high SRL and high motivation. Moreover, the profiles differed with regard to personality as more skilled self-regulators showed lower test anxiety, lower neuroticism, and higher values in extraversion, conscientiousness, agreeableness, and openness to experiences. Using a sample of n = 55 students who participated in an eight-week SRL training, we investigated differential effects of the SRL profiles. Students with moderate and motivated SRL profiles benefited from the intervention, whereas students with low and high SRL profiles did not. The results speak in favor of developing adaptive training programs depending on SRL profiles.},
	language     = {en}
}
@article{Fichten_Pickup_Asunsion_Jorgensen_Vo_Legault_Libman_2022,
	title        = {State of the research on artificial intelligence based apps for post-secondary students with disabilities},
	author       = {Fichten, Catherine and Pickup, David and Asunsion, Jennison and Jorgensen, Mary and Vo, Christine and Legault, Anick and Libman, Eva},
	year         = 2022,
	month        = {Jan},
	journal      = {Exceptionality education international},
	publisher    = {University of Western Ontario, Western Libraries},
	volume       = 31,
	number       = 1,
	pages        = {62–76},
	doi          = {10.5206/eei.v31i1.14089},
	issn         = {1918-5227},
	url          = {http://dx.doi.org/10.5206/eei.v31i1.14089},
	abstractnote = {We conducted a general Google search and a scoping review of various types of artificial intelligence (AI) based technology – mobile, web-based, software, hardware – used by college and university students to do schoolwork. The main findings indicate that (1) there is no generally agreed upon definition of AI, and (2) there is a huge discrepancy between the popular press articles that are behind the AI hype and the scientific literature. The popular press provides an overview of the AI tools available to students with disabilities and discusses how students can use these tools. The scientific literature is primarily devoted to tool development and has poor methodology. We conclude that the potential of AI for post-secondary students with disabilities is enormous, but that informed research about these tools is scant, with a profound lack of demonstrated scalability. Research needs to address “real-world” uses of AI-based tools by post-secondary students with disabilities.}
}
@article{Gašević_Dawson_Rogers_Gasevic_2016,
	title        = {Learning analytics should not promote one size fits all: The effects of instructional conditions in predicting academic success},
	author       = {Gašević, Dragan and Dawson, Shane and Rogers, Tim and Gasevic, Danijela},
	year         = 2016,
	month        = {Jan},
	journal      = {The internet and higher education},
	publisher    = {Elsevier BV},
	volume       = 28,
	pages        = {68–84},
	doi          = {10.1016/j.iheduc.2015.10.002},
	issn         = {1096-7516},
	url          = {http://dx.doi.org/10.1016/j.iheduc.2015.10.002},
	abstractnote = {This study examined the extent to which instructional conditions influence the prediction of academic success in nine undergraduate courses offered in a blended learning model (n = 4134). The study illustrates the differences in predictive power and significant predictors between course-specific models and generalized predictive models. The results suggest that it is imperative for learning analytics research to account for the diverse ways technology is adopted and applied in course-specific contexts. The differences in technology use, especially those related to whether and how learners use the learning management system, require consideration before the log-data can be merged to create a generalized model for predicting academic success. A lack of attention to instructional conditions can lead to an over or under estimation of the effects of LMS features on students’ academic success. These findings have broader implications for institutions seeking generalized and portable models for identifying students at risk of academic failure.},
	language     = {en}
}
@article{Gillani_Eynon_Chiabaut_Finkel_2023,
	title        = {Unpacking the “Black Box” of AI in Education},
	author       = {Gillani, Nabeel and Eynon, Rebecca and Chiabaut, Catherine and Finkel, Kelsey},
	year         = 2023,
	journal      = {Journal of educational technology & society},
	publisher    = {International Forum of Educational Technology & Society, National Taiwan Normal University, Taiwan},
	volume       = 26,
	number       = 1,
	pages        = {99–111},
	issn         = {1176-3647},
	url          = {https://www.jstor.org/stable/48707970},
	abstractnote = {Recent advances in Artificial Intelligence (AI) have sparked renewed interest in its potential to improve education. However, AI is a loose umbrella term that refers to a collection of methods, capabilities, and limitations—many of which are often not explicitly articulated by researchers, education technology companies, or other AI developers. In this paper, we seek to clarify what “AI” is and the potential it holds to both advance and hamper educational opportunities that may improve the human condition. We offer a basic introduction to different methods and philosophies underpinning AI, discuss recent advances, explore applications to education, and highlight key limitations and risks. We conclude with a set of questions that educationalists may ask as they encounter AI in their research and practice. Our hope is to make often jargon-laden terms and concepts accessible, so that all are equipped to understand, interrogate, and ultimately shape the development of human-centered AI in education.}
}
@article{Guan_Mou_Jiang_2020,
	title        = {Artificial intelligence innovation in education: A twenty-year data-driven historical analysis},
	author       = {Guan, Chong and Mou, Jian and Jiang, Zhiying},
	year         = 2020,
	month        = {Dec},
	journal      = {International journal of innovation studies},
	publisher    = {Elsevier BV},
	volume       = 4,
	number       = 4,
	pages        = {134–147},
	doi          = {10.1016/j.ijis.2020.09.001},
	issn         = {2096-2487},
	url          = {https://www.sciencedirect.com/science/article/pii/S2096248720300369},
	language     = {en}
}
@article{Guidotti_2024,
	title        = {Counterfactual explanations and how to find them: literature review and benchmarking},
	author       = {Guidotti, Riccardo},
	year         = 2024,
	month        = {Sep},
	journal      = {Data mining and knowledge discovery},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 38,
	number       = 5,
	pages        = {2770–2824},
	doi          = {10.1007/s10618-022-00831-6},
	issn         = {1384-5810},
	url          = {https://link.springer.com/article/10.1007/s10618-022-00831-6},
	abstractnote = {AbstractInterpretable machine learning aims at unveiling the reasons behind predictions returned by uninterpretable classifiers. One of the most valuable types of explanation consists of counterfactuals. A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome. For instance, a bank customer asks for a loan that is rejected. The counterfactual explanation consists of what should have been different for the customer in order to have the loan accepted. Recently, there has been an explosion of proposals for counterfactual explainers. The aim of this work is to survey the most recent explainers returning counterfactual explanations. We categorize explainers based on the approach adopted to return the counterfactuals, and we label them according to characteristics of the method and properties of the counterfactuals returned. In addition, we visually compare the explanations, and we report quantitative benchmarking assessing minimality, actionability, stability, diversity, discriminative power, and running time. The results make evident that the current state of the art does not provide a counterfactual explainer able to guarantee all these properties simultaneously.},
	language     = {en}
}
@inproceedings{Hellas_Ihantola_Petersen_Ajanovski_Gutica_Hynninen_Knutas_Leinonen_Messom_Liao_2018,
	title        = {Predicting academic performance: a systematic literature review},
	author       = {Hellas, Arto and Ihantola, Petri and Petersen, Andrew and Ajanovski, Vangel V. and Gutica, Mirela and Hynninen, Timo and Knutas, Antti and Leinonen, Juho and Messom, Chris and Liao, Soohyun Nam},
	year         = 2018,
	month        = {Jul},
	booktitle    = {Proceedings Companion of the 23rd Annual ACM Conference on Innovation and Technology in Computer Science Education},
	publisher    = {ACM},
	address      = {New York, NY, USA},
	doi          = {10.1145/3293881.3295783},
	isbn         = 9781450362238,
	url          = {http://dx.doi.org/10.1145/3293881.3295783},
	abstractnote = {The ability to predict student performance in a course or program creates opportunities to improve educational outcomes. With effective performance prediction approaches, instructors can allocate resources and instruction more accurately. Research in this area seeks to identify features that can be used to make predictions, to identify algorithms that can improve predictions, and to quantify aspects of student performance. Moreover, research in predicting student performance seeks to determine interrelated features and to identify the underlying reasons why certain features work better than others. This working group report presents a systematic literature review of work in the area of predicting student performance. Our analysis shows a clearly increasing amount of research in this area, as well as an increasing variety of techniques used. At the same time, the review uncovered a number of issues with research quality that drives a need for the community to provide more detailed reporting of methods and results and to increase efforts to validate and replicate work.}
}
@article{Imani_Montazer_2019,
	title        = {A survey of emotion recognition methods with emphasis on E-Learning environments},
	author       = {Imani, Maryam and Montazer, Gholam Ali},
	year         = 2019,
	month        = {Dec},
	journal      = {Journal of network and computer applications},
	publisher    = {Elsevier BV},
	volume       = 147,
	number       = 102423,
	pages        = 102423,
	doi          = {10.1016/j.jnca.2019.102423},
	issn         = {1084-8045},
	url          = {https://www.sciencedirect.com/science/article/pii/S1084804519302759?casa_token=UdaZRpauvNQAAAAA:1r23_2mmwBGv59sJ7HcN3v4EqSPsLekv-xNEvaiM5E4Jdz88UvLrfyI28u5eFC1_SHdGbkBpCw},
	language     = {en}
}
@unpublished{Jacovi_Swayamdipta_Ravfogel_Elazar_Choi_Goldberg_2021,
	title        = {Contrastive Explanations for Model Interpretability},
	author       = {Jacovi, Alon and Swayamdipta, Swabha and Ravfogel, Shauli and Elazar, Yanai and Choi, Yejin and Goldberg, Yoav},
	year         = 2021,
	month        = {Mar},
	journal      = {arXiv \[cs.CL]},
	url          = {http://arxiv.org/abs/2103.01378},
	abstractnote = {Contrastive explanations clarify why an event occurred in contrast to another. They are more inherently intuitive to humans to both produce and comprehend. We propose a methodology to produce contrastive explanations for classification models by modifying the representation to disregard non-contrastive information, and modifying model behavior to only be based on contrastive reasoning. Our method is based on projecting model representation to a latent space that captures only the features that are useful (to the model) to differentiate two potential decisions. We demonstrate the value of contrastive explanations by analyzing two different scenarios, using both high-level abstract concept attribution and low-level input token/span attribution, on two widely used text classification tasks. Specifically, we produce explanations for answering: for which label, and against which alternative label, is some aspect of the input useful? And which aspects of the input are useful for and against particular decisions? Overall, our findings shed light on the ability of label-contrastive explanations to provide a more accurate and finer-grained interpretability of a model’s decision.}
}
@article{Jukiewicz_2024,
	title        = {The future of grading programming assignments in education: The role of ChatGPT in automating the assessment and feedback process},
	author       = {Jukiewicz, Marcin},
	year         = 2024,
	month        = {Jun},
	journal      = {Thinking skills and creativity},
	publisher    = {Elsevier BV},
	volume       = 52,
	number       = 101522,
	pages        = 101522,
	doi          = {10.1016/j.tsc.2024.101522},
	issn         = {1871-1871},
	url          = {http://dx.doi.org/10.1016/j.tsc.2024.101522},
	abstractnote = {This research evaluated ChatGPT’s potential as a tool for grading programming tasks, exploring its capability to understand and assess code quality. T…},
	language     = {en}
}
@article{Khalil_Prinsloo_Slade_2023,
	title        = {Fairness, trust, transparency, equity, and responsibility in learning analytics},
	author       = {Khalil, Mohammad and Prinsloo, Paul and Slade, Sharon},
	year         = 2023,
	month        = {Mar},
	journal      = {Journal of learning analytics},
	publisher    = {Society for Learning Analytics Research},
	volume       = 10,
	number       = 1,
	pages        = {1–7},
	doi          = {10.18608/jla.2023.7983},
	issn         = {1929-7750},
	url          = {http://dx.doi.org/10.18608/jla.2023.7983},
	abstractnote = {Learning analytics has the capacity to provide potential benefit to a wide range of stakeholders within a range of educational contexts. It can provide prompt support to students, facilitate effective teaching, highlight aspects of course content that might be adapted, and predict a range of possible outcomes, such as students registering for more appropriate courses, supporting students’ self-efficacy, or redesigning a course’s pedagogical strategy. It will do all these things based on the assumptions and rules that learning analytics developers set out. As such, learning analytics can exacerbate existing inequalities such as unequal access to support or opportunities based on (any combination of) race, gender, culture, age, socioeconomic status, etc., or work to overcome the impact of such inequalities on realizing student potential. In this editorial, we introduce several selected articles that explore the principles of fairness, equity, and responsibility in the context of learning analytics. We discuss existing research and summarize the papers within this special section to outline what is known, and what remains to be explored. This editorial concludes by celebrating the breadth of work set out here, but also by suggesting that there are no simple answers to ensuring fairness, trust, transparency, equity, and responsibility in learning analytics. More needs to be done to ensure that our mutual understanding of responsible learning analytics continues to be embedded in the learning analytics research and design practice.}
}
@article{Khosravi_Shum_Chen_Conati_Tsai_Kay_Knight_Martinez-Maldonado_Sadiq_Gašević_2022,
	title        = {Explainable artificial intelligence in education},
	author       = {Khosravi, Hassan and Shum, Simon Buckingham and Chen, Guanliang and Conati, Cristina and Tsai, Yi-Shan and Kay, Judy and Knight, Simon and Martinez-Maldonado, Roberto and Sadiq, Shazia and Gašević, Dragan},
	year         = 2022,
	journal      = {Computers and Education: Artificial Intelligence},
	publisher    = {Elsevier BV},
	volume       = 3,
	number       = 100074,
	pages        = 100074,
	doi          = {10.1016/j.caeai.2022.100074},
	issn         = {2666-920X},
	url          = {http://dx.doi.org/10.1016/j.caeai.2022.100074},
	language     = {en}
}
@article{Kumar_Boulanger_2020,
	title        = {Explainable automated essay scoring: Deep learning really has pedagogical value},
	author       = {Kumar, Vivekanandan and Boulanger, David},
	year         = 2020,
	month        = {Oct},
	journal      = {Frontiers in education},
	publisher    = {Frontiers Media SA},
	volume       = 5,
	pages        = 572367,
	doi          = {10.3389/feduc.2020.572367},
	issn         = {2504-284X},
	url          = {https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2020.572367/pdf},
	abstractnote = {Automated essay scoring (AES) is a compelling topic in Learning Analytics for the primary reason that recent advances in AI find it as a good testbed to explore artificial supplementation of human creativity. However, a vast swath of research tackles AES only holistically; few have even developed AES models at the rubric level, the very first layer of explanation underlying the prediction of holistic scores. Consequently, the AES black box has remained impenetrable. Although several algorithms from Explainable Artificial Intelligence have recently been published, no research has yet investigated the role that these explanation models can play in: (a) discovering the decision-making process that drives AES, (b) fine-tuning predictive models to improve generalizability and interpretability, and (c) providing personalized, formative, and fine-grained feedback to students during the writing process. Building on previous studies where models were trained to predict both the holistic and rubric scores of essays, using the Automated Student Assessment Prize’s essay datasets, this study focuses on predicting the quality of the writing style of Grade-7 essays and exposes the decision processes that lead to these predictions. In doing so, it evaluates the impact of deep learning (multi-layer perceptron neural networks) on the performance of AES. It has been found that the effect of deep learning can be best viewed when assessing the trustworthiness of explanation models. As more hidden layers were added to the neural network, the descriptive accuracy increased by about 10%. This study shows that faster (up to three orders of magnitude) SHAP implementations are as accurate as the slower model-agnostic one. It leverages the state-of-the-art in natural language processing, applying feature selection on a pool of 1592 linguistic indices that measure aspects of text cohesion, lexical diversity, lexical sophistication, and syntactic sophistication and complexity. In addition to the list of most globally important features, this study reports (a) a list of features that are important for a specific essay (locally), (b) a range of values for each feature that contribute to higher or lower rubric scores, and (c) a model that allows to quantify the impact of the implementation of formative feedback.},
	keywords     = {Explainable artificial intelligence; Shap; automated essay scoring; deep learning; Trust; Learning analytics; Feedback; rubric; style},
	language     = {en}
}
@article{Labadze_Grigolia_Machaidze_2023,
	title        = {Role of AI chatbots in education: systematic literature review},
	author       = {Labadze, Lasha and Grigolia, Maya and Machaidze, Lela},
	year         = 2023,
	month        = {Oct},
	journal      = {International journal of educational technology in higher education},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 20,
	number       = 1,
	doi          = {10.1186/s41239-023-00426-1},
	issn         = {2365-9440},
	url          = {http://dx.doi.org/10.1186/s41239-023-00426-1},
	abstractnote = {AbstractAI chatbots shook the world not long ago with their potential to revolutionize education systems in a myriad of ways. AI chatbots can provide immediate support by answering questions, offering explanations, and providing additional resources. Chatbots can also act as virtual teaching assistants, supporting educators through various means. In this paper, we try to understand the full benefits of AI chatbots in education, their opportunities, challenges, potential limitations, concerns, and prospects of using AI chatbots in educational settings. We conducted an extensive search across various academic databases, and after applying specific predefined criteria, we selected a final set of 67 relevant studies for review. The research findings emphasize the numerous benefits of integrating AI chatbots in education, as seen from both students’ and educators’ perspectives. We found that students primarily gain from AI-powered chatbots in three key areas: homework and study assistance, a personalized learning experience, and the development of various skills. For educators, the main advantages are the time-saving assistance and improved pedagogy. However, our research also emphasizes significant challenges and critical factors that educators need to handle diligently. These include concerns related to AI applications such as reliability, accuracy, and ethical considerations.},
	language     = {en}
}
@inbook{Lamti_El_Malhi_Sekhsoukh_Kerzazi_2024,
	title        = {Intelligent tutoring system for medical students: Opportunities and challenges},
	author       = {Lamti, Sanae and El Malhi, Marouane and Sekhsoukh, Rachid and Kerzazi, Noureddine},
	year         = 2024,
	booktitle    = {Information Systems Engineering and Management},
	publisher    = {Springer Nature Switzerland},
	address      = {Cham},
	pages        = {242–251},
	doi          = {10.1007/978-3-031-66850-0_27},
	isbn         = 9783031668494,
	issn         = {3004-958X},
	url          = {https://link.springer.com/chapter/10.1007/978-3-031-66850-0_27},
	language     = {en}
}
@article{Leichtmann_Humer_Hinterreiter_Streit_Mara_2023,
	title        = {Effects of Explainable Artificial Intelligence on trust and human behavior in a high-risk decision task},
	author       = {Leichtmann, Benedikt and Humer, Christina and Hinterreiter, Andreas and Streit, Marc and Mara, Martina},
	year         = 2023,
	month        = {Feb},
	journal      = {Computers in human behavior},
	publisher    = {Elsevier BV},
	volume       = 139,
	number       = 107539,
	pages        = 107539,
	doi          = {10.1016/j.chb.2022.107539},
	issn         = {0747-5632},
	url          = {http://dx.doi.org/10.1016/j.chb.2022.107539},
	abstractnote = {Understanding the recommendations of an artificial intelligence (AI) based assistant for decision-making is especially important in high-risk tasks, s…},
	language     = {en}
}
@article{Lin_Huang_Lu_2023,
	title        = {Artificial intelligence in intelligent tutoring systems toward sustainable education: a systematic review},
	author       = {Lin, Chien-Chang and Huang, Anna Y. Q. and Lu, Owen H. T.},
	year         = 2023,
	month        = {Aug},
	journal      = {Smart learning environments},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 10,
	number       = 1,
	pages        = {1–22},
	doi          = {10.1186/s40561-023-00260-y},
	issn         = {2196-7091},
	url          = {https://slejournal.springeropen.com/articles/10.1186/s40561-023-00260-y},
	abstractnote = {AbstractSustainable education is a crucial aspect of creating a sustainable future, yet it faces several key challenges, including inadequate infrastructure, limited resources, and a lack of awareness and engagement. Artificial intelligence (AI) has the potential to address these challenges and enhance sustainable education by improving access to quality education, creating personalized learning experiences, and supporting data-driven decision-making. One outcome of using AI and Information Technology (IT) systems in sustainable education is the ability to provide students with personalized learning experiences that cater to their unique learning styles and preferences. Additionally, AI systems can provide teachers with data-driven insights into student performance, emotions, and engagement levels, enabling them to tailor their teaching methods and approaches or provide assistance or intervention accordingly. However, the use of AI and IT systems in sustainable education also presents challenges, including issues related to privacy and data security, as well as potential biases in algorithms and machine learning models. Moreover, the deployment of these systems requires significant investments in technology and infrastructure, which can be a challenge for educators. In this review paper, we will provide different perspectives from educators and information technology solution architects to connect education and AI technology. The discussion areas include sustainable education concepts and challenges, technology coverage and outcomes, as well as future research directions. By addressing these challenges and pursuing further research, we can unlock the full potential of these technologies and support a more equitable and sustainable education system.},
	language     = {en}
}
@article{Liz-Domínguez_Caeiro-Rodríguez_Llamas-Nistal_Mikic-Fonte_2019,
	title        = {Systematic literature review of predictive analysis tools in higher education},
	author       = {Liz-Domínguez, Martín and Caeiro-Rodríguez, Manuel and Llamas-Nistal, Martín and Mikic-Fonte, Fernando A.},
	year         = 2019,
	month        = {Dec},
	journal      = {Applied sciences (Basel, Switzerland)},
	publisher    = {MDPI AG},
	volume       = 9,
	number       = 24,
	pages        = 5569,
	doi          = {10.3390/app9245569},
	issn         = {2076-3417},
	url          = {http://dx.doi.org/10.3390/app9245569},
	abstractnote = {The topic of predictive algorithms is often regarded among the most relevant fields of study within the data analytics discipline. They have applications in multiple contexts, education being an important one of them. Focusing on higher education scenarios, most notably universities, predictive analysis techniques are present in studies that estimate academic outcomes using different kinds of student-related data. Furthermore, predictive algorithms are the basis of tools such as early warning systems (EWS): applications able to foresee future risks, such as the likelihood of students failing or dropping out of a course, and alert of such risks so that corrective measures can be taken. The purpose of this literature review is to provide an overview of the current state of research activity regarding predictive analytics in higher education, highlighting the most relevant instances of predictors and EWS that have been used in practice. The PRISMA guidelines for systematic literature reviews were followed in this study. The document search process yielded 1382 results, out of which 26 applications were selected as relevant examples of predictors and EWS, each of them defined by the contexts where they were applied and the data that they used. However, one common shortcoming is that they are usually applied in limited scenarios, such as a single course, evidencing that building a predictive application able to work well under different teaching and learning methodologies is an arduous task.},
	language     = {en}
}
@article{López-Pernas_Saqr_2024,
	title        = {How the dynamics of engagement explain the momentum of achievement and the inertia of disengagement: A complex systems theory approach},
	author       = {López-Pernas, Sonsoles and Saqr, Mohammed},
	year         = 2024,
	month        = {Apr},
	journal      = {Computers in human behavior},
	volume       = 153,
	pages        = 108126,
	doi          = {10.1016/j.chb.2023.108126},
	issn         = {0747-5632},
	url          = {https://www.sciencedirect.com/science/article/pii/S0747563223004776},
	abstractnote = {Engagement can be understood as a complex dynamic system that unfolds over time and interacts within the student, school, and environment. Most of the research on the dynamics of engagement comes from classroom settings and it is so far inconclusive how and why engagement and disengagement evolve over time. Using person-centered methods, sequence, transition, and covariate analysis, we examined a large dataset of 18 consecutive courses of 245 students over a full program. We identified three engagement states (active, average, and disengaged), as well as three distinct longitudinal engagement trajectories (engaged, fluctuating, and mostly disengaged). Taken together, our results showed that engagement trajectories are rather stable over time conforming to the universal dynamics of complex systems. Engaged students were driven by course materials, their achievement, and their previous engaged states (momentum). Most importantly, our results offer a novel theoretical grounding for the understanding of disengagement which has so far remained unexplained. According to our results, disengagement follows the dynamics of a complex system where stability does not require a hard-wired causal mechanism but rather, it is an attractor state that pulls the system to settle in (inertia). Thus, disengagement becomes a hard to change equilibrium state for those students (or a stuck state).},
	keywords     = {Learning analytics; Engagement; Markov modeling; Sequence analysis; Longitudinal study; Complex dynamic systems; Individual differences; Disengagement}
}
@inbook{Lundberg_Lee_2017,
	title        = {A Unified Approach to Interpreting Model Predictions},
	author       = {Lundberg, Scott M. and Lee, Su-In},
	year         = 2017,
	booktitle    = {Advances in Neural Information Processing Systems 30},
	publisher    = {Curran Associates, Inc.},
	pages        = {4765–4774},
	url          = {http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf},
	editor       = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.}
}
@misc{Lytton_2024,
	title        = {AI hiring tools may be filtering out the best job applicants},
	author       = {Lytton, Charlotte},
	year         = 2024,
	month        = {Feb},
	journal      = {BBC},
	url          = {https://www.bbc.com/worklife/article/20240214-ai-recruiting-hiring-software-bias-discrimination},
	abstractnote = {As firms increasingly rely on artificial intelligence-driven hiring platforms, many highly qualified candidates are finding themselves on the cutting room floor.},
	language     = {en}
}
@inbook{Madaio_Blodgett_Mayfield_Dixon-Román_2022,
	title        = {Beyond “fairness”},
	author       = {Madaio, Michael and Blodgett, Su Lin and Mayfield, Elijah and Dixon-Román, Ezekiel},
	year         = 2022,
	month        = {Aug},
	booktitle    = {The Ethics of Artificial Intelligence in Education},
	publisher    = {Routledge},
	address      = {New York},
	pages        = {203–239},
	doi          = {10.4324/9780429329067-11},
	isbn         = 9780429329067,
	url          = {https://api.taylorfrancis.com/content/chapters/edit/download?identifierName=doi&identifierValue=10.4324/9780429329067-11&type=chapterpdf},
	edition      = {1st Edition},
	abstractnote = {Educational technologies, and the systems of schooling in which they are deployed, enact particular ideologies about what is important to know and how learners should learn. As Artificial Intelligence—in education and beyond—may contribute to inequitable outcomes for marginalized communities, approaches have been developed to evaluate and mitigate AI’s harmful impacts. However, we argue in this chapter that the dominant paradigm of evaluating fairness on the basis of performance disparities in AI models is inadequate for confronting the systemic inequities that educational AI systems (re)produce. We draw on lenses of structural injustice informed by critical theory and Black feminist scholarship to critically interrogate several widely studied and adopted categories of educational AI; and we explore how they are bound up in and reproduce historical legacies of structural injustice and inequity, regardless of the parity of their models’ performance. We close with alternative visions for a more equitable future for educational AI.}
}
@article{McCallum_2024,
	title        = {Payout for Uber Eats driver over face scan bias case},
	author       = {McCallum, Shiona},
	year         = 2024,
	month        = {Mar},
	journal      = {BBC News},
	publisher    = {BBC News},
	url          = {https://www.bbc.com/news/technology-68655429},
	abstractnote = {A black delivery driver’s account was removed after facial-recognition software failed to recognise him.}
}
@article{Miller_2019,
	title        = {Explanation in artificial intelligence: Insights from the social sciences},
	author       = {Miller, Tim},
	year         = 2019,
	month        = {Feb},
	journal      = {Artificial intelligence},
	publisher    = {Elsevier BV},
	volume       = 267,
	pages        = {1–38},
	doi          = {10.1016/j.artint.2018.07.007},
	issn         = {0004-3702},
	url          = {https://www.sciencedirect.com/science/article/pii/S0004370218305988},
	language     = {en}
}
@inproceedings{Miller_2023,
	title        = {Explainable AI is Dead, Long Live Explainable AI!: Hypothesis-driven Decision Support using Evaluative AI},
	author       = {Miller, Tim},
	year         = 2023,
	month        = {Jun},
	booktitle    = {2023 ACM Conference on Fairness, Accountability, and Transparency},
	publisher    = {ACM},
	address      = {New York, NY, USA},
	pages        = {333–342},
	doi          = {10.1145/3593013.3594001},
	url          = {https://dl.acm.org/doi/10.1145/3593013.3594001}
}
@inbook{Monett_Lewis_2018,
	title        = {Getting clarity by defining artificial intelligence—A survey},
	author       = {Monett, Dagmar and Lewis, Colin W. P.},
	year         = 2018,
	booktitle    = {Studies in Applied Philosophy, Epistemology and Rational Ethics},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	series       = {Studies in applied philosophy, epistemology and rational ethics},
	pages        = {212–214},
	doi          = {10.1007/978-3-319-96448-5_21},
	isbn         = 9783319964478,
	issn         = {2192-6255},
	url          = {http://dx.doi.org/10.1007/978-3-319-96448-5_21},
	collection   = {Studies in applied philosophy, epistemology and rational ethics}
}
@article{Mustafa_Tlili_Lampropoulos_Huang_Jandrić_Zhao_Salha_Xu_Panda_Kinshuk_et_al_2024,
	title        = {A systematic review of literature reviews on artificial intelligence in education (AIED): a roadmap to a future research agenda},
	author       = {Mustafa, Muhammad Yasir and Tlili, Ahmed and Lampropoulos, Georgios and Huang, Ronghuai and Jandrić, Petar and Zhao, Jialu and Salha, Soheil and Xu, Lin and Panda, Santosh and Kinshuk and López-Pernas, Sonsoles and Saqr, Mohammed},
	year         = 2024,
	month        = {Dec},
	journal      = {Smart learning environments},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 11,
	number       = 1,
	pages        = {1–33},
	doi          = {10.1186/s40561-024-00350-5},
	issn         = {2196-7091},
	url          = {https://scholar.google.com/citations?view_op=view_citation&hl=en&citation_for_view=67eUI4gAAAAJ:BUYA1_V_uYcC},
	abstractnote = {AbstractDespite the increased adoption of Artificial Intelligence in Education (AIED), several concerns are still associated with it. This has motivated researchers to conduct (systematic) reviews aiming at synthesizing the AIED findings in the literature. However, these AIED reviews are diversified in terms of focus, stakeholders, educational level and region, and so on. This has made the understanding of the overall landscape of AIED challenging. To address this research gap, this study proceeds one step forward by systematically meta-synthesizing the AIED literature reviews. Specifically, 143 literature reviews were included and analyzed according to the technology-based learning model. It is worth noting that most of the AIED research has been from China and the U.S. Additionally, when discussing AIED, strong focus was on higher education, where less attention is paid to special education. The results also reveal that AI is used mostly to support teachers and students in education with less focus on other educational stakeholders (e.g. school leaders or administrators). The study provides a possible roadmap for future research agenda on AIED, facilitating the implementation of effective and safe AIED.},
	language     = {en}
}
@article{Pinto_Abreu_Costa_Paiva_2023,
	title        = {How machine learning (ML) is transforming higher education: A systematic literature review},
	author       = {Pinto, Agostinho Sousa and Abreu, António and Costa, Eusébio and Paiva, Jerónimo},
	year         = 2023,
	month        = {Apr},
	journal      = {Journal of information systems engineering & management},
	publisher    = {International Association for Digital Transformation and Technological Innovation},
	volume       = 8,
	number       = 2,
	pages        = 21168,
	doi          = {10.55267/iadt.07.13227},
	issn         = {2468-2071},
	url          = {http://dx.doi.org/10.55267/iadt.07.13227},
	abstractnote = {In the last decade, artificial intelligence (AI), machine learning (ML) and learning data analytics have been introduced with great effect in the field of higher education. However, despite the potential benefits for higher education institutions (HIE´s) of these emerging technologies, most of them are still in the early stages of adoption of these technologies. Thus, a systematic literature review (SLR) on the literature published over the last 5 years on potential applications of machine learning in higher education is necessary. Following the PRISMA guidelines, out of the 1887 initially identified SCOPUS-indexed publications on the topic, 171 articles were selected for review. To screen the abstracts and titles of each citation, Rayyan QCRI was used. VOSViewer, a software tool for constructing and visualizing bibliometric networks, and Microsoft Excel were used to generate charts and figures. The findings show that the most widely researched application of ML in higher education is related to the prediction of academic performance and employability of students. The implications will be invaluable for researchers and practitioners to explore how ML and AI technologies ,in the era of ChatGPT, can be used in universities without jeopardizing academic integrity.}
}
@article{Ramesh_Sanampudi_2022,
	title        = {An automated essay scoring systems: a systematic literature review},
	author       = {Ramesh, Dadi and Sanampudi, Suresh Kumar},
	year         = 2022,
	journal      = {Artificial intelligence review},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 55,
	number       = 3,
	pages        = {2495–2527},
	doi          = {10.1007/s10462-021-10068-2},
	issn         = {0269-2821},
	url          = {http://dx.doi.org/10.1007/s10462-021-10068-2},
	abstractnote = {Assessment in the Education system plays a significant role in judging student performance. The present evaluation system is through human assessment. As the number of teachers’ student ratio is gradually increasing, the manual evaluation process becomes complicated. The drawback of manual evaluation is that it is time-consuming, lacks reliability, and many more. This connection online examination system evolved as an alternative tool for pen and paper-based methods. Present Computer-based evaluation system works only for multiple-choice questions, but there is no proper evaluation system for grading essays and short answers. Many researchers are working on automated essay grading and short answer scoring for the last few decades, but assessing an essay by considering all parameters like the relevance of the content to the prompt, development of ideas, Cohesion, and Coherence is a big challenge till now. Few researchers focused on Content-based evaluation, while many of them addressed style-based assessment. This paper provides a systematic literature review on automated essay scoring systems. We studied the Artificial Intelligence and Machine Learning techniques used to evaluate automatic essay scoring and analyzed the limitations of the current studies and research trends. We observed that the essay evaluation is not done based on the relevance of the content and coherence. SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1007/s10462-021-10068-2.},
	keywords     = {Assessment; Deep learning; Essay grading; Natural language processing; Short answer scoring},
	language     = {en}
}
@inproceedings{Ribeiro_Singh_Guestrin_2016,
	title        = {Why should I trust you?: Explaining the predictions of any classifier},
	author       = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	year         = 2016,
	month        = {Aug},
	booktitle    = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	publisher    = {ACM},
	address      = {New York, NY, USA},
	doi          = {10.1145/2939672.2939778},
	isbn         = 9781450342322,
	url          = {https://dl.acm.org/doi/10.1145/2939672.2939778},
	language     = {en}
}
@unpublished{Roshan_Zafar_2023,
	title        = {Using kernel SHAP XAI method to optimize the network anomaly detection model},
	author       = {Roshan, Khushnaseeb and Zafar, Aasim},
	year         = 2023,
	month        = {Jul},
	journal      = {arXiv \[cs.LG]},
	url          = {https://ieeexplore.ieee.org/abstract/document/9763241/},
	abstractnote = {Anomaly detection and its explanation is important in many research areas such as intrusion detection, fraud detection, unknown attack detection in network traffic and logs. It is challenging to identify the cause or explanation of why one instance is an anomaly? and the other is not due to its unbounded and lack of supervisory nature. The answer to this question is possible with the emerging technique of explainable artificial intelligence (XAI). XAI provides tools and techniques to interpret and explain the output and working of complex models such as Deep Learning (DL). This paper aims to detect and explain network anomalies with XAI, kernelSHAP method. The same approach is used to improve the network anomaly detection model in terms of accuracy, recall, precision and f score. The experiment is conduced with the latest CICIDS2017 dataset. Two models are created (Model_1 and OPT_Model) and compared. The overall accuracy and F score of OPT_Model (when trained in unsupervised way) are 0.90 and 0.76, respectively.}
}
@article{Saeed_Omlin_2023,
	title        = {Explainable AI (XAI): A systematic meta-survey of current challenges and future opportunities},
	author       = {Saeed, Waddah and Omlin, Christian},
	year         = 2023,
	month        = {Mar},
	journal      = {Knowledge-based systems},
	publisher    = {Elsevier BV},
	volume       = 263,
	number       = 110273,
	pages        = 110273,
	doi          = {10.1016/j.knosys.2023.110273},
	issn         = {0950-7051},
	url          = {http://dx.doi.org/10.1016/j.knosys.2023.110273},
	abstractnote = {The past decade has seen significant progress in artificial intelligence (AI), which has resulted in algorithms being adopted for resolving a variety …},
	language     = {en}
}
@article{Saqr_Cheng_López-Pernas_Beck_2024,
	title        = {Idiographic artificial intelligence to explain students’ self-regulation: Toward precision education},
	author       = {Saqr, Mohammed and Cheng, Rongxin and López-Pernas, Sonsoles and Beck, Emorie D.},
	year         = 2024,
	month        = {Aug},
	journal      = {Learning and individual differences},
	publisher    = {Elsevier BV},
	volume       = 114,
	number       = 102499,
	pages        = 102499,
	doi          = {10.1016/j.lindif.2024.102499},
	issn         = {1041-6080},
	url          = {http://dx.doi.org/10.1016/j.lindif.2024.102499},
	language     = {en}
}
@article{Saqr_Jovanović_Viberg_Gašević_2022,
	title        = {Is there order in the mess? A single paper meta-analysis approach to identification of predictors of success in learning analytics},
	author       = {Saqr, Mohammed and Jovanović, Jelena and Viberg, Olga and Gašević, Dragan},
	year         = 2022,
	month        = {Dec},
	journal      = {Studies in Higher Education},
	publisher    = {Routledge},
	volume       = 47,
	number       = 12,
	pages        = {2370–2391},
	doi          = {10.1080/03075079.2022.2061450},
	issn         = {0307-5079},
	url          = {https://doi.org/10.1080/03075079.2022.2061450},
	abstractnote = {ABSTRACTPredictors of student academic success do not always replicate well across different learning designs, subject areas, or educational institutions. This suggests that characteristics of a particular discipline and learning design have to be carefully considered when creating predictive models in order to scale up learning analytics. This study aimed to examine if and to what extent frequently used predictors of study success are portable across a homogenous set of courses. The research was conducted in an integrated blended problem-based curriculum with trace data (n?=?2,385 students) from 50 different course offerings across four academic years. We applied the statistical method of single paper meta-analysis to combine correlations of several indicators with students? success. Total activity and the forum indicators exhibited the highest prediction intervals, where the former represented proxies of the overall engagement with online tasks, and the latter with online collaborative learning activities. Indicators of lecture reading (frequency of lecture view) showed statistically insignificant prediction intervals and, therefore, are less likely to be portable across course offerings. The findings show moderate amounts of variability both within iterations of the same course and across courses. The results suggest that the use of the meta-analytic statistical method for the examination of study success indicators across courses with similar learning design and subject area can offer valuable quantitative means for the identification of predictors that reasonably well replicate and consequently can be reliably portable in the future.}
}
@article{Saqr_López-Pernas_2022,
	title        = {How CSCL roles emerge, persist, transition, and evolve over time: A four-year longitudinal study},
	author       = {Saqr, Mohammed and López-Pernas, Sonsoles},
	year         = 2022,
	journal      = {Computers & education},
	pages        = 104581,
	doi          = {10.1016/j.compedu.2022.104581},
	issn         = {0360-1315},
	url          = {http://dx.doi.org/10.1016/j.compedu.2022.104581}
}
@article{Saqr_López-Pernas_2024,
	title        = {Why explainable AI may not be enough: predictions and mispredictions in decision making in education},
	author       = {Saqr, Mohammed and López-Pernas, Sonsoles},
	year         = 2024,
	journal      = {Smart learning environments},
	number       = {in–press},
	doi          = {10.1186/s40561-024-00343-4},
	issn         = {2196-7091},
	url          = {http://dx.doi.org/10.1186/s40561-024-00343-4},
	abstractnote = {In learning analytics and in education at large, AI explanations are always computed from aggregate data of all the students to offer the “average” picture. Whereas the average may work for most students, it does not reflect or capture the individual differences or the variability among students. Therefore, instance-level predictions— where explanations for each particular student are presented according to their own data—may help understand how and why predictions were estimated and how a student or teacher may act or make decisions. This study aims to examine the utility of individualized instance-level AI, its value in informing decision-making, and—more importantly—how they can be used to offer personalized feedback. Furthermore, the study examines mispredictions, their explanations and how they offer explanations or affect decision making. Using data from a full course with 126 students, five ML algorithms were implemented with explanatory mechanisms, compared and the best performing algorithm (Random Forest) was therefore selected. The results show that AI explanations, while useful, cannot achieve their full potential without a nuanced human involvement (i.e., hybrid human AI collaboration). Instance-level explainability may allow us to understand individual algorithmic decisions but may not very helpful for personalization or individualized support. In case of mispredictions, the explanations show that algorithms decide based on the “wrong predictors” which underscores the fact that a full data-driven approach cannot be fully trusted with generating plausible recommendations completely on its own and may require human assistance.}
}
@inbook{Saqr_López-Pernas_2025a,
	title        = {Explainable Artificial Intelligence in Education: A Tutorial for Identifying the Variables that Matter},
	author       = {Saqr, Mohammed and López-Pernas, Sonsoles},
	year         = 2025,
	booktitle    = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
	publisher    = {Springer}
}
@inbook{Saqr_López-Pernas_2025b,
	title        = {Explainable Artificial Intelligence: Evaluating Individual And Local Machine Learning Predictions In R},
	author       = {Saqr, Mohammed and López-Pernas, Sonsoles},
	year         = 2025,
	booktitle    = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
	publisher    = {Springer}
}
@article{Saqr_López-Pernas_Jovanović_Gašević_2023,
	title        = {Intense, turbulent, or wallowing in the mire: A longitudinal study of cross-course online tactics, strategies, and trajectories},
	author       = {Saqr, Mohammed and López-Pernas, Sonsoles and Jovanović, Jelena and Gašević, Dragan},
	year         = 2023,
	journal      = {The Internet and Higher Education},
	volume       = 57,
	pages        = 100902,
	doi          = {10.1016/j.iheduc.2022.100902},
	issn         = {1096-7516},
	url          = {https://www.sciencedirect.com/science/article/pii/S1096751622000586},
	abstractnote = {Research has repeatedly demonstrated that students with effective learning strategies are more likely to have better academic achievement. Existing research has mostly focused on a single course or two, while longitudinal studies remain scarce. The present study examines the longitudinal sequence of students’ strategies, their succession, consistency, temporal unfolding, and whether students tend to retain or adapt strategies between courses. We use a large dataset of online traces from 135 students who completed 10 successive courses (i.e., 1350 course enrollments) in a higher education program. The methods used in this study have shown the feasibility of using trace data recorded by learning management systems to unobtrusively trace and model the longitudinal learning strategies across a program. We identified three program-level strategy trajectories: a stable and intense trajectory related to deep learning where students used diverse strategies and scored the highest grades; a fluctuating interactive trajectory, where students focused on course requirements, scored average grades, and were relatively fluctuating; and a light trajectory related to surface learning where students invested the least effort, scored the lowest grades, and had a relatively stable pathway. Students who were intensely active were more likely to transfer the intense strategies and therefore, they were expected to require less support or guidance. Students focusing on course requirements were not as effective self-regulators as they seemed and possibly required early guidance and support from teachers. Students with consistent light strategies or low effort needed proactive guidance and support.},
	keywords     = {Learning analytics; Learning strategies; Sequence analysis; Longitudinal studies}
}
@article{Saqr_Vogelsmeier_López-Pernas_2024,
	title        = {Capturing where the learning process takes place: A person-specific and person-centered primer},
	author       = {Saqr, Mohammed and Vogelsmeier, Leonie V. D. E. and López-Pernas, Sonsoles},
	year         = 2024,
	month        = {Jul},
	journal      = {Learning and individual differences},
	publisher    = {Elsevier BV},
	volume       = 113,
	number       = 102492,
	pages        = 102492,
	doi          = {10.1016/j.lindif.2024.102492},
	issn         = {1041-6080},
	url          = {http://dx.doi.org/10.1016/j.lindif.2024.102492},
	language     = {en}
}
@inproceedings{Selvaraju_Cogswell_Das_Vedantam_Parikh_Batra_2017,
	title        = {Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization},
	author       = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	year         = 2017,
	booktitle    = {Proceedings of the IEEE International Conference on Computer Vision},
	pages        = {618–626},
	url          = {http://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf}
}
@article{Shafiq_Marjani_Habeeb_Asirvatham_2022,
	title        = {Student Retention Using Educational Data Mining and Predictive Analytics: A Systematic Literature Review},
	author       = {Shafiq, Dalia Abdulkareem and Marjani, Mohsen and Habeeb, Riyaz Ahamed Ariyaluran and Asirvatham, David},
	year         = 2022,
	journal      = {IEEE Access},
	volume       = 10,
	pages        = {72480–72503},
	doi          = {10.1109/ACCESS.2022.3188767},
	issn         = {2169-3536},
	url          = {http://dx.doi.org/10.1109/ACCESS.2022.3188767},
	abstractnote = {Student retention is an essential measurement metric in education, indicated by retention rates, which are accumulated as students re-enroll from one academic year to the next. High retention rates can be obtained if institutions aim to provide appropriate support and teaching methods among the various practices to prevent students from deferring their studies. To address this pressing challenge faced by educational institutions, the underlying factors and the methodological aspects of building robust predictive models are reviewed and scrutinized. Educational Data Mining (EDM) and Learning Analytics (LA) have been widely adopted for knowledge discovery from educational data sources, improving the teaching practice, and identifying at-risk students. Various predictive techniques are applied in LA, such as Machine Learning (ML), Statistical Analysis, and Deep Learning (DL). To gain an in-depth review of these techniques, academic publications have been reviewed to highlight their potential to resolve Student Retention issues in education. Additionally, the paper presents a taxonomy of ML approaches and a comprehensive review of the success factors and the features that are not indicative of student performance in three different learning environments: Traditional Learning, Blended Learning, and Online Learning. The survey reveals that supervised ML and DL techniques are broadly applied in Student Retention. However, the application of ensemble and unsupervised learning clustering techniques supporting the heterogenous and homogenous groups of students is generally lacking. Moreover, static and traditional features are commonly used in student performance, ignoring vital factors such as educators-related, cognitive, and personal data. Furthermore, the paper highlights open challenges for future research directions.},
	keywords     = {Education;Data mining;Systematics;Predictive models;Prediction algorithms;Bibliographies;Soft sensors;Educational data mining;learning analytics;machine learning;predictive models;student retention}
}
@inbook{Siiman_Rannastu-Avalos_Pöysä-Tarhonen_Häkkinen_Pedaste_2023,
	title        = {Opportunities and challenges for AI-assisted qualitative data analysis: An example from collaborative problem-solving discourse data},
	author       = {Siiman, Leo A. and Rannastu-Avalos, Meeli and Pöysä-Tarhonen, Johanna and Häkkinen, Päivi and Pedaste, Margus},
	year         = 2023,
	booktitle    = {Lecture Notes in Computer Science},
	publisher    = {Springer Nature Switzerland},
	address      = {Cham},
	series       = {Lecture notes in computer science},
	pages        = {87–96},
	doi          = {10.1007/978-3-031-40113-8_9},
	isbn         = 9783031401121,
	issn         = {0302-9743},
	url          = {https://link.springer.com/chapter/10.1007/978-3-031-40113-8_9},
	collection   = {Lecture notes in computer science}
}
@article{Susnjak_2024,
	title        = {Beyond predictive Learning Analytics modelling and onto eXplainable artificial intelligence with prescriptive analytics and ChatGPT},
	author       = {Susnjak, Teo},
	year         = 2024,
	month        = {Jun},
	journal      = {International journal of artificial intelligence in education},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 34,
	number       = 2,
	pages        = {452–482},
	doi          = {10.1007/s40593-023-00336-3},
	issn         = {1560-4292},
	url          = {https://link.springer.com/article/10.1007/s40593-023-00336-3},
	abstractnote = {AbstractA significant body of recent research in the field of Learning Analytics has focused on leveraging machine learning approaches for predicting at-risk students in order to initiate timely interventions and thereby elevate retention and completion rates. The overarching feature of the majority of these research studies has been on the science of prediction only. The component of predictive analytics concerned with interpreting the internals of the models and explaining their predictions for individual cases to stakeholders has largely been neglected. Additionally, works that attempt to employ data-driven prescriptive analytics to automatically generate evidence-based remedial advice for at-risk learners are in their infancy. eXplainable AI is a field that has recently emerged providing cutting-edge tools which support transparent predictive analytics and techniques for generating tailored advice for at-risk students. This study proposes a novel framework that unifies both transparent machine learning as well as techniques for enabling prescriptive analytics, while integrating the latest advances in large language models for communicating the insights to learners. This work demonstrates a predictive modelling framework for identifying learners at risk of qualification non-completion based on a real-world dataset comprising $$sim $$ ∼ 7000 learners with their outcomes, covering 2018 - 2022. The study further demonstrates how predictive modelling can be augmented with prescriptive analytics on two case studies to generate human-readable prescriptive feedback for those who are at risk using ChatGPT.},
	language     = {en}
}
@inbook{Taylor_Yeung_Bashet_2021,
	title        = {Personalized and adaptive learning},
	author       = {Taylor, Deborah L. and Yeung, Michelle and Bashet, A. Z.},
	year         = 2021,
	booktitle    = {Innovative Learning Environments in STEM Higher Education},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	series       = {SpringerBriefs in statistics},
	pages        = {17–34},
	doi          = {10.1007/978-3-030-58948-6_2},
	isbn         = 9783030589479,
	issn         = {2191-544X},
	url          = {http://dx.doi.org/10.1007/978-3-030-58948-6_2},
	abstractnote = {AbstractPersonalized and adaptive learning has been touted to be one of the most promising emerging tools for increasing student learning and student success. Yet, the terms are neither precise nor clearly defined at this time, thus making it difficult for institutions of higher education to adopt and implement a learning approach using technology that is in its infancy and not clearly understood by those who will be utilizing it. One goal of this chapter is to define adaptive and personalized learning as it is used at this time in the hopes that as the technology evolves the promise of increased student learning can come to fruition. Adaptive learning personalizes learning by continuously evaluating each student’s performance in real time and creating an ever-changing individualized learning pathway as directed by artificial intelligence and machine learning, thus increasing learning and student satisfaction.},
	collection   = {SpringerBriefs in statistics}
}
@inproceedings{Tiukhova_Vemuri_Óskarsdóttir_Poelmans_Baesens_Snoeck_2024,
	title        = {Discovering Unusual Study Patterns Using Anomaly Detection and XAI},
	author       = {Tiukhova, Elena and Vemuri, Pavani and Óskarsdóttir, María and Poelmans, Stephan and Baesens, Bart and Snoeck, Monique},
	year         = 2024,
	booktitle    = {Hawaii International Conference on System Sciences 2024 (HICSS-57)},
	url          = {https://aisel.aisnet.org/hicss-57/da/learning_analytics/3},
	abstractnote = {Learning Analytics (LA) has been leveraged as a tool to analyze and improve educational processes by informing its stakeholders. LA for student profiling focuses on discovering learning patterns and trends based on diverse features extracted from trace data. Prior studies have used classical clustering methods to group students and understand the study patterns of each cluster. However, variations within the clusters are still large making it difficult to draw concrete insights into the relation between study behaviors and learning outcomes. In this work, we leverage anomaly detection and eXplainable AI techniques to distinguish between normal and abnormal study patterns and to possibly discover unexpected patterns that are not apparent from clustering alone. We perform external validation to check the generalizability and compare the insights on study patterns from our method to be at par with insights gained from previous studies.}
}
@article{Urdaneta-Ponte_Mendez-Zorrilla_Oleagordia-Ruiz_2021,
	title        = {Recommendation systems for education: Systematic review},
	author       = {Urdaneta-Ponte, María Cora and Mendez-Zorrilla, Amaia and Oleagordia-Ruiz, Ibon},
	year         = 2021,
	month        = {Jul},
	journal      = {Electronics},
	publisher    = {MDPI AG},
	volume       = 10,
	number       = 14,
	pages        = 1611,
	doi          = {10.3390/electronics10141611},
	issn         = {2079-9292},
	url          = {http://dx.doi.org/10.3390/electronics10141611},
	abstractnote = {Recommendation systems have emerged as a response to overload in terms of increased amounts of information online, which has become a problem for users regarding the time spent on their search and the amount of information retrieved by it. In the field of recommendation systems in education, the relevance of recommended educational resources will improve the student’s learning process, and hence the importance of being able to suitably and reliably ensure relevant, useful information. The purpose of this systematic review is to analyze the work undertaken on recommendation systems that support educational practices with a view to acquiring information related to the type of education and areas dealt with, the developmental approach used, and the elements recommended, as well as being able to detect any gaps in this area for future research work. A systematic review was carried out that included 98 articles from a total of 2937 found in main databases (IEEE, ACM, Scopus and WoS), about which it was able to be established that most are geared towards recommending educational resources for users of formal education, in which the main approaches used in recommendation systems are the collaborative approach, the content-based approach, and the hybrid approach, with a tendency to use machine learning in the last two years. Finally, possible future areas of research and development in this field are presented.},
	language     = {en}
}
@article{Wang_Wang_Zhu_Wang_Tran_Du_2024,
	title        = {Artificial intelligence in education: A systematic literature review},
	author       = {Wang, Shan and Wang, Fang and Zhu, Zhen and Wang, Jingxuan and Tran, Tam and Du, Zhao},
	year         = 2024,
	month        = {Oct},
	journal      = {Expert systems with applications},
	publisher    = {Elsevier BV},
	volume       = 252,
	number       = 124167,
	pages        = 124167,
	doi          = {10.1016/j.eswa.2024.124167},
	issn         = {0957-4174},
	url          = {http://dx.doi.org/10.1016/j.eswa.2024.124167},
	language     = {en}
}
@article{Wei_Lu_Song_2015,
	title        = {Variable importance analysis: A comprehensive review},
	author       = {Wei, Pengfei and Lu, Zhenzhou and Song, Jingwen},
	year         = 2015,
	month        = {Oct},
	journal      = {Reliability Engineering & System Safety},
	publisher    = {Elsevier BV},
	volume       = 142,
	pages        = {399–432},
	doi          = {10.1016/j.ress.2015.05.018},
	issn         = {0951-8320},
	url          = {http://dx.doi.org/10.1016/j.ress.2015.05.018},
	abstractnote = {Measuring variable importance for computational models or measured data is an important task in many applications. It has drawn our attention that the variable importance analysis (VIA) techniques were developed independently in many disciplines. We are strongly aware of the necessity to aggregate all the good practices in each discipline, and compare the relative merits of each method, so as to instruct the practitioners to choose the optimal methods to meet different analysis purposes, and to guide current research on VIA. To this end, all the good practices, including seven groups of methods, i.e., the difference-based variable importance measures (VIMs), parametric regression and related VIMs, nonparametric regression techniques, hypothesis test techniques, variance-based VIMs, moment-independent VIMs and graphic VIMs, are reviewed and compared with a numerical test example set in two situations (independent and dependent cases). For ease of use, the recommendations are provided for different types of applications, and packages as well as software for implementing these VIA techniques are collected. Prospects for future study of VIA techniques are also proposed.},
	language     = {en}
}
@article{Williams_2023,
	title        = {AI, analytics and a new assessment model for universities},
	author       = {Williams, Peter},
	year         = 2023,
	month        = {Oct},
	journal      = {Education sciences},
	publisher    = {MDPI AG},
	volume       = 13,
	number       = 10,
	pages        = 1040,
	doi          = {10.3390/educsci13101040},
	issn         = {2227-7102},
	url          = {http://dx.doi.org/10.3390/educsci13101040},
	abstractnote = {As the COVID-19 pandemic recedes, its legacy has been to disrupt universities across the world, most immediately in developing online adjuncts to face-to-face teaching. Behind these problems lie those of assessment, particularly traditional summative assessment, which has proved more difficult to implement. This paper models the current practice of assessment in higher education as influenced by ten factors, the most important of which are the emerging technologies of artificial intelligence (AI) and learning analytics (LA). Using this model and a SWOT analysis, the paper argues that the pressures of marketisation and demand for nontraditional and vocationally oriented provision put a premium on courses offering a more flexible and student-centred assessment. This could be facilitated through institutional strategies enabling assessment for learning: an approach that employs formative assessment supported by AI and LA, together with collaborative working in realistic contexts, to facilitate students’ development as flexible and sustainable learners. While literature in this area tends to focus on one or two aspects of technology or assessment, this paper aims to be integrative by drawing upon more comprehensive evidence to support its thesis.},
	language     = {en}
}
@inproceedings{Yan_Martinez-Maldonado_Gasevic_2024,
	title        = {Generative artificial intelligence in learning analytics: Contextualising opportunities and challenges through the learning analytics cycle},
	author       = {Yan, Lixiang and Martinez-Maldonado, Roberto and Gasevic, Dragan},
	year         = 2024,
	month        = {Mar},
	booktitle    = {Proceedings of the 14th Learning Analytics and Knowledge Conference},
	publisher    = {ACM},
	address      = {New York, NY, USA},
	doi          = {10.1145/3636555.3636856},
	url          = {https://dl.acm.org/doi/10.1145/3636555.3636856},
	language     = {en}
}
@article{Yang_Ye_Xia_2022,
	title        = {Unbox the black-box for the medical explainable AI via multi-modal and multi-centre data fusion: A mini-review, two showcases and beyond},
	author       = {Yang, Guang and Ye, Qinghao and Xia, Jun},
	year         = 2022,
	month        = {Jan},
	journal      = {An international journal on information fusion},
	publisher    = {Elsevier BV},
	volume       = 77,
	pages        = {29–52},
	doi          = {10.1016/j.inffus.2021.07.016},
	issn         = {1566-2535},
	url          = {http://dx.doi.org/10.1016/j.inffus.2021.07.016},
	abstractnote = {Explainable Artificial Intelligence (XAI) is an emerging research topic of machine learning aimed at unboxing how AI systems’ black-box choices are made. This research field inspects the measures and models involved in decision-making and seeks solutions to explain them explicitly. Many of the machine learning algorithms cannot manifest how and why a decision has been cast. This is particularly true of the most popular deep neural network approaches currently in use. Consequently, our confidence in AI systems can be hindered by the lack of explainability in these black-box models. The XAI becomes more and more crucial for deep learning powered applications, especially for medical and healthcare studies, although in general these deep neural networks can return an arresting dividend in performance. The insufficient explainability and transparency in most existing AI systems can be one of the major reasons that successful implementation and integration of AI tools into routine clinical practice are uncommon. In this study, we first surveyed the current progress of XAI and in particular its advances in healthcare applications. We then introduced our solutions for XAI leveraging multi-modal and multi-centre data fusion, and subsequently validated in two showcases following real clinical scenarios. Comprehensive quantitative and qualitative analyses can prove the efficacy of our proposed XAI solutions, from which we can envisage successful applications in a broader range of clinical questions.},
	keywords     = {Explainable AI; Information fusion; Medical image analysis; Multi-domain information fusion; Weakly supervised learning},
	language     = {en}
}
@article{Zawacki-Richter_Marín_Bond_Gouverneur_2019,
	title        = {Systematic review of research on artificial intelligence applications in higher education – where are the educators?},
	author       = {Zawacki-Richter, Olaf and Marín, Victoria I. and Bond, Melissa and Gouverneur, Franziska},
	year         = 2019,
	month        = {Dec},
	journal      = {International journal of educational technology in higher education},
	publisher    = {Springer Science and Business Media LLC},
	volume       = 16,
	number       = 1,
	doi          = {10.1186/s41239-019-0171-0},
	issn         = {2365-9440},
	url          = {http://dx.doi.org/10.1186/s41239-019-0171-0},
	abstractnote = {Abstract According to various international reports, Artificial Intelligence in Education (AIEd) is one of the currently emerging fields in educational technology. Whilst it has been around for about 30 years, it is still unclear for educators how to make pedagogical advantage of it on a broader scale, and how it can actually impact meaningfully on teaching and learning in higher education. This paper seeks to provide an overview of research on AI applications in higher education through a systematic review. Out of 2656 initially identified publications for the period between 2007 and 2018, 146 articles were included for final synthesis, according to explicit inclusion and exclusion criteria. The descriptive results show that most of the disciplines involved in AIEd papers come from Computer Science and STEM, and that quantitative methods were the most frequently used in empirical studies. The synthesis of results presents four areas of AIEd applications in academic support services, and institutional and administrative services: 1. profiling and prediction, 2. assessment and evaluation, 3. adaptive systems and personalisation, and 4. intelligent tutoring systems. The conclusions reflect on the almost lack of critical reflection of challenges and risks of AIEd, the weak connection to theoretical pedagogical perspectives, and the need for further exploration of ethical and educational approaches in the application of AIEd in higher education.},
	language     = {en}
}
@article{Zhai_Chu_Chai_Jong_Istenic_Spector_Liu_Yuan_Li_2021,
	title        = {A review of artificial intelligence (AI) in education from 2010 to 2020},
	author       = {Zhai, Xuesong and Chu, Xiaoyan and Chai, Ching Sing and Jong, Morris Siu Yung and Istenic, Andreja and Spector, Michael and Liu, Jia-Bao and Yuan, Jing and Li, Yan},
	year         = 2021,
	month        = {Jan},
	journal      = {Complexity},
	publisher    = {Wiley},
	volume       = 2021,
	number       = 1,
	pages        = {1–18},
	doi          = {10.1155/2021/8812542},
	issn         = {1076-2787},
	url          = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2021/8812542},
	abstractnote = {This study provided a content analysis of studies aiming to disclose how artificial intelligence (AI) has been applied to the education sector and explore the potential research trends and challenges of AI in education. A total of 100 papers including 63 empirical papers (74 studies) and 37 analytic papers were selected from the education and educational research category of Social Sciences Citation Index database from 2010 to 2020. The content analysis showed that the research questions could be classified into development layer (classification, matching, recommendation, and deep learning), application layer (feedback, reasoning, and adaptive learning), and integration layer (affection computing, role‐playing, immersive learning, and gamification). Moreover, four research trends, including Internet of Things, swarm intelligence, deep learning, and neuroscience, as well as an assessment of AI in education, were suggested for further investigation. However, we also proposed the challenges in education may be caused by AI with regard to inappropriate use of AI techniques, changing roles of teachers and students, as well as social and ethical issues. The results provide insights into an overview of the AI used for education domain, which helps to strengthen the theoretical foundation of AI in education and provides a promising channel for educators and AI engineers to carry out further collaborative research.},
	language     = {en}
}
@article{Zhong_2023,
	title        = {A systematic review of personalized learning in higher education: learning content structure, learning materials sequence, and learning readiness support},
	author       = {Zhong, Lin},
	year         = 2023,
	month        = {Dec},
	journal      = {Interactive learning environments},
	publisher    = {Informa UK Limited},
	volume       = 31,
	number       = 10,
	pages        = {7053–7073},
	doi          = {10.1080/10494820.2022.2061006},
	issn         = {1049-4820},
	url          = {http://dx.doi.org/10.1080/10494820.2022.2061006},
	language     = {en}
}
