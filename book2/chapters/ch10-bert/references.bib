@article{karadzhov2023delidata,
    title={DeliData: A dataset for deliberation in multi-party problem solving},
    author={Karadzhov, Georgi and Stafford, Tom and Vlachos, Andreas},
    journal={Proceedings of the ACM on Human-Computer Interaction},
    volume={7},
    number={CSCW2},
    pages={1--25},
    year={2023},
    publisher={ACM New York, NY, USA}
  }
  
@article{Kjell2023-rx,
  title     = "The text-package: An {R}-package for analyzing and visualizing
               human language using natural language processing and transformers",
  author    = "Kjell, Oscar and Giorgi, Salvatore and Schwartz, H Andrew",
  journal   = "Psychol. Methods",
  publisher = "American Psychological Association (APA)",
  volume    =  28,
  number    =  6,
  pages     = "1478--1498",
  year      =  2023,
  doi       = "10.1037/met0000542",
  pmid      =  37126041,
  issn      = "1082-989X,1939-1463",
  language  = "en"
}

@INPROCEEDINGS{Yan2024-jw,
  title     = "Generative artificial intelligence in learning analytics:
               Contextualising opportunities and challenges through the learning
               analytics cycle",
  author    = "Yan, Lixiang and Martinez-Maldonado, Roberto and Gasevic, Dragan",
  booktitle = "Proceedings of the 14th Learning Analytics and Knowledge
               Conference",
  publisher = "ACM",
  address   = "New York, NY, USA",
  year      =  2024,
  doi       = "10.1145/3636555.3636856",
  language  = "en"
}
@INCOLLECTION{Menzel2023-mz,
  title     = "Why you should give your students automatic process feedback on
               their collaboration: Evidence from a randomized experiment",
  author    = "Menzel, Lukas and Gombert, Sebastian and Weidlich, Joshua and
               Fink, Aron and Frey, Andreas and Drachsler, Hendrik",
  booktitle = "Lecture Notes in Computer Science",
  publisher = "Springer Nature Switzerland",
  address   = "Cham",
  pages     = "198--212",
  series    = "Lecture notes in computer science",
  year      =  2023,
  doi       = "10.1007/978-3-031-42682-7\_14",
  isbn      = "9783031426810,9783031426827",
  issn      = "0302-9743,1611-3349"
}

@INPROCEEDINGS{Pinargote2024-kb,
  title     = "Automating data narratives in Learning Analytics Dashboards using
               {GenAI}",
  author    = "Pinargote, Adriano and Calderón, Eddy and Cevallos, Kevin and
               Carrillo, Gladys and Chiluiza, Katherine and Echeverria, Vanessa",
  booktitle = "2024 Joint of International Conference on Learning Analytics and
               Knowledge Workshops",
  publisher = "CEUR-WS",
  pages     = "150--161",
  year      =  2024,
  url       = "https://ceur-ws.org/Vol-3667/DS-LAK24_paper_5.pdf",
  language  = "en"
}

@INPROCEEDINGS{Pan2020-xa,
  title     = "Learning analytics dashboard for problem-based learning",
  author    = "Pan, Zilong and Li, Chenglu and Liu, Min",
  booktitle = "Proceedings of the Seventh ACM Conference on Learning @ Scale",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  aug,
  year      =  2020,
  doi       = "10.1145/3386527.3406751",
  isbn      =  9781450379519 
}

@INPROCEEDINGS{Milesi2024-rj,
  title     = "``it's really enjoyable to see me solve the problem like a
               hero'': {GenAI}-enhanced data comics as a learning analytics tool",
  author    = "Milesi, Mikaela E and Alfredo, Riordan and Echeverria, Vanessa
               and Yan, Lixiang and Zhao, Linxuan and Tsai, Yi-Shan and
               Martinez-Maldonado, Roberto",
  booktitle = "Extended Abstracts of the CHI Conference on Human Factors in
               Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  may,
  year      =  2024,
  doi       = "10.1145/3613905.3651111",
  language  = "en"
}

@INPROCEEDINGS{Garg2024-wi,
  title     = "Automated discourse analysis via generative artificial
               intelligence",
  author    = "Garg, Ryan and Han, Jaeyoung and Cheng, Yixin and Fang, Zheng and
               Swiecki, Zachari",
  booktitle = "Proceedings of the 14th Learning Analytics and Knowledge
               Conference",
  publisher = "ACM",
  address   = "New York, NY, USA",
  year      =  2024,
  doi       = "10.1145/3636555.3636879",
  language  = "en"
}

@ARTICLE{Misiejuk2024-wl,
  title     = "Augmenting assessment with {AI} coding of online student
               discourse: A question of reliability",
  author    = "Misiejuk, Kamila and Kaliisa, Rogers and Scianna, Jennifer",
  journal   = "Computers and Education: Artificial Intelligence",
  publisher = "Elsevier",
  volume    =  6,
  number    =  100216,
  year      =  2024,
  doi       = "10.1016/j.caeai.2024.100216",
  issn      = "2666-920X",
  language  = "en"
}

@ARTICLE{Pugh2021-bz,
  title   = "Say what? Automatic modeling of collaborative problem solving
             skills from student speech in the wild",
  author  = "Pugh, Samuel L and Subburaj, Shree Krishna and Rao, A and Stewart,
             Angela E B and Andrews-Todd, Jessica and D'Mello, S",
  journal = "Int Conf Young Spéc Micro/nanotechnologies Electron Device",
  year    =  2021,
  url     = "https://files.eric.ed.gov/fulltext/ED615653.pdf"
}



@Manual{dplyr,
  title = {dplyr: A Grammar of Data Manipulation},
  author = {Hadley Wickham and Romain François and Lionel Henry and Kirill Müller and Davis Vaughan},
  year = {2023},
  note = {R package version 1.1.4},
  url = {https://CRAN.R-project.org/package=dplyr},
}


@Article{caret,
  title = {Building Predictive Models in R Using the caret Package},
  volume = {28},
  doi = {10.18637/jss.v028.i05},
  number = {5},
  journal = {Journal of Statistical Software},
  author = {{Kuhn} and {Max}},
  year = {2008},
  pages = {1–26},
}

@Article{text,
  title = {The text-package: An R-package for Analyzing and Visualizing Human Language Using Natural Language Processing and Deep Learning},
  author = {Oscar Kjell and Salvatore Giorgi and H. Andrew Schwartz},
  journal = {Psychological Methods},
  doi = {10.1037/met0000542},
  year = {2023} 
}

@Manual{reticulate,
  title = {reticulate: Interface to 'Python'},
  author = {Kevin Ushey and JJ Allaire and Yuan Tang},
  year = {2024},
  note = {R package version 1.38.0},
  url = {https://CRAN.R-project.org/package=reticulate},
}


@Manual{tibble,
    title = {tibble: Simple Data Frames},
    author = {Kirill Müller and Hadley Wickham},
    year = {2023},
    note = {R package version 3.2.1},
    url = {https://CRAN.R-project.org/package=tibble},
  }
  
  
 @Manual{tidyr,
    title = {tidyr: Tidy Messy Data},
    author = {Hadley Wickham and Davis Vaughan and Maximilian Girlich},
    year = {2024},
    note = {R package version 1.3.1},
    url = {https://CRAN.R-project.org/package=tidyr},
  }
  
  @ARTICLE{Saqr2024-ug,
  title     = "Sequence analysis in education: principles, technique, and
               tutorial with {R}",
  author    = "Saqr, Mohammed and López-Pernas, Sonsoles and Helske, Satu and
               Durand, Marion and Murphy, Keefe and Studer, Matthias and
               Ritschard, Gilbert",
  publisher = "Springer Nature Switzerland",
  pages     = "321--354",
  abstract  = "Patterns exist everywhere in our life, from the sequence of genes
               to the order of steps in cooking recipes. Discovering patterns,
               variations, regularities, or irregularities is at the heart of
               scientific inquiry and, therefore, several data mining methods
               have been developed to understand patterns. Sequence analysis—or
               sequence mining—was developed almost four decades ago to address
               the increasing needs for pattern mining [1]. Ever since, a wealth
               of applications, algorithms, and statistical tools have been
               developed, adapted, or incorporated into the array of sequence
               analysis. Since sequence mining has been conceptualized, it has
               grown in scale of adoption and range of applications across life
               and social sciences [2] and education research was no exception
               (eg,[3]). As a data mining technique, sequence mining has been
               commonly implemented to identify hidden patterns that would
               otherwise be missed using other analytical techniques and find
               interesting subsequences (parts of the sequence) that have
               practical significance or unexpected sequences that we did not
               know existed [4]. For instance, by mining sequences of
               collaborative dialogue, we",
  month     =  feb,
  year      =  2024,
  doi = "10.1007/978-3-031-54464-4_10"
  
}

@ARTICLE{Saqr2023-nv,
  title    = "The temporal dynamics of online problem-based learning: Why and
              when sequence matters",
  author   = "Saqr, Mohammed and López-Pernas, Sonsoles",
  journal  = "International Journal of Computer-Supported Collaborative Learning",
  volume   =  18,
  number   =  1,
  pages    = "11--37",
  abstract = "Early research on online PBL explored student satisfaction,
              effectiveness, and design. The temporal aspect of online PBL has
              rarely been addressed. Thus, a gap exists in our knowledge
              regarding how online PBL unfolds: when and for how long a group
              engages in collaborative discussions. Similarly, little is known
              about whether and what sequence of interactions could predict
              higher achievement. This study aims to bridge such a gap by
              implementing the latest advances in temporal learning analytics to
              analyze the sequential and temporal aspects of online PBL across a
              large sample (n = 204 students) of qualitatively coded
              interactions (8,009 interactions). We analyzed interactions at the
              group level to understand the group dynamics across whole problem
              discussions, and at the student level to understand the students’
              contribution dynamics across different episodes. We followed such
              analyses by examining the association of interaction types and the
              sequences thereof with students’ performance using multilevel
              linear regression models. The analysis of the interactions
              reflected that the scripted PBL process is followed a logical
              sequence, yet often lacked enough depth. When cognitive
              interactions (e.g., arguments, questions, and evaluations)
              occurred, they kindled high cognitive interactions, when low
              cognitive and social interactions dominated, they kindled low
              cognitive interactions. The order and sequence of interactions
              were more predictive of performance, and with a higher explanatory
              power as compared to frequencies. Starting or initiating
              interactions (even with low cognitive content) showed the highest
              association with performance, pointing to the importance of
              initiative and sequencing.",
  month    =  mar,
  year     =  2023,
  url      = "https://doi.org/10.1007/s11412-023-09385-1",
  doi      = "10.1007/s11412-023-09385-1",
  issn     = "1556-1615"
}



@ARTICLE{Gabadinho2011-lm,
  title   = "Analyzing and visualizing state sequences in {R} with {TraMineR}",
  author  = "Gabadinho, Alexis and Ritschard, Gilbert and Mueller, Nicolas
             Séverin and Studer, Matthias",
  journal = "J. Stat. Softw.",
  volume  =  40,
  number  =  4,
  pages   = "1--37",
  year    =  2011,
  doi     = "10.18637/jss.v040.i04"
}

@INCOLLECTION{Helske2024-ar,
  title     = "A modern approach to transition analysis and process mining with
               Markov models in education",
  author    = "Helske, Jouni and Helske, Satu and Saqr, Mohammed and
               López-Pernas, Sonsoles and Murphy, Keefe",
  booktitle = "Learning Analytics Methods and Tutorials",
  publisher = "Springer Nature Switzerland",
  address   = "Cham",
  pages     = "381--427",
  abstract  = "AbstractThis chapter presents an introduction to Markovian
               modelling for the analysis of sequence data. Contrary to the
               deterministic approach seen in the previous sequence analysis
               chapters, Markovian models are probabilistic models, focusing on
               the transitions between states instead of studying sequences as a
               whole. The chapter provides an introduction to this method and
               differentiates between its most common variations: first-order
               Markov models, hidden Markov models, mixture Markov models, and
               mixture hidden Markov models. In addition to a thorough
               explanation and contextualisation within the existing literature,
               the chapter provides a step-by-step tutorial on how to implement
               each type of Markovian model using the R package seqHMM. The
               chapter also provides a complete guide to performing stochastic
               process mining with Markovian models as well as plotting,
               comparing and clustering different process models.",
  year      =  2024,
  url       = "https://link.springer.com/chapter/10.1007/978-3-031-54464-4_12",
  doi       = "10.1007/978-3-031-54464-4\_12",
  isbn      = "9783031544637,9783031544644",
  language  = "en"
}



@INCOLLECTION{Tan2024-qu,
  title     = "Epistemic network analysis and ordered network analysis in
               learning analytics",
  author    = "Tan, Yuanru and Swiecki, Zachari and Ruis, A R and Shaffer, David",
  booktitle = "Learning Analytics Methods and Tutorials",
  publisher = "Springer Nature Switzerland",
  address   = "Cham",
  pages     = "569--636",
  abstract  = "AbstractThis chapter provides a tutorial on conducting epistemic
               network analysis (ENA) and ordered network analysis (ONA) using
               R. We introduce these two techniques together because they share
               similar theoretical foundations, but each addresses a different
               challenge for analyzing large-scale qualitative data on learning
               processes. ENA and ONA are methods for quantifying, visualizing,
               and interpreting network data. Taking coded data as input, ENA
               and ONA represent associations between codes in undirected or
               directed weighted network models, respectively. Both techniques
               measure the strength of association among codes and illustrate
               the structure of connections in network graphs, and they quantify
               changes in the composition and strength of those connections over
               time. Importantly, ENA and ONA enable comparison of networks both
               visually and via summary statistics, so they can be used to
               explore a wide range of research questions in contexts where
               patterns of association in coded data are hypothesized to be
               meaningful and where comparing those patterns across individuals
               or groups is important.",
  year      =  2024,
  url       = "https://link.springer.com/chapter/10.1007/978-3-031-54464-4_18",
  doi       = "10.1007/978-3-031-54464-4\_18",
  isbn      = "9783031544637,9783031544644",
  language  = "en"
}


@INPROCEEDINGS{Saqr2025-ku,
  title     = "Transition Network Analysis: A Novel Framework for Modeling,
               Visualizing, and Identifying the Temporal Patterns of Learners
               and Learning Processes",
  author    = "Saqr, Mohammed and López-Pernas, Sonsoles and Törmänen, Tiina and
               Kaliisa, Rogers and Misiejuk, Kamila and Tikka, Santtu",
  booktitle = "Proceedings of Learning Analytics \& Knowledge (LAK '25)",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "in--press",
  abstract  = "This paper proposes a novel analytical framework: Transition
               Network Analysis (TNA), an approach that integrates Stochastic
               Process Mining and probabilistic graph representation to model,
               visualize, and identify transition patterns in the learning
               process data. Combining the relational and temporal aspects into
               a single lens offers capabilities beyond either framework,
               including centralities to capture important learning events,
               community finding to identify patterns of behavior, and
               clustering to reveal temporal patterns. This paper introduces the
               theoretical and mathematical foundations of TNA. To demonstrate
               the functionalities of TNA, we present a case study with students
               (n=191) engaged in small-group collaboration to map patterns of
               group dynamics using the theories of co-regulation and
               socially-shared regulated learning. The analysis revealed that
               TNA could reveal the regulatory processes and identify important
               events, temporal patterns and clusters. Bootstrap validation
               established the significant transitions and eliminated spurious
               transitions. In doing so, we showcase TNA’s utility to capture
               learning dynamics and provide a robust framework for
               investigating the temporal evolution of learning processes.
               Future directions include —inter alia— advancing estimation
               methods, expanding reliability assessment, exploring longitudinal
               TNA, and comparing TNA networks using permutation tests.",
  year      =  2025
}

@article{elliott2018thinking,
title={Thinking about the coding process in qualitative data analysis},
author={Elliott, Victoria},
journal={Qualitative report},
volume={23},
number={11},
year={2018},
publisher={Nova Southeastern University}
}



@article{thomas2006general,
  title={A general inductive approach for analyzing qualitative evaluation data},
  author={Thomas, David R},
  journal={American journal of evaluation},
  volume={27},
  number={2},
  pages={237--246},
  year={2006},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}



@article{elliott2018thinking,
  title={Thinking about the coding process in qualitative data analysis},
  author={Elliott, Victoria},
  journal={Qualitative report},
  volume={23},
  number={11},
  year={2018},
  publisher={Nova Southeastern University}
}



@article{lefort2024small,
  title={When Small Wins Big: Classification Tasks Where Compact Models Outperform Original GPT-4},
  author={Lefort, Baptiste and Benhamou, Eric and Ohana, Jean-Jacques and Guez, Beatrice and Saltiel, David and Challet, Damien},
  journal={Available at SSRN 4780454},
  year={2024}
}



@article{davison2024ethics,
  title={The ethics of using generative AI for qualitative data analysis},
  author={Davison, Robert M and Chughtai, Hameed and Nielsen, Petter and Marabelli, Marco and Iannacci, Federico and van Offenbeek, Marjolein and Tarafdar, Monideepa and Trenz, Manuel and Techatassanasoontorn, Angsana and Diaz Andrade, Antonio and others},
  year={2024},
  publisher={John Wiley \& Sons Ltd}
}



@article{morgan2023exploring,
  title={Exploring the use of artificial intelligence for qualitative data analysis: The case of ChatGPT},
  author={Morgan, David L},
  journal={International journal of qualitative methods},
  volume={22},
  pages={16094069231211248},
  year={2023},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}



@article{prescott2024comparing,
  title={Comparing the efficacy and efficiency of human and generative AI: Qualitative thematic analyses},
  author={Prescott, Maximo R and Yeager, Samantha and Ham, Lillian and Rivera Saldana, Carlos D and Serrano, Vanessa and Narez, Joey and Paltin, Dafna and Delgado, Jorge and Moore, David J and Montoya, Jessica},
  journal={JMIR AI},
  volume={3},
  pages={e54482},
  year={2024},
  publisher={JMIR Publications Toronto, Canada}
}



@inproceedings{ganesh2024prompting,
  title={Prompting as Panacea? A Case Study of In-Context Learning Performance for Qualitative Coding of Classroom Dialog},
  author={Ganesh, Ananya and Chandler, Chelsea and D'Mello, Sidney and Palmer, Martha and Kann, Katharina},
  booktitle={Proceedings of the 17th International Conference on Educational Data Mining},
  pages={835--843},
  year={2024}
}



@inproceedings{barany2024chatgpt,
  title={ChatGPT for education research: exploring the potential of large language models for qualitative codebook development},
  author={Barany, Amanda and Nasiar, Nidhi and Porter, Chelsea and Zambrano, Andres Felipe and Andres, Alexandra L and Bright, Dara and Shah, Mamta and Liu, Xiner and Gao, Sabrina and Zhang, Jiayi and others},
  booktitle={International conference on artificial intelligence in education},
  pages={134--149},
  year={2024},
  organization={Springer}
}



@article{raiaan2024review,
  title={A review on large Language Models: Architectures, applications, taxonomies, open issues and challenges},
  author={Raiaan, Mohaimenul Azam Khan and Mukta, Md Saddam Hossain and Fatema, Kaniz and Fahad, Nur Mohammad and Sakib, Sadman and Mim, Most Marufatul Jannat and Ahmad, Jubaer and Ali, Mohammed Eunus and Azam, Sami},
  journal={IEEE Access},
  year={2024},
  publisher={IEEE}
}

@article{huang2023survey,
  title={A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others},
  journal={ACM Transactions on Information Systems},
  year={2023},
  publisher={ACM New York, NY}
}



@article{bosley2023we,
  title={Do we still need BERT in the age of GPT? Comparing the benefits of domain-adaptation and in-context-learning approaches to using LLMs for Political Science Research},
  author={Bosley, Mitchell and Jacobs-Harukawa, Musashi and Licht, Hauke and Hoyle, Alexander},
  year={2023}
}



@article{palmer2024using,
  title={Using proprietary language models in academic research requires explicit justification},
  author={Palmer, Alexis and Smith, Noah A and Spirling, Arthur},
  journal={Nature Computational Science},
  volume={4},
  number={1},
  pages={2--3},
  year={2024},
  publisher={Nature Publishing Group US New York}
}



@bookchapter{amaratunga2023understanding,
  title={What Makes LLMs Large?},
  book={Understanding large language models: Learning their underlying concepts and technologies},
  author={Amaratunga, Thimira},
  year={2023},
  publisher={Springer},
  pages={81--177},
  publisher={Apress: Berkeley, CA}
}



@article{chae2023large,
  title={Large language models for text classification: From zero-shot learning to fine-tuning},
  author={Chae, Youngjin and Davidson, Thomas},
  journal={Open Science Foundation},
  year={2023}
}



@article{periti2024lexical,
  title={Lexical Semantic Change through Large Language Models: a Survey},
  author={Periti, Francesco and Montanelli, Stefano},
  journal={ACM Computing Surveys},
  year={2024},
  publisher={ACM New York, NY}
}


jsx
@bookchapter{saqr2025classification,
  title={Artificial Intelligence: Student Classification With Machine Learning in R},
  book={Advanced learning analytics methods and tutorials: A practical guide using R},
  author={Mohammed Saqr and Kamila Misiejuk and Santtu Tikka and Sonsoles López-Pernas},
  year={2025},
  publisher={Springer}
}



@inproceedings{shaffer2021we,
  title={How we code},
  author={Shaffer, David Williamson and Ruis, Andrew R},
  booktitle={Advances in Quantitative Ethnography: Second International Conference, ICQE 2020, Malibu, CA, USA, February 1-3, 2021, Proceedings 2},
  pages={62--77},
  year={2021},
  organization={Springer}
}



@article{hayes2023conversing,
  title={“Conversing” with Qualitative Data: Enhancing Qualitative Research through Large Language Models (LLMs)},
  author={Hayes, Adam},
  publisher={OSF preprint},
  doi={10.31235/osf.io/yms8p}
}



@article{tai2024examination,
  title={An examination of the use of large language models to aid analysis of textual data},
  author={Tai, Robert H and Bentley, Lillian R and Xia, Xin and Sitt, Jason M and Fankhauser, Sarah C and Chicas-Mosier, Ana M and Monteith, Barnas G},
  journal={International Journal of Qualitative Methods},
  volume={23},
  pages={16094069241231168},
  year={2024},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}



@inproceedings{xiao2023supporting,
  title={Supporting qualitative analysis with large language models: Combining codebook with GPT-3 for deductive coding},
  author={Xiao, Ziang and Yuan, Xingdi and Liao, Q Vera and Abdelghani, Rania and Oudeyer, Pierre-Yves},
  booktitle={Companion proceedings of the 28th international conference on intelligent user interfaces},
  pages={75--78},
  year={2023}
}



@article{mathis2024inductive,
  title={Inductive thematic analysis of healthcare qualitative interviews using open-source large language models: How does it compare to traditional methods?},
  author={Mathis, Walter S and Zhao, Sophia and Pratt, Nicholas and Weleff, Jeremy and De Paoli, Stefano},
  journal={Computer Methods and Programs in Biomedicine},
  volume={255},
  pages={108356},
  year={2024},
  publisher={Elsevier}
}



@inproceedings{spinoso2023qualitative,
  title={Qualitative Code Suggestion: A Human-Centric Approach to Qualitative Coding},
  author={Spinoso-Di Piano, Cesare and Rahimi, Samira and Cheung, Jackie Chi Kit},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={14887--14909},
  year={2023}
}



@article{katz2024using,
  title={Using Generative Text Models to Create Qualitative Codebooks for Student Evaluations of Teaching},
  author={Katz, Andrew and Gerhardt, Mitch and Soledad, Michelle},
  journal={International Journal of Qualitative Methods},
  volume={23},
  pages={16094069241293283},
  year={2024},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}



@article{van2023automating,
  title={Automating the identification of feedback quality criteria and the CanMEDS roles in written feedback comments using natural language processing},
  author={Van Ostaeyen, Sofie and De Langhe, Loic and De Clercq, Orph{\'e}e and Embo, Mieke and Schellens, Tammy and Valcke, Martin},
  journal={Perspectives on Medical Education},
  volume={12},
  number={1},
  pages={540},
  year={2023},
  publisher={Ubiquity Press}
}



@inproceedings{ganesh2024prompting,
  title={Prompting as Panacea? A Case Study of In-Context Learning Performance for Qualitative Coding of Classroom Dialog},
  author={Ganesh, Ananya and Chandler, Chelsea and D'Mello, Sidney and Palmer, Martha and Kann, Katharina},
  booktitle={Proceedings of the 17th International Conference on Educational Data Mining},
  pages={835--843},
  year={2024}
}



@inproceedings{ma2024my,
  title={(Why) Is My Prompt Getting Worse? Rethinking Regression Testing for Evolving LLM APIs},
  author={Ma, Wanqin and Yang, Chenyang and K{\"a}stner, Christian},
  booktitle={Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering-Software Engineering for AI},
  pages={166--171},
  year={2024}
}



@inproceedings{jang2023can,
  title={Can large language models truly understand prompts? a case study with negated prompts},
  author={Jang, Joel and Ye, Seonghyeon and Seo, Minjoon},
  booktitle={Transfer learning for natural language processing workshop},
  pages={52--62},
  year={2023},
  organization={PMLR}
}



@article{yan2024practical,
  title={Practical and ethical challenges of large language models in education: A systematic scoping review},
  author={Yan, Lixiang and Sha, Lele and Zhao, Linxuan and Li, Yuheng and Martinez-Maldonado, Roberto and Chen, Guanliang and Li, Xinyu and Jin, Yueqiao and Ga{\v{s}}evi{\'c}, Dragan},
  journal={British Journal of Educational Technology},
  volume={55},
  number={1},
  pages={90--112},
  year={2024},
  publisher={Wiley Online Library}
}



@inproceedings{liu2024confronting,
  title={Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications},
  author={Liu, Yanchen and Gautam, Srishti and Ma, Jiaqi and Lakkaraju, Himabindu},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={3603--3620},
  year={2024}
}



@article{gallegos2024bias,
  title={Bias and fairness in large language models: A survey},
  author={Gallegos, Isabel O and Rossi, Ryan A and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K},
  journal={Computational Linguistics},
  pages={1--79},
  year={2024},
  publisher={MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA~…}
}
@inproceedings{pangakis2024knowledge,
  title={Knowledge Distillation in Automated Annotation: Supervised Text Classification with LLM-Generated Training Labels},
  author={Pangakis, Nicholas and Wolken, Samuel},
  booktitle={The Sixth Workshop on Natural Language Processing and Computational Social Science},
  pages={113},
  year={2024}
}


@article{chang2024survey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={3},
  pages={1--45},
  year={2024},
  publisher={ACM New York, NY}
}

@article{delgado2019cohen,
  title={Why Cohen’s Kappa should be avoided as performance measure in classification},
  author={Delgado, Rosario and Tibau, Xavier-Andoni},
  journal={PloS one},
  volume={14},
  number={9},
  pages={e0222916},
  year={2019},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{kolesnyk2022justification,
  title={Justification for the use of Cohen’s Kappa statistic in experimental studies of NLP and text mining},
  author={Kolesnyk, AS and Khairova, NF},
  journal={Cybernetics and Systems Analysis},
  volume={58},
  number={2},
  pages={280--288},
  year={2022},
  publisher={Springer}
}

@inproceedings{blood2007disagreement,
  title={Disagreement on agreement: two alternative agreement coefficients},
  author={Blood, Emily and Spratt, Kevin F},
  booktitle={SAS Global Forum},
  volume={186},
  pages={1--12},
  year={2007},
  organization={Citeseer}
}

@article{gwet2008computing,
  title={Computing inter-rater reliability and its variance in the presence of high agreement},
  author={Gwet, Kilem Li},
  journal={British Journal of Mathematical and Statistical Psychology},
  volume={61},
  number={1},
  pages={29--48},
  year={2008},
  publisher={Wiley Online Library}
}

@article{chicco2021matthews,
  title={The Matthews correlation coefficient (MCC) is more informative than Cohen’s Kappa and Brier score in binary classification assessment},
  author={Chicco, Davide and Warrens, Matthijs J and Jurman, Giuseppe},
  journal={Ieee Access},
  volume={9},
  pages={78368--78381},
  year={2021},
  publisher={IEEE}
}

@article{sokolova2009systematic,
  title={A systematic analysis of performance measures for classification tasks},
  author={Sokolova, Marina and Lapalme, Guy},
  journal={Information processing \& management},
  volume={45},
  number={4},
  pages={427--437},
  year={2009},
  publisher={Elsevier}
}

@article{ziems2024can,
  title={Can large language models transform computational social science?},
  author={Ziems, Caleb and Held, William and Shaikh, Omar and Chen, Jiaao and Zhang, Zhehao and Yang, Diyi},
  journal={Computational Linguistics},
  volume={50},
  number={1},
  pages={237--291},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{wang2023sight,
  title={SIGHT: A Large Annotated Dataset on Student Insights Gathered from Higher Education Transcripts},
  author={Wang, Rose and Wirawarn, Pawan and Goodman, Noah and Demszky, Dorottya},
  booktitle={Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)},
  pages={315--351},
  year={2023}
}

@Manual{mltools,
    title = {mltools: Machine Learning Tools},
    author = {Ben Gorman},
    year = {2018},
    note = {R package version 0.3.5},
    url = {https://CRAN.R-project.org/package=mltools},
}
  
@INCOLLECTION{LABOOK2_Chapter_1,
	title = {Unpacking learning in the age of AI: Bridging AI, Complexity, and Precision Education},
	author = "López-Pernas, Sonsoles and Tlili, Ahmed and Majumdar, Rwitajit and Heikkinen, Sami and Saqr, Mohammed",
	booktitle = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
  	editor    = "Saqr, Mohammed and López-Pernas, Sonsoles",
	publisher = "Springer Nature Switzerland",
	address   = "Cham",
	year      =  2025,
	language  = "en"
}
@INCOLLECTION{LABOOK2_Chapter_2,
	title = {AI, Explainable AI and Evaluative AI: An Introduction to Informed Data-Driven Decision-Making in Education},
	author = "López-Pernas, Sonsoles and Oliveira, Eduardo and Song, Yige and Saqr, Mohammed",
	booktitle = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
  	editor    = "Saqr, Mohammed and López-Pernas, Sonsoles",
	publisher = "Springer Nature Switzerland",
	address   = "Cham",
	year      =  2025,
	language  = "en"
}
@INCOLLECTION{LABOOK2_Chapter_3,
	title = {Artificial Intelligence: Using Machine Learning to Predict Students' Performance},
	author = "Saqr, Mohammed and Misiejuk, Kamila and Tikka, Santtu and López-Pernas, Sonsoles",
	booktitle = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
  	editor    = "Saqr, Mohammed and López-Pernas, Sonsoles",
	publisher = "Springer Nature Switzerland",
	address   = "Cham",
	year      =  2025,
	language  = "en"
}
@INCOLLECTION{LABOOK2_Chapter_4,
	title = {Artificial Intelligence: Using Machine Learning to Classify Students and Predict Low Achievers},
	author = "Saqr, Mohammed and Misiejuk, Kamila and Tikka, Santtu and López-Pernas, Sonsoles",
	booktitle = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
  	editor    = "Saqr, Mohammed and López-Pernas, Sonsoles",
	publisher = "Springer Nature Switzerland",
	address   = "Cham",
	year      =  2025,
	language  = "en"
}
@INCOLLECTION{LABOOK2_Chapter_5,
	title = {Comparative Analysis of Regularization Methods for Predicting Student Certification in Online Courses},
	author = "Li, Tian and Han, Feifei and Guo, Jiesi and Wu	, Jinran",
	booktitle = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
  	editor    = "Saqr, Mohammed and López-Pernas, Sonsoles",
	publisher = "Springer Nature Switzerland",
	address   = "Cham",
	year      =  2025,
	language  = "en"
}
@INCOLLECTION{LABOOK2_Chapter_6,
	title = {Explainable Artificial Intelligence in Education: A Tutorial for Identifying the Variables that Matter },
	author = "Saqr, Mohammed and López-Pernas	, Sonsoles",
	booktitle = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
  	editor    = "Saqr, Mohammed and López-Pernas, Sonsoles",
	publisher = "Springer Nature Switzerland",
	address   = "Cham",
	year      =  2025,
	language  = "en"
}
@INCOLLECTION{LABOOK2_Chapter_7,
	title = {Individualized Explainable Artificial Intelligence: A Tutorial For Identifying Local and Individual Predictions},
	author = "Saqr, Mohammed and López-Pernas	, Sonsoles",
	booktitle = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
  	editor    = "Saqr, Mohammed and López-Pernas, Sonsoles",
	publisher = "Springer Nature Switzerland",
	address   = "Cham",
	year      =  2025,
	language  = "en"
}
@INCOLLECTION{LABOOK2_Chapter_8,
	title = {An Introduction to Large Language Models in Education},
	author = "Oliveira, Eduardo and Song, Yige and Saqr, Mohammed and López-Pernas, Sonsoles",
	booktitle = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
  	editor    = "Saqr, Mohammed and López-Pernas, Sonsoles",
	publisher = "Springer Nature Switzerland",
	address   = "Cham",
	year      =  2025,
	language  = "en"
}
@INCOLLECTION{LABOOK2_Chapter_9,
	title = {The Use of Natural Language Processing in Learning Analytics},
	author = "Wongvorachan, Tarid and Bulut	, Okan",
	booktitle = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
  	editor    = "Saqr, Mohammed and López-Pernas, Sonsoles",
	publisher = "Springer Nature Switzerland",
	address   = "Cham",
	year      =  2025,
	language  = "en"
}
@INCOLLECTION{LABOOK2_Chapter_10,
	title = { Using Language Models for Automated Discourse Coding: A Primer and Tutorial},
	author = "López-Pernas, Sonsoles and Misiejuk, Kamila and Saqr, Mohammed",
	booktitle = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
  	editor    = "Saqr, Mohammed and López-Pernas, Sonsoles",
	publisher = "Springer Nature Switzerland",
	address   = "Cham",
	year      =  2025,
	language  = "en"
}
@INCOLLECTION{LABOOK2_Chapter_11,
	title = {LLMs for Explainable Artificial Intelligence: Automating Natural Language Explanations of Predictive Analytics Models},
	author = "López-Pernas, Sonsoles and Song, Yige and Oliveira, Eduardo and Saqr, Mohammed",
	booktitle = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
  	editor    = "Saqr, Mohammed and López-Pernas, Sonsoles",
	publisher = "Springer Nature Switzerland",
	address   = "Cham",
	year      =  2025,
	language  = "en"
}
@INCOLLECTION{LABOOK2_Chapter_12,
	title = { Complex Dynamic Systems in Education: Beyond the Static, the Linear and the Causal Reductionism},
	author = "Saqr, Mohammed and Dever, Daryn and López-Pernas, Sonsoles and Gernigon, Christophe and Marchand, Gwen and Kaplan, Avi",
	booktitle = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
  	editor    = "Saqr, Mohammed and López-Pernas, Sonsoles",
	publisher = "Springer Nature Switzerland",
	address   = "Cham",
	year      =  2025,
	language  = "en"
}
@INCOLLECTION{LABOOK2_Chapter_13,
	title = { The Advanced Applications of Psychological Networks with EGA},
	author = "Wongvorachan, Tarid and Bulut, Okan",
	booktitle = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
  	editor    = "Saqr, Mohammed and López-Pernas, Sonsoles",
	publisher = "Springer Nature Switzerland",
	address   = "Cham",
	year      =  2025,
	language  = "en"
}
@INCOLLECTION{LABOOK2_Chapter_14,
	title = {Detecting Nonlinear Patterns in Education Research: A tutorial on Recurrence Quantification Analysis},
	author = "Dever, Daryn",
	booktitle = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
  	editor    = "Saqr, Mohammed and López-Pernas, Sonsoles",
	publisher = "Springer Nature Switzerland",
	address   = "Cham",
	year      =  2025,
	language  = "en"
}
@INCOLLECTION{LABOOK2_Chapter_15,
	title = { Mapping Relational Dynamics with Transition Network Analysis: A Primer and Tutorial},
	author = "Saqr, Mohammed and López-Pernas, Sonsoles and Tikka, Santtu",
	booktitle = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
  	editor    = "Saqr, Mohammed and López-Pernas, Sonsoles",
	publisher = "Springer Nature Switzerland",
	address   = "Cham",
	year      =  2025,
	language  = "en"
}
@INCOLLECTION{LABOOK2_Chapter_16,
	title = { Capturing The Breadth and Dynamics of the Temporal Processes with Frequency Transition Network Analysis: A Primer and Tutorial},
	author = "Saqr, Mohammed and López-Pernas, Sonsoles and Tikka, Santtu",
	booktitle = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
  	editor    = "Saqr, Mohammed and López-Pernas, Sonsoles",
	publisher = "Springer Nature Switzerland",
	address   = "Cham",
	year      =  2025,
	language  = "en"
}
@INCOLLECTION{LABOOK2_Chapter_17,
	title = { Mining Patterns and Clusters with Transition Network Analysis: A Heterogeneity Approach},
	author = "López-Pernas, Sonsoles and Tikka, Santtu and Saqr, Mohammed",
	booktitle = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
  	editor    = "Saqr, Mohammed and López-Pernas, Sonsoles",
	publisher = "Springer Nature Switzerland",
	address   = "Cham",
	year      =  2025,
	language  = "en"
}
@INCOLLECTION{LABOOK2_Chapter_18,
	title = { A Comprehensive Introduction to Idiographic and Within-Person Analytics},
	author = "Saqr, Mohammed and Ito, Hibiki and López-Pernas, Sonsoles",
	booktitle = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
  	editor    = "Saqr, Mohammed and López-Pernas, Sonsoles",
	publisher = "Springer Nature Switzerland",
	address   = "Cham",
	year      =  2025,
	language  = "en"
}
@INCOLLECTION{LABOOK2_Chapter_19,
	title = { The Three Levels of Analysis: Variable-centered, Person-centered and Person-specific Analysis in Education},
	author = "López-Pernas, Sonsoles and Kayaduman, Halil and Leonie V.D.Vogelsmeier, E.}, Saqr, Mohammed",
	booktitle = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
  	editor    = "Saqr, Mohammed and López-Pernas, Sonsoles",
	publisher = "Springer Nature Switzerland",
	address   = "Cham",
	year      =  2025,
	language  = "en"
}
@INCOLLECTION{LABOOK2_Chapter_20,
	title = { Idiographic Networks: A tutorial on Graphical Vector Autoregression and Unified Structural Equation Modeling},
	author = "Saqr, Mohammed and Dever, Daryn and López-Pernas	, Sonsoles",
	booktitle = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
  	editor    = "Saqr, Mohammed and López-Pernas, Sonsoles",
	publisher = "Springer Nature Switzerland",
	address   = "Cham",
	year      =  2025,
	language  = "en"
}
@INCOLLECTION{LABOOK2_Chapter_21,
	title = { Detecting Long-Memory Psychological Processes in Academic Settings Using Whittle’s Maximum Likelihood Estimator: An Application with R},
	author = "Altamore, Rémi and Roume, Clément and Teboul, Anne and Gernigon, Christophe",
	booktitle = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
  	editor    = "Saqr, Mohammed and López-Pernas, Sonsoles",
	publisher = "Springer Nature Switzerland",
	address   = "Cham",
	year      =  2025,
	language  = "en"
}
@INCOLLECTION{LABOOK2_Chapter_22,
	title = { Automating Individualized Machine Learning and AI Prediction using AutoML: The Case of Idiographic Predictions},
	author = "Saqr, Mohammed and Tlili, Ahmed and López-Pernas, Sonsoles",
	booktitle = {Advanced Learning Analytics Methods: AI, Precision and Complexity},
  	editor    = "Saqr, Mohammed and López-Pernas, Sonsoles",
	publisher = "Springer Nature Switzerland",
	address   = "Cham",
	year      =  2025,
	language  = "en"
}  

@ARTICLE{Liu2019-bz,
  title         = "{RoBERTa}: A robustly optimized {BERT} pretraining approach",
  author        = "Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei
                   and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis,
                   Mike and Zettlemoyer, Luke and Stoyanov, Veselin",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language model pretraining has led to significant performance
                   gains but careful comparison between different approaches is
                   challenging. Training is computationally expensive, often
                   done on private datasets of different sizes, and, as we will
                   show, hyperparameter choices have significant impact on the
                   final results. We present a replication study of BERT
                   pretraining (Devlin et al., 2019) that carefully measures the
                   impact of many key hyperparameters and training data size. We
                   find that BERT was significantly undertrained, and can match
                   or exceed the performance of every model published after it.
                   Our best model achieves state-of-the-art results on GLUE,
                   RACE and SQuAD. These results highlight the importance of
                   previously overlooked design choices, and raise questions
                   about the source of recently reported improvements. We
                   release our models and code.",
  month         =  jul,
  year          =  2019,
  url           = "https://scholar.google.es/citations?user=H9buyroAAAAJ&hl=en&oi=sra",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1907.11692"
}


@ARTICLE{Yang2019-rt,
  title         = "{XLNet}: Generalized Autoregressive Pretraining for Language
                   Understanding",
  author        = "Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell,
                   Jaime and Salakhutdinov, Ruslan and Le, Quoc V",
  journal       = "arXiv [cs.CL]",
  abstract      = "With the capability of modeling bidirectional contexts,
                   denoising autoencoding based pretraining like BERT achieves
                   better performance than pretraining approaches based on
                   autoregressive language modeling. However, relying on
                   corrupting the input with masks, BERT neglects dependency
                   between the masked positions and suffers from a
                   pretrain-finetune discrepancy. In light of these pros and
                   cons, we propose XLNet, a generalized autoregressive
                   pretraining method that (1) enables learning bidirectional
                   contexts by maximizing the expected likelihood over all
                   permutations of the factorization order and (2) overcomes the
                   limitations of BERT thanks to its autoregressive formulation.
                   Furthermore, XLNet integrates ideas from Transformer-XL, the
                   state-of-the-art autoregressive model, into pretraining.
                   Empirically, under comparable experiment settings, XLNet
                   outperforms BERT on 20 tasks, often by a large margin,
                   including question answering, natural language inference,
                   sentiment analysis, and document ranking.",
  month         =  jun,
  year          =  2019,
  url           = "http://arxiv.org/abs/1906.08237",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1906.08237"
}
