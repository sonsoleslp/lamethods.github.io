<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sonsoles López-Pernas">
<meta name="author" content="Kamila Misiejuk">
<meta name="author" content="Mohammed Saqr">
<meta name="keywords" content="automatic coding, large language models, BERT, learning analytics, natural language processing, artificial intelligence">

<title>Advanced learning analytics methods - 10&nbsp; Using BERT-like Language Models for Automated Discourse Coding: A Primer and Tutorial</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/ch11-llmsxai/ch11-llmsxai.html" rel="next">
<link href="../../chapters/ch09-nlp/ch09-nlp.html" rel="prev">
<script src="../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-Y4VBV3J9WD"></script>

<script type="text/plain" cookie-consent="tracking">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Y4VBV3J9WD', { 'anonymize_ip': true});
</script>

<script type="text/javascript" charset="UTF-8">
document.addEventListener('DOMContentLoaded', function () {
cookieconsent.run({
  "notice_banner_type":"simple",
  "consent_type":"implied",
  "palette":"light",
  "language":"en",
  "page_load_consent_levels":["strictly-necessary","functionality","tracking","targeting"],
  "notice_banner_reject_button_hide":false,
  "preferences_center_close_button_hide":false,
  "website_name":""
  });
});
</script> 
  

<link href="../../site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet">
<script src="../../site_libs/pagedtable-1.1/js/pagedtable.js"></script>


<meta name="twitter:title" content="Advanced learning analytics methods - 10&nbsp; Using BERT-like Language Models for Automated Discourse Coding: A Primer and Tutorial">
<meta name="twitter:description" content="Coding text data in qualitative research is a labor-intensive and error-prone process that requires meticulous attention to detail as well as consistency in the coding criteria.">
<meta name="twitter:card" content="summary">
<meta name="citation_title" content="[10]{.chapter-number}&nbsp; [Using BERT-like Language Models for Automated Discourse Coding: A Primer and Tutorial]{.chapter-title}">
<meta name="citation_abstract" content="Coding text data in qualitative research is a labor-intensive and error-prone process that requires meticulous attention to detail as well as consistency in the coding criteria. Large Language Models (LLMs) present a promising solution to alleviate some of these challenges by automating part of the coding process. This tutorial explores the application of LLMs for automated text classification using word embeddings through the R package `text` and different BERT-like large language models. We implement a machine learning pipeline that combines word embeddings with supervised machine learning algorithms to code text data with high accuracy. We present a case study on collaborative problem-solving in which we train a classification model on a small portion of manually coded data and then apply it to classify the remaining data. The tutorial also covers the evaluation of coding accuracy by comparing human and machine-coded data using classic machine learning performance metrics as well as Cohen&amp;amp;#039;s kappa, Matthews' correlation coefficient, and Gwet AC1, measures commonly used to assess interrater reliability in qualitative research. Lastly, we apply different learning analytics techniques to compare the findings obtained from human-coded data and automatically coded data.">
<meta name="citation_keywords" content="automatic coding, large language models, BERT, learning analytics, natural language processing, artificial intelligence">
<meta name="citation_author" content="Sonsoles López-Pernas">
<meta name="citation_author" content="Kamila Misiejuk">
<meta name="citation_author" content="Mohammed Saqr">
<meta name="citation_fulltext_html_url" content="https://lamethods.github.io/ch10-bert.html">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=DeliData: A dataset for deliberation in multi-party problem solving;,citation_author=Georgi Karadzhov;,citation_author=Tom Stafford;,citation_author=Andreas Vlachos;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_issue=CSCW2;,citation_volume=7;,citation_journal_title=Proceedings of the ACM on Human-Computer Interaction;,citation_publisher=ACM New York, NY, USA;">
<meta name="citation_reference" content="citation_title=The text-package: An R-package for analyzing and visualizing human language using natural language processing and transformers;,citation_author=Oscar Kjell;,citation_author=Salvatore Giorgi;,citation_author=H Andrew Schwartz;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_issue=6;,citation_doi=10.1037/met0000542;,citation_issn=1082-989X,1939-1463;,citation_pmid=37126041;,citation_volume=28;,citation_journal_title=Psychol. Methods;,citation_publisher=American Psychological Association (APA);">
<meta name="citation_reference" content="citation_title=Generative artificial intelligence in learning analytics: Contextualising opportunities and challenges through the learning analytics cycle;,citation_author=Lixiang Yan;,citation_author=Roberto Martinez-Maldonado;,citation_author=Dragan Gasevic;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_doi=10.1145/3636555.3636856;,citation_conference_title=Proceedings of the 14th learning analytics and knowledge conference;,citation_conference=ACM;">
<meta name="citation_reference" content="citation_title=Why you should give your students automatic process feedback on their collaboration: Evidence from a randomized experiment;,citation_author=Lukas Menzel;,citation_author=Sebastian Gombert;,citation_author=Joshua Weidlich;,citation_author=Aron Fink;,citation_author=Andreas Frey;,citation_author=Hendrik Drachsler;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_doi=10.1007/978-3-031-42682-7\_14;,citation_isbn=9783031426810,9783031426827;,citation_issn=0302-9743,1611-3349;,citation_inbook_title=Lecture notes in computer science;,citation_series_title=Lecture notes in computer science;">
<meta name="citation_reference" content="citation_title=Automating data narratives in learning analytics dashboards using GenAI;,citation_author=Adriano Pinargote;,citation_author=Eddy Calderón;,citation_author=Kevin Cevallos;,citation_author=Gladys Carrillo;,citation_author=Katherine Chiluiza;,citation_author=Vanessa Echeverria;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://ceur-ws.org/Vol-3667/DS-LAK24_paper_5.pdf;,citation_conference_title=2024 joint of international conference on learning analytics and knowledge workshops;,citation_conference=CEUR-WS;">
<meta name="citation_reference" content="citation_title=Learning analytics dashboard for problem-based learning;,citation_author=Zilong Pan;,citation_author=Chenglu Li;,citation_author=Min Liu;,citation_publication_date=2020-08;,citation_cover_date=2020-08;,citation_year=2020;,citation_doi=10.1145/3386527.3406751;,citation_isbn=9781450379519;,citation_conference_title=Proceedings of the seventh ACM conference on learning @ scale;,citation_conference=ACM;">
<meta name="citation_reference" content="citation_title=“It’s really enjoyable to see me solve the problem like a hero”: GenAI-enhanced data comics as a learning analytics tool;,citation_author=Mikaela E Milesi;,citation_author=Riordan Alfredo;,citation_author=Vanessa Echeverria;,citation_author=Lixiang Yan;,citation_author=Linxuan Zhao;,citation_author=Yi-Shan Tsai;,citation_author=Roberto Martinez-Maldonado;,citation_publication_date=2024-05;,citation_cover_date=2024-05;,citation_year=2024;,citation_doi=10.1145/3613905.3651111;,citation_conference_title=Extended abstracts of the CHI conference on human factors in computing systems;,citation_conference=ACM;">
<meta name="citation_reference" content="citation_title=Automated discourse analysis via generative artificial intelligence;,citation_author=Ryan Garg;,citation_author=Jaeyoung Han;,citation_author=Yixin Cheng;,citation_author=Zheng Fang;,citation_author=Zachari Swiecki;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_doi=10.1145/3636555.3636879;,citation_conference_title=Proceedings of the 14th learning analytics and knowledge conference;,citation_conference=ACM;">
<meta name="citation_reference" content="citation_title=Augmenting assessment with AI coding of online student discourse: A question of reliability;,citation_author=Kamila Misiejuk;,citation_author=Rogers Kaliisa;,citation_author=Jennifer Scianna;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=100216;,citation_doi=10.1016/j.caeai.2024.100216;,citation_issn=2666-920X;,citation_volume=6;,citation_journal_title=Computers and Education: Artificial Intelligence;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=Say what? Automatic modeling of collaborative problem solving skills from student speech in the wild;,citation_author=Samuel L Pugh;,citation_author=Shree Krishna Subburaj;,citation_author=A Rao;,citation_author=Angela E B Stewart;,citation_author=Jessica Andrews-Todd;,citation_author=S D’Mello;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://files.eric.ed.gov/fulltext/ED615653.pdf;,citation_journal_title=Int Conf Young Spéc Micro/nanotechnologies Electron Device;">
<meta name="citation_reference" content="citation_title=Dplyr: A grammar of data manipulation;,citation_author=Hadley Wickham;,citation_author=Romain François;,citation_author=Lionel Henry;,citation_author=Kirill Müller;,citation_author=Davis Vaughan;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://CRAN.R-project.org/package=dplyr;">
<meta name="citation_reference" content="citation_title=Building predictive models in r using the caret package;,citation_author=Kuhn;,citation_author=Max;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_issue=5;,citation_doi=10.18637/jss.v028.i05;,citation_volume=28;,citation_journal_title=Journal of Statistical Software;">
<meta name="citation_reference" content="citation_title=The text-package: An r-package for analyzing and visualizing human language using natural language processing and deep learning;,citation_author=Oscar Kjell;,citation_author=Salvatore Giorgi;,citation_author=H. Andrew Schwartz;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_doi=10.1037/met0000542;,citation_journal_title=Psychological Methods;">
<meta name="citation_reference" content="citation_title=Reticulate: Interface to ’python’;,citation_author=Kevin Ushey;,citation_author=JJ Allaire;,citation_author=Yuan Tang;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://CRAN.R-project.org/package=reticulate;">
<meta name="citation_reference" content="citation_title=Tibble: Simple data frames;,citation_author=Kirill Müller;,citation_author=Hadley Wickham;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://CRAN.R-project.org/package=tibble;">
<meta name="citation_reference" content="citation_title=Tidyr: Tidy messy data;,citation_author=Hadley Wickham;,citation_author=Davis Vaughan;,citation_author=Maximilian Girlich;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://CRAN.R-project.org/package=tidyr;">
<meta name="citation_reference" content="citation_title=Sequence analysis in education: Principles, technique, and tutorial with R;,citation_abstract=Patterns exist everywhere in our life, from the sequence of genes to the order of steps in cooking recipes. Discovering patterns, variations, regularities, or irregularities is at the heart of scientific inquiry and, therefore, several data mining methods have been developed to understand patterns. Sequence analysis—or sequence mining—was developed almost four decades ago to address the increasing needs for pattern mining [1]. Ever since, a wealth of applications, algorithms, and statistical tools have been developed, adapted, or incorporated into the array of sequence analysis. Since sequence mining has been conceptualized, it has grown in scale of adoption and range of applications across life and social sciences [2] and education research was no exception (eg,[3]). As a data mining technique, sequence mining has been commonly implemented to identify hidden patterns that would otherwise be missed using other analytical techniques and find interesting subsequences (parts of the sequence) that have practical significance or unexpected sequences that we did not know existed [4]. For instance, by mining sequences of collaborative dialogue, we;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_author=Satu Helske;,citation_author=Marion Durand;,citation_author=Keefe Murphy;,citation_author=Matthias Studer;,citation_author=Gilbert Ritschard;,citation_publication_date=2024-02;,citation_cover_date=2024-02;,citation_year=2024;,citation_doi=10.1007/978-3-031-54464-4_10;,citation_publisher=Springer Nature Switzerland;">
<meta name="citation_reference" content="citation_title=The temporal dynamics of online problem-based learning: Why and when sequence matters;,citation_abstract=Early research on online PBL explored student satisfaction, effectiveness, and design. The temporal aspect of online PBL has rarely been addressed. Thus, a gap exists in our knowledge regarding how online PBL unfolds: when and for how long a group engages in collaborative discussions. Similarly, little is known about whether and what sequence of interactions could predict higher achievement. This study aims to bridge such a gap by implementing the latest advances in temporal learning analytics to analyze the sequential and temporal aspects of online PBL across a large sample (n = 204 students) of qualitatively coded interactions (8,009 interactions). We analyzed interactions at the group level to understand the group dynamics across whole problem discussions, and at the student level to understand the students’ contribution dynamics across different episodes. We followed such analyses by examining the association of interaction types and the sequences thereof with students’ performance using multilevel linear regression models. The analysis of the interactions reflected that the scripted PBL process is followed a logical sequence, yet often lacked enough depth. When cognitive interactions (e.g., arguments, questions, and evaluations) occurred, they kindled high cognitive interactions, when low cognitive and social interactions dominated, they kindled low cognitive interactions. The order and sequence of interactions were more predictive of performance, and with a higher explanatory power as compared to frequencies. Starting or initiating interactions (even with low cognitive content) showed the highest association with performance, pointing to the importance of initiative and sequencing.;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_publication_date=2023-03;,citation_cover_date=2023-03;,citation_year=2023;,citation_fulltext_html_url=https://doi.org/10.1007/s11412-023-09385-1;,citation_issue=1;,citation_doi=10.1007/s11412-023-09385-1;,citation_issn=1556-1615;,citation_volume=18;,citation_journal_title=International Journal of Computer-Supported Collaborative Learning;">
<meta name="citation_reference" content="citation_title=Analyzing and visualizing state sequences in R with TraMineR;,citation_author=Alexis Gabadinho;,citation_author=Gilbert Ritschard;,citation_author=Nicolas Séverin Mueller;,citation_author=Matthias Studer;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_issue=4;,citation_doi=10.18637/jss.v040.i04;,citation_volume=40;,citation_journal_title=J. Stat. Softw.;">
<meta name="citation_reference" content="citation_title=A modern approach to transition analysis and process mining with markov models in education;,citation_abstract=AbstractThis chapter presents an introduction to Markovian modelling for the analysis of sequence data. Contrary to the deterministic approach seen in the previous sequence analysis chapters, Markovian models are probabilistic models, focusing on the transitions between states instead of studying sequences as a whole. The chapter provides an introduction to this method and differentiates between its most common variations: first-order Markov models, hidden Markov models, mixture Markov models, and mixture hidden Markov models. In addition to a thorough explanation and contextualisation within the existing literature, the chapter provides a step-by-step tutorial on how to implement each type of Markovian model using the R package seqHMM. The chapter also provides a complete guide to performing stochastic process mining with Markovian models as well as plotting, comparing and clustering different process models.;,citation_author=Jouni Helske;,citation_author=Satu Helske;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_author=Keefe Murphy;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://link.springer.com/chapter/10.1007/978-3-031-54464-4_12;,citation_doi=10.1007/978-3-031-54464-4\_12;,citation_isbn=9783031544637,9783031544644;,citation_inbook_title=Learning analytics methods and tutorials;">
<meta name="citation_reference" content="citation_title=Epistemic network analysis and ordered network analysis in learning analytics;,citation_abstract=AbstractThis chapter provides a tutorial on conducting epistemic network analysis (ENA) and ordered network analysis (ONA) using R. We introduce these two techniques together because they share similar theoretical foundations, but each addresses a different challenge for analyzing large-scale qualitative data on learning processes. ENA and ONA are methods for quantifying, visualizing, and interpreting network data. Taking coded data as input, ENA and ONA represent associations between codes in undirected or directed weighted network models, respectively. Both techniques measure the strength of association among codes and illustrate the structure of connections in network graphs, and they quantify changes in the composition and strength of those connections over time. Importantly, ENA and ONA enable comparison of networks both visually and via summary statistics, so they can be used to explore a wide range of research questions in contexts where patterns of association in coded data are hypothesized to be meaningful and where comparing those patterns across individuals or groups is important.;,citation_author=Yuanru Tan;,citation_author=Zachari Swiecki;,citation_author=A R Ruis;,citation_author=David Shaffer;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://link.springer.com/chapter/10.1007/978-3-031-54464-4_18;,citation_doi=10.1007/978-3-031-54464-4\_18;,citation_isbn=9783031544637,9783031544644;,citation_inbook_title=Learning analytics methods and tutorials;">
<meta name="citation_reference" content="citation_title=Transition network analysis: A novel framework for modeling, visualizing, and identifying the temporal patterns of learners and learning processes;,citation_abstract=This paper proposes a novel analytical framework: Transition Network Analysis (TNA), an approach that integrates Stochastic Process Mining and probabilistic graph representation to model, visualize, and identify transition patterns in the learning process data. Combining the relational and temporal aspects into a single lens offers capabilities beyond either framework, including centralities to capture important learning events, community finding to identify patterns of behavior, and clustering to reveal temporal patterns. This paper introduces the theoretical and mathematical foundations of TNA. To demonstrate the functionalities of TNA, we present a case study with students (n=191) engaged in small-group collaboration to map patterns of group dynamics using the theories of co-regulation and socially-shared regulated learning. The analysis revealed that TNA could reveal the regulatory processes and identify important events, temporal patterns and clusters. Bootstrap validation established the significant transitions and eliminated spurious transitions. In doing so, we showcase TNA’s utility to capture learning dynamics and provide a robust framework for investigating the temporal evolution of learning processes. Future directions include —inter alia— advancing estimation methods, expanding reliability assessment, exploring longitudinal TNA, and comparing TNA networks using permutation tests.;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_author=Tiina Törmänen;,citation_author=Rogers Kaliisa;,citation_author=Kamila Misiejuk;,citation_author=Santtu Tikka;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_conference_title=Proceedings of learning analytics &amp;amp;amp; knowledge (LAK ’25);,citation_conference=ACM;">
<meta name="citation_reference" content="citation_title=Thinking about the coding process in qualitative data analysis;,citation_author=Victoria Elliott;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=11;,citation_volume=23;,citation_journal_title=Qualitative report;,citation_publisher=Nova Southeastern University;">
<meta name="citation_reference" content="citation_title=A general inductive approach for analyzing qualitative evaluation data;,citation_author=David R Thomas;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_issue=2;,citation_volume=27;,citation_journal_title=American journal of evaluation;,citation_publisher=Sage Publications Sage CA: Thousand Oaks, CA;">
<meta name="citation_reference" content="citation_title=Thinking about the coding process in qualitative data analysis;,citation_author=Victoria Elliott;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_issue=11;,citation_volume=23;,citation_journal_title=Qualitative report;,citation_publisher=Nova Southeastern University;">
<meta name="citation_reference" content="citation_title=When small wins big: Classification tasks where compact models outperform original GPT-4;,citation_author=Baptiste Lefort;,citation_author=Eric Benhamou;,citation_author=Jean-Jacques Ohana;,citation_author=Beatrice Guez;,citation_author=David Saltiel;,citation_author=Damien Challet;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_journal_title=Available at SSRN 4780454;">
<meta name="citation_reference" content="citation_title=The ethics of using generative AI for qualitative data analysis;,citation_author=Robert M Davison;,citation_author=Hameed Chughtai;,citation_author=Petter Nielsen;,citation_author=Marco Marabelli;,citation_author=Federico Iannacci;,citation_author=Marjolein Offenbeek;,citation_author=Monideepa Tarafdar;,citation_author=Manuel Trenz;,citation_author=Angsana Techatassanasoontorn;,citation_author=Antonio Diaz Andrade;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_publisher=John Wiley &amp;amp;amp; Sons Ltd;">
<meta name="citation_reference" content="citation_title=Exploring the use of artificial intelligence for qualitative data analysis: The case of ChatGPT;,citation_author=David L Morgan;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_volume=22;,citation_journal_title=International journal of qualitative methods;,citation_publisher=SAGE Publications Sage CA: Los Angeles, CA;">
<meta name="citation_reference" content="citation_title=Comparing the efficacy and efficiency of human and generative AI: Qualitative thematic analyses;,citation_author=Maximo R Prescott;,citation_author=Samantha Yeager;,citation_author=Lillian Ham;,citation_author=Carlos D Rivera Saldana;,citation_author=Vanessa Serrano;,citation_author=Joey Narez;,citation_author=Dafna Paltin;,citation_author=Jorge Delgado;,citation_author=David J Moore;,citation_author=Jessica Montoya;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_volume=3;,citation_journal_title=JMIR AI;,citation_publisher=JMIR Publications Toronto, Canada;">
<meta name="citation_reference" content="citation_title=Prompting as panacea? A case study of in-context learning performance for qualitative coding of classroom dialog;,citation_author=Ananya Ganesh;,citation_author=Chelsea Chandler;,citation_author=Sidney D’Mello;,citation_author=Martha Palmer;,citation_author=Katharina Kann;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_conference_title=Proceedings of the 17th international conference on educational data mining;">
<meta name="citation_reference" content="citation_title=ChatGPT for education research: Exploring the potential of large language models for qualitative codebook development;,citation_author=Amanda Barany;,citation_author=Nidhi Nasiar;,citation_author=Chelsea Porter;,citation_author=Andres Felipe Zambrano;,citation_author=Alexandra L Andres;,citation_author=Dara Bright;,citation_author=Mamta Shah;,citation_author=Xiner Liu;,citation_author=Sabrina Gao;,citation_author=Jiayi Zhang;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_conference_title=International conference on artificial intelligence in education;,citation_conference=Springer;">
<meta name="citation_reference" content="citation_title=A review on large language models: Architectures, applications, taxonomies, open issues and challenges;,citation_author=Mohaimenul Azam Khan Raiaan;,citation_author=Md Saddam Hossain Mukta;,citation_author=Kaniz Fatema;,citation_author=Nur Mohammad Fahad;,citation_author=Sadman Sakib;,citation_author=Most Marufatul Jannat Mim;,citation_author=Jubaer Ahmad;,citation_author=Mohammed Eunus Ali;,citation_author=Sami Azam;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_journal_title=IEEE Access;,citation_publisher=IEEE;">
<meta name="citation_reference" content="citation_title=A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions;,citation_author=Lei Huang;,citation_author=Weijiang Yu;,citation_author=Weitao Ma;,citation_author=Weihong Zhong;,citation_author=Zhangyin Feng;,citation_author=Haotian Wang;,citation_author=Qianglong Chen;,citation_author=Weihua Peng;,citation_author=Xiaocheng Feng;,citation_author=Bing Qin;,citation_author=others;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=ACM Transactions on Information Systems;,citation_publisher=ACM New York, NY;">
<meta name="citation_reference" content="citation_title=Do we still need BERT in the age of GPT? Comparing the benefits of domain-adaptation and in-context-learning approaches to using LLMs for political science research;,citation_author=Mitchell Bosley;,citation_author=Musashi Jacobs-Harukawa;,citation_author=Hauke Licht;,citation_author=Alexander Hoyle;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;">
<meta name="citation_reference" content="citation_title=Using proprietary language models in academic research requires explicit justification;,citation_author=Alexis Palmer;,citation_author=Noah A Smith;,citation_author=Arthur Spirling;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=4;,citation_journal_title=Nature Computational Science;,citation_publisher=Nature Publishing Group US New York;">
<meta name="citation_reference" content="citation_title=What makes LLMs large?;,citation_author=Thimira Amaratunga;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_publisher=Apress: Berkeley, CA;">
<meta name="citation_reference" content="citation_title=Large language models for text classification: From zero-shot learning to fine-tuning;,citation_author=Youngjin Chae;,citation_author=Thomas Davidson;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=Open Science Foundation;">
<meta name="citation_reference" content="citation_title=Lexical semantic change through large language models: A survey;,citation_author=Francesco Periti;,citation_author=Stefano Montanelli;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_journal_title=ACM Computing Surveys;,citation_publisher=ACM New York, NY;">
<meta name="citation_reference" content="citation_title=Artificial intelligence: Student classification with machine learning in r;,citation_author=Mohammed Saqr;,citation_author=Kamila Misiejuk;,citation_author=Santtu Tikka;,citation_author=Sonsoles López-Pernas;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_publisher=Springer;">
<meta name="citation_reference" content="citation_title=How we code;,citation_author=David Williamson Shaffer;,citation_author=Andrew R Ruis;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_conference_title=Advances in quantitative ethnography: Second international conference, ICQE 2020, malibu, CA, USA, february 1-3, 2021, proceedings 2;,citation_conference=Springer;">
<meta name="citation_reference" content="citation_title=“Conversing” with qualitative data: Enhancing qualitative research through large language models (LLMs);,citation_author=Adam Hayes;,citation_doi=10.31235/osf.io/yms8p;,citation_publisher=OSF preprint;">
<meta name="citation_reference" content="citation_title=An examination of the use of large language models to aid analysis of textual data;,citation_author=Robert H Tai;,citation_author=Lillian R Bentley;,citation_author=Xin Xia;,citation_author=Jason M Sitt;,citation_author=Sarah C Fankhauser;,citation_author=Ana M Chicas-Mosier;,citation_author=Barnas G Monteith;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_volume=23;,citation_journal_title=International Journal of Qualitative Methods;,citation_publisher=SAGE Publications Sage CA: Los Angeles, CA;">
<meta name="citation_reference" content="citation_title=Supporting qualitative analysis with large language models: Combining codebook with GPT-3 for deductive coding;,citation_author=Ziang Xiao;,citation_author=Xingdi Yuan;,citation_author=Q Vera Liao;,citation_author=Rania Abdelghani;,citation_author=Pierre-Yves Oudeyer;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_conference_title=Companion proceedings of the 28th international conference on intelligent user interfaces;">
<meta name="citation_reference" content="citation_title=Inductive thematic analysis of healthcare qualitative interviews using open-source large language models: How does it compare to traditional methods?;,citation_author=Walter S Mathis;,citation_author=Sophia Zhao;,citation_author=Nicholas Pratt;,citation_author=Jeremy Weleff;,citation_author=Stefano De Paoli;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_volume=255;,citation_journal_title=Computer Methods and Programs in Biomedicine;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=Qualitative code suggestion: A human-centric approach to qualitative coding;,citation_author=Cesare Spinoso-Di Piano;,citation_author=Samira Rahimi;,citation_author=Jackie Chi Kit Cheung;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_conference_title=Findings of the association for computational linguistics: EMNLP 2023;">
<meta name="citation_reference" content="citation_title=Using generative text models to create qualitative codebooks for student evaluations of teaching;,citation_author=Andrew Katz;,citation_author=Mitch Gerhardt;,citation_author=Michelle Soledad;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_volume=23;,citation_journal_title=International Journal of Qualitative Methods;,citation_publisher=SAGE Publications Sage CA: Los Angeles, CA;">
<meta name="citation_reference" content="citation_title=Automating the identification of feedback quality criteria and the CanMEDS roles in written feedback comments using natural language processing;,citation_author=Sofie Van Ostaeyen;,citation_author=Loic De Langhe;,citation_author=Orphée De Clercq;,citation_author=Mieke Embo;,citation_author=Tammy Schellens;,citation_author=Martin Valcke;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_issue=1;,citation_volume=12;,citation_journal_title=Perspectives on Medical Education;,citation_publisher=Ubiquity Press;">
<meta name="citation_reference" content="citation_title=Prompting as panacea? A case study of in-context learning performance for qualitative coding of classroom dialog;,citation_author=Ananya Ganesh;,citation_author=Chelsea Chandler;,citation_author=Sidney D’Mello;,citation_author=Martha Palmer;,citation_author=Katharina Kann;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_conference_title=Proceedings of the 17th international conference on educational data mining;">
<meta name="citation_reference" content="citation_title=(Why) is my prompt getting worse? Rethinking regression testing for evolving LLM APIs;,citation_author=Wanqin Ma;,citation_author=Chenyang Yang;,citation_author=Christian Kästner;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_conference_title=Proceedings of the IEEE/ACM 3rd international conference on AI engineering-software engineering for AI;">
<meta name="citation_reference" content="citation_title=Can large language models truly understand prompts? A case study with negated prompts;,citation_author=Joel Jang;,citation_author=Seonghyeon Ye;,citation_author=Minjoon Seo;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_conference_title=Transfer learning for natural language processing workshop;,citation_conference=PMLR;">
<meta name="citation_reference" content="citation_title=Practical and ethical challenges of large language models in education: A systematic scoping review;,citation_author=Lixiang Yan;,citation_author=Lele Sha;,citation_author=Linxuan Zhao;,citation_author=Yuheng Li;,citation_author=Roberto Martinez-Maldonado;,citation_author=Guanliang Chen;,citation_author=Xinyu Li;,citation_author=Yueqiao Jin;,citation_author=Dragan Gašević;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=55;,citation_journal_title=British Journal of Educational Technology;,citation_publisher=Wiley Online Library;">
<meta name="citation_reference" content="citation_title=Confronting LLMs with traditional ML: Rethinking the fairness of large language models in tabular classifications;,citation_author=Yanchen Liu;,citation_author=Srishti Gautam;,citation_author=Jiaqi Ma;,citation_author=Himabindu Lakkaraju;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_conference_title=Proceedings of the 2024 conference of the north american chapter of the association for computational linguistics: Human language technologies (volume 1: Long papers);">
<meta name="citation_reference" content="citation_title=Bias and fairness in large language models: A survey;,citation_author=Isabel O Gallegos;,citation_author=Ryan A Rossi;,citation_author=Joe Barrow;,citation_author=Md Mehrab Tanjim;,citation_author=Sungchul Kim;,citation_author=Franck Dernoncourt;,citation_author=Tong Yu;,citation_author=Ruiyi Zhang;,citation_author=Nesreen K Ahmed;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_journal_title=Computational Linguistics;,citation_publisher=MIT Press 255 Main Street, 9th Floor, Cambridge, Massachusetts 02142, USA&nbsp;…;">
<meta name="citation_reference" content="citation_title=Knowledge distillation in automated annotation: Supervised text classification with LLM-generated training labels;,citation_author=Nicholas Pangakis;,citation_author=Samuel Wolken;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_conference_title=The sixth workshop on natural language processing and computational social science;">
<meta name="citation_reference" content="citation_title=A survey on evaluation of large language models;,citation_author=Yupeng Chang;,citation_author=Xu Wang;,citation_author=Jindong Wang;,citation_author=Yuan Wu;,citation_author=Linyi Yang;,citation_author=Kaijie Zhu;,citation_author=Hao Chen;,citation_author=Xiaoyuan Yi;,citation_author=Cunxiang Wang;,citation_author=Yidong Wang;,citation_author=others;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=3;,citation_volume=15;,citation_journal_title=ACM Transactions on Intelligent Systems and Technology;,citation_publisher=ACM New York, NY;">
<meta name="citation_reference" content="citation_title=Why cohen’s kappa should be avoided as performance measure in classification;,citation_author=Rosario Delgado;,citation_author=Xavier-Andoni Tibau;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=9;,citation_volume=14;,citation_journal_title=PloS one;,citation_publisher=Public Library of Science San Francisco, CA USA;">
<meta name="citation_reference" content="citation_title=Justification for the use of cohen’s kappa statistic in experimental studies of NLP and text mining;,citation_author=AS Kolesnyk;,citation_author=NF Khairova;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_issue=2;,citation_volume=58;,citation_journal_title=Cybernetics and Systems Analysis;,citation_publisher=Springer;">
<meta name="citation_reference" content="citation_title=Disagreement on agreement: Two alternative agreement coefficients;,citation_author=Emily Blood;,citation_author=Kevin F Spratt;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_volume=186;,citation_conference_title=SAS global forum;,citation_conference=Citeseer;">
<meta name="citation_reference" content="citation_title=Computing inter-rater reliability and its variance in the presence of high agreement;,citation_author=Kilem Li Gwet;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_issue=1;,citation_volume=61;,citation_journal_title=British Journal of Mathematical and Statistical Psychology;,citation_publisher=Wiley Online Library;">
<meta name="citation_reference" content="citation_title=The matthews correlation coefficient (MCC) is more informative than cohen’s kappa and brier score in binary classification assessment;,citation_author=Davide Chicco;,citation_author=Matthijs J Warrens;,citation_author=Giuseppe Jurman;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_volume=9;,citation_journal_title=Ieee Access;,citation_publisher=IEEE;">
<meta name="citation_reference" content="citation_title=A systematic analysis of performance measures for classification tasks;,citation_author=Marina Sokolova;,citation_author=Guy Lapalme;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_issue=4;,citation_volume=45;,citation_journal_title=Information processing &amp;amp;amp; management;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=Can large language models transform computational social science?;,citation_author=Caleb Ziems;,citation_author=William Held;,citation_author=Omar Shaikh;,citation_author=Jiaao Chen;,citation_author=Zhehao Zhang;,citation_author=Diyi Yang;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_issue=1;,citation_volume=50;,citation_journal_title=Computational Linguistics;,citation_publisher=MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA&nbsp;…;">
<meta name="citation_reference" content="citation_title=SIGHT: A large annotated dataset on student insights gathered from higher education transcripts;,citation_author=Rose Wang;,citation_author=Pawan Wirawarn;,citation_author=Noah Goodman;,citation_author=Dorottya Demszky;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_conference_title=Proceedings of the 18th workshop on innovative use of NLP for building educational applications (BEA 2023);">
<meta name="citation_reference" content="citation_title=Mltools: Machine learning tools;,citation_author=Ben Gorman;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_fulltext_html_url=https://CRAN.R-project.org/package=mltools;">
<meta name="citation_reference" content="citation_title=Unpacking learning in the age of AI: Bridging AI, complexity, and precision education;,citation_author=Sonsoles López-Pernas;,citation_author=Ahmed Tlili;,citation_author=Rwitajit Majumdar;,citation_author=Sami Heikkinen;,citation_author=Mohammed Saqr;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_inbook_title=Advanced learning analytics methods: AI, precision and complexity;">
<meta name="citation_reference" content="citation_title=AI, explainable AI and evaluative AI: An introduction to informed data-driven decision-making in education;,citation_author=Sonsoles López-Pernas;,citation_author=Eduardo Oliveira;,citation_author=Yige Song;,citation_author=Mohammed Saqr;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_inbook_title=Advanced learning analytics methods: AI, precision and complexity;">
<meta name="citation_reference" content="citation_title=Artificial intelligence: Using machine learning to predict students’ performance;,citation_author=Mohammed Saqr;,citation_author=Kamila Misiejuk;,citation_author=Santtu Tikka;,citation_author=Sonsoles López-Pernas;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_inbook_title=Advanced learning analytics methods: AI, precision and complexity;">
<meta name="citation_reference" content="citation_title=Artificial intelligence: Using machine learning to classify students and predict low achievers;,citation_author=Mohammed Saqr;,citation_author=Kamila Misiejuk;,citation_author=Santtu Tikka;,citation_author=Sonsoles López-Pernas;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_inbook_title=Advanced learning analytics methods: AI, precision and complexity;">
<meta name="citation_reference" content="citation_title=Comparative analysis of regularization methods for predicting student certification in online courses;,citation_author=Tian Li;,citation_author=Feifei Han;,citation_author=Jiesi Guo;,citation_author=Jinran Wu;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_inbook_title=Advanced learning analytics methods: AI, precision and complexity;">
<meta name="citation_reference" content="citation_title=Explainable artificial intelligence in education: A tutorial for identifying the variables that matter;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_inbook_title=Advanced learning analytics methods: AI, precision and complexity;">
<meta name="citation_reference" content="citation_title=Individualized explainable artificial intelligence: A tutorial for identifying local and individual predictions;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_inbook_title=Advanced learning analytics methods: AI, precision and complexity;">
<meta name="citation_reference" content="citation_title=An introduction to large language models in education;,citation_author=Eduardo Oliveira;,citation_author=Yige Song;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_inbook_title=Advanced learning analytics methods: AI, precision and complexity;">
<meta name="citation_reference" content="citation_title=The use of natural language processing in learning analytics;,citation_author=Tarid Wongvorachan;,citation_author=Okan Bulut;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_inbook_title=Advanced learning analytics methods: AI, precision and complexity;">
<meta name="citation_reference" content="citation_title=Using language models for automated discourse coding: A primer and tutorial;,citation_author=Sonsoles López-Pernas;,citation_author=Kamila Misiejuk;,citation_author=Mohammed Saqr;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_inbook_title=Advanced learning analytics methods: AI, precision and complexity;">
<meta name="citation_reference" content="citation_title=LLMs for explainable artificial intelligence: Automating natural language explanations of predictive analytics models;,citation_author=Sonsoles López-Pernas;,citation_author=Yige Song;,citation_author=Eduardo Oliveira;,citation_author=Mohammed Saqr;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_inbook_title=Advanced learning analytics methods: AI, precision and complexity;">
<meta name="citation_reference" content="citation_title=Complex dynamic systems in education: Beyond the static, the linear and the causal reductionism;,citation_author=Mohammed Saqr;,citation_author=Daryn Dever;,citation_author=Sonsoles López-Pernas;,citation_author=Christophe Gernigon;,citation_author=Gwen Marchand;,citation_author=Avi Kaplan;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_inbook_title=Advanced learning analytics methods: AI, precision and complexity;">
<meta name="citation_reference" content="citation_title=The advanced applications of psychological networks with EGA;,citation_author=Tarid Wongvorachan;,citation_author=Okan Bulut;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_inbook_title=Advanced learning analytics methods: AI, precision and complexity;">
<meta name="citation_reference" content="citation_title=Detecting nonlinear patterns in education research: A tutorial on recurrence quantification analysis;,citation_author=Daryn Dever;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_inbook_title=Advanced learning analytics methods: AI, precision and complexity;">
<meta name="citation_reference" content="citation_title=Mapping relational dynamics with transition network analysis: A primer and tutorial;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_author=Santtu Tikka;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_inbook_title=Advanced learning analytics methods: AI, precision and complexity;">
<meta name="citation_reference" content="citation_title=Capturing the breadth and dynamics of the temporal processes with frequency transition network analysis: A primer and tutorial;,citation_author=Mohammed Saqr;,citation_author=Sonsoles López-Pernas;,citation_author=Santtu Tikka;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_inbook_title=Advanced learning analytics methods: AI, precision and complexity;">
<meta name="citation_reference" content="citation_title=Mining patterns and clusters with transition network analysis: A heterogeneity approach;,citation_author=Sonsoles López-Pernas;,citation_author=Santtu Tikka;,citation_author=Mohammed Saqr;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_inbook_title=Advanced learning analytics methods: AI, precision and complexity;">
<meta name="citation_reference" content="citation_title=A comprehensive introduction to idiographic and within-person analytics;,citation_author=Mohammed Saqr;,citation_author=Hibiki Ito;,citation_author=Sonsoles López-Pernas;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_inbook_title=Advanced learning analytics methods: AI, precision and complexity;">
<meta name="citation_reference" content="citation_title=The three levels of analysis: Variable-centered, person-centered and person-specific analysis in education;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_inbook_title=Advanced learning analytics methods: AI, precision and complexity;">
<meta name="citation_reference" content="citation_title=Idiographic networks: A tutorial on graphical vector autoregression and unified structural equation modeling;,citation_author=Mohammed Saqr;,citation_author=Daryn Dever;,citation_author=Sonsoles López-Pernas;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_inbook_title=Advanced learning analytics methods: AI, precision and complexity;">
<meta name="citation_reference" content="citation_title=Detecting long-memory psychological processes in academic settings using whittle’s maximum likelihood estimator: An application with r;,citation_author=Rémi Altamore;,citation_author=Clément Roume;,citation_author=Anne Teboul;,citation_author=Christophe Gernigon;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_inbook_title=Advanced learning analytics methods: AI, precision and complexity;">
<meta name="citation_reference" content="citation_title=Automating individualized machine learning and AI prediction using AutoML: The case of idiographic predictions;,citation_author=Mohammed Saqr;,citation_author=Ahmed Tlili;,citation_author=Sonsoles López-Pernas;,citation_editor=Mohammed Saqr;,citation_editor=Sonsoles López-Pernas;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_inbook_title=Advanced learning analytics methods: AI, precision and complexity;">
<meta name="citation_reference" content="citation_title=RoBERTa: A robustly optimized BERT pretraining approach;,citation_abstract=Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.;,citation_author=Yinhan Liu;,citation_author=Myle Ott;,citation_author=Naman Goyal;,citation_author=Jingfei Du;,citation_author=Mandar Joshi;,citation_author=Danqi Chen;,citation_author=Omer Levy;,citation_author=Mike Lewis;,citation_author=Luke Zettlemoyer;,citation_author=Veselin Stoyanov;,citation_publication_date=2019-07;,citation_cover_date=2019-07;,citation_year=2019;,citation_fulltext_html_url=https://scholar.google.es/citations?user=H9buyroAAAAJ&amp;amp;amp;hl=en&amp;oi=sra;,citation_journal_title=arXiv [cs.CL];">
<meta name="citation_reference" content="citation_title=XLNet: Generalized autoregressive pretraining for language understanding;,citation_abstract=With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.;,citation_author=Zhilin Yang;,citation_author=Zihang Dai;,citation_author=Yiming Yang;,citation_author=Jaime Carbonell;,citation_author=Ruslan Salakhutdinov;,citation_author=Quoc V Le;,citation_publication_date=2019-06;,citation_cover_date=2019-06;,citation_year=2019;,citation_fulltext_html_url=http://arxiv.org/abs/1906.08237;,citation_journal_title=arXiv [cs.CL];">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">LA Methods</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../book1/index.html">
 <span class="menu-text">Learning Analytics Methods and Tutorials</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../book2/index.html">
 <span class="menu-text">Advanced Learning Analytics Methods</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/lamethods/code2/"><i class="bi bi-github" role="img" aria-label="Source Code">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Using BERT-like Language Models for Automated Discourse Coding: A Primer and Tutorial</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">Welcome</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../contributors.html" class="sidebar-item-text sidebar-link">Contributors</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch01-intro/ch01-intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Artificial Intelligence</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch02-AIxAI/ch02-aixai.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">AI and XAI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch03-prediction/ch03-prediction.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Prediction</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch04-classification/ch04-classification.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Classification</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch05-regularization/ch05-regularization.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Regularization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch06-xai-global/ch06-xai-global.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Global XAI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch07-xai-local/ch07-xai-local.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Local XAI</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Large Language Models</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch08-llms/ch08-llms.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Large Language Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch09-nlp/ch09-nlp.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch10-bert/ch10-bert.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Classification with BERT</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch11-llmsxai/ch11-llmsxai.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Automated feedback with XAI and LLMs</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Complex Systems</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch12-cds/ch12-cds.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Complex Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch13-ega/ch13-ega.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Exploratory Graph Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch14-rqa/ch14-rqa.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Recurrent Quantification Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch15-tna/ch15-tna.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Transition Network Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch16-ftna/ch16-ftna.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Frequency-based Transition Network Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch17-tna-clusters/ch17-tna-clusters.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Transition Network Analysis Clusters</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">Idiographic</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch18-idio/ch18-idio.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Within-person analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch19-three-levels/ch19-three-levels.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Heterogeneity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch20-var/ch20-var.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Vector Autoregression and uSEM</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch21-mle/ch21-mle.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Maximum Likelihood Estimator</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/ch22-automl/ch22-automl.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Automated Machine Learning</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="toc-section-number">1</span>  Introduction</a></li>
  <li><a href="#related-work-on-using-llms-to-support-text-coding" id="toc-related-work-on-using-llms-to-support-text-coding" class="nav-link" data-scroll-target="#related-work-on-using-llms-to-support-text-coding"><span class="toc-section-number">2</span>  Related work on using LLMs to support text coding</a></li>
  <li><a href="#a-case-study-on-collaborative-problem-solving" id="toc-a-case-study-on-collaborative-problem-solving" class="nav-link" data-scroll-target="#a-case-study-on-collaborative-problem-solving"><span class="toc-section-number">3</span>  A case study on collaborative problem solving</a>
  <ul class="collapse">
  <li><a href="#the-dataset" id="toc-the-dataset" class="nav-link" data-scroll-target="#the-dataset"><span class="toc-section-number">3.1</span>  The dataset</a></li>
  <li><a href="#automated-discourse-coding" id="toc-automated-discourse-coding" class="nav-link" data-scroll-target="#automated-discourse-coding"><span class="toc-section-number">3.2</span>  Automated discourse coding</a></li>
  <li><a href="#setting-up-the-enviroment" id="toc-setting-up-the-enviroment" class="nav-link" data-scroll-target="#setting-up-the-enviroment"><span class="toc-section-number">3.3</span>  Setting up the enviroment</a></li>
  <li><a href="#splitting-the-dataset" id="toc-splitting-the-dataset" class="nav-link" data-scroll-target="#splitting-the-dataset"><span class="toc-section-number">3.4</span>  Splitting the dataset</a></li>
  <li><a href="#sec-wordembd" id="toc-sec-wordembd" class="nav-link" data-scroll-target="#sec-wordembd"><span class="toc-section-number">3.5</span>  Word embeddings</a></li>
  <li><a href="#training-the-model" id="toc-training-the-model" class="nav-link" data-scroll-target="#training-the-model"><span class="toc-section-number">3.6</span>  Training the model</a></li>
  <li><a href="#using-the-model-to-predict" id="toc-using-the-model-to-predict" class="nav-link" data-scroll-target="#using-the-model-to-predict"><span class="toc-section-number">3.7</span>  Using the model to predict</a></li>
  <li><a href="#evaluating-the-model" id="toc-evaluating-the-model" class="nav-link" data-scroll-target="#evaluating-the-model"><span class="toc-section-number">3.8</span>  Evaluating the model</a></li>
  <li><a href="#impact-on-findings" id="toc-impact-on-findings" class="nav-link" data-scroll-target="#impact-on-findings"><span class="toc-section-number">3.9</span>  Impact on findings</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="toc-section-number">4</span>  Conclusion</a></li>
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">References</a></li>
  </ul>
</nav>
    <div class="quarto-margin-footer"><div class="margin-footer-item">
<a href="https://github.com/lamethods/code2" target="_blank"> <button class="btn btn-outline-dark"> <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 496 512" style="width: 22px;vertical-align: text-top;margin-right: 9px;"> <path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3 .3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5 .3-6.2 2.3zm44.2-1.7c-2.9 .7-4.9 2.6-4.6 4.9 .3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3 .7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3 .3 2.9 2.3 3.9 1.6 1 3.6 .7 4.3-.7 .7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3 .7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3 .7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z" style="width: 24px;"> </path> </svg>Download code </button> </a>
<div style="padding: 10px;">
Check out our previous book! <br> <a href="../../../book1/index.html"><img src="../../../book1/1712067211600.jpeg" style="
     width: 70%;
 "></a>
</div>
<p><br> <small>© 2025 The authors</small></p>
</div></div></div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Using BERT-like Language Models for Automated Discourse Coding: A Primer and Tutorial</span></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-contents">
             <p>Sonsoles López-Pernas </p>
             <p>Kamila Misiejuk </p>
             <p>Mohammed Saqr </p>
          </div>
  </div>
    
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="abstract-title">Abstract</div>
    Coding text data in qualitative research is a labor-intensive and error-prone process that requires meticulous attention to detail as well as consistency in the coding criteria. Large Language Models (LLMs) present a promising solution to alleviate some of these challenges by automating part of the coding process. This tutorial explores the application of LLMs for automated text classification using word embeddings through the R package <code>text</code> and different BERT-like large language models. We implement a machine learning pipeline that combines word embeddings with supervised machine learning algorithms to code text data with high accuracy. We present a case study on collaborative problem-solving in which we train a classification model on a small portion of manually coded data and then apply it to classify the remaining data. The tutorial also covers the evaluation of coding accuracy by comparing human and machine-coded data using classic machine learning performance metrics as well as Cohen’s kappa, Matthews’ correlation coefficient, and Gwet AC1, measures commonly used to assess interrater reliability in qualitative research. Lastly, we apply different learning analytics techniques to compare the findings obtained from human-coded data and automatically coded data.
  </div>
</div>

</header>

<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>Coding text data is an effortful and time-consuming process and constitutes one of the major challenges faced by qualitative researchers. Coding refers to the process of “essentially indexing or mapping data, to provide an overview of disparate data that allows the researcher to make sense of them in relation to their research questions” <span class="citation" data-cites="elliott2018thinking">[<a href="#ref-elliott2018thinking" role="doc-biblioref">1</a>]</span>. Coding requires defining codes, i.e.&nbsp;the concepts of interest, and localizing places in text where codes occur <span class="citation" data-cites="shaffer2021we">[<a href="#ref-shaffer2021we" role="doc-biblioref">2</a>]</span>. There are two main approaches of deriving at the codes: <em>inductive coding</em>, where codes emerge from the data, and <em>deductive coding</em>, which uses a set of predetermined codes to classify the data <span class="citation" data-cites="thomas2006general">[<a href="#ref-thomas2006general" role="doc-biblioref">3</a>]</span>.</p>
<p>The coding process requires close inspection of data before it can be analyzed. Researchers often have to scroll through large volumes of text, identify the main concepts and themes, and assign codes that best describe them. This process requires a high level of attention, as every line of text must be carefully examined. Furthermore, maintaining consistency in coding across different parts of the data is difficult, especially if more than one researcher is involved, often requiring multiple rounds of review, discussion, and revision <span class="citation" data-cites="elliott2018thinking">[<a href="#ref-elliott2018thinking" role="doc-biblioref">1</a>]</span>. Coding of text is vital in qualitative research; however, it is also an important part of many quantitative or mixed-methods studies, where text data coding is often delegated as a part of data preprocessing before it can be analyzed by other, often quantitative, methods. In terms of machine learning (ML), deriving codes inductively can be similar to text summarization tasks, where an algorithm discovers topics within a text, while localizing a presence of a specific code in text can match annotation or classification tasks, where an algorithm assigns a class label to a given data point based on its characteristics.</p>
<p>Large Language Models (LLMs) offer a promising solution to ease some of the challenges related to coding of text data. LLMs are trained on vast amounts of text data and have the ability to process and generate human-like language <span class="citation" data-cites="LABOOK2_Chapter_8">[<a href="#ref-LABOOK2_Chapter_8" role="doc-biblioref">4</a>]</span>. When presented with the task of coding text data, LLMs can significantly reduce the time and effort required by automating —at least part of— the process, as this technology can quickly scan large volumes of text, as well as identify the main underlying concepts. The introduction of the transformer architecture enabled the development of of different types of LLMS such as Google’s Bidirectional Encoder Representations from Transformers (BERT) trained using masked language modeling and open AI’s Generative Pre-trained Transformers (GPT) trained using causal language modeling (see <span class="citation" data-cites="raiaan2024review">[<a href="#ref-raiaan2024review" role="doc-biblioref">5</a>]</span>, <span class="citation" data-cites="bosley2023we">[<a href="#ref-bosley2023we" role="doc-biblioref">6</a>]</span> for more details). Although both models can be adapted to a variety of tasks, BERT models are typically recommended for text classification, while GPT models for text summarization <span class="citation" data-cites="amaratunga2023understanding">[<a href="#ref-amaratunga2023understanding" role="doc-biblioref">7</a>]</span>.</p>
<p>LLMs are capable of performing zero-shot classification, which means that without the need for specifically trained data for certain categories <span class="citation" data-cites="LABOOK2_Chapter_8 chae2023large">[<a href="#ref-LABOOK2_Chapter_8" role="doc-biblioref">4</a>, <a href="#ref-chae2023large" role="doc-biblioref">8</a>]</span>, the model can classify text based on its broad understanding of language and knowledge. This capability stems from the extensive and diverse textual data encountered during the pretraining phase, enabling the model to make reasonable inferences and predictions for categories it has not been explicitly trained on through contextual understanding and knowledge transfer.</p>
<p>While zero-shot classification significantly reduces both the technical and intellectual effort required from the researchers, it often lacks the precision and reliability that might be required for domain-specific tasks. A possible middle ground solution that achieves a good trade-off between manual coding and completely automated coding, is the combination of word embeddings —implemented in LLMs— with supervised ML. Word embeddings capture the semantic meaning of text in a way that is informed by the context in which words appear <span class="citation" data-cites="periti2024lexical">[<a href="#ref-periti2024lexical" role="doc-biblioref">9</a>]</span> (the concept is explained more thoroughly in <a href="#sec-wordembd"><span>Section&nbsp;10.3.5</span></a>). Generating these embeddings allows us to use the textual information as features in a classification model (such as Random Forest or Support Vector Machine) <span class="citation" data-cites="saqr2025classification">[<a href="#ref-saqr2025classification" role="doc-biblioref">10</a>]</span>. This approach requires that researchers to manually code part of the data, while the major part of the process is delegated to the LLM, achieving —allegedly— more accurate and more consistent classifications, as the model is explicitly trained on data that reflects the task at hand.</p>
<p>In this tutorial, we will learn how to implement automated discourse classification using word embeddings. We will rely on the R package <code>text</code> and BERT-like models. We will train a classification ML model with a small percentage of a dataset on collaborative problem solving, and we will automatically classify the remaining text. We will evaluate the classification performance and compare the agreement of the human-coded and LLM-coded text using Cohen’s kappa, Gwet’s AC1 and Matthews Correlation Coefficient. We will also compare the performance of different BERT-like algorithms using traditional metrics. Lastly, we will investigate how the insights gathered from applying different learning analytics techniques to the automatically coded data differ from those obtained from human-coded data.</p>
</section>
<section id="related-work-on-using-llms-to-support-text-coding" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="related-work-on-using-llms-to-support-text-coding"><span class="header-section-number">2</span> Related work on using LLMs to support text coding</h2>
<p>The rise in popularity of LLMs has caught the attention of learning analytics researchers for their potential to assist researchers at all stages of the learning analytics cycle including synthetic data generation and automatic feedback generation, as well as automatic coding, which is the goal of our tutorial <span class="citation" data-cites="Yan2024-jw">[<a href="#ref-Yan2024-jw" role="doc-biblioref">11</a>]</span>. LLMs can be used to support developing code definitions, i.e.&nbsp;a codebook, and identifying said codes in the text segments.</p>
<p>Emerging research shows that LLMs has the potential to support developing a codebook. In the context of qualitative coding, a codebook is a structured document that outlines the codes, categories, and their definitions, serving as a guide for systematically categorizing, analyzing and interpreting qualitative data. The authors of <span class="citation" data-cites="barany2024chatgpt">[<a href="#ref-barany2024chatgpt" role="doc-biblioref">12</a>]</span> compared three codebook development settings: 1) fully automated approach with ChatGPT, 2) hybrid ChatGPT-human approach, and 3) fully human approach. The hybrid approach resulted in best utility ratings, highest conceptual overlap and higher inter-rater reliability than other approaches. Interestingly, both hybrid and human approaches developed similar codebooks, while the fully automated approach generated a codebook less aligned with the study goals and was an outlier in terms of conceptual similarity. Similarly, the hybrid approach has yielded positive results to code a small sample of interviews in a study by <span class="citation" data-cites="hayes2023conversing">[<a href="#ref-hayes2023conversing" role="doc-biblioref">13</a>]</span>. The authors of <span class="citation" data-cites="mathis2024inductive">[<a href="#ref-mathis2024inductive" role="doc-biblioref">14</a>]</span> reported moderate to substantial similarity of themes generated by an LLM and humans. However, there are still open questions about scalability and about whether the use of AI for that task will not lead to superficial understanding of the data <span class="citation" data-cites="davison2024ethics morgan2023exploring">[<a href="#ref-davison2024ethics" role="doc-biblioref">15</a>, <a href="#ref-morgan2023exploring" role="doc-biblioref">16</a>]</span>. The increasing familiarity with the data is expected to lead changes in the codebook over time; the coding process is supposed to be iterative and may lead to revalidation of earlier coding <span class="citation" data-cites="elliott2018thinking">[<a href="#ref-elliott2018thinking" role="doc-biblioref">1</a>]</span>. Hence, implementation of LLMs at this stage of text data coding should be carefully considered.</p>
<p>Early studies using LLMs for identifying codes in text segments showed mixed results. A recent work <span class="citation" data-cites="Garg2024-wi">[<a href="#ref-Garg2024-wi" role="doc-biblioref">17</a>]</span> evaluated the capability of generative artificial intelligence (AI), particularly GPT models, to automatically code discourse within a learning analytics study, employing various prompting and training strategies. Their findings indicated that fine-tuning methods (refer to <span class="citation" data-cites="LABOOK2_Chapter_8">[<a href="#ref-LABOOK2_Chapter_8" role="doc-biblioref">4</a>]</span> to learn more about this concept) yielded the most favorable outcomes, though none of the results met the standard reliability thresholds typically expected in the field. <span class="citation" data-cites="Misiejuk2024-wl">[<a href="#ref-Misiejuk2024-wl" role="doc-biblioref">18</a>]</span> investigated the reliability of automatic discourse coding by applying a range of LLMs from the GPT family, along with various prompting techniques, to an asynchronous online discussion dataset. The authors determined that “the agreement with human coding was substantial at best”, and do not recommend that AI replaces humans in coding, at least not yet in a reliable way. Another study by <span class="citation" data-cites="Pugh2021-bz">[<a href="#ref-Pugh2021-bz" role="doc-biblioref">19</a>]</span> operationalized BERT models to create word embeddings, and used them to train a Random Forest classifier to automatically code collaborative problem-solving conversations. Although findings are not yet beyond ‘substantial’ accuracy, the rapid advance of LLMs suggests that these models could soon achieve higher levels of reliability and accuracy in automatic coding tasks. Also, the selection of a specific LLM may affect the performance. <span class="citation" data-cites="prescott2024comparing">[<a href="#ref-prescott2024comparing" role="doc-biblioref">20</a>]</span> reported that ChatGPT and Bard performed similarly across both types inductive and deductive thematic analysis. Higher performance of BERT-like models for text classification tasks than the GPT models was reported in several studies <span class="citation" data-cites="lefort2024small ganesh2024prompting">[e.g., <a href="#ref-lefort2024small" role="doc-biblioref">21</a>, <a href="#ref-ganesh2024prompting" role="doc-biblioref">22</a>]</span>. Finally, using LLM models for text coding may require decomposing this complex task into multiple simple binary classifications (i.e., processing each code at a time and classifying each text unit regarding whether the code is included or not). Complex codes are more challenging for LLMs to codes <span class="citation" data-cites="van2023automating ganesh2024prompting Misiejuk2024-wl">[<a href="#ref-Misiejuk2024-wl" role="doc-biblioref">18</a>, <a href="#ref-ganesh2024prompting" role="doc-biblioref">22</a>, <a href="#ref-van2023automating" role="doc-biblioref">23</a>]</span>.</p>
<p>Adjusting the temperature setting or conducting multiple iterations of coding are some of the strategies used to improve LLM’s performance in classification tasks <span class="citation" data-cites="tai2024examination Misiejuk2024-wl">[<a href="#ref-Misiejuk2024-wl" role="doc-biblioref">18</a>, <a href="#ref-tai2024examination" role="doc-biblioref">24</a>]</span>. Another strategy is to improve the LLM prompt. For instance, <span class="citation" data-cites="xiao2023supporting">[<a href="#ref-xiao2023supporting" role="doc-biblioref">25</a>]</span> tested codebook-centered or example-centered prompts, as well as experimented with the number of examples used in a prompt. The results showed that the codebook-centered design performed better than the example-centered designs, while adding even one example of a code to a prompt increased the performance notably. Wording of a prompt is crucial. A recent work <span class="citation" data-cites="jang2023can">[<a href="#ref-jang2023can" role="doc-biblioref">26</a>]</span> demonstrated that using negated instructions (e.g., “Generate the incorrect solution..”) instead of positive instructions (e.g., “Generate the correct solution..”) decreases the performance of all LLMs tested. Decreased performance could be also caused by changes in the LLM model itself <span class="citation" data-cites="ma2024my">[<a href="#ref-ma2024my" role="doc-biblioref">27</a>]</span>. This is particularly important in case of proprietary models (i.e., any LLMs that cannot be downloaded, run offline and shared <span class="citation" data-cites="palmer2024using">[<a href="#ref-palmer2024using" role="doc-biblioref">28</a>]</span>). For example, the majority of BERT models are an open-source, while most GPT models are not publicly available <span class="citation" data-cites="bosley2023we">[<a href="#ref-bosley2023we" role="doc-biblioref">6</a>]</span>. Using proprietary models requires researchers to concede a degree of control to developers. Changes made to proprietary models are not transparent and may effect the text coding results and, in effect, the final analysis.</p>
<p>As LLMs may be prone to hallucinations, it is critical to evaluate model’s performance before relying on its results for analysis. One method to do it, is to use human-coded data as a baseline to assess performance <span class="citation" data-cites="chang2024survey">[<a href="#ref-chang2024survey" role="doc-biblioref">29</a>]</span>. However, this method requires available data that was collected in a similar context, which may be unavailable in educational settings. Another approach is to manually check the LLM coding <span class="citation" data-cites="ziems2024can">[<a href="#ref-ziems2024can" role="doc-biblioref">30</a>]</span>, which may be applicable for smaller dataset; however, it may as effortful as manually coding the dataset instead of using an LLM. Furthermore, there are many discussions about the best metrics that can be used to assess the performance of LLMs in text coding tasks. Metrics such as accuracy, precision or recall are typical measurements used in machine learning for classification tasks (see <span class="citation" data-cites="sokolova2009systematic">[<a href="#ref-sokolova2009systematic" role="doc-biblioref">31</a>]</span> for an overview). At the same time, other metrics, such as Cohen’s kappa, are applied in the qualitative research to ensure the level of agreement among multiple coders <span class="citation" data-cites="shaffer2021we">[<a href="#ref-shaffer2021we" role="doc-biblioref">2</a>]</span>. Although Cohen’s kappa is often used to evaluate text classification task <span class="citation" data-cites="kolesnyk2022justification">[<a href="#ref-kolesnyk2022justification" role="doc-biblioref">32</a>]</span>, it has been critiqued as not an appropriate approach to this task. Instead metrics like Matthews’ correlation coefficient <span class="citation" data-cites="delgado2019cohen chicco2021matthews">[<a href="#ref-delgado2019cohen" role="doc-biblioref">33</a>, <a href="#ref-chicco2021matthews" role="doc-biblioref">34</a>]</span> or Gwet AC1 are suggested <span class="citation" data-cites="gwet2008computing blood2007disagreement">[<a href="#ref-gwet2008computing" role="doc-biblioref">35</a>, <a href="#ref-blood2007disagreement" role="doc-biblioref">36</a>]</span>.</p>
<p>Finally, ethical issues are important to consider in case of coding education text data using LLMs. We addressed some of these issues throughout this chapter (for a comprehensive review see <span class="citation" data-cites="yan2024practical">[<a href="#ref-yan2024practical" role="doc-biblioref">37</a>]</span>). There are two ethical issues that need to be highlighted in case of using LLMs for text coding. First, educational data may include sensitive information and feeding it to a proprietary LLM can be a breach student privacy rights depending on local regulations. Second, inherent bias in the LLM from its pre-training data was proven to affect classification tasks <span class="citation" data-cites="liu2024confronting">[<a href="#ref-liu2024confronting" role="doc-biblioref">38</a>]</span>. There are several techniques and recommendations that can be considered to address bias in LLMs (see <span class="citation" data-cites="gallegos2024bias">[<a href="#ref-gallegos2024bias" role="doc-biblioref">39</a>]</span> for more details).</p>
</section>
<section id="a-case-study-on-collaborative-problem-solving" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="a-case-study-on-collaborative-problem-solving"><span class="header-section-number">3</span> A case study on collaborative problem solving</h2>
<section id="the-dataset" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="the-dataset"><span class="header-section-number">3.1</span> The dataset</h3>
<p>The dataset that we use in this tutorial is based on an experiment in which small groups participated in a group discussion. In this discussion, the groups aimed to solve the Wason card selection task: a logic puzzle used to study reasoning and decision-making <span class="citation" data-cites="karadzhov2023delidata">[<a href="#ref-karadzhov2023delidata" role="doc-biblioref">40</a>]</span>. The discussion data has been qualitatively coded according to the nature of participants’ contributions Specifically, 500 dialogues, containing 14,000 utterances were coded using a novel annotation schema designed to capture deliberation cues. For more details about the experiment, refer to the original article <span class="citation" data-cites="karadzhov2023delidata">[<a href="#ref-karadzhov2023delidata" role="doc-biblioref">40</a>]</span>.</p>
</section>
<section id="automated-discourse-coding" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="automated-discourse-coding"><span class="header-section-number">3.2</span> Automated discourse coding</h3>
<p>The process followed in the tutorial is depicted in <a href="#fig-flow">Figure&nbsp;<span>10.1</span></a>. We will first divide our dataset in training and testing sets (I). Contrary to other ML tasks, we use only a small part of the data for training (15%), and the rest for testing. The goal is to minimize the number of pieces of text that we have to manually code. Then, we will create word embeddings from the text data (II). Word embeddings are numerical representations of words that capture their semantic meaning, enabling the model to understand and process the text in a way that reflects the relationships between words. We will try different existing LLMs to create the embeddings. We then use the word embeddings corresponding to the training data and their manual codes as an input to train our classifier ML model (III). Lastly, we use the word embeddings of the test dataset to predict their codes using the trained model and evaluate the results (IV). We compare the predicted codes with the actual ones using both traditional ML performance metrics based on the confusion matrix, as well as several measures of interrater reliability to assess the agreement between human and AI coding. We will compare the performance of the different BERT-like language models to assess which one is more suitable for the classification task. For a beginners’ tutorial on ML classification tasks, refer to Chapter 4 <span class="citation" data-cites="LABOOK2_Chapter_4">[<a href="#ref-LABOOK2_Chapter_4" role="doc-biblioref">41</a>]</span> in this book.</p>
<div id="fig-flow" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/process.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;1<strong>.</strong> Process followed in the tutorial</figcaption><p></p>
</figure>
</div>
</section>
<section id="setting-up-the-enviroment" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="setting-up-the-enviroment"><span class="header-section-number">3.3</span> Setting up the enviroment</h3>
<p>Before starting our task, we need to install and load the necessary packages, which are described below:</p>
<ul>
<li><p><code>dplyr</code>: A powerful and popular R package used for data manipulation and transformation <span class="citation" data-cites="dplyr">[<a href="#ref-dplyr" role="doc-biblioref">42</a>]</span>. It provides a set of intuitive functions that make it easy to filter, select, mutate, arrange, and summarize data within a data frame. It is especially useful for cleaning and preparing data before analysis.</p></li>
<li><p><code>tidyr</code>: An essential R package for tidying data, <code>tidyr</code> helps reshape and clean data sets so they can be analyzed more effectively <span class="citation" data-cites="tidyr">[<a href="#ref-tidyr" role="doc-biblioref">43</a>]</span>. It provides functions to pivot data between wide and long formats, split or combine columns, and ensure that data is in a consistent, tidy format suitable for analysis.</p></li>
<li><p><code>tibble</code>: A modern take on data frames in R, <code>tibble</code> <span class="citation" data-cites="tibble">[<a href="#ref-tibble" role="doc-biblioref">44</a>]</span> provides a lightweight and faster alternative to the traditional <code>data.frame</code> object.</p></li>
<li><p><code>reticulate</code>: A package that provides an interface between R and Python <span class="citation" data-cites="reticulate">[<a href="#ref-reticulate" role="doc-biblioref">45</a>]</span>. It allows users to run Python code within R and seamlessly exchange data between the two languages. This is particularly useful when combining the strengths of R and Python in a single analysis pipeline, taking advantage of Python’s ML libraries, such as TensorFlow or Scikit-learn, while still working within an R environment.</p></li>
<li><p><code>caret</code>: A package used for building and evaluating ML models in R <span class="citation" data-cites="caret">[<a href="#ref-caret" role="doc-biblioref">46</a>]</span>. It simplifies the process of training models, performing cross-validation, and optimizing model parameters. The package supports a wide range of algorithms and provides tools for data splitting, pre-processing, feature selection, and model evaluation.</p></li>
<li><p><code>text</code>: The <code>text</code> package is designed for text analysis in R <span class="citation" data-cites="text">[<a href="#ref-text" role="doc-biblioref">47</a>]</span>. It provides tools for processing and analyzing textual data, such as tokenization, vectorization, sentiment analysis, and topic modeling. It uses transformers, natural language processing (NLP) and ML methods to examine text and numerical variables.</p></li>
<li><p><code>mltools</code>: The <code>mltools</code> package in R <span class="citation" data-cites="mltools">[<a href="#ref-mltools" role="doc-biblioref">48</a>]</span> provides a collection of machine learning tools and utilities designed to simplify common preprocessing and data transformation tasks for machine learning workflows. We will use it for some of its evaluation functions.</p></li>
</ul>
<p>Below is the code to load the libraries. Install them first if you do not have them already using the commented code.</p>
<div class="cell" data-hash="ch10-bert_cache/html/unnamed-chunk-1_f3e3d02605f61258db1f723c4209b58a">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr) <span class="co"># install.packages("dplyr")</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyr) <span class="co"># install.packages("tidyr")</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tibble) <span class="co"># install.packages("tibble")</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(reticulate) <span class="co"># install.packages("reticulate")</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(text) <span class="co"># install.packages("text")</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret) <span class="co"># install.packages("caret")</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mltools) <span class="co"># install.packages("mltools")</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="ch10-bert_cache/html/unnamed-chunk-2_59baa040c28b9d625983745ce343207e">

</div>
<p>In addition to the previous libraries, since the <code>text</code> library relies on Python underneath, we need to make sure we have Python installed (preferrably together with the <code>conda</code> environment). If you do not have it and you do not wish to install the complete Python environment, you can install <code>miniconda</code> through the package <code>reticulate</code> as follows (uncomment to run):</p>
<div class="cell" data-hash="ch10-bert_cache/html/unnamed-chunk-3_bd1bf5fbaa5efbea1b543eb5d118afb7">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># reticulate::install_miniconda()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Once done, you need to initialize the Python session as follows. This is an interactive command so you will need to enter “yes” in the terminal to advance.</p>
<div class="cell" data-hash="ch10-bert_cache/html/unnamed-chunk-4_e64be255c99b0591d3a23b6607fd9390">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">textrpp_initialize</span>(<span class="at">save_profile =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now that we have all the R packages, we will download our data. The dataset described earlier is available in Hugging Face and therefore we can download it using the <code>datasets</code> package in Python. We need to install it before using it for the first time with the commented command. After importing <code>datasets</code>, we download the dataset that we will use in the tutorial: <code>"gkaradzhov/DeliData"</code>. The <code>cache_dir</code> argument indicates where the data will be stored in your computer; feel free to change the name from <code>"cache"</code> to something else.</p>
<div class="cell" data-hash="ch10-bert_cache/html/unnamed-chunk-5_c6092ad211097515f4f7ce18601b58b3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># reticulate::py_install("datasets")</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>datasets <span class="ot">&lt;-</span> <span class="fu">import</span>(<span class="st">"datasets"</span>) </span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>dataset <span class="ot">&lt;-</span> datasets<span class="sc">$</span><span class="fu">load_dataset</span>(<span class="st">"gkaradzhov/DeliData"</span>, <span class="at">cache_dir =</span> <span class="st">"cache"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now extract the data in an R dataframe as follows:</p>
<div class="cell" data-hash="ch10-bert_cache/html/unnamed-chunk-6_9422eb58fd81e207f4fb4e0da57bcfae">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>df_raw <span class="ot">&lt;-</span> dataset<span class="sc">$</span>train<span class="sc">$</span><span class="fu">to_pandas</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If we inspect the dataframe, we see that the it contains a column to identify the team (<code>group_id</code>) and the message (<code>message_id</code>), a column containing the message original text (<code>original_text</code>), and a cleaned one (<code>clean_text</code>). It also contains the code used to annotate the message (<code>annotation_target</code>), and two other annotation fields. In addition, it contains several other columns related to the solution of the task and the team’s performance. Let us focus on the column that contains the manual code (<code>annotation_target</code>). We can inspect it to see the different codes that have been used to classify the discussion utterances:</p>
<div class="cell" data-hash="ch10-bert_cache/html/unnamed-chunk-9_4f155c0a2cb5d0edc4f6e3561d60728f">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(df_raw<span class="sc">$</span>annotation_target)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
         0      Agree   Disagree Moderation  Reasoning   Solution 
      3274       2047        111        785       4382       3403 </code></pre>
</div>
</div>
<p>We see that there are many rows that have not been assigned a code. They correspond to messages that do not fall under any of the deliberation codes. We can remove them from our dataset to work only with the coded text as follows:</p>
<div class="cell" data-hash="ch10-bert_cache/html/unnamed-chunk-10_cf1723c6ece30aba746fb6f8512c6a59">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>classification <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Solution"</span>, <span class="st">"Reasoning"</span>, <span class="st">"Agree"</span>, <span class="st">"Disagree"</span>, <span class="st">"Moderation"</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> df_raw <span class="sc">|&gt;</span> <span class="fu">filter</span>(annotation_target <span class="sc">%in%</span> classification) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we are ready to start working with the data.</p>
</section>
<section id="splitting-the-dataset" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="splitting-the-dataset"><span class="header-section-number">3.4</span> Splitting the dataset</h3>
<p>As is common in most ML tasks, we split our data into a training dataset and a testing dataset. We will use the training dataset to train an ML model to classify text data into one of the five codes and we will use the test dataset to evaluate how accurately our model classifies “unseen” text. This strategy is designed to mimic a realistic coding scenario where the objective is to reduce the amount of text that needs to be manually coded. We use the function <code>createDataPartition</code> from the <code>caret</code> package to split our data into a training dataset (consisting of 15% of the data) and a testing dataset (the remaining 85% of the data). We do so in a way that the two partitions are balanced in term of proportion of codes.</p>
<div class="cell" data-hash="ch10-bert_cache/html/unnamed-chunk-11_1a4435d8e886bea291fde46e8f8d4f0e">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>seed <span class="ot">=</span> <span class="dv">123</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(seed) <span class="co"># for reproducibility</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>trainIndex <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(df[[<span class="st">"annotation_target"</span>]], <span class="at">p =</span> <span class="fl">0.15</span>, </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">list =</span> <span class="cn">FALSE</span>, </span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">times =</span> <span class="dv">1</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating training and testing datasets</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>df_train <span class="ot">&lt;-</span> df[trainIndex, ]</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>df_test <span class="ot">&lt;-</span> df[<span class="sc">-</span>trainIndex, ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="sec-wordembd" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="sec-wordembd"><span class="header-section-number">3.5</span> Word embeddings</h3>
<p>Our next task is to convert our text data into numeric representations (i.e., word embeddings) to be able to apply common ML techniques. Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation. Word embeddings can be defined as dense vectors of real numbers that represent words in a way in which words that share similar contexts are placed closer together. Unlike traditional one-hot encoding, where each word is represented as a unique binary vector, word embeddings capture semantic relationships between words, making them powerful for various NLP tasks. In simpler terms, word embeddings are a way to represent words as numbers so that words with similar meanings end up having similar numerical patterns.</p>
<p>In simple models like Word2Vec, word embeddings are created by training on large text corpora. These models learn to map each word to a vector in such a way that words used in similar contexts have similar vectors. For example, the vectors for “king” and “queen” might be close to each other because these words often appear in similar contexts (e.g., both might appear near words like “monarch,” “crown,” and “royalty”). BERT models take the concept of word embeddings one step further. BERT is a transformer-based model that understands the context of a word in a sentence by looking at both its left and right sides simultaneously (bidirectional). This is in contrast to earlier models like Word2Vec, which only considered the context in a single direction.</p>
<p><strong>BERT</strong> starts by <em>tokenizing</em> the input text. This process breaks the text into smaller units called tokens. BERT uses a technique called WordPiece tokenization, where words are broken down into subword units. This helps BERT handle out-of-vocabulary words by representing them as combinations of known subwords. BERT has an <em>embedding layer</em> that converts these tokens into dense vectors. However, unlike static word embeddings (e.g., Word2Vec, where each word has a single vector), BERT’s embeddings are dynamic. The embedding for a word depends not just on the word itself but also on the entire context of the sentence. In addition to token embeddings, BERT includes <em>positional embeddings</em>, which provide the model with information about the position of each token in the sentence. This is crucial because, unlike recurrent neural networks for example, transformers do not inherently handle the order of words. BERT then passes these embeddings through multiple <em>layers of transformers</em>, where self-attention mechanisms allow the model to focus on different parts of the sentence. This enables BERT to capture complex dependencies and relationships between words in a sentence. The output from the final transformer layer provides the <em>contextualized word embeddings</em>. These embeddings are rich in context and meaning because they take into account the entire sentence. For example, the word “bank” will have different embeddings in the sentences “He sat on the river bank” and “He went to the bank to withdraw money,” reflecting the different meanings of the word in these contexts.</p>
<p>Several variations and enhancements have been created based on BERT. For instance, <strong>RoBERTa</strong> (A Robustly Optimized BERT Pretraining Approach) <span class="citation" data-cites="Liu2019-bz">[<a href="#ref-Liu2019-bz" role="doc-biblioref">49</a>]</span> builds on BERT’s architecture by enhancing the pretraining process with more data, among other improvements. These modifications, along with larger batch sizes and learning rates, as well as the introduction of dynamic masking, make RoBERTa more robust and better performing than BERT on various natural language processing tasks. Another example is <strong>XLNet</strong> <span class="citation" data-cites="Yang2019-rt">[<a href="#ref-Yang2019-rt" role="doc-biblioref">50</a>]</span>, which improves upon BERT by using permutation language modeling, allowing it to capture bidirectional context without the need for masking. It builds on the Transformer-XL architecture, enabling the handling of longer sequences and dependencies beyond the sentence level. XLNet’s approach combines autoregressive training with a permutation-based technique, making it particularly effective for tasks requiring complex context understanding, such as question answering and language modeling.</p>
<p>We will create word embeddings for the training and testing datasets using different variations of BERT in order to compare performance between models. For this purpose, we will use the <code>textEmbed</code> function from the <code>text</code> package. By default, this function uses the <code>"bert-base-uncased"</code> language model to create the word embeddings. We will use this model as a baseline, and compare it with two other models: <code>"roberta-base"</code> and <code>"xlnet-base-cased"</code>. You can try other language models available in Hugging Face that support the task of creating word embeddings (text classification category). You should take into account that this process takes a considerable amount of run time. As an input to the function we provide the <code>"clean_text"</code> column of our training and testing data, which contains the text to be coded.</p>
<div class="cell" data-hash="ch10-bert_cache/html/unnamed-chunk-12_4bf7ca08b92e99a48d6ceeec26d56f85">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating embeddings for the training data</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="do">## BERT</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>word_embeddings_bert <span class="ot">&lt;-</span> <span class="fu">textEmbed</span>(df_train[,<span class="st">"clean_text"</span>], </span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>                            <span class="at">model =</span> <span class="st">"bert-base-uncased"</span>,</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>                            <span class="at">aggregation_from_tokens_to_word_types =</span> <span class="st">"mean"</span>,</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>                            <span class="at">keep_token_embeddings =</span> <span class="cn">FALSE</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="do">## RoBERTa</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>word_embeddings_roberta <span class="ot">&lt;-</span> <span class="fu">textEmbed</span>(df_train[,<span class="st">"clean_text"</span>], </span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>                            <span class="at">model =</span> <span class="st">"roberta-base"</span>,</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>                            <span class="at">aggregation_from_tokens_to_word_types =</span> <span class="st">"mean"</span>,</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>                            <span class="at">keep_token_embeddings =</span> <span class="cn">FALSE</span>)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="do">## XLNet</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>word_embeddings_xlnet <span class="ot">&lt;-</span> <span class="fu">textEmbed</span>(df_train[,<span class="st">"clean_text"</span>], </span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>                            <span class="at">model =</span> <span class="st">"xlnet-base-cased"</span>,</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>                            <span class="at">aggregation_from_tokens_to_word_types =</span> <span class="st">"mean"</span>,</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>                            <span class="at">keep_token_embeddings =</span> <span class="cn">FALSE</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating embeddings for the testing data</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="do">## BERT</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>word_embeddings_bert_test <span class="ot">&lt;-</span> <span class="fu">textEmbed</span>(df_test[,<span class="st">"clean_text"</span>], </span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>                            <span class="at">model =</span> <span class="st">"bert-base-uncased"</span>,</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>                            <span class="at">aggregation_from_tokens_to_word_types =</span> <span class="st">"mean"</span>,</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>                            <span class="at">keep_token_embeddings =</span> <span class="cn">FALSE</span>)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="do">## RoBERTa</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>word_embeddings_roberta_test <span class="ot">&lt;-</span> <span class="fu">textEmbed</span>(df_test[,<span class="st">"clean_text"</span>], </span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>                            <span class="at">model =</span> <span class="st">"roberta-base"</span>,</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>                            <span class="at">aggregation_from_tokens_to_word_types =</span> <span class="st">"mean"</span>,</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>                            <span class="at">keep_token_embeddings =</span> <span class="cn">FALSE</span>)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a><span class="do">## XLNet</span></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>word_embeddings_xlnet_test <span class="ot">&lt;-</span> <span class="fu">textEmbed</span>(df_test[,<span class="st">"clean_text"</span>], </span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>                            <span class="at">model =</span> <span class="st">"xlnet-base-cased"</span>,</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>                            <span class="at">aggregation_from_tokens_to_word_types =</span> <span class="st">"mean"</span>,</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>                            <span class="at">keep_token_embeddings =</span> <span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If we inspect the generated embeddings, we see that there are 1,612 rows in the texts of the training embeddings: one per row of the training dataset. However, instead of one column with the text, we have a large number of numerical columns that represent the text. For example, in BERT, we have 768, which corresponds to the size of the BERT token embedding vector.</p>
<div class="cell" data-hash="ch10-bert_cache/html/unnamed-chunk-15_58f5ed3ba24d8f107ed75462425a516d">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">as_tibble</span>(word_embeddings_bert<span class="sc">$</span>texts<span class="sc">$</span>texts)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1,612 × 768
   Dim1_texts Dim2_texts Dim3_texts Dim4_texts Dim5_texts Dim6_texts Dim7_texts
        &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;
 1      0.528    -0.0955     0.0600     0.124      0.533      -0.420     0.0723
 2      0.507    -0.0987     0.0220    -0.188      0.181      -0.655     0.356 
 3      0.448    -0.0161     0.257     -0.485     -0.310      -0.194     0.190 
 4     -0.117    -0.318      0.418     -0.119      0.216      -0.511     0.287 
 5     -0.629    -0.0899     0.0623    -0.0132     0.143       0.292    -0.210 
 6      0.421    -0.211      0.318      0.104     -0.0157     -0.366     0.231 
 7      0.615     0.0502     0.561     -0.406      0.0821     -0.309    -0.0774
 8      0.126     0.0952     0.146      0.113     -0.318      -0.142    -0.350 
 9      0.287     0.0764     0.261     -0.0609     0.0596     -0.345     0.0266
10      0.547    -0.272      0.163      0.0564    -0.197      -0.391     0.0832
# ℹ 1,602 more rows
# ℹ 761 more variables: Dim8_texts &lt;dbl&gt;, Dim9_texts &lt;dbl&gt;, Dim10_texts &lt;dbl&gt;,
#   Dim11_texts &lt;dbl&gt;, Dim12_texts &lt;dbl&gt;, Dim13_texts &lt;dbl&gt;, Dim14_texts &lt;dbl&gt;,
#   Dim15_texts &lt;dbl&gt;, Dim16_texts &lt;dbl&gt;, Dim17_texts &lt;dbl&gt;, Dim18_texts &lt;dbl&gt;,
#   Dim19_texts &lt;dbl&gt;, Dim20_texts &lt;dbl&gt;, Dim21_texts &lt;dbl&gt;, Dim22_texts &lt;dbl&gt;,
#   Dim23_texts &lt;dbl&gt;, Dim24_texts &lt;dbl&gt;, Dim25_texts &lt;dbl&gt;, Dim26_texts &lt;dbl&gt;,
#   Dim27_texts &lt;dbl&gt;, Dim28_texts &lt;dbl&gt;, Dim29_texts &lt;dbl&gt;, …</code></pre>
</div>
</div>
</section>
<section id="training-the-model" class="level3" data-number="3.6">
<h3 data-number="3.6" class="anchored" data-anchor-id="training-the-model"><span class="header-section-number">3.6</span> Training the model</h3>
<p>Now that we have converted our text data into numeric variables through word embeddings, we are ready to train our ML model. Specifically, we will train a Random Forest classifier algorithm. We will rely on the <code>textTrainRandomForest</code> function from the <code>text</code> package for this purpose. As training data, we will enter the embeddings from the previous step (for each of the three BERT-like models). As an output, we will enter the column that contains the codes in the original training data: <code>"annotation_target"</code>. We will use 5-fold cross validation to train the model (<code>outside_folds</code> argument). For more information about the training function, refer to the <code>text</code> package documentation.</p>
<div class="cell" data-hash="ch10-bert_cache/html/unnamed-chunk-16_4ec76b72d5b702533d3813c95e278664">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>trained_model_bert <span class="ot">&lt;-</span> <span class="fu">textTrainRandomForest</span>(</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> word_embeddings_bert<span class="sc">$</span>texts,</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> <span class="fu">data.frame</span>(<span class="fu">as.factor</span>(df_train[[<span class="st">"annotation_target"</span>]])),</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">outside_folds =</span> <span class="dv">5</span>,</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">simulate.p.value =</span> <span class="cn">TRUE</span>,</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">append_first =</span> <span class="cn">TRUE</span>,</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">multi_cores =</span> <span class="cn">TRUE</span>,</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">seed =</span> seed</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>trained_model_roberta <span class="ot">&lt;-</span> <span class="fu">textTrainRandomForest</span>(</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> word_embeddings_roberta<span class="sc">$</span>texts,</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> <span class="fu">data.frame</span>(<span class="fu">as.factor</span>(df_train[[<span class="st">"annotation_target"</span>]])),</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>  <span class="at">outside_folds =</span> <span class="dv">5</span>,</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">simulate.p.value =</span> <span class="cn">TRUE</span>,</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>  <span class="at">append_first =</span> <span class="cn">TRUE</span>,</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">multi_cores =</span> <span class="cn">TRUE</span>,</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">seed =</span> seed</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>trained_model_xlnet <span class="ot">&lt;-</span> <span class="fu">textTrainRandomForest</span>(</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">x =</span> word_embeddings_xlnet<span class="sc">$</span>texts,</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> <span class="fu">data.frame</span>(<span class="fu">as.factor</span>(df_train[[<span class="st">"annotation_target"</span>]])),</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>  <span class="at">outside_folds =</span> <span class="dv">5</span>,</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>  <span class="at">simulate.p.value =</span> <span class="cn">TRUE</span>,</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>  <span class="at">append_first =</span> <span class="cn">TRUE</span>,</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>  <span class="at">multi_cores =</span> <span class="cn">TRUE</span>,</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>  <span class="at">seed =</span> seed</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="using-the-model-to-predict" class="level3" data-number="3.7">
<h3 data-number="3.7" class="anchored" data-anchor-id="using-the-model-to-predict"><span class="header-section-number">3.7</span> Using the model to predict</h3>
<p>We can now use the trained models to code the remainder of the data (i.e., the word embeddings of the test dataset).</p>
<div class="cell" data-hash="ch10-bert_cache/html/unnamed-chunk-19_9cb754057c99b85d814a36b454afe001">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>predicted_bert <span class="ot">&lt;-</span> <span class="fu">textPredict</span>(</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">model_info =</span> trained_model_bert,</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">word_embeddings =</span> word_embeddings_bert_test<span class="sc">$</span>texts</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>predicted_roberta <span class="ot">&lt;-</span> <span class="fu">textPredict</span>(</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">model_info =</span> trained_model_roberta,</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">word_embeddings =</span> word_embeddings_roberta_test<span class="sc">$</span>texts</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>predicted_xlnet <span class="ot">&lt;-</span> <span class="fu">textPredict</span>(</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">model_info =</span> trained_model_xlnet,</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">word_embeddings =</span> word_embeddings_xlnet_test<span class="sc">$</span>texts</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="evaluating-the-model" class="level3" data-number="3.8">
<h3 data-number="3.8" class="anchored" data-anchor-id="evaluating-the-model"><span class="header-section-number">3.8</span> Evaluating the model</h3>
<p>As a first measure of the performance of our models, we can use the results of the training data. To ease the comparison, let us first combine all the results in a single dataframe (<code>rbind</code>) and adding the name of the model <code>mutate</code> to be able to identify each set of results. We then pivot our data (<code>pivot_wider</code>) to show all the metrics of each model as columns, and each model as a row.</p>
<p>As we can see in <a href="#tbl-perff">Table&nbsp;<span>10.1</span></a>, RoBERTa consistently outperforms BERT and XLNet across most key metrics, showcasing its superior performance. For accuracy, RoBERTa achieves the highest value (0.788), followed closely by BERT (0.770), while XLNet (0.734) lags behind. This indicates RoBERTa’s ability to make correct predictions more consistently across the dataset. When considering balanced accuracy, which accounts for both sensitivity and specificity, RoBERTa (0.769) again leads, demonstrating its effectiveness at handling imbalanced classes. BERT performs respectably (0.748), while XLNet (0.717) shows the weakest balance between true positive and true negative rates. In terms of sensitivity (recall), RoBERTa (0.605) outperforms BERT (0.568) and XLNet (0.517), highlighting its ability to correctly identify positive instances. Meanwhile, for specificity, RoBERTa (0.934) slightly surpasses BERT (0.928) and XLNet (0.917), reinforcing its capacity to avoid false positives. For precision, RoBERTa (0.857) maintains its advantage over BERT (0.843) and XLNet (0.809), reflecting fewer false positives. The F1 score, which balances precision and recall, also favors RoBERTa (0.645), with BERT (0.610) trailing and XLNet (0.547) performing the weakest. Looking at kappa, a measure of agreement between predicted and actual labels beyond chance, RoBERTa (0.685) once more outperforms BERT (0.654) and XLNet (0.600), showcasing its robustness in classification consistency. Finally, ROC-AUC values indicate RoBERTa (0.910) as the strongest model, effectively balancing sensitivity and specificity, followed by BERT (0.889) and XLNet (0.843). Overall, the results strongly favor RoBERTa across all metrics, making it the most suitable model for text dataset coding in this scenario. BERT provides solid performance but falls slightly behind in several areas, while XLNet struggles particularly with sensitivity, F1 score, and class balance.</p>
<div class="cell" data-hash="ch10-bert_cache/html/unnamed-chunk-22_5e0388008c52ae5bdac5f7d92b1cb3a3">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rbind</span>( <span class="co"># Combine the performance results of each model</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>  trained_model_bert<span class="sc">$</span>results <span class="sc">|&gt;</span> <span class="fu">mutate</span>(<span class="at">Model =</span> <span class="st">"BERT"</span>),</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  trained_model_roberta<span class="sc">$</span>results <span class="sc">|&gt;</span> <span class="fu">mutate</span>(<span class="at">Model =</span> <span class="st">"RoBERTa"</span>),</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>  trained_model_xlnet<span class="sc">$</span>results <span class="sc">|&gt;</span> <span class="fu">mutate</span>(<span class="at">Model =</span> <span class="st">"XLNet"</span>)) <span class="sc">|&gt;</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>.estimator) <span class="sc">|&gt;</span> </span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_wider</span>(<span class="at">names_from =</span> <span class="st">".metric"</span>, <span class="at">values_from =</span> <span class="st">".estimate"</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="ch10-bert_cache/html/tbl-perff_7f0fe51ccf9d7a1b586906de8976e9f9">
<div class="cell-output-display">
<div id="tbl-perff" class="anchored">
<table class="table table-sm table-striped">
<caption>Table&nbsp;1<strong>.</strong> Performance of each model</caption>
<colgroup>
<col style="width: 10%">
<col style="width: 12%">
<col style="width: 17%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 13%">
<col style="width: 8%">
<col style="width: 9%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Model</th>
<th style="text-align: right;">accuracy</th>
<th style="text-align: right;">bal_accuracy</th>
<th style="text-align: right;">sens</th>
<th style="text-align: right;">spec</th>
<th style="text-align: right;">precision</th>
<th style="text-align: right;">kap</th>
<th style="text-align: right;">f_meas</th>
<th style="text-align: right;">roc_auc</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">BERT</td>
<td style="text-align: right;">0.770</td>
<td style="text-align: right;">0.748</td>
<td style="text-align: right;">0.568</td>
<td style="text-align: right;">0.928</td>
<td style="text-align: right;">0.843</td>
<td style="text-align: right;">0.654</td>
<td style="text-align: right;">0.610</td>
<td style="text-align: right;">0.889</td>
</tr>
<tr class="even">
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: right;">0.788</td>
<td style="text-align: right;">0.769</td>
<td style="text-align: right;">0.605</td>
<td style="text-align: right;">0.934</td>
<td style="text-align: right;">0.857</td>
<td style="text-align: right;">0.685</td>
<td style="text-align: right;">0.645</td>
<td style="text-align: right;">0.910</td>
</tr>
<tr class="odd">
<td style="text-align: left;">XLNet</td>
<td style="text-align: right;">0.734</td>
<td style="text-align: right;">0.717</td>
<td style="text-align: right;">0.517</td>
<td style="text-align: right;">0.917</td>
<td style="text-align: right;">0.809</td>
<td style="text-align: right;">0.600</td>
<td style="text-align: right;">0.547</td>
<td style="text-align: right;">0.843</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>In a real setting, if we were to choose one of the three models to code our text dataset based solely on the performance metrics obtained from the training process, we would choose RoBERTa. Since in this case we have access to the fully coded dataset, let us verify if RoBERTa is still the best choice when faced with the remainder of the dataset —still unseen by any model. For this purpose, we will compare the manual codes with the automatic codes classified by our models. First, let us combine all codes into a single dataframe. We obtain the manually coded data from the test data (<code>df_test</code>), and the automatically coded data from the prediction results of each model (e.g., <code>predicted_bert</code>). We also need to convert the five possible codes to factors to be able to operate with them as “categories”.</p>
<div class="cell" data-hash="ch10-bert_cache/html/unnamed-chunk-24_4eeecec1208f37c809b22af6e205fd45">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">manual =</span> <span class="fu">as.factor</span>(df_test<span class="sc">$</span>annotation_target),</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">bert =</span> <span class="fu">as.factor</span>(predicted_bert<span class="sc">$</span><span class="st">`</span><span class="at">texts__cv_method="validation_split"pred</span><span class="st">`</span>),</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">roberta =</span> <span class="fu">as.factor</span>(predicted_roberta<span class="sc">$</span><span class="st">`</span><span class="at">texts__cv_method="validation_split"pred</span><span class="st">`</span>),</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">xlnet =</span> <span class="fu">as.factor</span>(predicted_xlnet<span class="sc">$</span><span class="st">`</span><span class="at">texts__cv_method="validation_split"pred</span><span class="st">`</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To evaluate the results, we can use the <code>confusionMatrix</code> function from the <code>caret</code> package, which takes as an input the results of the automatic coding and the manual codes. We can combine again all the results together using <code>rbind</code>.</p>
<div class="cell" data-hash="ch10-bert_cache/html/unnamed-chunk-25_a93e5203843655312e65c3a2c10ad451">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>cm_bert <span class="ot">&lt;-</span> <span class="fu">confusionMatrix</span>(results<span class="sc">$</span>bert, results<span class="sc">$</span>manual)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>cm_roberta <span class="ot">&lt;-</span> <span class="fu">confusionMatrix</span>(results<span class="sc">$</span>roberta, results<span class="sc">$</span>manual)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>cm_xlnet <span class="ot">&lt;-</span> <span class="fu">confusionMatrix</span>(results<span class="sc">$</span>xlnet, results<span class="sc">$</span>manual)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="fu">rbind</span>(<span class="at">Bert =</span> cm_bert<span class="sc">$</span>overall, </span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>      <span class="at">RoBERTa =</span> cm_roberta<span class="sc">$</span>overall, </span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>      <span class="at">XLnet =</span> cm_xlnet<span class="sc">$</span>overall)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>When comparing the accuracy and Cohen’s Kappa, after predicting the code of the test data, RoBERTa is still ahead of the two other models. The accuracy is quite high and the Cohen’s Kappa is substantial (&gt; 0.6). Therefore, our initial choice was correct.</p>
<div class="cell" data-hash="ch10-bert_cache/html/tbl-after-perf_30b09aec38d140f953c0c8c52dfad254">
<div class="cell-output-display">
<div id="tbl-after-perf" class="anchored">
<table class="table table-sm table-striped">
<caption>Table&nbsp;2<strong>.</strong> Performance of each model calculated using the test data</caption>
<colgroup>
<col style="width: 10%">
<col style="width: 11%">
<col style="width: 7%">
<col style="width: 17%">
<col style="width: 17%">
<col style="width: 16%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">Accuracy</th>
<th style="text-align: right;">Kappa</th>
<th style="text-align: right;">AccuracyLower</th>
<th style="text-align: right;">AccuracyUpper</th>
<th style="text-align: right;">AccuracyNull</th>
<th style="text-align: right;">AccuracyPValue</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Bert</td>
<td style="text-align: right;">0.737</td>
<td style="text-align: right;">0.599</td>
<td style="text-align: right;">0.728</td>
<td style="text-align: right;">0.746</td>
<td style="text-align: right;">0.409</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: right;">0.770</td>
<td style="text-align: right;">0.654</td>
<td style="text-align: right;">0.761</td>
<td style="text-align: right;">0.779</td>
<td style="text-align: right;">0.409</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">XLnet</td>
<td style="text-align: right;">0.744</td>
<td style="text-align: right;">0.616</td>
<td style="text-align: right;">0.734</td>
<td style="text-align: right;">0.752</td>
<td style="text-align: right;">0.409</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>In addition, we calculate Gwet’s AC1 and Matthews Correlation Coefficient (MCC). Gwet’s AC1 provides a robust measure of inter-rater agreement for multiclass settings, addressing the limitations of traditional kappa statistics. MCC, extended to handle multiclass classifications, assesses the quality of the classifier by considering the entire confusion matrix, providing a balanced evaluation of performance across all classes.</p>
<div class="cell" data-hash="ch10-bert_cache/html/unnamed-chunk-27_8ecc73dd3d1023c941872711569f5cf2">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Gwet AC1</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>calculate_gwets_ac1 <span class="ot">&lt;-</span> <span class="cf">function</span>(table) {</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">sum</span>(table)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>  po <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">diag</span>(table)) <span class="sc">/</span> n</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>  pe <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">pmax</span>(<span class="fu">rowSums</span>(table), <span class="fu">colSums</span>(table))) <span class="sc">/</span> n<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>  (po <span class="sc">-</span> pe) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">-</span> pe)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>extra_results <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">gwets_ac1 =</span> <span class="fu">c</span>(<span class="fu">calculate_gwets_ac1</span>(cm_bert<span class="sc">$</span>table),</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>                <span class="fu">calculate_gwets_ac1</span>(cm_roberta<span class="sc">$</span>table),</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>                <span class="fu">calculate_gwets_ac1</span>(cm_xlnet<span class="sc">$</span>table)),</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>  <span class="at">mcc =</span> <span class="fu">c</span>(<span class="fu">mcc</span>(results<span class="sc">$</span>bert, results<span class="sc">$</span>manual),</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>          <span class="fu">mcc</span>(results<span class="sc">$</span>roberta, results<span class="sc">$</span>manual),</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>          <span class="fu">mcc</span>(results<span class="sc">$</span>xlnet, results<span class="sc">$</span>manual)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(extra_results) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"Bert"</span>, <span class="st">"RoBERTa"</span>, <span class="st">"XLnet"</span>)</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>extra_results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="ch10-bert_cache/html/tbl-after-perf-extra_a093295605bba681f55f8902e8c85094">
<div class="cell-output-display">
<div id="tbl-after-perf-extra" class="anchored">
<table class="table table-sm table-striped">
<caption>Table&nbsp;3<strong>.</strong> Additional performance-metrics</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">gwets_ac1</th>
<th style="text-align: right;">mcc</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Bert</td>
<td style="text-align: right;">0.737</td>
<td style="text-align: right;">0.611</td>
</tr>
<tr class="even">
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: right;">0.770</td>
<td style="text-align: right;">0.664</td>
</tr>
<tr class="odd">
<td style="text-align: left;">XLnet</td>
<td style="text-align: right;">0.743</td>
<td style="text-align: right;">0.620</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>All three models demonstrate substantial inter-rater reliability (<a href="#tbl-after-perf-extra">Table&nbsp;<span>10.3</span></a>) as per the benchmarks commonly used for interpreting AC1 (values above 0.70 are typically considered strong). RoBERTa is the best-performing model in this evaluation metric, followed by XLNet, with BERT slightly trailing, where the results are almost exactly matching the accuracy metric. Regarding MCC, all values here fall between 0.6 and 0.7, indicating moderate to strong correlations between the model predictions and the manual labels. Similar to Gwet’s AC1 results, RoBERTa achieves the highest MCC score, indicating the best performance in terms of balanced prediction quality.</p>
<p>We can also extract the results by class to gain more knowledge about the potential classification errors. Some models might be better at capturing some types of discussion contributions than others. The code below extracts the performance metrics by class and creates a long data frame containing the model name, the code (agree, disagree, etc.) the metric name (sensitivity, precision, etc.), and the value of the metric. We can combine the results for each model in a single dataframe using <code>rbind</code>.</p>
<div class="cell" data-hash="ch10-bert_cache/html/unnamed-chunk-29_d90f7e2bf45e8a1427d660fb5aea2661">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># BERT</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>byclass_bert <span class="ot">&lt;-</span> cm_bert<span class="sc">$</span>byClass <span class="sc">|&gt;</span> <span class="fu">data.frame</span>() <span class="sc">|&gt;</span> </span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rownames_to_column</span>() <span class="sc">|&gt;</span> <span class="co"># Convert rowname (code) to new column</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">12</span>) <span class="sc">|&gt;</span> <span class="co"># Convert to a long format where each metric is a new row</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Model =</span> <span class="st">"BERT"</span>) <span class="co"># Add the model name as a column</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co"># RoBERTa</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>byclass_roberta <span class="ot">&lt;-</span> cm_roberta<span class="sc">$</span>byClass <span class="sc">|&gt;</span> <span class="fu">data.frame</span>() <span class="sc">|&gt;</span> </span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rownames_to_column</span>() <span class="sc">|&gt;</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">12</span>) <span class="sc">|&gt;</span> </span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Model =</span> <span class="st">"RoBERTa"</span>)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="co"># XLNet</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>byclass_xlnet <span class="ot">&lt;-</span> cm_xlnet<span class="sc">$</span>byClass <span class="sc">|&gt;</span> <span class="fu">data.frame</span>() <span class="sc">|&gt;</span> </span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rownames_to_column</span>() <span class="sc">|&gt;</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">12</span>) <span class="sc">|&gt;</span> </span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Model =</span> <span class="st">"XLNet"</span>)</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine all results together</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>byclass_all <span class="ot">&lt;-</span> <span class="fu">rbind</span>(byclass_bert, byclass_roberta, byclass_xlnet)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can also plot the results using <code>ggplot2</code>. We use a bar plot (<code>geom_col</code>) to plot all the metrics grouped by model. We use <code>facet_wrap</code> to separate the plot by coding class (agree, disagree, moderation, etc.).</p>
<div class="cell" data-hash="ch10-bert_cache/html/fig-byclass_d9cf076ae1d6b8420a1465006899b425">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(byclass_all, <span class="fu">aes</span>(<span class="at">x =</span> name, <span class="at">y =</span> value, <span class="at">group =</span> Model, <span class="at">fill =</span> Model)) <span class="sc">+</span> </span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_col</span>(<span class="at">position =</span> <span class="st">"dodge2"</span>, <span class="at">width =</span> <span class="fl">0.9</span>) <span class="sc">+</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="st">"rowname"</span>, <span class="at">ncol =</span> <span class="dv">1</span>) <span class="sc">+</span> </span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"#76B7B2"</span>, <span class="st">"#B07AA1"</span>, <span class="st">"#FF9DA7"</span>)) <span class="sc">+</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>,</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>        <span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">45</span>, <span class="at">hjust =</span> <span class="dv">1</span>)) <span class="sc">+</span> </span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">fill =</span> <span class="st">"Model"</span>) <span class="sc">+</span> <span class="fu">xlab</span>(<span class="st">""</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">""</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-byclass" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/ch10-bert_files/figure-html/fig-byclass-1.png" class="img-fluid figure-img" width="768"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;2<strong>.</strong> Model performance comparison by class</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>When we visualize the different performance metrics of each model per class (<a href="#fig-byclass">Figure&nbsp;<span>10.2</span></a>), we can see that RoBERTa outperforms or equals the other model in 31 of the 55 comparisons, XLNet 21, and BERT 16. These findings further confirm that RoBERTa is the most suitable model for the classification task.</p>
<p>In addition, we can also test whether the combination of all three models yields better results. First we create an option to obtain the vote from all three algorithms (i.e., the statistical mode):</p>
<div class="cell" data-hash="ch10-bert_cache/html/unnamed-chunk-31_b3857e6ea32bffb3f9c4364c39ccbe25">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculates with class is selected by most models</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>getmode <span class="ot">&lt;-</span> <span class="cf">function</span>(v) {</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>   uniqv <span class="ot">&lt;-</span> <span class="fu">unique</span>(v)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>   uniqv[<span class="fu">which.max</span>(<span class="fu">tabulate</span>(<span class="fu">match</span>(v, uniqv)))]</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we iterate through our results and, for each coded, we gather the voting of all three models. We input RoBERTa in the first position so that in case all models have come up with a different code, the one created by RoBERTa is chosen:</p>
<div class="cell" data-hash="ch10-bert_cache/html/unnamed-chunk-32_27fe0315a4642a2e158128e7440fbc7a">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>results_with_vote <span class="ot">&lt;-</span> results <span class="sc">|&gt;</span> </span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rowwise</span>() <span class="sc">|&gt;</span> </span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">vote =</span> <span class="fu">getmode</span>(<span class="fu">c</span>(roberta, xlnet, bert))) <span class="sc">|&gt;</span> </span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now create the confusion matrix for the voted code. We can see that the results are slightly below those achieved by RoBERTa alone, so we are not getting any advantage by voting.</p>
<div class="cell" data-hash="ch10-bert_cache/html/unnamed-chunk-33_ee5099a300c49f1d8edfdcfd700f7639">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>cm_vote <span class="ot">&lt;-</span> <span class="fu">confusionMatrix</span>(results_with_vote<span class="sc">$</span>vote, results_with_vote<span class="sc">$</span>manual)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>cm_vote<span class="sc">$</span>overall</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull 
     0.7646994      0.6455472      0.7558531      0.7733763      0.4085125 
AccuracyPValue  McnemarPValue 
     0.0000000            NaN </code></pre>
</div>
</div>
</section>
<section id="impact-on-findings" class="level3" data-number="3.9">
<h3 data-number="3.9" class="anchored" data-anchor-id="impact-on-findings"><span class="header-section-number">3.9</span> Impact on findings</h3>
<p>To complement the evaluation of our models, we assess the impact of using an automatically coded dataset on the findings derived from applying common learning analytics methods to the data. We choose RoBERTa, as it was our best performing model, to compare the results with those obtained from manually coded data.</p>
<p>First, we need to construct the complete AI-generated dataset which contains all of the data classified by RoBERTa and the small percentage classified manually used as training. We store it in the <code>complete_ai</code> variable.</p>
<div class="cell" data-hash="ch10-bert_cache/html/unnamed-chunk-34_b9d2ff0870b753ad6ac078811186ce86">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>df_test_recoded <span class="ot">&lt;-</span> df_test</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>df_test_recoded<span class="sc">$</span>annotation_target <span class="ot">&lt;-</span> results<span class="sc">$</span>roberta</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>complete_ai <span class="ot">&lt;-</span> <span class="fu">rbind</span>(df_train, df_test_recoded)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We start by analyzing our data using sequence analysis, which is a method that is commonly used to study temporally-ordered data, and specifically collaborative learning (e.g., <span class="citation" data-cites="Saqr2023-nv">[<a href="#ref-Saqr2023-nv" role="doc-biblioref">51</a>]</span>). For a full tutorial about sequence analysis consult <span class="citation" data-cites="Saqr2024-ug">[<a href="#ref-Saqr2024-ug" role="doc-biblioref">52</a>]</span>. The first step is to order the messages in each group and to transpose the data so that is in a wide format (i.e., each column represents one message order: message 1, message 2, etc.). Then, we use <code>seqdef</code> from the <code>TraMineR</code> package <span class="citation" data-cites="Gabadinho2011-lm">[<a href="#ref-Gabadinho2011-lm" role="doc-biblioref">53</a>,]</span>, to construct a sequence object, where each group is represented as a sequence of all its contributions. We use <code>seqdplot</code> to plot the sequence distribution plot (<a href="#fig-seqdplot-compare">Figure&nbsp;<span>10.3</span></a>). From visual inspection, we can see that the sequence profiles are similar, although the RoBERTa side has a lower presence of the minority codes.</p>
<div class="cell" data-hash="ch10-bert_cache/html/unnamed-chunk-35_8707edaf1d024763ea7535067e6f6431">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(TraMineR)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>wider_human <span class="ot">&lt;-</span> df <span class="sc">|&gt;</span> <span class="fu">group_by</span>(group_id) <span class="sc">|&gt;</span> </span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">order =</span> <span class="fu">seq_along</span>(message_id)) <span class="sc">|&gt;</span> </span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_wider</span>(<span class="at">id_cols=</span><span class="st">"group_id"</span>, <span class="at">names_from=</span><span class="st">"order"</span>,<span class="at">values_from =</span> <span class="st">"annotation_target"</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>wider_ai <span class="ot">&lt;-</span> complete_ai <span class="sc">|&gt;</span> <span class="fu">group_by</span>(group_id) <span class="sc">|&gt;</span> </span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">order =</span> <span class="fu">seq_along</span>(message_id)) <span class="sc">|&gt;</span> </span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_wider</span>(<span class="at">id_cols=</span><span class="st">"group_id"</span>, <span class="at">names_from=</span><span class="st">"order"</span>,<span class="at">values_from =</span> <span class="st">"annotation_target"</span>)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>seq_human <span class="ot">&lt;-</span> <span class="fu">seqdef</span>(wider_human, <span class="dv">2</span><span class="sc">:</span><span class="fu">ncol</span>(wider_human))</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>seq_ai <span class="ot">&lt;-</span> <span class="fu">seqdef</span>(wider_ai, <span class="dv">2</span><span class="sc">:</span><span class="fu">ncol</span>(wider_ai))</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a><span class="fu">seqdplot</span>(seq_human) </span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a><span class="fu">seqdplot</span>(seq_ai)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-hash="ch10-bert_cache/html/fig-seqdplot-compare_94fed766ebf31e9eb73ad1b8553b8d49">
<div class="cell-output-display">
<div id="fig-seqdplot-compare" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/ch10-bert_files/figure-html/fig-seqdplot-compare-1.png" class="img-fluid figure-img" width="960"></p>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;3<strong>.</strong> Comparing the sequence distribution plots between the AI-coded and human-coded datasets</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Next, we use transition network analysis, a novel method that implements stochastic process mining through Markov models <span class="citation" data-cites="Saqr2025-ku">[<a href="#ref-Saqr2025-ku" role="doc-biblioref">54</a>]</span>, to model the transitions between codes and assess whether there are differences between the two datasets. For a full tutorial, refer to <span class="citation" data-cites="LABOOK2_Chapter_15">[<a href="#ref-LABOOK2_Chapter_15" role="doc-biblioref">55</a>]</span>. We use the <code>tna</code> function to generate the Markov model containing the probability of transitioning between each pair of codes. We do so for the human coded and for the data coded with the RoBERTa model. We can visualize each model using the <code>plot</code> function. We can also plot the difference between the transitions using <code>plot_compare</code>. However, this is only a visual comparison. We can run a permutation test (<code>permutation_test</code>) to statistically compare the two models and visualize only the statistically significant differences (<a href="#fig-qgraph-compare">Figure&nbsp;<span>10.4</span></a>).</p>
<div>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tna)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute transition probabilities</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>transitions_ai <span class="ot">&lt;-</span>  <span class="fu">tna</span>(seq_ai)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>transitions_human <span class="ot">&lt;-</span>  <span class="fu">tna</span>(seq_human)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot human coded transition network</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(transitions_human)</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot RoBERTa transition network</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(transitions_ai)</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot difference network</span></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a><span class="fu">plot_compare</span>(transitions_human, transitions_ai)</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a permutation test to assess which differences are statistically significant</span></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>permutation <span class="ot">&lt;-</span> <span class="fu">permutation_test</span>(transitions_human, transitions_ai, <span class="at">it =</span> <span class="dv">1000</span>)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the significant differences identified in the permutation test</span></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(permutation, <span class="at">minimum =</span> <span class="fl">0.01</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="fig-qgraph-compare" class="cell quarto-layout-panel" data-hash="ch10-bert_cache/html/fig-qgraph-compare_8c2bc05132eefd8141024214c305c5b9">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-qgraph-compare-1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/ch10-bert_files/figure-html/fig-qgraph-compare-1.png" class="img-fluid figure-img" data-ref-parent="fig-qgraph-compare" width="480"></p>
<p></p><figcaption class="figure-caption">(a) Human-coded</figcaption><p></p>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-qgraph-compare-2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/ch10-bert_files/figure-html/fig-qgraph-compare-2.png" class="img-fluid figure-img" data-ref-parent="fig-qgraph-compare" width="480"></p>
<p></p><figcaption class="figure-caption">(b) RoBERTa</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-qgraph-compare-3" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/ch10-bert_files/figure-html/fig-qgraph-compare-3.png" class="img-fluid figure-img" data-ref-parent="fig-qgraph-compare" width="480"></p>
<p></p><figcaption class="figure-caption">(c) Difference network</figcaption><p></p>
</figure>
</div>
</div>
<div class="cell-output-display quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-qgraph-compare-4" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="img/ch10-bert_files/figure-html/fig-qgraph-compare-4.png" class="img-fluid figure-img" data-ref-parent="fig-qgraph-compare" width="480"></p>
<p></p><figcaption class="figure-caption">(d) Permutation test</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption"><strong>Figure</strong>&nbsp;4<strong>.</strong> Comparing the transition plots between the AI-coded and human-coded datasets</figcaption><p></p>
</figure>
</div>
</div>
<p>In the transition network created from human codes, <em>Solution</em> and <em>Agree</em> are central nodes with strong self-loops and frequent transitions between them. <em>Moderation</em> shows significant connections from <em>Agree</em> and <em>Solution</em>, while <em>Disagree</em> is weakly integrated. In the RoBERTa network, <em>Solution</em> and <em>Agree</em> <em>remain</em> central, and <em>Disagree</em> still has low connections. The difference network highlights that <em>Reasoning</em> plays a stronger role in RoBERTa’s coding, especially in its links from <em>Agree</em> and <em>Solution</em>. <em>Disagree</em> is less integrated, and some transitions, like from <em>Moderation</em> to <em>Reasoning</em>, are significantly reduced.</p>
<p>We could continue with other relevant methods, like clustering participants according to their argumentation profiles or role in the discussion, but we have already demonstrated that the results somewhat differ between the two sets of codes. However, the overall takeaway for both versions is that <em>Reasoning</em> is the driver of the conversation. It is not expected that two human-coded datasets yielded more similar results.</p>
</section>
</section>
<section id="conclusion" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">4</span> Conclusion</h2>
<p>This tutorial examined the use of BERT-like language models for automating discourse coding in R, showcasing how these models can expedite qualitative text classification while maintaining an acceptable level of accuracy. We illustrated this with a case study on collaborative problem-solving. Among the models tested, RoBERTa demonstrated better performance compared to BERT and XLNet, making it the most effective model for the classification task. We exemplified how applying different analytical techniques from the learning analytics repertoire, including sequence analysis and transition network analysis, revealed somewhat different insights from the human coded data than from the automatically coded data. However, there was an overall alignment between both in capturing the broad patterns of discourse and the disagreement was comparable to the one expected between two humans. As large language models continue to advance, we anticipate even greater accuracy in automated discourse coding, making automated coding an increasingly reliable tool for qualitative research.</p>
<p>Although automated coding with large language models offers promising efficiency and consistency, caution is warranted when interpreting the results. As pointed out earlier in this chapter, the reliability of automated methods can vary based on the context, dataset characteristics, and the specific model used. Differences in coding accuracy may introduce biases or overlook human judgment, which could impact downstream analyses. We must critically evaluate the output of automated coding by comparing it against human-coded benchmarks and incorporating rigorous metrics like Gwet’s AC1 or Matthews Correlation Coefficient. Additionally, researchers should remain mindful of the iterative and reflexive nature of qualitative coding, which automated systems cannot fully replicate. Therefore, automated methods should be viewed as complementary to, rather than replacements for, human expertise in the coding process.</p>


</section>
<section id="bibliography" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-elliott2018thinking" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Elliott V (2018) Thinking about the coding process in qualitative data analysis. Qualitative report 23:</div>
</div>
<div id="ref-shaffer2021we" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">Shaffer DW, Ruis AR (2021) How we code. In: Advances in quantitative ethnography: Second international conference, ICQE 2020, malibu, CA, USA, february 1-3, 2021, proceedings 2. Springer, pp 62–77</div>
</div>
<div id="ref-thomas2006general" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">3. </div><div class="csl-right-inline">Thomas DR (2006) A general inductive approach for analyzing qualitative evaluation data. American journal of evaluation 27:237–246</div>
</div>
<div id="ref-LABOOK2_Chapter_8" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">4. </div><div class="csl-right-inline">Oliveira E, Song Y, Saqr M, López-Pernas S (2025) An introduction to large language models in education. In: Saqr M, López-Pernas S (eds) Advanced learning analytics methods: AI, precision and complexity. Springer Nature Switzerland, Cham</div>
</div>
<div id="ref-raiaan2024review" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">5. </div><div class="csl-right-inline">Raiaan MAK, Mukta MSH, Fatema K, Fahad NM, Sakib S, Mim MMJ, Ahmad J, Ali ME, Azam S (2024) A review on large language models: Architectures, applications, taxonomies, open issues and challenges. IEEE Access</div>
</div>
<div id="ref-bosley2023we" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">6. </div><div class="csl-right-inline">Bosley M, Jacobs-Harukawa M, Licht H, Hoyle A (2023) Do we still need BERT in the age of GPT? Comparing the benefits of domain-adaptation and in-context-learning approaches to using LLMs for political science research</div>
</div>
<div id="ref-amaratunga2023understanding" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">7. </div><div class="csl-right-inline">Amaratunga T (2023) What makes LLMs large? 81–177</div>
</div>
<div id="ref-chae2023large" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">8. </div><div class="csl-right-inline">Chae Y, Davidson T (2023) Large language models for text classification: From zero-shot learning to fine-tuning. Open Science Foundation</div>
</div>
<div id="ref-periti2024lexical" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">9. </div><div class="csl-right-inline">Periti F, Montanelli S (2024) Lexical semantic change through large language models: A survey. ACM Computing Surveys</div>
</div>
<div id="ref-saqr2025classification" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">10. </div><div class="csl-right-inline">Saqr M, Misiejuk K, Tikka S, López-Pernas S (2025) Artificial intelligence: Student classification with machine learning in r</div>
</div>
<div id="ref-Yan2024-jw" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">11. </div><div class="csl-right-inline">Yan L, Martinez-Maldonado R, Gasevic D (2024) <a href="https://doi.org/10.1145/3636555.3636856">Generative artificial intelligence in learning analytics: Contextualising opportunities and challenges through the learning analytics cycle</a>. In: Proceedings of the 14th learning analytics and knowledge conference. ACM, New York, NY, USA</div>
</div>
<div id="ref-barany2024chatgpt" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">12. </div><div class="csl-right-inline">Barany A, Nasiar N, Porter C, Zambrano AF, Andres AL, Bright D, Shah M, Liu X, Gao S, Zhang J, others (2024) ChatGPT for education research: Exploring the potential of large language models for qualitative codebook development. In: International conference on artificial intelligence in education. Springer, pp 134–149</div>
</div>
<div id="ref-hayes2023conversing" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">13. </div><div class="csl-right-inline">Hayes A <span>“Conversing”</span> with qualitative data: Enhancing qualitative research through large language models (LLMs). <a href="https://doi.org/10.31235/osf.io/yms8p">https://doi.org/10.31235/osf.io/yms8p</a></div>
</div>
<div id="ref-mathis2024inductive" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">14. </div><div class="csl-right-inline">Mathis WS, Zhao S, Pratt N, Weleff J, De Paoli S (2024) Inductive thematic analysis of healthcare qualitative interviews using open-source large language models: How does it compare to traditional methods? Computer Methods and Programs in Biomedicine 255:108356</div>
</div>
<div id="ref-davison2024ethics" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">15. </div><div class="csl-right-inline">Davison RM, Chughtai H, Nielsen P, Marabelli M, Iannacci F, Offenbeek M van, Tarafdar M, Trenz M, Techatassanasoontorn A, Diaz Andrade A, others (2024) The ethics of using generative AI for qualitative data analysis</div>
</div>
<div id="ref-morgan2023exploring" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">16. </div><div class="csl-right-inline">Morgan DL (2023) Exploring the use of artificial intelligence for qualitative data analysis: The case of ChatGPT. International journal of qualitative methods 22:16094069231211248</div>
</div>
<div id="ref-Garg2024-wi" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">17. </div><div class="csl-right-inline">Garg R, Han J, Cheng Y, Fang Z, Swiecki Z (2024) <a href="https://doi.org/10.1145/3636555.3636879">Automated discourse analysis via generative artificial intelligence</a>. In: Proceedings of the 14th learning analytics and knowledge conference. ACM, New York, NY, USA</div>
</div>
<div id="ref-Misiejuk2024-wl" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">18. </div><div class="csl-right-inline">Misiejuk K, Kaliisa R, Scianna J (2024) Augmenting assessment with <span>AI</span> coding of online student discourse: A question of reliability. Computers and Education: Artificial Intelligence 6: https://doi.org/<a href="https://doi.org/10.1016/j.caeai.2024.100216">10.1016/j.caeai.2024.100216</a></div>
</div>
<div id="ref-Pugh2021-bz" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">19. </div><div class="csl-right-inline">Pugh SL, Subburaj SK, Rao A, Stewart AEB, Andrews-Todd J, D’Mello S (2021) <a href="https://files.eric.ed.gov/fulltext/ED615653.pdf">Say what? Automatic modeling of collaborative problem solving skills from student speech in the wild</a>. Int Conf Young Spéc Micro/nanotechnologies Electron Device</div>
</div>
<div id="ref-prescott2024comparing" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">20. </div><div class="csl-right-inline">Prescott MR, Yeager S, Ham L, Rivera Saldana CD, Serrano V, Narez J, Paltin D, Delgado J, Moore DJ, Montoya J (2024) Comparing the efficacy and efficiency of human and generative AI: Qualitative thematic analyses. JMIR AI 3:e54482</div>
</div>
<div id="ref-lefort2024small" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">21. </div><div class="csl-right-inline">Lefort B, Benhamou E, Ohana J-J, Guez B, Saltiel D, Challet D (2024) When small wins big: Classification tasks where compact models outperform original GPT-4. Available at SSRN 4780454</div>
</div>
<div id="ref-ganesh2024prompting" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">22. </div><div class="csl-right-inline">Ganesh A, Chandler C, D’Mello S, Palmer M, Kann K (2024) Prompting as panacea? A case study of in-context learning performance for qualitative coding of classroom dialog. In: Proceedings of the 17th international conference on educational data mining. pp 835–843</div>
</div>
<div id="ref-van2023automating" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">23. </div><div class="csl-right-inline">Van Ostaeyen S, De Langhe L, De Clercq O, Embo M, Schellens T, Valcke M (2023) Automating the identification of feedback quality criteria and the CanMEDS roles in written feedback comments using natural language processing. Perspectives on Medical Education 12:540</div>
</div>
<div id="ref-tai2024examination" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">24. </div><div class="csl-right-inline">Tai RH, Bentley LR, Xia X, Sitt JM, Fankhauser SC, Chicas-Mosier AM, Monteith BG (2024) An examination of the use of large language models to aid analysis of textual data. International Journal of Qualitative Methods 23:16094069241231168</div>
</div>
<div id="ref-xiao2023supporting" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">25. </div><div class="csl-right-inline">Xiao Z, Yuan X, Liao QV, Abdelghani R, Oudeyer P-Y (2023) Supporting qualitative analysis with large language models: Combining codebook with GPT-3 for deductive coding. In: Companion proceedings of the 28th international conference on intelligent user interfaces. pp 75–78</div>
</div>
<div id="ref-jang2023can" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">26. </div><div class="csl-right-inline">Jang J, Ye S, Seo M (2023) Can large language models truly understand prompts? A case study with negated prompts. In: Transfer learning for natural language processing workshop. PMLR, pp 52–62</div>
</div>
<div id="ref-ma2024my" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">27. </div><div class="csl-right-inline">Ma W, Yang C, Kästner C (2024) (Why) is my prompt getting worse? Rethinking regression testing for evolving LLM APIs. In: Proceedings of the IEEE/ACM 3rd international conference on AI engineering-software engineering for AI. pp 166–171</div>
</div>
<div id="ref-palmer2024using" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">28. </div><div class="csl-right-inline">Palmer A, Smith NA, Spirling A (2024) Using proprietary language models in academic research requires explicit justification. Nature Computational Science 4:2–3</div>
</div>
<div id="ref-chang2024survey" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">29. </div><div class="csl-right-inline">Chang Y, Wang X, Wang J, Wu Y, Yang L, Zhu K, Chen H, Yi X, Wang C, Wang Y, others (2024) A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology 15:1–45</div>
</div>
<div id="ref-ziems2024can" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">30. </div><div class="csl-right-inline">Ziems C, Held W, Shaikh O, Chen J, Zhang Z, Yang D (2024) Can large language models transform computational social science? Computational Linguistics 50:237–291</div>
</div>
<div id="ref-sokolova2009systematic" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">31. </div><div class="csl-right-inline">Sokolova M, Lapalme G (2009) A systematic analysis of performance measures for classification tasks. Information processing &amp; management 45:427–437</div>
</div>
<div id="ref-kolesnyk2022justification" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">32. </div><div class="csl-right-inline">Kolesnyk A, Khairova N (2022) Justification for the use of cohen’s kappa statistic in experimental studies of NLP and text mining. Cybernetics and Systems Analysis 58:280–288</div>
</div>
<div id="ref-delgado2019cohen" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">33. </div><div class="csl-right-inline">Delgado R, Tibau X-A (2019) Why cohen’s kappa should be avoided as performance measure in classification. PloS one 14:e0222916</div>
</div>
<div id="ref-chicco2021matthews" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">34. </div><div class="csl-right-inline">Chicco D, Warrens MJ, Jurman G (2021) The matthews correlation coefficient (MCC) is more informative than cohen’s kappa and brier score in binary classification assessment. Ieee Access 9:78368–78381</div>
</div>
<div id="ref-gwet2008computing" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">35. </div><div class="csl-right-inline">Gwet KL (2008) Computing inter-rater reliability and its variance in the presence of high agreement. British Journal of Mathematical and Statistical Psychology 61:29–48</div>
</div>
<div id="ref-blood2007disagreement" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">36. </div><div class="csl-right-inline">Blood E, Spratt KF (2007) Disagreement on agreement: Two alternative agreement coefficients. In: SAS global forum. Citeseer, pp 1–12</div>
</div>
<div id="ref-yan2024practical" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">37. </div><div class="csl-right-inline">Yan L, Sha L, Zhao L, Li Y, Martinez-Maldonado R, Chen G, Li X, Jin Y, Gašević D (2024) Practical and ethical challenges of large language models in education: A systematic scoping review. British Journal of Educational Technology 55:90–112</div>
</div>
<div id="ref-liu2024confronting" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">38. </div><div class="csl-right-inline">Liu Y, Gautam S, Ma J, Lakkaraju H (2024) Confronting LLMs with traditional ML: Rethinking the fairness of large language models in tabular classifications. In: Proceedings of the 2024 conference of the north american chapter of the association for computational linguistics: Human language technologies (volume 1: Long papers). pp 3603–3620</div>
</div>
<div id="ref-gallegos2024bias" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">39. </div><div class="csl-right-inline">Gallegos IO, Rossi RA, Barrow J, Tanjim MM, Kim S, Dernoncourt F, Yu T, Zhang R, Ahmed NK (2024) Bias and fairness in large language models: A survey. Computational Linguistics 1–79</div>
</div>
<div id="ref-karadzhov2023delidata" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">40. </div><div class="csl-right-inline">Karadzhov G, Stafford T, Vlachos A (2023) DeliData: A dataset for deliberation in multi-party problem solving. Proceedings of the ACM on Human-Computer Interaction 7:1–25</div>
</div>
<div id="ref-LABOOK2_Chapter_4" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">41. </div><div class="csl-right-inline">Saqr M, Misiejuk K, Tikka S, López-Pernas S (2025) Artificial intelligence: Using machine learning to classify students and predict low achievers. In: Saqr M, López-Pernas S (eds) Advanced learning analytics methods: AI, precision and complexity. Springer Nature Switzerland, Cham</div>
</div>
<div id="ref-dplyr" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">42. </div><div class="csl-right-inline">Wickham H, François R, Henry L, Müller K, Vaughan D (2023) <a href="https://CRAN.R-project.org/package=dplyr">Dplyr: A grammar of data manipulation</a></div>
</div>
<div id="ref-tidyr" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">43. </div><div class="csl-right-inline">Wickham H, Vaughan D, Girlich M (2024) <a href="https://CRAN.R-project.org/package=tidyr">Tidyr: Tidy messy data</a></div>
</div>
<div id="ref-tibble" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">44. </div><div class="csl-right-inline">Müller K, Wickham H (2023) <a href="https://CRAN.R-project.org/package=tibble">Tibble: Simple data frames</a></div>
</div>
<div id="ref-reticulate" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">45. </div><div class="csl-right-inline">Ushey K, Allaire J, Tang Y (2024) <a href="https://CRAN.R-project.org/package=reticulate">Reticulate: Interface to ’python’</a></div>
</div>
<div id="ref-caret" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">46. </div><div class="csl-right-inline">Kuhn, Max (2008) Building predictive models in r using the caret package. Journal of Statistical Software 28:1–26. https://doi.org/<a href="https://doi.org/10.18637/jss.v028.i05">10.18637/jss.v028.i05</a></div>
</div>
<div id="ref-text" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">47. </div><div class="csl-right-inline">Kjell O, Giorgi S, Schwartz HA (2023) The text-package: An r-package for analyzing and visualizing human language using natural language processing and deep learning. Psychological Methods. <a href="https://doi.org/10.1037/met0000542">https://doi.org/10.1037/met0000542</a></div>
</div>
<div id="ref-mltools" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">48. </div><div class="csl-right-inline">Gorman B (2018) <a href="https://CRAN.R-project.org/package=mltools">Mltools: Machine learning tools</a></div>
</div>
<div id="ref-Liu2019-bz" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">49. </div><div class="csl-right-inline">Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis M, Zettlemoyer L, Stoyanov V (2019) <a href="https://scholar.google.es/citations?user=H9buyroAAAAJ&amp;hl=en&amp;oi=sra"><span>RoBERTa</span>: A robustly optimized <span>BERT</span> pretraining approach</a>. arXiv [csCL]</div>
</div>
<div id="ref-Yang2019-rt" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">50. </div><div class="csl-right-inline">Yang Z, Dai Z, Yang Y, Carbonell J, Salakhutdinov R, Le QV (2019) <a href="http://arxiv.org/abs/1906.08237"><span>XLNet</span>: Generalized autoregressive pretraining for language understanding</a>. arXiv [csCL]</div>
</div>
<div id="ref-Saqr2023-nv" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">51. </div><div class="csl-right-inline">Saqr M, López-Pernas S (2023) The temporal dynamics of online problem-based learning: Why and when sequence matters. International Journal of Computer-Supported Collaborative Learning 18:11–37. https://doi.org/<a href="https://doi.org/10.1007/s11412-023-09385-1">10.1007/s11412-023-09385-1</a></div>
</div>
<div id="ref-Saqr2024-ug" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">52. </div><div class="csl-right-inline">Saqr M, López-Pernas S, Helske S, Durand M, Murphy K, Studer M, Ritschard G (2024) Sequence analysis in education: Principles, technique, and tutorial with <span>R</span>. 321–354. https://doi.org/<a href="https://doi.org/10.1007/978-3-031-54464-4_10">10.1007/978-3-031-54464-4_10</a></div>
</div>
<div id="ref-Gabadinho2011-lm" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">53. </div><div class="csl-right-inline">Gabadinho A, Ritschard G, Mueller NS, Studer M (2011) Analyzing and visualizing state sequences in <span>R</span> with <span>TraMineR</span>. J Stat Softw 40:1–37. https://doi.org/<a href="https://doi.org/10.18637/jss.v040.i04">10.18637/jss.v040.i04</a></div>
</div>
<div id="ref-Saqr2025-ku" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">54. </div><div class="csl-right-inline">Saqr M, López-Pernas S, Törmänen T, Kaliisa R, Misiejuk K, Tikka S (2025) Transition network analysis: A novel framework for modeling, visualizing, and identifying the temporal patterns of learners and learning processes. In: Proceedings of learning analytics &amp; knowledge (LAK ’25). ACM, New York, NY, USA, pp in–press</div>
</div>
<div id="ref-LABOOK2_Chapter_15" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">55. </div><div class="csl-right-inline">Saqr M, López-Pernas S, Tikka S (2025) Mapping relational dynamics with transition network analysis: A primer and tutorial. In: Saqr M, López-Pernas S (eds) Advanced learning analytics methods: AI, precision and complexity. Springer Nature Switzerland, Cham</div>
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../../chapters/ch09-nlp/ch09-nlp.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Natural Language Processing</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/ch11-llmsxai/ch11-llmsxai.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Automated feedback with XAI and LLMs</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center"><div class="cookie-consent-footer"><a href="#" id="open_preferences_center">Cookie Preferences</a></div></div>
  </div>
</footer>
<script>
  document.querySelector(".quarto-title").innerHTML =  '<div class="badge bs-warning bg-warning text-dark" style="float:right;">Pre-print</div>' +  document.querySelector(".quarto-title").innerHTML
  var keywords = document.querySelector('meta[name="keywords"]')
  if (keywords && keywords.content) {
    document.getElementById("title-block-header").innerHTML = document.getElementById("title-block-header").innerHTML + 
      '<div class="abstract"><div class="abstract-title">Keywords</div><div class="quarto-title-meta-contents"><p>'+
      keywords.content +
      '</p></div></div>'
  }
  function insertAfter(referenceNode, newNode) {
      referenceNode.parentNode.insertBefore(newNode, referenceNode.nextSibling);
  }
  var authors = document.querySelectorAll('meta[name="author"]')
  if (authors) {
    var authorlist = Array.from(authors).map(e=>e.content).reduce((accum, curr) =>  accum + curr + ", ", "","").replace(/\,\s$/,"")
    var citt = `<div class="card border-primary mb-3" style=;">
      <div class="card-header bg-primary">To cite this chapter</div>
      <div class="card-body small">
        <p class="card-text">${authorlist} (2025).
        <b>${document.getElementsByClassName("chapter-title")[0].innerText}</b>. 
        In M. Saqr & S. López-Pernas (Eds.), <i>Advanced Learning Analytics Methods: AI, Precision and Complexity</i> 
        (in – press). Springer. <a href="${window.location.href}">${window.location.href}</a></p>
      </div>
    </div>`;
    insertAfter(document.getElementsByTagName("HEADER")[1],new DOMParser().parseFromString(citt, 'text/html').body.childNodes[0])
  }
</script>



</body></html>