[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced learning analytics methods",
    "section": "",
    "text": "Learning Analytics Unit, University of Eastern Finland\n\n\n\n\nThis is the website for the book “Advanced learning analytics methods: AI, precision and complexity” which will be published by Springer in 2025.\n\n\nLearning analytics (LA) has grown over the years, both in methods and practices. As an applied field, LA embraced a wide range of research methodologies and traditions, including those from computer science, education, statistics, and psychology. While such growth has produced a vast body of research and brought significant progress, the past few years have raised several critical questions: What has LA achieved? What is the current status of the field? And, more importantly, what is LA as a field?\n\n\nThe meteoric rise of artificial intelligence (AI), and generative AI in particular, has sent shockwaves through many disciplines, and LA is no exception. As such the field has further expanded to encompass the new technology as it has always done. In doing so, LA has grown even larger and extended to the new areas which raised more questions about what LA is after all. In fact, the borders between LA, AI, educational data mining, and quantitative methods in general have always been unclear.\n\n\nNotwithstanding the LA identity debate, this book addresses several novel issues in LA and education, as well as the overlapping technologies and fields, from a methodological perspective. The book covers a range of topics related to advanced LA methods, including AI, precision education, and complex systems. While we do not claim to define the future of the field, our aim is to guide researchers in exploring some of these barely untrodden territories. We refrain from emphasizing or asserting that any topic lies within the definitive borders of LA. Instead, the book presents these advanced topics of significant interest to educational researchers, without placing undue focus on taxonomy or rigid classifications. Such borders have always been —at least to us— more artificial than real. In fact, almost every methodology we covered in the previous LA methods book came years before LA included them under its veneer. As such, the current book you are reading comprises four overarching methodological themes that reflect its title. While the said title says “advanced LA” we make no stress on LA in its narrow definition, and in fact, many chapters and discussions address the educational communities at large without being restrictive to LA.\n\n\nThe first theme of the book addresses AI and machine learning (ML) which both have become increasingly important nowadays. Including AI and ML topics fills a gap in our previous book and introduces several current topics that are of interest to all researchers and practitioners. We also made sure to include introductory chapters that present these topics in considerable detail to the readers so they can equip themselves with the basic knowledge required to read the dense methodological chapters. This enhancement over the previous book makes it easier for early researchers to have a swift introduction.\n\n\nThe book then introduces the “talk of the town”: Large Language Models (LLMs) and Natural Language Processing (NLP) topics. The second section introduces the foundational concepts of Large Language Models and their potential applications in education in an introductory chapter that discusses the main concepts. Then, several tutorials cover areas like using LLMs to explain predictive models, code text and offer insights to researchers. Relevant to this theme is the use of NLP to understand students’ text which is presented with several advanced techniques.\n\n\nThe editors\n\n\nMohammed Saqr and Sonsoles López-Pernas"
  },
  {
    "objectID": "contributors.html#editors",
    "href": "contributors.html#editors",
    "title": "Contributors",
    "section": "Editors",
    "text": "Editors\nMohammed Saqr, University of Eastern Finland\nSonsoles López-Pernas, University of Eastern Finland"
  },
  {
    "objectID": "contributors.html#associate-editors",
    "href": "contributors.html#associate-editors",
    "title": "Contributors",
    "section": "Associate Editors",
    "text": "Associate Editors\nSami Heikkinen, LAB University of Applied Sciences\nRwitajit Majumdar, Kumamoto University\nAhmed Tlili, Beijing Normal University"
  },
  {
    "objectID": "contributors.html#authors",
    "href": "contributors.html#authors",
    "title": "Contributors",
    "section": "Authors",
    "text": "Authors\nRémi Altamore, Université de Montpellier\nOkan Bulut, University of Alberta\nDaryn Dever, University of Florida\nChristophe Gernigon, University of Montpellier\nJiesi Guo, Australian Catholic University\nFeifei Han, Australian Catholic University\nSami Heikkinen, LAB University of Applied Sciences\nHibiki Ito, University of Helsinki\nAvi Kaplan, Temple University\nHalil Kayaduman, University of Eastern Finland\nTian Li, University of Queensland\nRwitajit Majumdar, Kumamoto University\nGwen Marchand, University of Nevada\nSonsoles López-Pernas, University of Eastern Finland\nKamila Misiejuk, FernUniversität in Hagen\nEduardo Oliveira, University of Melbourne\nClément Roume, Université de Haute-Alsace\nMohammed Saqr, University of Eastern Finland\nYige Song, University of Melbourne\nAnne Teboul, Université de Montpellier\nSanttu Tikka, University of Jyväskylä\nAhmed Tlili, Beijing Normal University\nLeonie V.D.E. Vogelsmeier, Tilburg University\nTarid Wongvorachan, University of Alberta\nJinran Wu, Australian Catholic University"
  },
  {
    "objectID": "contributors.html#reviewers",
    "href": "contributors.html#reviewers",
    "title": "Contributors",
    "section": "Reviewers",
    "text": "Reviewers\nRémi Altamore, University of Montpellier\nOkan Bulut, University of Alberta\nShaun Cavestany, University of Eastern Finland\nJavier Conde, Universidad Politécnica de Madrid\nTudor Cristea, Eindhoven University of Technology\nMouna Denden, Université Polytechnique Hauts-de-France\nMelis Dülger, Utrecht University\nRamy Elmoazen, University of Eastern Finland\nHamza Hammami, University of Tunis El Manar\nSami Heikkinen, LAB University of Applied Sciences\nHalil Kayaduman, Inonu University\nVille Kivimäki, Aalto University\nQinyi Liu, University of Bergen\nNidia Guadalupe López Flores, Reykjavik University\nSonsoles López-Pernas, University of Eastern Finland\nJoonas Merikko, Annie Advisor\nKamila Misiejuk, FernUniversität in Hagen\nRamkumar Rajendran, IIT Bombay\nDaevesh Singh, IIT Bombay\nJaani Väisänen, LAB University of Applied Sciences\nPavani Vemuri, KU Leuven\nEsteban Villalobos, Université Toulouse III\nMegan Wiedbusch, University of Central Florida\nTarid Wongvorachan, University of Alberta"
  },
  {
    "objectID": "contributors.html#editorial-support",
    "href": "contributors.html#editorial-support",
    "title": "Contributors",
    "section": "Editorial support",
    "text": "Editorial support\nCyprien Dujardin, Université de Technologie de Compiègne\nJules Delannoy, Université de Technologie de Compiègne\nIbrahim Belayachi, Université de Technologie de Compiègne"
  },
  {
    "objectID": "chapters/ch01-intro/ch01-intro.html",
    "href": "chapters/ch01-intro/ch01-intro.html",
    "title": "1  Unpacking learning in the age of AI: Bridging AI, Complexity, and Precision Education",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch01-intro/ch01-intro.html#introduction",
    "href": "chapters/ch01-intro/ch01-intro.html#introduction",
    "title": "1  Unpacking learning in the age of AI: Bridging AI, Complexity, and Precision Education",
    "section": "1 Introduction",
    "text": "1 Introduction\nThe birth of learning analytics as a field marked a vast surge of data-intensive studies and exponential growth of methods and applications. However, there was a lack of reliable methodological guidance for researchers and practitioners that remained unfulfilled for more than a decade. As a response, we released the first methodological book in 2024 [1]. The book Learning Analytics Methods and Tutorials: A Practical Guide Using R offered a much-needed resource for both newcomers and experts in the learning analytics field. The book started by introducing the R programming language to help beginners get up to speed with their R programming skills [2–4]. Most importantly, the book covered the foundational methods in learning analytics, such as sequence analysis, process mining, predictive modeling, Markov models, and social network analysis [5–8]. While those methods constitute the bedrock of learning analytics research and much of the advanced techniques, several aspects still require coverage. During the last few years —and especially the previous two— the field has witnessed remarkable changes, especially those related to artificial intelligence (AI) and the emergence of generative AI [9, 10]. Taken together, there is still a gap in methods and a need to catch up with the fast-advancing field and the recent surge in AI.\nTo fill this gap, we set out on a journey to create a new book: Advanced Learning Analytics Methods: AI, Precision and Complexity. This book aims to expand on foundational methods and integrate emerging innovations such as AI-driven insights, complexity science, and idiographic approaches, providing a comprehensive toolset to map the learning process. The book builds on the foundation laid by its predecessor, taking a significant step forward both methodologically —covering AI methods and their application in education— and theoretically — exploring idiographic analysis and complexity science. Given the novelty of several topics, the book included a mixture of foundational chapters that offered a comprehensive introduction to these novel topics in thematic sections, i.e., a chapter on complexity, two others on AI, and one on idiographic analysis."
  },
  {
    "objectID": "chapters/ch01-intro/ch01-intro.html#thematic-overview-of-the-book",
    "href": "chapters/ch01-intro/ch01-intro.html#thematic-overview-of-the-book",
    "title": "1  Unpacking learning in the age of AI: Bridging AI, Complexity, and Precision Education",
    "section": "2 Thematic Overview of the Book",
    "text": "2 Thematic Overview of the Book\nThe book is structured in four sections covering four main themes. This thematic categorization –albeit imperfect due to the unavoidable overlap– helps align the chapters within a common narrative to facilitate the book organization. Below, we describe each of these sections as well as the included chapters and give an overview of what these chapters discuss, how they are related together, and which parts of the methodological story they tell.\n\n2.1 Section I: Artificial Intelligence\nThe first section of the book covers the synergies between AI, learning analytics, and education.\nChapter 2 [11] kicks off this section with an overview of AI applications in education, including adaptive learning systems, profiling, and predictive modeling. Moreover, it introduces the main concepts and techniques related to explainable AI (XAI) and how AI can be designed to ensure trustworthiness and equity in AI-driven educational environments. The chapter also provides readers with an understanding of AI in education and XAI’s potentials and serves as a foundation for the remaining chapters presented in this section.\nAnother focus of this section is predictive modeling with AI, one of the central applications of AI in education [12, 13]. While our previous book covered this topic [14], it was approached from a statistical lens without introducing more recent developments in machine learning (ML). This book section provides a more modern approach to predictive modeling, focusing on methods and workflows to perform ML regression and classification tasks with R. The ability to predict student performance has been a main motivator for adopting learning analytics, and one of the central applications of AI in education. Predictive modeling uses historical data to forecast outcomes such as student grades, providing timely support to at-risk students. Chapter 3 [15] presents ML predictive methods to forecast continuous variables like grades. The methods discussed include Random Forest, K-Nearest Neighbor, Linear Regression, Neural Networks, and Support Vector Machines. The chapter presents two approaches: the traditional predictive modeling workflow and a more contemporary approach using the tidymodels framework. Chapter 4 [16] moves on to classify students based on academic performance levels, such as identifying low achievers or potential dropouts. The chapter begins by illustrating the process with a Random Forest classifier, relying on engagement indicators to distinguish between high and low achievers. It covers the entire modeling workflow, including data preparation, model training, and evaluation with performance metrics, and Goes beyond the classic ML prediction algorithms. Chapter 5 [17] introduces advanced regularization techniques—Least Absolute Shrinkage and Selection Operator (LASSO), Smoothly Clipped Absolute Deviation (SCAD), and Minimax Concave Penalty (MCP)—to construct predictive models. These methods are particularly useful when dealing with a large number of potential predictors, enabling the selection of the most relevant variables to create effective and interpretable models. As such, Chapter 5 is particularly useful in scenarios with complex and high-dimensional data.\nThe focus is then shifted to XAI as a solution to address some of the issues that arise from applying AI in education [18]. A key barrier to AI adoption is the “opaque” nature of many ML models, which can —and actually do— hinder trust, fairness, and accountability. To address this issue, explainability methods are essential for making predictions —such as identifying at-risk students, grading essays, or detecting plagiarism— that are more transparent and interpretable. Chapter 6 [19] introduces the DALEX R package as a tool for explaining ML models in educational settings and demonstrates how to interpret regression models (e.g., predicting student grades) and classification models (e.g., distinguishing high from low achievers) using variable importance measures, partial dependence plots, and accumulated local effects. Even though the aforementioned global explanation methods can offer aggregate insights into a model’s overall behavior, local explanations are needed to uncover the reasons behind specific outcomes, such as why a particular student was identified as at risk of dropping out. Chapter 7 [20] places the focus on the importance of local explanations in educational settings and explores three widely used techniques: Break Down plots, SHAP (SHapley Additive exPlanations), and LIME (Local Interpretable Model-agnostic Explanations).\nThese techniques are essential to understand why a certain decision has been made, e.g., why a student has been flagged as “at-risk”, why an essay has been marked as “excellent”, or why an assignment has been labeled as “plagiarized”.\n\n\n2.2 Section II: Large Language Models and Natural Language Processing\nMoving on from predictive modeling, the second section of the book focuses on the transformative role of another aspect of AI —natural language processing (NLP) and large language models (LLMs). NLP offers new opportunities to analyze and interpret textual data, such as student essays, discussion forums, and feedback, which provide deeper insights into learning processes and outcomes. The first chapter [21] introduces LLMs as the foundation of modern advancements in education. LLMs enable a wide range of applications, including automated feedback, question generation, sentiment analysis, and multilingual support, making them invaluable in diverse educational contexts. This chapter explains the foundational mechanisms behind LLMs, such as the transformer architecture. It also highlights the varied applications of LLMs in education, many of which will be explored in detail in later tutorial chapters. The next chapter [22] stresses the importance of utilizing textual data in education to enhance learning analytics systems and support informed pedagogical decisions. It introduces foundational concepts in NLP, including lexical and syntax analysis, corpus creation, and the significance of text preprocessing techniques like tokenization, stemming, lemmatization, and stop word removal, with practical demonstrations. The chapter offers a tutorial on the main NLP applications, including Term Frequency-Inverse Document Frequency (TF-IDF) for keyword identification, Latent Dirichlet Allocation (LDA) for topic modeling, and text summarization and demonstrates how these techniques reveal themes and insights in educational texts. The chapter concludes by showcasing sentiment analysis to evaluate student feedback, providing actionable insights into student perceptions and attitudes.\nChapter 10 [23] addresses the challenges of coding text data in qualitative research, a process often prone to errors and labor-intensive efforts. The chapter explores using LLMs to automate text classification, leveraging word embeddings and BERT-like models through the R package text. The chapter demonstrates an ML pipeline that combines word embeddings with supervised algorithms to achieve accurate text coding. A case study on collaborative problem-solving illustrates how a small sample of manually coded data can train a model to classify the remaining data. The tutorial also includes evaluating coding accuracy using standard machine learning metrics and measures of inter-rater reliability. Finally, the chapter applies learning analytics techniques to compare insights from human-coded and machine-coded data and shows the potential of LLMs to streamline and enhance qualitative research processes.\nTo conclude this section, Chapter 11 [24] investigates the integration of LLMs into the explainable AI pipeline. The chapter demonstrates how LLMs can automate the transformation of explanations—such as feature importance, partial dependence profiles, and local explanations— into natural language narratives that are accessible to non-technical stakeholders. This work advances the previous chapters on predictive ML, XAI, and learning analytics and emphasizes transparency, inclusivity, and fairness in educational technologies. In doing so, it offers a case study of using LLMs to push the boundaries of traditional predictive analytics and bring nuance to these models.\n\n\n2.3 Section III: Complex Dynamic Systems\nComplex dynamic systems as an analytical framework have lately gained traction in educational research by offering a set of methods that uncover the temporal and relational patterns within the learning process [18, 25–27]. This complex dynamic systems lens has been used to analyze complex learning-related constructs such as engagement [28, 29], self-regulation [26, 30, 31], and language learning [32]. The first chapter in this section [33] addresses the limitations of traditional methods in capturing the dynamic and evolving nature of learning processes by introducing complex systems theory for educational research. It outlines the key characteristics of complex systems, including non-linear relationships, emergent properties, and feedback mechanisms, to explain how educational phenomena develop over time. Methodological approaches such as network analysis and recurrence quantification analysis are described as tools to study relationships and patterns in learning. The chapter concludes by discussing data collection methods that align with a complex systems perspective, providing a foundation for studying learning processes through this innovative lens.\nChapter 13 [34] explores the role of network psychometrics as a complement to traditional factor analysis in education research. It goes one step further from the psychological network chapter in the previous book [35]. While factor analysis simplifies relationships by modeling latent variables like cognitive ability, engagement, and motivation, the network approach examines dynamic, unrestricted associations among observed variables. Building on concepts introduced in our previous book [35], this chapter presents advanced applications of psychological networks in learning analytics. The chapter introduces unique variable analysis to identify the main contributors within complex systems, reducing redundancy and simplifying datasets. Additionally, the chapter demonstrates methods for assessing system integrity through exploratory graph analysis (EGA), hierarchical EGA, and dynamic EGA, which enables tracking changes in system structures over time. Finally, the chapter highlights how psychological networks derived from EGA can be transformed into factor analytic formats, illustrating how these two approaches complement each other in studying the complex structures of learning processes.\nAnother method suitable for analyzing complex systems is Recurrence Quantification Analysis (RQA), a technique used to identify and quantify the repetition of patterns within nonlinear dynamical systems [36, 37] described in Chapter 14 [38]. RQA examines how system behaviors transition between repetitive and novel sequences over time, analyzing revisits to prior states or the evolution into new behaviors across various time lags. Applicable to both categorical and continuous data, RQA supports the analysis of single or multiple time series across diverse contexts. Although less commonly used in education, RQA has been applied to study collaborative learning, self-regulated learning, and literacy processes, using data sources such as eye tracking, log files, physiology, and verbalizations. This chapter reviews RQA methodologies, highlights its applications in educational research, and discusses how it can quantify learning dynamics, distinguish functional from dysfunctional learning behaviors, and enhance collaborative learning.\nChapter 15 [39] introduces Transition Network Analysis (TNA), a new method for capturing the relational dynamics of temporal processes by modeling transitions between events as a weighted directed network. TNA leverages the potentials of network analysis and process mining, offering insights through graph, node, and edge-level metrics [7, 40], and also provides tools for detecting recurring patterns such as dyads and triads, and for identifying communities [41]. More importantly, TNA incorporates statistical validation methods— e.g., bootstrapping, permutation, and case-dropping techniques—to ensure the robustness of research findings. TNA also enables the inclusion of covariates to explain emergent patterns or examine subgroup differences, enhancing its utility for hypothesis testing and theory development. The chapter includes a practical tutorial using the tna R package and demonstrates these features through a case study on group regulation dynamics [42]. Chapter 16 [43] builds on these foundations and presents Frequency-Based Transition Network Analysis (FTNA) as an alternative approach for modeling the relational dynamics and transitions between states or events. While TNA leverages probabilistic assumptions inherent in Markov models [5], FTNA focuses on summarizing, describing, and visually analyzing observed data without such constraints. The section concluded with Chapter 17 [44], which explores clustering within TNA to uncover heterogeneity in learners’ behavioral patterns. The focus is on using Mixture Markov Models (MMM) [5] to identify latent subgroups with distinct transition probabilities, incorporating covariates to explain the clusters. Through the tna R package [42], the chapter demonstrates how to compare and contrast unique transition dynamics in each cluster and examine centrality measures, communities, and cliques. Additionally, it provides examples of alternative clustering methods, such as distance-based grouping [8], and extends the discussion to other types of transition networks, including frequency-based approaches.\n\n\n2.4 Section IV: Idiographic Analysis\nThe last section of the book zooms in on the individual learner through idiographic and individualized approaches that emphasize within-person dynamics [29, 45]. The section explores methods and frameworks designed to capture the unique, dynamic processes of individual learners and provides deeper insights into their cognitive, emotional, and behavioral patterns. The section closes the book by exploring methodologies that prioritize the variability inherent in human learning processes [46], offering practical tools for researchers and educators seeking to tailor interventions to each individual.\nAn introductory chapter [47] presents the two primary research paradigms in educational research: between-person (nomothetic) and within-person (idiographic) approaches. The between-person approach examines variations across individuals, while the within-person approach focuses on variations within the same individual over time. These paradigms align only if ergodicity holds—when individuals are similar and remain unchanged over time—conditions that are rarely met in educational contexts. The chapter emphasizes the need for within-person analysis and zooms in on idiographic analysis, which focuses on a single individual (N=1). The chapter also outlines the key characteristics, data sources, and methods associated with both approaches and sets the stage for personalized analyses in educational research.\nBased on these principles, Chapter 19 [48] examines three analytical approaches—variable-centered, person-centered, and person-specific—that differ in their balance between generalizability and individualization [46]. The variable-centered approach seeks generalizable insights by analyzing relationships across a population, often at the expense of capturing individual differences. The person-centered approach identifies distinct subgroups within the population, highlighting shared patterns among similar individuals. The person-specific, or idiographic, approach focuses entirely on the individual, providing detailed insights into intra-individual variability and processes over time. Through a practical tutorial in R, the chapter demonstrates these methods using dynamic networks and Experience Sampling Method (ESM) data on students’ task value and happiness.\nChapter 20 [49] proposes network analysis methods, specifically Graphical Vector Autoregression (graphicalVAR) and Unified Structural Equation Modeling (uSEM), to model learning processes as complex, dynamic systems [30]. These methods allow researchers to examine both temporal and contemporaneous relationships among variables within individual learners over time. The chapter begins by conceptualizing learning as a networked system, reviewing relevant literature, and discussing the advantages of probabilistic network models in educational contexts. A step-by-step tutorial in R is included, guiding readers through the estimation of idiographic models, visualization of dynamic relationships, and interpretation of results. The tools presented to analyze multivariate time-series data lay the foundation for personalized interventions in educational settings.\nChapter 21 [50] presents Whittle’s approximation of the Maximum Likelihood Estimator (MLE) and highlights its ability to detect long-term memory processes within relatively short time series. The chapter explains the foundational principles of Whittle’s MLE and demonstrates its application in R to analyze motivational dynamics, specifically the interplay between approach and avoidance behaviors in academic settings. Finally, the chapter explores the theoretical and practical implications of identifying long-memory processes in education, shedding light on how these insights can inform research and practice in understanding sustained learning and behavior patterns.\nThe last chapter of this section [51] presents a tutorial on using AutoML to streamline and scale predictive modeling in education, with a focus on idiographic analysis. AutoML simplifies the ML pipeline, enabling the creation of individually optimized models tailored to each student’s unique data. The tutorial demonstrates how to use AutoML to fit and manage multiple datasets, automating the process of developing personalized models. Additionally, it showcases explainable AI techniques to interpret key predictors, providing transparent insights into the variables that drive individual outcomes [52]. This complete pipeline highlights the potential of AutoML to deliver automated, real-time, and trustworthy insights for idiographic educational research and practice. As such, this chapter combines ML, explainable AI, idiographic analysis and complex systems."
  },
  {
    "objectID": "chapters/ch01-intro/ch01-intro.html#a-call-to-innovate",
    "href": "chapters/ch01-intro/ch01-intro.html#a-call-to-innovate",
    "title": "1  Unpacking learning in the age of AI: Bridging AI, Complexity, and Precision Education",
    "section": "3 A Call to Innovate",
    "text": "3 A Call to Innovate\nThe convergence of AI, complex systems theory, and idiographic analysis opens new paths for expanding the boundaries of education research in general and learning analytics in particular. This section discusses some of these potentials, aligning with Clow’s learning analytics lifecycle [53] (Figure 1.1).\n\n\n\nFigure 1. Learning analytics lifecycle by Clow [53]\n\n\nFirst, regarding learners, the chapters of this book have discussed several important synergy points where innovation is ready to take place. The AI technologies discussed in Chapter 2 [11], and especially those powered by LLMs (discussed in Chapter 8 [21]), offer unprecedented opportunities for personalized learning, tailoring learning experiences to each specific learner. As a result, adopting an idiographic lens [47] to treat each individual learner as a single unit of analysis is needed to better understand individual learning trajectories enabled by such personalization. This lens allows to identify patterns in learners’ behaviors, preferences, and progress, which are often concealed by aggregate analyses [48]. Moreover, learning processes usually involve multiple interacting constructs, such as motivation, engagement, and prior knowledge, which influence each other and evolve over time. Viewing these interactions through the lens of complexity allows us to recognize emergent patterns that are not apparent when constructs are studied in isolation [33]. Combining both approaches (idiographic and complexity) in light of recent innovations in AI in education is a first step towards studying personalized learning [49].\nWhen it comes to data, the second stage of the learning analytics lifecycle, the focus shifts to the collection, integration, and preparation of diverse data sources to support meaningful analyses. A longstanding problem in learning analytics is balancing the tradeoff between gathering meaningful, high-quality data and automating its analysis to enable timely interventions. While multimodal data—such as behavioral logs, textual inputs, and even physiological signals—can provide a deeper understanding of learning processes, the complexity of such data often requires sophisticated preprocessing and modeling techniques and/or manual coding or inspection, which can delay actionable insights.\nTo address this challenge, recent advancements in AI, particularly LLMs, have demonstrated significant potential in automating the interpretation of unstructured data [21]. These models can extract meaningful patterns from text and synthesize multimodal information. Some chapters of the book deal with the problem of automatically synthesizing or coding textual information [22, 23]. However, it should be reiterated that automated analysis must be accompanied by careful consideration of data quality, representativeness, and fairness. Poorly integrated, unbalanced, or incomplete data can lead to biased models and misguided interventions, which risks compromising the learner-centered approach central to fair and effective learning analytics. In this regard, several chapters of the book cover XAI [11, 19, 20, 24], a central tool to audit AI decisions and detect potential biases in ML models.\nThe metrics phase of the learning analytics lifecycle deals with the selection, development, and application of methods and metrics that are capable of capturing, analyzing, and interpreting the complexity of educational data. As a methodological tutorial-style book, we make our most impactful contributions in this phase. While our previous book focused on the foundational methods of learning analytics, this new volume contributes state-of-the-art techniques and tools for advancing the metrics phase of the learning analytics lifecycle. The most important contribution is a novel method —TNA—, presented for the first time at LAK 2025 [40], which combines Markov models and network analysis, offering a strong statistical framework to model temporal processes [39, 44].\nIn addition to TNA, this book sheds light on other new methods that have yet to gain widespread adoption in the learning analytics community. EGA aims to identify latent constructs and their relationships within psychological and educational data [34]. EGA’s can detect hidden structures in learner interactions and behaviors, particularly when coupled with idiographic analysis. Similarly, RQA [38] enables the study of the recurrence, complexity, and predictability of learning processes over time. RQA offers insights into how learning trajectories evolve by examining patterns of stability and change within temporal sequences, making it an invaluable method for both idiographic and complex systems research. Its emphasis on temporal recurrences aligns with the growing demand for methods strategies that can capture the emergent and non-linear nature of learning.\nLastly, the intervention phase crowns the learning analytics lifecycle, where the insights gained from data and metrics are applied to create actionable strategies that support learners and educators. This phase tests the practical value of the analytics process to ensure that the methods and models lead to meaningful interventions. For instance, learning analytics enhanced learning platforms like LEAF [54] considered learning logs to automatically conduct statistical modeling to extract the effectiveness of teaching-learning cases [55] and extract students’ learning patterns [56] to aim for precision education. One particularly promising direction is the development of hybrid human-AI systems that combine the computational power and pattern recognition capabilities of AI with the contextual understanding, ethical judgment, and adaptability of human decision-makers. These systems could dynamically adapt to individual learners’ needs while maintaining human oversight and interpreting complex learning processes. For instance, AI could analyze real-time learning patterns using the advanced methods described in this book —from transition networks to recurrence quantification— while educators use their expertise to contextualize these insights —perhaps with the help of XAI techniques— and make interventions [13]. This hybrid approach could help bridge the gap between sophisticated analytics and practical pedagogical applications, especially in areas where purely automated systems might miss important contextual or emotional factors.\nFurthermore, as learning environments become increasingly digitized and interconnected, we need innovative methods that can handle multimodal data streams while respecting individual learning trajectories. This calls for developing integrated frameworks that can simultaneously analyze different types of data - from clickstream logs to natural language interactions to physiological signals - while maintaining both the precision of idiographic analysis and the scalability of AI-driven approaches, such as AutoML and learning platforms such as LA-ReflecT [57]. Such frameworks could help us better understand the complex interplay between cognitive, emotional, and social aspects of learning, leading to more holistic and effective educational interventions."
  },
  {
    "objectID": "chapters/ch01-intro/ch01-intro.html#how-to-use-this-book",
    "href": "chapters/ch01-intro/ch01-intro.html#how-to-use-this-book",
    "title": "1  Unpacking learning in the age of AI: Bridging AI, Complexity, and Precision Education",
    "section": "4 How to use this book",
    "text": "4 How to use this book\nTo support your learning and provide hands-on experience with the concepts discussed in this book, we have created a companion code repository for each chapter. This repository includes all the code from the step-by-step tutorials, demonstrating the implementation of various learning analytics and AI methods covered in the chapters. Additionally, the code provides guidance on generating visualizations, graphs, and plots to help interpret and communicate findings effectively. You can access the companion code repository at:\n https://github.com/lamethods/data2\nMoreover, the data used to demonstrate the methods in most of the chapters have been made available in the following repository:\n https://github.com/lamethods/code2"
  },
  {
    "objectID": "chapters/ch02-AIxAI/ch02-aixai.html",
    "href": "chapters/ch02-AIxAI/ch02-aixai.html",
    "title": "2  AI, Explainable AI and Evaluative AI: Informed Data-Driven Decision-Making in Education",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch02-AIxAI/ch02-aixai.html#introduction",
    "href": "chapters/ch02-AIxAI/ch02-aixai.html#introduction",
    "title": "2  AI, Explainable AI and Evaluative AI: Informed Data-Driven Decision-Making in Education",
    "section": "1 Introduction",
    "text": "1 Introduction\nAI has a long and rich history in education. In fact, artificial intelligence in education predates several other technologies and disciplines e.g., online learning. Among the first examples of artificial intelligence applications in education is the creation of the PLATO system at the University of Illinois around the 1960s which allowed for interactive learning and automated feedback [1]. While PLATO was limited in scope, it laid the foundation for future developments to follow and encouraged other researchers to explore the nascent technology in education. Most importantly, interest started to grow in developing intelligent tutoring systems (ITS) and AI powered educational solutions [1].\nHowever, it was not until the early 1980s when the field of artificial intelligence in education saw its major milestone with the launch of the first International Conference on Artificial Intelligence in Education (AIED) which was held in Montreal [2]. AIED conference brought together researchers from both AI and educational psychology, marking the formal recognition of AI as a significant discipline. Furthermore, AIED set the stage for collaboration between the fields of AI and education, which led to several significant developments in intelligent tutoring systems, adaptive learning technologies, and educational software [3].\nThe last two decades have witnessed several remarkable and fast paced developments in the field of AIED fueled by the rapid adoption of the internet, the increase in computation power and advances in methodology [4]. Such advances led to the birth of learning analytics as a field —along with educational data mining— which all contributed to wide use of machine learning and data analytics, alongside other uses of AIED that extends to almost all fields of research and practice [5].\nAs AI continues to play an increasingly central role in education, concerns around transparency, and fairness have become more prominent [6]. These issues stem from the inherently complex and often opaque nature of AI systems, particularly those powered by machine learning and deep learning [7]. For instance, while AI-driven systems can offer personalized recommendations and adaptive learning experiences, their underlying decision-making processes are not always clear to educators or learners. This lack of transparency can prevent users from trusting AI systems and hinder their adoption in educational contexts [8]. Worse even, they might follow AI decisions without knowing if they are wrong or right [9].\nOne of the reasons for the mistrust in AI is that its performance is commonly affected by bias, as AI models are only as unbiased as the data they are trained on [6]. Historical inequities or imbalances in training datasets can perpetuate and even amplify existing disparities, leading to outcomes that disproportionately disadvantage certain groups of students [10]. For example, adaptive systems might inadvertently favor learners whose behavior aligns with the dominant patterns in the training data, leaving those with less conventional learning trajectories underserved. Such risks highlight the importance of rigorous evaluation, diverse datasets, and ethical oversight in the design and deployment of AI systems in education.\nAddressing these challenges requires the integration of explainable AI (XAI) techniques, which aim to make AI outputs interpretable and accountable to users. XAI tools can provide insights into how AI systems reach their conclusions, enabling educators to critically evaluate and trust these technologies [11]. Moreover, the adoption of fairness-aware machine learning practices and inclusive design principles can help mitigate biases, ensuring that AI-driven solutions support equitable outcomes for all learners. These considerations are particularly relevant when it comes to personalized education, where the promise of tailored interventions must be balanced with a commitment to fairness and transparency.\nThis chapter explores the current state of artificial intelligence in education, as well as explainable AI (XAI). AI and XAI techniques and tools are illustrated in the subsequent tutorials in the book and this chapter provides an introduction for the reader to have an overarching view on their main characteristics and potential. We further introduce the concept of evaluative AI [12], which aims to address some of the limitations of XAI."
  },
  {
    "objectID": "chapters/ch02-AIxAI/ch02-aixai.html#an-overview-of-ai-applications-in-education",
    "href": "chapters/ch02-AIxAI/ch02-aixai.html#an-overview-of-ai-applications-in-education",
    "title": "2  AI, Explainable AI and Evaluative AI: Informed Data-Driven Decision-Making in Education",
    "section": "2 An overview of AI applications in education",
    "text": "2 An overview of AI applications in education\nAI is an extremely broad field that permeates many disciplines, encompassing a wide range of techniques and applications that aim to replicate or enhance human intelligence. In spite of —or maybe because of— its widespread influence across fields such as healthcare, finance and, of course, education, there is no universally agreed-upon definition of AI [13], nor a standardized taxonomy to categorize its many methods and applications. In the context of education, this lack of clarity is furthered by the diverse range of technologies that fall —sometimes wrongly— under the label of AI —from rule-based systems to complex neural networks— each serving diverse purposes such as personalization, prediction, and automation. In this section, we aim to provide an overview of the key AI applications that have influenced education, presenting relevant use cases in the field. We follow the structure of the recent meta-review by Bond et al. [14], which is based on the original typology by [15], and add an additional section on AI in education research.\n\n2.1 Adaptive systems and personalization\nOne of the main promises of AI in education is personalization. Personalized learning focuses on tailoring educational experiences to individual learners’ needs, preferences, and readiness, and it can be implemented through various methods and technologies [16]. For example, adaptive systems implement personalized learning by constructing a model of each learner using artificial intelligence and machine learning as well as continuous data collection of students’ activities and performance [16]. According to Zhong [17], personalized learning has been mainly implemented through learning content structuring and sequencing (organizing and ordering learning materials according to individual needs) as well as readiness support (adapting to students’ performance).\nAnother form of personalization —closely related to adaptive systems— is the use of recommender systems. Recommender systems are algorithms designed to suggest learning resources, activities, or courses to students based on their individual profiles, past performance, and expressed preferences. According to the review by Urdaneta-Ponte et al. [18], most recommender systems in the education literature use collaborative filtering, which entails recommending resources based on similarity to other students.\nIn recent years, personalization has expanded beyond adaptivity and recommendation through the provision of truly personalized learning experiences, made possible thanks to the latest AI breakthroughs. Chatbots and virtual assistants currently represent one of the most prominent applications of personalization of education using AI. These tools take advantage of the advances in natural language processing —and most recently Large language Models (LLMs)— to interact with students in a conversational manner. A recent review by Labadze et al. [19] discovered that students benefit most from AI-powered chatbots in three main areas: support with homework and studying, personalized learning experiences, and skill development [20].\nOther AI-powered technologies like facial recognition or mood detection are emerging as relevant tools for tailoring educational experiences [21]. These technologies can complement traditional adaptive systems through the incorporation of real-time emotional and behavioral information on students’ facial expressions, tone of voice, and body language. Despite their potential, these technologies raise concerns regarding privacy, consent, and data security [22]. Furthermore, the accuracy of emotion recognition algorithms and the potential for cultural or individual biases remain critical challenges to address.\n\n\n2.2 Intelligent tutoring systems ITS)\nClosely related to adaptive systems are Intelligent Tutoring Systems (ITSs). ITSs use AI techniques to provide personalized tutoring based on a model of the learner’s knowledge, behavior, and learning progress [23]. ITSs aim to emulate one-on-one human tutoring through the adaptation of instructional strategies, delivering tailored feedback, and offering customized learning experiences to help students achieve specific educational goals. ITSs often include features such as error diagnosis, hint generation, and real-time adaptation to individual learner needs, typically in well-defined domains. For example, virtual patients are a form of ITS used in healthcare to practice clinical reasoning [24]. ITSs are also extensively used in programming education as a step-by-step tutor to master a programming language [25].\n\n\n2.3 Profiling and prediction\nPredictive modeling has been a primary focus of AI in education. Early attempts dealt with the use of demographic and administrative data from learners to predict performance and dropouts [26]. With the birth of the educational data mining and learning analytics fields, utilizing intensive log data about learners’ online activities became the norm in the field, which came accompanied to increased predictive accuracy thanks to advances in ML algorithms [27].\nThe goals of performing predictive modeling have been diverse. A central goal has been the early identification of students who may be at risk of failing or dropping out [28]. Based on data from previous iterations of a course, researchers attempt to predict students’ performance early on in the current course to be able to conduct remedial interventions. Other less frequent goals of predictive modeling are related to career placement [29], admission [15], or student satisfaction [27].\nResearch suggests that predictive modeling is hardly generalizable and lacks portability between courses [30], or even between course iterations [31]. Thus, recent research has shifted the focus from predicting performance to understanding learners based on their behavioral data. Clustering algorithms have been operationalized to detect distinct patterns or profiles of engagement [32], collaboration roles [33], self-regulated learning [34], or learning strategies [35].\nLastly, recent research strands that are gathering increased attention focus on identifying and mitigating biases in AI algorithms [36], ensuring equitable treatment of all learners, and enhancing the interpretability of AI systems through eXplainable Artificial Intelligence (XAI) [11].\n\n\n2.4 Assessment and evaluation\nStudents often long for formative feedback that offers actionable points to improve their learning and performance. This has traditionally come at the expense of overburdening teachers with a higher workload. AI offers a lot of promise to alleviate this challenge by providing automated, personalized feedback to students [37]. Although automated assessment has been present for decades, e.g., through multiple-choice questionnaires that can be automatically graded or autograders for programming assignments, advancements in artificial intelligence, particularly in NLP, have led to the development of systems capable of analyzing student work and generating constructive feedback.\nAI assessment and feedback have been especially relevant in the evaluation of artifacts that do not have a deterministic correct solution. In fact, the most widely used application of AI-powered feedback in education is automated essay scoring [38]. Another important focus has been the evaluation of programming code. Although non-AI automated assessment tools have existed for decades [39], the recent advances in AI have allowed the provision of more relevant and detailed feedback to students [40]. The main advantage of AI-powered assessment systems is that they can provide immediate feedback, allowing students to understand their mistakes and learn from them in real-time. In addition, such systems enable educators to support a larger number of students without a proportional increase in workload, making personalized education more accessible.\nBeyond evaluating students’ assignments and artifacts, AI has also opened new avenues for evaluating students’ learning process. For instance, automated discourse coding allows educators and researchers to analyze conversations or discussion forums in order to identify patterns of collaboration, critical thinking, and engagement [41].\n\n\n2.5 AI in Education Research and Learning Analytics\nLastly, AI has not only brought opportunities to education practice but also to education research and learning analytics. Advances in AI, such as deep learning, natural language processing (NLP), and more recently, LLMs, have significantly expanded the possibilities for analyzing unstructured data such as text, speech, and video, and facilitating new approaches to personalization and feedback in education [42, 43].\nMoreover, the increasing role of AI as an educational tool has further expanded the possibilities of education and learning analytics researchers for understanding learners’ behaviors in the presence of these new learning technologies [44]. For example, researchers can examine the types of prompts students pose to AI systems, providing fine-grained information about common misconceptions, knowledge gaps, or areas where additional support may be needed. These analyses can also showcase how students respond to feedback or recommendations generated by AI, and hence whether AI’s guidance promotes better learning outcomes or contributes to over-reliance on AI-generated solutions [45]."
  },
  {
    "objectID": "chapters/ch02-AIxAI/ch02-aixai.html#explainable-ai",
    "href": "chapters/ch02-AIxAI/ch02-aixai.html#explainable-ai",
    "title": "2  AI, Explainable AI and Evaluative AI: Informed Data-Driven Decision-Making in Education",
    "section": "3 Explainable AI",
    "text": "3 Explainable AI\nHeadlines around the globe are increasingly spotlighting instances of AI bias, from skewed hiring algorithms [46] to discriminatory facial recognition systems [47], exposing the urgent need to address the unintended consequences of AI in our daily lives. These incidents highlight not only the limitations of AI but also the opaque nature of many of these systems, which makes identifying and mitigating bias a significant challenge. Explainable AI (XAI) emerges as a promising solution to shed light on the ‘black box’ of AI decision-making. In a nutshell, XAI can be defined as a set of methods and tools that enables humans to understand the outcomes of AI models [48]. Making the processes behind AI decisions transparent and interpretable through XAI allows all involved stakeholders—developers, users, and policymakers —to detect bias and assess fairness, and hence build trust in AI systems [49]. XAI aims to answer questions such as: why did the AI make this particular decision?; how would changes in the input data influence the decision?, and in what changes are needed in AI’s behavior to align with ethical and societal values? This is achieved through techniques such as feature importance rankings, model-agnostic interpretability methods, and visualization tools that reveal the relationships between inputs and outputs [50].\nAs we have seen in the previous section, the adoption of AI tools and systems in the field of education has opened new opportunities for personalized learning, automated assessments, and early intervention for at-risk students [4]. However, as these systems increasingly influence critical decisions—such as grading, admissions, and resource allocation—they also bring the risk of embedding and perpetuating biases present in training data or algorithms. As such, XAI is of particular relevance in education, where fairness, transparency, and trust should be the basis of an equitable learning environment [11]. For instance, XAI techniques can reveal which factors are most influential in predicting a student’s performance [51], enabling interventions that are data-informed yet account for individual differences. Similarly, transparency in automated grading systems ensures that assessments are objective [52], and therefore they can be trusted by students and educators. Moreover, XAI can provide students with actionable feedback that is both personalized and understandable [53], helping them detect areas for improvement.\nIn the remainder of this section, we provide an overview of the main concepts, techniques, and tools associated with XAI. We describe the main AI methodologies, including model-agnostic interpretability, feature importance analysis, and counterfactual reasoning, with an emphasis on how they enable transparency and accountability in AI systems.\n\n3.1 The trade-off between explainability and performance\nWe start our journey into XAI by talking about explainability. We must differentiate between two broad categories of AI models: intrinsically explainable algorithms and black-box models. This distinction is critical for contextualizing the need for explainability tools and techniques discussed in later sections.\n\n3.1.1 Intrinsically Explainable Algorithms\nIntrinsically explainable (or white-box) algorithms are designed to be interpretable by their very nature [49]. These algorithms have transparent structures and straightforward mechanisms for decision-making, making it easy to understand how input features contribute to output predictions. Examples include linear regression, logistic regression, decision trees, and rule-based systems [54]. Stakeholders can directly observe relationships, such as the weight of a variable in a regression model (Figure 2.1 (a)) or the sequence of decisions in a tree structure (Figure 2.1 (b)).\n\n\n\n\n\n\n\n(a) Explanation of a Linear Regression model.\n\n\n\n\n\n\n\n(b) Explanation of a Decision Tree model\n\n\n\n\nFigure 1. Example of using intrinsically explainable algorithms to predict students’ grades.\n\n\nIn education, intrinsically explainable algorithms are particularly valuable when transparency is a priority. For instance, a linear or logistic regression model predicting a student’s grade based on attendance and participation provides a clear and intuitive explanation for its outcomes. Educators and other stakeholders can readily interpret the model and take actionable steps, such as encouraging students to engage in self-assessment activities.\nHowever, intrinsically explainable models often come with limitations. Their simplicity can result in lower predictive accuracy when dealing with complex, non-linear relationships or high-dimensional data [54], which are common in educational contexts and a common problem in learning analytics research and practice. As a result, their applicability is sometimes constrained when achieving high accuracy is critical.\n\n\n3.1.2 Black-Box Models\nBlack-box models, such as deep neural networks, gradient boosting machines, and ensemble methods [54], are characterized by their complex internal structures, which are not inherently interpretable. These models are often chosen for their ability to capture highly complex patterns and relationships in data, hence yielding higher predictive performance in many applications [49]. However, their complexity creates a significant barrier to understanding how decisions are made.\nIn education, black-box models might be needed when dealing with more complex unstructured data such as students’ free-form text or images [26]. Although these models may achieve higher accuracy than intrinsically explainable algorithms, their opacity raises concerns. Without insights into how predictions are generated, educators and administrators may struggle to trust or act on these outputs, especially when high-stakes decisions that could be consequential for students are involved.\n\n\n3.1.3 Balancing Explainability and Predictive Power\nThe choice between intrinsically explainable algorithms and black-box models often involves a trade-off between interpretability and performance [54]. Simpler models provide clarity and ease of use, at the expense of failing to capture the complexity of educational data. On the other hand, black-box models excel in handling such complexity but require additional methods to make their decisions interpretable [50]. XAI aims to bridge this gap by providing explainability techniques for black-box models. These tools enable stakeholders to take advantage of the high performance of black box models while maintaining transparency and accountability such as in the case of white-box models. For example, black-box models predicting student dropout can be paired with interpretability methods to reveal the main factors driving each prediction, hence allowing educators to make informed, data-driven decisions.\n\n\n\n3.2 Global vs. Local Explainability: XAI tools\nAs we have seen in the previous section, we need XAI tools to be able to explain the outcomes of black-box AI models. We can approach explainability from two complementary perspectives: global and local. Both play a crucial role in understanding and effectively applying AI systems, since both macro-level patterns and individual-level insights are essential for achieving trust and fairness. In this section, we describe each of these perspectives and the XAI tools used in each of them.\n\n3.2.1 Global Explainability\nGlobal explainability focuses on understanding the overall behavior of a model. It provides insights into how the model makes decisions across the entire dataset, pointing to the features that are most influential and the patterns that drive predictions. For instance, in a model designed to predict student performance (such as the ones shown in Figure 1) global explainability might reveal that the average quiz score, attendance, and assignment submission are consistently the most important factors that predict performance. This “big picture” explanation enables educators and administrators to evaluate the model’s logic and ensure it aligns with their expectations. Moreover, global explainability is instrumental in assessing whether the model behaves fairly across different demographic groups, helping to identify and address potential biases. However, its high-level nature means that it cannot explain why specific predictions are made, which limits its usefulness in scenarios requiring personalized action. It is, though, useful to take general actions that would benefit “the majority”, for example changing the course design or learning materials. Chapter 6 [55] in this book offers a tutorial on global explainability using the R programming language. Below, we overview the main techniques for global explainability\n\n3.2.1.1 Variable/Feature Importance\nFeature importance measures how much each input variable contributes to a model’s predictions, helping to identify the most influential factors driving its decisions [56]. In educational applications, feature importance provides actionable insights into what matters most for student outcomes. For example, a model predicting grades (see Figure 4.4) might show that the frequency of forum contributions is the most important factor. This insight helps educators prioritize interventions where they are most needed, such as encouraging participation .\n\n\n\nFigure 2. Variable importance plot when predicting students’ grades\n\n\nIn models like decision trees or random forests, feature importance is calculated based on how often a variable is used in splits and how much it reduces prediction error (e.g., impurity). In linear regression, the magnitude of coefficients represents the relative weight of each variable, directly showing how much they influence the target outcome. These metrics are intrinsic to these models, making them straightforward to compute and interpret.\nFor black-box models, such as neural networks, gradient boosting, or ensemble methods, feature importance is not inherently available and must be derived through additional techniques. Unlike intrinsically interpretable models, black-box models process data in ways that obscure the direct contribution of individual features. To estimate feature importance, methods like permutation importance are often used [56]. This technique evaluates how model performance changes when the values of a feature are randomly shuffled, effectively breaking its relationship with the target variable. Features that cause a significant drop in performance when permuted are deemed more important. The derived feature importance remains an approximation, as black-box models can capture nonlinear interactions that are difficult to fully interpret without additional tools like SHAP or LIME. Thus, while feature importance provides valuable insights into black-box models, it often requires advanced methods to ensure reliability and transparency.\nIt is important to bear in mind that, while feature importance highlights what influences predictions, it does not fully explain how features influence prediction and how features interact with one another, underlining its value as a foundational but limited interpretability tool.\n\n\n3.2.1.2 Partial Dependence Plots\nPartial Dependence Plots (PDPs) depict how changes in a feature influence predictions while accounting for the average behavior of all other features. They isolate the effect of one feature at a time, making it easier to understand the relationship between input variables and model outcomes. For instance, Figure Figure 2.3 shows how increasing the number of reads of the forum —while the rest of the features remain the same— has barely any effect in the prediction of students’ grades, while increasing the number of contributions results in a higher grade prediction. This insight allows educators to focus interventions in the variables that are most effective.\n\n\n\nFigure 3. PDP of frequency of forum consume and contribute\n\n\nAlthough PDPs are useful for understanding non-linear relationships and interactions, they rely on the assumption that features are independent. In reality features like reading and writing forum contributions may be correlated, so this assumption can lead to misleading interpretations. Additionally, PDPs represent average effects, which may not fully capture individual variations in how a feature influences predictions. Despite these limitations, PDPs remain a valuable tool for interpreting complex models, especially in contexts like education where understanding feature contributions can inform better decisions.\n\n\n3.2.1.3 SHAP Summary Plots\nA SHAP summary plot combines two critical aspects of global feature importance: the magnitude of a feature’s impact (ordered from top to bottom in the figure) on the predictions and the range of each feature’s effects (the horizontal axis). For each feature, the plot shows the SHAP values across all instances in the dataset, where each SHAP value represents the contribution of the feature to a single prediction (i.e. individual dots) [57]. In the figure, top features such as the frequency of forum contributions have greater importance, its wide horizontal dispersion shows that the feature’s contribution varies greatly across instances. Additionally, the color of each point typically reflects the feature value (e.g., high values in red and low values in blue), which helps identify whether higher or lower feature values drive predictions.\nIn educational contexts, SHAP summary plots can provide insights not only into which factors are most critical in predicting outcomes like grades, engagement, or dropout risk, but also about the direction towards which each feature pushes the prediction. For instance, a summary plot (Figure 2.4) might reveal that frequency of forum contributions has the highest average SHAP values, indicating it is the most influential feature. The plot might also show that a higher number of active days consistently contributes to lower predictions, while regularity in viewing lectures has a more variable effect depending on other factors. This level of detail helps educators and administrators not only understand which features matter most but also how their impact differs across students.\n\n\n\nFigure 4. SHAP summary plot\n\n\nIt is worth mentioning that SHAP plots can also be used for explainability in other ML tasks such as unsupervised learning (e.g., clustering). For instance, [58] used clustering to detect study patterns, and leveraged XAI to discover unexpected patterns that are not apparent from the clustering alone.\nLastly, other variants of SHAP exist, such as Kernel SHAP [59], which extends SHAP by using kernel methods to efficiently approximate feature attributions, making it suitable for complex models. Permutation SHAP, another variant, evaluates the importance of features by analyzing the change in model performance when feature values are randomly permuted, providing an intuitive measure of feature impact.\n\n\n\n3.2.2 Local Explainability\nContrary to global explainability, local explainability provides a detailed understanding of individual predictions, explaining why the model arrived at a specific outcome for a single student or instance. For example, if a student is flagged as being at risk of failing or dropping out, local explainability can identify that the prediction was driven by a drop in attendance and low quiz scores, for example. This level of granularity is of the utmost importance in education, where interventions are often targeted at individuals. Teachers, parents, counselors, and administrators can use these explanations to issue tailored interventions, such as reaching out to the student about attendance or offering extra tutoring for quizzes. Furthermore, local explainability builds trust in AI systems by making their decisions transparent and actionable, particularly in high-stakes contexts like grading or admission. Chapter 7 in this book [60] offers a tutorial on local explainability using the R programming language.\n\n3.2.2.1 SHAP Values for Individual Predictions\nSHAP values for individual predictions provide a breakdown of how each feature contributes to a specific prediction made by a machine learning model. Unlike SHAP summary plots, which summarize the overall behavior of the model across all predictions, individual SHAP values focus on explaining why the model made a particular prediction for a single instance. These explanations highlight the direction and magnitude of each feature’s impact on the prediction, making them invaluable for personalized decision-making.\nFor an individual prediction, SHAP values calculate the contribution of each feature by comparing the model’s output when the feature is included versus when it is excluded, averaged across all possible subsets of features. The SHAP values sum up to the difference between the model’s baseline prediction (the average prediction for all instances) and the specific prediction for the instance. For example, in a model predicting a student’s grade (see Figure 2.5), if the baseline prediction is 68.35 and the model predicts 55.22 for the particular student under examination, the SHAP values will explain the 13-point difference by assigning contributions to features such as frequency and regularity of forum contribute, lecture view, etc. In the plot, we can see that almost all features have a negative value, thus contributing to a lower grade, except for frequency of viewing lectures.\n\n\n\nFigure 5. SHAP instance explanations\n\n\n\n\n3.2.2.2 Local Interpretable Model-Agnostic Explanations\nLIME (Local Interpretable Model-Agnostic Explanations) is a method used to explain individual predictions by approximating the behavior of a complex machine learning model with a simpler, interpretable model within a localized region around the specific instance being explained [61]. Unlike SHAP, which relies on theoretical foundations from cooperative game theory, LIME focuses on practical, local interpretability by creating a surrogate model, such as linear regression, to approximate the original model’s decisions in the vicinity of a single instance.\nLIME works by perturbing the input data around the instance to generate a synthetic dataset. The model’s predictions for these perturbed data points are then used to train a simple, interpretable model that mimics the original model locally. For instance, in a model predicting a student’s grade, LIME might generate slightly varied versions of the student’s features (e.g., tweaking the frequency of forum contributions and session count) and observe how the predictions change (see Figure 2.6). The surrogate model identifies the main features driving the prediction, providing an interpretable explanation of the outcome.\n\n\n\nFigure 6. LIME explanations\n\n\n\n\n3.2.2.3 Counterfactual Explanations\nCounterfactual explanations offer a way to understand model predictions by identifying the minimal changes in input features needed to achieve a different outcome [62]. Instead of explaining why a particular prediction was made, counterfactual explanations answer the question: What could have been different to achieve the desired result? This makes them distinctly actionable, as they focus on what is required to alter the prediction. In other words, they offer “prescriptive analytics” in which a specific course of action is recommended [63].\nA counterfactual explanation involves generating a hypothetical instance similar to the original input but modified to produce the desired prediction. For example, in an educational context, if a student is predicted to score below a passing grade, a counterfactual explanation might suggest that increasing forum participation by 20% and improving the regularity of viewing the lectures by 10% would lead to a passing grade. These explanations are of particular value when the focus is often on understanding how to improve outcomes for individual students. Counterfactual explanations are generated using optimization techniques that find the smallest or most plausible changes to input features to alter the prediction.\nLLMs can play a significant role in enhancing the usability and accessibility of counterfactual explanations [63], particularly in education. Although counterfactual explanations are inherently actionable, their technical nature can make them difficult for educators or learners to interpret and apply without additional support. LLMs, with their natural language generation capabilities, can translate counterfactual outputs into easily understandable, context-specific recommendations. For instance, rather than presenting raw numerical changes in input features, an LLM can reframe the explanation as a conversational suggestion: “To improve your chances of passing, consider increasing your forum participation by 20% and attending lectures more regularly by 10%.”\nA related alternative to counterfactual explanations is the Contrastive Explanation Method (CEM) [64], which complements counterfactual explanations by focusing on contrastive reasoning. Rather than identifying minimal changes to alter a prediction, CEM emphasizes what features must be present (pertinent positives) and what features must be absent (pertinent negatives) for the current prediction to occur. For instance, in an educational setting, if a student is predicted to excel in a course, CEM might highlight pertinent positives such as consistent lecture engagement and forum participation, while pertinent negatives could include the absence of frequent late submissions. A question CEM aims to answer is: “Why this prediction instead of another?”. Thus, CEM provides a broader understanding of model behavior, offering both prescriptive insights and a deeper contextual explanation.\n\n\n3.2.2.4 Saliency Maps and Grad-CAM\nSaliency maps and Grad-CAM (Gradient-weighted Class Activation Mapping) are visualization techniques primarily used to explain predictions made by deep learning models [65]. These methods highlight the parts of the input data (e.g., pixels in an image or words in a text) that are most influential in the model’s decision-making process, providing localized and intuitive explanations of how the model interprets its inputs.\nSaliency maps visualize how sensitive a model’s prediction is to small changes in the input features. They compute the gradient of the model’s output with respect to the input, capturing how each input feature contributes to the prediction. The resulting visualization highlights areas of the input data that, if altered, would most significantly change the prediction. In text data, a saliency map might underline specific words or phrases in a reflective essay that contributed most to predicting a student’s understanding of a concept. Grad-CAM extends saliency maps by providing class-specific explanations. It focuses on the deeper convolutional layers of a neural network, computing gradients with respect to specific target classes. Grad-CAM generates a heatmap that overlays the input data, indicating regions that contributed most strongly to the prediction for a given class [65]. For instance, in an image classification model predicting whether a student is engaged in a video lecture, Grad-CAM might highlight that the model focused on the student’s face and eyes when assessing engagement. This is extremely relevant to assess bias in AI models. For example, in the automated engagement detection system depicted in Figure 7, it is clear that the fact that the student is wearing an eye patch is affecting the AI decision-making.\n\n\n\nFigure 7. Illustration of how a saliency map can assist in explaining an image classification model for student lecture engagement. The student image has been AI-generated."
  },
  {
    "objectID": "chapters/ch02-AIxAI/ch02-aixai.html#why-xai-may-not-be-enough",
    "href": "chapters/ch02-AIxAI/ch02-aixai.html#why-xai-may-not-be-enough",
    "title": "2  AI, Explainable AI and Evaluative AI: Informed Data-Driven Decision-Making in Education",
    "section": "4 Why XAI may not be enough",
    "text": "4 Why XAI may not be enough\nRecent research on AI in education has shown that algorithms alone cannot fully capture the full gamut of complexity of learners, learning and the way they learn. While AI can identify important variables that might affect a student’s performance, it may —and often does— miss the meaningful connections to educational theories. For instance, recent work has shown that while XAI may offer insights into the factors that predict student performance, it failed to account for the heterogeneity of learners and their learning approaches. Such limitation is rather consequential given that educational interventions are often effective on average but may not be suitable for every student and such, may harm some [51, 66]. The researchers also demonstrated that when instance-level explainability was considered, it showed that AI made decisions based on incorrect predictors, leading to mis-predictions. These incorrect predictions show the inherent risk in a fully data-driven approach and the necessity for human oversight or judgment [51]. As such, a hybrid human-AI collaboration, where the expertise of educators complements —and critically evaluates— the explainability of AI might be needed. In doing so, hybrid human-AI collaboration could lead to more effective educational strategies that are responsive to the diverse needs of each learner and less prone to problematic conclusions. Such an approach would improve the reliability of AI predictions and also ensure that AI serves as an aid rather than a substitute for human expertise. It also further emphasizes the central role of critical AI literacy where AI decisions are understood and cortically evaluated.\n\n4.1 Evaluative AI: A New Perspective on XAI\nTraditional XAI techniques, such as SHAP and LIME, provide explanations for the single most likely prediction or recommendation done by a system. For example, if an AI system predicts that a student is likely to drop out of a course, the XAI techniques that we have reviewed before can help understand which engagement indicators caused the AI to make that prediction. This can lead to overtrust or misinterpretation by users, especially in complex decision-making environments, since alternative hypotheses are not inspected. A recent approach by Miller [12] highlights the limitations of single outputs and introduces the concept of evaluative AI.\nInstead of providing definitive explanations, evaluative AI presents evidence for and against multiple hypotheses, allowing users to critically assess plausible outcomes. This approach could address trust issues by ensuring that AI-driven insights, such as student performance predictions, do not dictate decisions but inform them. For example, the same AI system described earlier could present arguments supporting and refuting the influence of forum participation, encouraging educators and learners to engage in critical analysis rather than passively accepting the AI’s output. This also aligns with Bearman et al. [67], who highlight the importance of developing students’ evaluative judgment —the ability to appraise AI outputs and processes critically. Evaluative AI can not only enhance transparency but also support the critical engagement required for students and educators to maintain autonomy over AI-augmented decisions.Advocates for evaluative AI recognize, however, that this novel paradigm poses an additional cognitive load on the decision-makers compared with usual XAI recommendation-driven outputs, although not as much as having no explanations whatsoever [12].\nLastly, it should be noted that evaluative AI does not compete with but rather builds on XAI methods [12] and techniques such as feature importance, SHAP, and LIME. Nonetheless, new tools are needed to apply these tools in the framework of evaluative AI. Most likely, these will be more interactive tools that enable testing specific hypotheses in a similar way as local explainability is assessed. Though a system that supports inquiry-based decision-making based on evaluative AI is easy to envision for classic regression or classification models in which the outcome is heavily constrained, it is not as straightforward to picture in the context of generative AI where the possibilities are endless and therefore the need for users’ evaluative judgmentis greater [67]."
  },
  {
    "objectID": "chapters/ch02-AIxAI/ch02-aixai.html#conclusion",
    "href": "chapters/ch02-AIxAI/ch02-aixai.html#conclusion",
    "title": "2  AI, Explainable AI and Evaluative AI: Informed Data-Driven Decision-Making in Education",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nThis chapter has provided an overview of the main applications of AI in education as well as an introduction to XAI as a set of tools and methods for providing transparency and interpretability to otherwise opaque AI-driven systems, which are critical requirements for enabling trust, mitigating bias, and ensuring an ethical use of AI in educational contexts.\nThe recent criticism about AI with regard to bias, fairness and transparency has also raised concerns in the education field [68]. Data used in AI models often reflects existing societal inequities, such as socioeconomic, racial, or gender disparities, which can inadvertently lead to models amplifying these biases. For example, patterns in historical educational data may disadvantage underrepresented groups, further perpetuating systemic inequities in learning environments [69]. Additionally, the aggregation of data across diverse populations can obscure the unique needs of specific learner subgroups, resulting in interventions that lack contextual relevance or efficacy [70]. A further challenge lies in the opacity of data processing and decision-making pipelines, which often leave educators and learners unaware of how their data influences predictions and recommendations.\nThe interplay between XAI and LLMs offers a promising synergy that enhances the transparency, usability, and adaptability of AI systems in education. On the one hand, LLMs can improve the accessibility of XAI outputs by converting abstract visualizations, such as feature importance plots or decision attribution maps, into clear textual explanations. This ability ensures that the rationale behind AI-driven decisions is not only available but also comprehensible to non-technical stakeholders. In this way, LLMs serve as an interface that bridges the gap between the technical workings of XAI tools and their practical applications in educational contexts. On the other hand, XAI enhances the application of LLMs by addressing challenges related to their opacity and potential biases. It can clarify why a specific response or recommendation was generated, identify patterns of bias or unfairness in outputs, and explain the logic behind personalization or content adaptations. As these technologies continue to evolve, their integration into education, research, and other fields will further redefine workflows and learning experiences, opening new opportunities for innovation and personalization.\nAs AI systems increasingly influence decision-making in education, it is critical to ensure that XAI tools promote autonomy rather than replace human judgment [51]. Developing evaluative judgment, as Bearman et al. [67] argue, is essential for enabling students and educators to critically engage with AI outputs. Emerging approaches, such as evaluative AI, suggest a shift from single-point explanations to evidence-based decision support, fostering critical thinking among educators and learners. Additionally, tools like LLMs can act as interpreters of XAI outputs, bridging data literacy gaps and encouraging users to challenge AI-driven recommendations. Fostering a culture of inquiry —where explanations are evaluated, not merely accepted— is necessary for XAI to truly empower stakeholders to make informed, ethical decisions in education."
  },
  {
    "objectID": "chapters/ch03-prediction/ch03-prediction.html",
    "href": "chapters/ch03-prediction/ch03-prediction.html",
    "title": "3  Artificial Intelligence: Using Machine Learning to Predict Students’ Performance",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch03-prediction/ch03-prediction.html#introduction",
    "href": "chapters/ch03-prediction/ch03-prediction.html#introduction",
    "title": "3  Artificial Intelligence: Using Machine Learning to Predict Students’ Performance",
    "section": "1 Introduction",
    "text": "1 Introduction\nPredicting student performance is one of the most common tasks in learning analytics [1]. Typically, student grades are used as an outcome variable of such modeling. Predicting a continuous variable like grades is called a regression problem, in contrast with classification that deals with nominal outcomes such as high and low achievers [2]. When we do predictive modeling, we assume that historical data can be used to build a model to predict the future using new data. In doing so, prediction goes beyond reporting what has happened but attempts to explain why an event happened, monitor what is currently happening and also attempts to foresee future developments [3]. As such, it provides a basis for actionable insights based on data or what we call data-informed decision making. This is particularly relevant for scaling up learning analytics applications. As such prediction has become a core step within the learning analytics process that aims to improve teaching or learning [4].\nThere are six main steps in predictive analytics [5]. First, the problem needs to be specified (e.g., improving student performance). Second, an outcome variable is defined (e.g., student grades), and data that can potentially predict the outcome variable is collected. Several types of data can be collected: prior academic data (e.g., student admission information), demographic data (e.g., student’s socio-economic status), academic data (e.g., results of formative assessment), behavioral data (e.g., student log data), and psychological data (e.g., a survey on student motivation) [6]. Third, raw data needs to be pre-processed to become suitable for modeling. This can include formatting the data or deriving features from raw data (e.g., calculating time spent in an online system by subtracting log-out time from log-in time) (see [7] for more details on data cleaning and wrangling). In order to choose the appropriate predictors for the predictive model, an exploratory analysis is conducted (see [8] for a tutorial in exploratory methods). Once the relevant indicators are selected, a predictive model can be built using one or more algorithms. The selection of an appropriate algorithm is determined by “the problem type, the nature of the outcome to be predicted, and the variables employed in the prediction” [6]. Typically, several algorithms would be compared to choose the method that models the data most accurately. The results from a predictive analysis can be used to develop preventive measures and interventions. Depending on the timing of the analysis, these interventions can be implemented on the current cohort of students, or the insights from the analytics can be used to shape future teaching and learning. Finally, it is important to build explainable prediction models —i.e., that provide an explanation for how a prediction was calculated— in order to increase the trust of the stakeholders in the model and increase awareness of what the model is actually predicting. In addition, explainable models can help validate model accuracy [9]. An example of an explainable predictive model is [10], who used data from formative assessment activities (e.g., the number of correct or incorrect answers to a question) to generate two predictions: of a grade in the midterm exam (on a scale of 0–20) and of a final grade (on a scale 0–40). The predictive model used the decision tree (DT) algorithm that supports the instructor’s sense-making and helps identify student groups in need of additional support.\nThe goal of this chapter is to explore advanced techniques of predictive analytics to model regression problems (see [11] for an introductory tutorial in predictive analytics). It is assumed that the reader is familiar with the R programming language. If that is not the case, it is recommended to refer to previous tutorials on the basics of R [12], data cleaning [13], basic statistics [14], and visualization [15]. The following section presents an overview of learning analytics research concerned with regression. Next, a synthetic dataset used in this tutorial is presented based on data of this study [16]. The tutorial section starts with an exploratory data analysis, including visualizing variable distribution and calculating correlations. It is followed by a description of the steps needed to create a predictive model. This tutorial explores the following predictive methods: Random Forest (RF), K-Nearest Neighbor (KNN), Linear Regression (LR), Neural Networks (NN), and Support Vector Machine (SVM). In addition, techniques to determine predictive model performance, such as Mean Absolute Error (MAE), Root-mean-square deviation (RMSE), and R-squared (RSQ), are presented and explained. The final part of this chapter shows how to create workflows to use multiple predictive algorithms."
  },
  {
    "objectID": "chapters/ch03-prediction/ch03-prediction.html#previous-research-on-student-performance-prediction",
    "href": "chapters/ch03-prediction/ch03-prediction.html#previous-research-on-student-performance-prediction",
    "title": "3  Artificial Intelligence: Using Machine Learning to Predict Students’ Performance",
    "section": "2 Previous research on student performance prediction",
    "text": "2 Previous research on student performance prediction\n[17] described three categories of regression techniques: 1) similarity-based approaches that are built upon identifying similar patterns in a dataset (e.g., KNN), 2) model-based approaches are developed by estimating implicit correlation among data samples and predictors (e.g., SVM, NN, DT, LR), 3) probabilistic approaches focus on examining probability distributions in the input dataset (e.g., Bayesian LR). Regression is the most commonly applied method in predictive analysis, while predicting student performance is one of the main regression problems prevalent in learning analytics research [18]. The vast majority of learning analytics papers focus on predicting student final grades [18]. Predictions in learning analytics are conducted in two main ways: 1) formative prediction examines student features at several points in the duration of the learning activity to provide several predictions over time, and 2) summative prediction aggregates data from the whole learning activity to predict the final outcome [19–21]. In addition, different levels of learning activity can be specified, e.g., degree, year, course, or exam level [22]. Log activity data is typically combined with other data sources, such as demographic or self-reported data [18]. Current evidence suggests that engagement level expressed through participation frequencies has a positive effect on performance [18]. Prior academic data, such as grades achieved in previous courses, was found to be the most influential factor in predicting student performance. An interesting meta-analytics study conducted by [16] to examine the reliability of predictors across courses found that overall engagement with online tasks and collaborative learning activities (measured by total activity and forum indicators) showed the highest prediction ranges, indicating their reliability as metrics. Lecture reading frequency had insignificant prediction ranges, suggesting limited applicability across courses.\nPredictive modeling can be challenging sometimes, specially when using low quality data for analysis or having a shallow understanding of the nature of the data and therefore can lead to untrustworthy findings. As learning processes are dynamic, using results from historical data to model current learners can be inaccurate and lead to the over-fitting or under-fitting of statistical models [23]. These issues can be mitigated by applying methods such as cross-validation or bootstrapping [4, 23, 24]. Another problem in predictive analytics is imbalanced datasets, where specific important values are not sufficiently represented in a dataset. This issue can be mitigated by over- or undersampling (for more details, see [25]). In addition, it is crucial to approach the findings from a point-in-time perspective that takes into consideration the temporality aspect of the findings’ accuracy [24, 26]. Also, it is important to consider the ethical implications of implementing predictive learning analytics so as not to cause harm to students or contribute to perpetuating social injustices [27, 28]. [17] identified three main issues with implementing predictive learning analytics. First, the issue of a cold start relates to the lack of data at the early stages of learning analytics deployment. This challenge can be mitigated by including demographic data or previous academic data. Second, the issue of scalability is connected to the increasing number of students, the complexity of algorithms used, and the computational requirements necessary to perform analytics. To address this issue, time constraints on the analysis or algorithm optimization need to be performed. Third, the issue of data sparsity refers to the lack of sufficient data about student activities, which can be caused by offline learning activities or passive interactions with available online learning systems. In order to increase available data, online learning activities can be made mandatory and embedded into a course design.\nDecisions regarding which data to collect, which features to select, which algorithm to use to build a predictive model, or how to evaluate model performance depend on the available data, dataset size, and study context [16]. A substantial body of research compares the performance of different predictive algorithms. For example, [29] collected student demographic and socio-economic data, self-reported motivation level, and assessment data from two evaluations during a semester. The authors predicted the student’s final grade (on a scale of 0–20) using the KNN and SVM algorithms. The performance of both algorithms was evaluated using 10-fold cross-validation and direct partition in the ratio methods, and it was found that SVM outperformed KNN. In another example, [30] collected student log data from a learning management system, such as the number of system logins the number of posts, or quiz grades, to model final exam grades using SVM and LR algorithms. The predictions were made multiple times during a semester and shown to the teacher. In this study, LR performed better than SVM, and the goodness-of-fit measure, R-squared, increased with each prediction in the semester. In addition, feature selection decreased model performance. [17] predicted student scores on the final exam (on a scale of 0–110) using KNN, SVM, Artificial NN, DT, Bayesian Regression, and LR. The dataset includes demographic data (e.g., gender, age), engagement data (sum of clicks), and performance data (e.g., scores per assessment). Artificial NN using engagement data and performance data had the highest precision out of all models. Including demographic data did not improve model precision significantly.\nOther papers focused on determining the most important features that predict the outcome variable. For example, [31] collected LMS activity data, such as login frequency or number of student posts, and student assignment and assessment grades to predict student final grades using multiple linear regression. The results showed that total login frequency in LMS, regularity of learning interval in LMS, and total assignments and assessment composites predicted the final grades. [32] collected two datasets: 1) where historical academic grades were available and limited demographic data was used; 2) where historical academic grades were not available, while demographic and socioeconomic data were extensively used. The SVM algorithm was applied to predict students’ high school final grades using both datasets. The authors reported greater errors in the case of the dataset without historical academic grades than in the dataset with historical academic grades.\nFinally, the issue of transferring predictive models within the same learning context and across different learning contexts was explored in the learning analytics research [16, 33, 34]. For example, [35] examined the relationship between LMS log data and final grade (on a scale of 0–100%) after adjusting to student characteristics. In addition, separate multiple linear regression models were developed to compare different courses. The study found that the impact of LMS features varied significantly on student performance across courses. The work by [33] applied mixed effect linear regression models to analyze a sample of data from log data from 15 courses that were homogeneous regarding the institutional settings, discipline, nominal learning design, and course size. The results highlighted that the same pedagogical model and study setting did not guarantee the same predictive power among courses, as there may be differences in the practical implementation of specific pedagogical models. In addition, it was found that the overall time spent in an LMS, regular discussion forum posts, and accessing course materials regularly were significant predictors of student final course grades across all courses."
  },
  {
    "objectID": "chapters/ch03-prediction/ch03-prediction.html#performance-prediction-with-r",
    "href": "chapters/ch03-prediction/ch03-prediction.html#performance-prediction-with-r",
    "title": "3  Artificial Intelligence: Using Machine Learning to Predict Students’ Performance",
    "section": "3 Performance prediction with R",
    "text": "3 Performance prediction with R\nIn this section we present two approaches to implement performance prediction in R: one following a more classic procedure, and the second one following a more updated procedure based on tidymodels. Broadly, the workflow followed is depicted in Figure 11.4, aligned with the common steps that are performed in any ML pipeline. First, we explore the data to gain an idea of the magnitude, format and content of the data, explore the relationships between the different variables, etc. This step is called exploratory data analysis. Next, we prepare the data (B) for subsequent steps. This might include removing or fixing incomplete records, converting data to different types, unifying values that are close to one another, or rescaling the data. The transformations that we need to do are commonly dictated by the type of data and the ML models that we will be using in subsequent steps. After data preparation, the next step is data splitting (C). Here, the dataset is divided into training and testing sets. The training set is used to fit the model, while the testing set is reserved for evaluating its performance. It is crucial to ensure that the testing set remains untouched until the evaluation stage to provide an unbiased estimate of the model’s generalization ability. Once the data is split, the process moves to model fitting (D). During this stage, the training data is used to train the selected machine learning models. This involves finding the optimal parameters that minimize error or maximize predictive accuracy for the given task. Depending on the algorithm, the model may identify different patterns, such as decision boundaries or relationships between features and the target variable. Finally, in the model evaluation step (E), the trained model is tested on the unseen testing data. Performance metrics that quantify the degree of error in the numeric predictions are computed to determine how well the model performs on new data. This step ensures that the model is not overfitting the training data and can generalize well to other datasets. We can repeat this process for several models with different underlying ML algorithms, or using only a subset of features of the data and compare the performance metrics among different models to select the one with the best fit.\n\n\n\nFigure 1. ML workflow implemented in the tutorial\n\n\n\n3.1 The dataset used in this chapter\nThe dataset we are using to illustrate the methods in this tutorial is a synthetic dataset based on the study by Jovanović et al. [33] and consists mostly of behavioral engagement indicators that can be obtained from learning management system (LMS) trace-log data. These variables capture both the engagement behavior of students (e.g., frequency and regularity of course and forum activities) and their participatory and time investment in the course (e.g., total duration, active days, session count) as well as the regularity of participation. Both time and regularity may be considered proxy indicators of cognitive engagement. We can also consider forum contributions as indicators of cognitive engagement since the context is problem-based learning, and student contributions require students to read, synthesize, critique, and formulate arguments [36].\nThe dataset has the following variables:\n\nFrequency Variables\n\nFreq_Course_View: The frequency of course page views by the student.\nFreq_Forum_Consume: The frequency with which the student consumes content in the forum (i.e., reads forum posts).\nFreq_Forum_Contribute: The frequency with which the student contributes to the forum (i.e., posts in the forum).\nFreq_Lecture_View: The frequency of lecture video views by the student.\n\nRegularity Variables\n\nRegularity_Course_View: The consistency of the student’s course page views.\nRegularity_Lecture_View: The consistency of the student’s lecture video views.\nRegularity_Forum_Consume: The consistency of the student’s forum content consumption.\nRegularity_Forum_Contribute: The consistency of the student’s forum contributions.\n\nTime Variables\n\nSession_Count: The total number of sessions the student has participated in.\nTotal_Duration: The total duration (in seconds) of all sessions participated by the student.\nActive_Days: The number of days the student was active in the course.\n\nOutcome Variables\n\nFinal_Grade: The final grade of the student in the course.\n\n\n\n3.1.1 Exploratory Data Analysis\nExploratory data analysis (EDA) is usually performed before doing the actual machine learning tasks (Figure 11.4–A). EDA includes examining the characteristics of the dataset to understand the distribution of the variables, and identify patterns and inconsistencies, if they may arise. Exploring the distribution and variance of each variable helps us identify any skewness, outliers, or anomalies in the data. Furthermore, EDA also allows us to examine the presence of missing values —or lack thereof—, handle distribution inconsistencies, and transform variables to balance their characteristics if needed. For instance, if we find that our variables have different measurement scales (one is measured in large numbers with vast variance and another in proportions and ranges between 0 and 1), we may need to transform or normalize the data to ensure that the two variables are not largely different. This process is particularly needed in some algorithms that are sensitive to discrepancies between variables. Additionally, we may have variables that are highly correlated (e.g., each is more or less a copy of the other), we may need to discard one of these similar variables as they do not offer any added information to the model and may bias the estimates. It should be mentioned that standardization will often result in less (direct) interpretability of the model, for example, when predicting final grade, there will be negative grades (below average) which might be hard to interpret, and also regression coefficients may be tricky to interpret. If standardization is carried out, the results may be presented on the original scales for better interpretation.\nThe first step in EDA is to load the R packages and use them to explore the data. We will need several R packages. Firstly, we will need tidyverse: a set of R packages designed to make it easy to read, edit, manipulate and visualize data through a large collection of functions for filtering, selecting and modifying the data [37]. As a collection of packages, tidyverse includes packages such as dplyr and tidyr for data manipulation and ggplot2 for data visualization, among others. The skimr package provides an easy, yet comprehensive, summary of a dataset with more detail than most R built-in summary functions [38]. The skimr package offers central tendency statistics like mean and median as well as the number and proportion of missing values for each variable, making it easy to get an overview of the data and detect any anomalies or patterns quickly. Besides, skimr calculates variance measures, maximum, minimum and displays a miniature histogram for each variable. The correlation package is an easy-to-use package for estimating and visualizing several types of correlations. We will need the package correlation to visualize the correlation between variables. In particular, it will be used to create a heatmap to understand relationships between variables [39]. We will also use the package rio to import the data into our code [40]. Lastly, we will use the performance package to evaluate our predictive models [41].\nThe next code starts by loading the R packages and reading the data.\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(correlation)\nlibrary(skimr)\nlibrary(rio)\nlibrary(performance)\n\nstudent_data <- import(\"https://github.com/lamethods/data2/raw/main/lms/lms.csv\")\n\nThe previous R code imports student data from a CSV file. Next, the following EDA tasks will be performed: 1) provide a detailed summary of the data using skimr, 2) create histograms for all variables to examine their distribution, 3) calculate and visualize correlations between variables, and 4) extract and display correlations with the final grade of the course (Final_Grade).\nIn our EDA, first, we get a quick and comprehensive overview of the dataset using the skim function from the skimr package. The function provides detailed information such as the number of rows, number of columns, type of variables, and most importantly, the number of missing values (n_missing). skimr also provides basic statistics for each variable like mean, standard deviation (sd), minimum (p0) and maximum (p100), first quartile (p25), median (p50), third quartile (p75), and a small histogram of the data.\n\n# 1. A detailed summary using skimr\nskim(student_data)\n\n\nData summary\n\n\nName\nstudent_data\n\n\nNumber of rows\n285\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n12\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nFreq_Course_View\n0\n1\n223.79\n59.33\n51.00\n187.00\n222.00\n264.00\n356.00\n▁▃▇▇▂\n\n\nFreq_Forum_Consume\n0\n1\n582.55\n175.55\n84.00\n465.00\n589.00\n703.00\n991.00\n▁▅▇▇▂\n\n\nFreq_Forum_Contribute\n0\n1\n178.07\n55.57\n17.00\n144.00\n175.00\n213.00\n347.00\n▁▅▇▃▁\n\n\nFreq_Lecture_View\n0\n1\n201.87\n56.68\n32.00\n162.00\n202.00\n247.00\n346.00\n▁▃▇▆▁\n\n\nRegularity_Course_View\n0\n1\n0.51\n0.14\n0.15\n0.42\n0.52\n0.61\n0.90\n▂▅▇▅▁\n\n\nRegularity_Lecture_View\n0\n1\n0.48\n0.15\n0.07\n0.38\n0.49\n0.57\n0.80\n▁▅▇▇▃\n\n\nRegularity_Forum_Consume\n0\n1\n0.49\n0.14\n0.13\n0.41\n0.50\n0.58\n0.95\n▁▅▇▂▁\n\n\nRegularity_Forum_Contribute\n0\n1\n0.48\n0.16\n0.03\n0.38\n0.47\n0.59\n0.90\n▁▃▇▅▂\n\n\nSession_Count\n0\n1\n189.54\n52.83\n53.00\n151.00\n191.00\n229.00\n306.00\n▂▅▇▇▂\n\n\nTotal_Duration\n0\n1\n83150.98\n23818.16\n25055.00\n68632.00\n83667.00\n98373.00\n147827.00\n▂▆▇▅▁\n\n\nActive_Days\n0\n1\n16.06\n4.15\n5.00\n14.00\n16.00\n19.00\n31.00\n▂▆▇▂▁\n\n\nFinal_Grade\n0\n1\n68.62\n8.86\n44.73\n62.42\n68.66\n75.26\n90.89\n▁▆▇▇▁\n\n\n\n\n\nThe results of the skim(student_data) command show that the data has 285 observations and no missing data in any variable. However, the variables have large differences in variance. For instance, the frequency variables are different from the regularity variables which are all below 1 and even more different if we compared them to the duration which has a mean of 83150.98 (SD = 23818.16). This information tells us that data may need to be normalized if the algorithm that we choose is sensitive to differences in variance.\nThe second step is to inspect the distribution of all variables (Figure 3.2) using the following code, which starts by using pivot_longer(everything()) to reshape our data, putting all variables into a single column. This makes it easier to create a faceted plot (multiple plots). We use ggplot2 to create histograms for each variable. The facet_wrap(~ name, scales = \"free\") line creates a separate plot for each variable, and scales = \"free\" allows each plot to have its own horizontal and vertical scales, which is often necessary when dealing with variables of different types or ranges and scales of measurement. For a tutorial about data visualization with R, refer to [15].\n\n# 2. Histograms of all variables\nstudent_data |>\n  pivot_longer(everything()) |>\n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 25, fill = \"skyblue\", color = \"black\") +\n  facet_wrap(~ name, scales = \"free\") +\n  theme_minimal()\n\n\n\n\nFigure 2. Histograms of all variables\n\n\n\n\nThe third step is to look at the correlations between variables (Figure 3.3). This allows us to see if there are any variables that are highly correlated to each other. If there are variables that are perfectly or almost perfectly correlated, they might be redundant or linear combinations of one another and therefore, do not add much to the model. The heatmap shows that most variables have correlations below the 0.8 level, and only a few cross this level. We may also examine the correlation with the final grade (our outcome variable that we want to predict) to explore variable importance.\n\n# 3. Relationship between Variables and `Final_Grade`\n# Calculate correlations with `Final_Grade`\ncorrelations <- correlation(student_data, method = \"pearson\")\ncorrelations |> summary() |> plot() + \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))\n\n\n\n\nFigure 3. Correlations between Variables and Final_Grade\n\n\n\n\n\n# Extract and display correlations with `Final_Grade`\nfinal_grade_correlations <- correlations |>\n  as.data.frame() |>\n  filter(Parameter2 == \"Final_Grade\") |>\n  arrange(desc(r))\n\n# Print the correlations with `Final_Grade`\nfinal_grade_correlations\n\n\n\n  \n\n\n\n\n\n3.1.2 Data preparation\nThe next stage in our workflow is data preparation (Figure 11.4–B), where we clean and transform our data to fix any problems and inconsistencies and to get it ready for subsequent steps. We can see two potential issues with our data: data with large differences in variance (e.g., duration and regularity) as well as potential high collinearity in some variables. We may need to standardize the data (subtract the mean and divide by the standard deviation) to make all variables have a comparable scale. The code in the next chunk does exactly that: it simply scales all numeric variables. You may need to re-run the EDA code again to verify:\n\n# Standardize numeric columns in the student_data data frame\nstudent_data_standardized <- student_data |>\n  mutate(across(\n    where(is.numeric),  # Select all numeric columns\n    ~scale(.) |> # Standardize each column (M=0, SD=1) \n      as.vector() # Convert to vector\n  ))\n\n# Use skimr package to get a summary of the standardized data\nskim(student_data_standardized)  \n\n\nData summary\n\n\nName\nstudent_data_standardized\n\n\nNumber of rows\n285\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n12\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nFreq_Course_View\n0\n1\n0\n1\n-2.9\n-0.62\n-0.03\n0.68\n2.2\n▁▃▇▆▂\n\n\nFreq_Forum_Consume\n0\n1\n0\n1\n-2.8\n-0.67\n0.04\n0.69\n2.3\n▁▅▇▇▂\n\n\nFreq_Forum_Contribute\n0\n1\n0\n1\n-2.9\n-0.61\n-0.06\n0.63\n3.0\n▁▅▇▃▁\n\n\nFreq_Lecture_View\n0\n1\n0\n1\n-3.0\n-0.70\n0.00\n0.80\n2.5\n▁▃▇▆▁\n\n\nRegularity_Course_View\n0\n1\n0\n1\n-2.5\n-0.62\n0.09\n0.73\n2.8\n▂▅▇▅▁\n\n\nRegularity_Lecture_View\n0\n1\n0\n1\n-2.8\n-0.66\n0.09\n0.63\n2.2\n▁▅▇▇▃\n\n\nRegularity_Forum_Consume\n0\n1\n0\n1\n-2.6\n-0.60\n0.05\n0.62\n3.3\n▁▅▇▂▁\n\n\nRegularity_Forum_Contribute\n0\n1\n0\n1\n-2.8\n-0.64\n-0.08\n0.66\n2.6\n▁▃▇▅▂\n\n\nSession_Count\n0\n1\n0\n1\n-2.6\n-0.73\n0.03\n0.75\n2.2\n▂▅▇▇▂\n\n\nTotal_Duration\n0\n1\n0\n1\n-2.4\n-0.61\n0.02\n0.64\n2.7\n▂▆▇▅▁\n\n\nActive_Days\n0\n1\n0\n1\n-2.7\n-0.50\n-0.02\n0.71\n3.6\n▂▆▇▂▁\n\n\nFinal_Grade\n0\n1\n0\n1\n-2.7\n-0.70\n0.00\n0.75\n2.5\n▁▆▇▇▁\n\n\n\n\n\nWe may also need to investigate the multicollinearity issue further. While multicollinearity is not a commonly discussed problem in machine learning, it is particularly important in explainable AI. If present, multicollinearity can lead to unstable estimates of model coefficients and inflated standard errors. One way to detect multicollinearity is by calculating the variance inflation factor (VIF). VIF measures the amount of inflation in the variance of a coefficient due to its correlation with other predictors. A high VIF value indicates that a variable is highly correlated with one or more other variables, suggesting multicollinearity. While there is no consensus about the VIF cutoff, in practice, a VIF value greater than 5 or 10 is often used as a cutoff to indicate multicollinearity. VIF is commonly computed in regression models, so we fit a regression model in the following code and then estimate the VIF from it. The code fits a linear regression model to predict the outcome variable (Final_Grade) using student LMS indicators as predictors. After fitting the model, it pipes the result into the check_collinearity() function from the performance package. The results of the VIF show that we have only four variables with moderate VIF values, so we can proceed with the analysis with no exclusion of any variable, given that we do not have a considerable collinearity problem.\n\n# Fit a linear regression model and check for multicollinearity\nlm(Final_Grade ~ Freq_Course_View + Freq_Lecture_View + Freq_Forum_Consume +\n                 Freq_Forum_Contribute + Regularity_Course_View + \n                 Regularity_Lecture_View + Regularity_Forum_Consume +\n                 Regularity_Forum_Contribute + Session_Count + \n                 Total_Duration + Active_Days,\n   data = student_data_standardized) |>  # Use standardized data for the model\ncheck_collinearity()    # Check for multicollinearity among predictors"
  },
  {
    "objectID": "chapters/ch03-prediction/ch03-prediction.html#tutorial-1-a-classic-approach-to-predictive-modeling",
    "href": "chapters/ch03-prediction/ch03-prediction.html#tutorial-1-a-classic-approach-to-predictive-modeling",
    "title": "3  Artificial Intelligence: Using Machine Learning to Predict Students’ Performance",
    "section": "4 Tutorial 1: A Classic Approach to Predictive Modeling",
    "text": "4 Tutorial 1: A Classic Approach to Predictive Modeling\nHaving explored and prepared the data, we now proceed to the next step of predicting students’ performance using machine learning. We will predict the final grades using the variables that represent the students’ engagement. The steps include loading the necessary R packages, splitting the data, fitting the model, and evaluating the model’s performance. First, we load the required R packages: the randomForest package, which is used to build the random forest model, the rsample package, which will be used to split the data into training and testing sets, and the yardstick package to facilitate model evaluation by calculating performance metrics.\nSecond, we split the dataset into two parts: a training set, which is used to train the machine learning model (perform the prediction), and a testing set (a holdout set for evaluation). The main rationale for data splitting is to use it to assess the performance of the estimated machine learning model on new, unseen data (the testing dataset). When keeping a testing dataset aside, we can evaluate how well the trained model generalizes to new data instances and estimate its accuracy. After all, we are creating the model with the hope that the same results will be obtainable in the future with new data. Furthermore, if we perform the prediction on all of the data, the model may —and most probably will— overfit. Overfitting occurs when a model learns the training data too well, including noise or irrelevant features, resulting in poor performance on new data. By having a separate testing set held out, we are increasing the likelihood that the model is learning the relevant patterns within the data not the noise. In practice, if a good model performs well on both the training and testing datasets, it is a good indication that the model is not overfitting and is likely to perform well in other situations with new data. Lastly, having a separate testing data set is rather valuable when our goal is to compare multiple machine learning algorithms to determine which model performs better on new data.\nThe code below uses the function initial_split function from the rsample package to perform an 80/20 split, where 80% of the data is used to train the model (perform the prediction), and the remaining 20% is kept aside as a testing set. While there is no consensus on the split ratio with many others using 70/30 and closer values, 80/20 is a reasonable starting point as it provides sufficient data for training while reserving a reasonable amount for evaluation.\nThird, we build the random forest model using the randomForest function from the randomForest package. The random forest algorithm is an ensemble learning method that creates multiple decision trees and combines their predictions to improve accuracy and control overfitting. This combination of multiple models helps reduce overfitting by averaging out the errors, resulting in an —arguably— more robust and generalizable model. More importantly, the random forest algorithm is an explainable model and provides feature importance scores, which can be used to identify the most relevant features for the problem at hand. In our case, it can help explain which variables help predict students’ performance. In this model, we predict the final grade (Final_Grade) based on the engagement variables such as the frequency and regularity of course views, lecture views, forum consumption and contributions, session count, total duration of activities, and the number of active days. We specify the number of trees (ntree) to be 1000, a parameter that controls the complexity and performance of the model.\nAfter fitting the model, we print the model summary using the functions print and importance (which shows the explanatory variables). The summary provides the model call, insights into the model’s performance, including error rates and the proportion of variance explained. Variable importance shows how much each predictor has contributed to the prediction accuracy of students’ performance. In doing so, it allows us to understand which features are more associated with higher grades. More accurately, which features the random forest algorithm associated with higher grades. Learning the important variables allows us to gain insights of what can help in improving teaching and learning based on the results of our model.\nFourth, we use the trained model with new unseen data instances that we held out (the testing dataset) to perform predictions. In other words, the already trained model will estimate the final grades in the testing data. In this step, we use the predict function to apply the model to the test dataset to generate predictions of final grades. These predictions will be compared against the actual grades to evaluate how close or far the algorithm was able to predict the grades and by that we evaluate the model’s performance. The closer the predictions are to the actual grade values, the better the performance of the algorithm. If the predicted grades differ considerably from the actual grades, it is an indication that the model is not good enough. It might be that we do not have enough data to make reliable predictions or, on the contrary, that our model is overfitted to the training data.\nIn the fifth step, we evaluate the model performance, given that there are multiple ways to assess performance, we will use different indicators, each of them provides different insights. Among the most common metrics when evaluating a regression machine learning model are the root mean squared error (RMSE), R-squared (R²), and mean absolute error (MAE). RMSE is a measure of the differences between predicted and actual values. It is calculated as the square root of the average of the squared differences between predicted and actual values. RMSE is always a positive value (because the errors are squared), where lower RMSE values indicate a better fit of the model to the data. Further, RMSE is sensitive to outliers because RMSE squares the errors, inflating the weight of larger errors.\nR-squared (R² or rsq) measures the proportion of variance in the dependent variable (the final grade in our case) that is explained by the independent variables. R² values range from 0 to 1, where values of 1 indicate that the model explains all the variability and R² of 0 indicates that the model explains none (bad model). However, although R² is popular, it can be misleading, pointing to erroneous conclusions. A high R² does not necessarily mean that the model is good; it only means that the model explains a large portion of the variance in the dependent variable. A major issue is that R² only measures the goodness of fit between the model and the data, but it does not account for the complexity or interpretability of the model. Adding more independent variables to a model, regardless of their relevance, will always increase or at least not decrease the R² value, leading to a more complex model that may be hard to interpret. As such, high R² values may give a false impression of a model’s predictive power. Furthermore, a high R² does not guarantee that the model will perform well on new, unseen data. Other metrics such as RMSE or MAE, are more useful in this regard. MAE measures the absolute average magnitude of the errors in a set of predictions (i.e., without considering the direction of the difference). It is calculated as the average of the absolute differences between predicted and actual values. Unlike RMSE, MAE is not sensitive to outliers (since no squaring is performed).\nThe sixth step in our analysis is creating better and enhanced model performance evaluation. In that, we visualize the predicted versus actual grades to see how the model fared. We also plot the variable importance in a better way using the ggplot2 package as well as the residuals. Here are the steps of the tutorial in detail:\n\n4.1 Loading the necessary libraries\nThe code below starts by loading the necessary libraries. The randomForest [42] library is used to build and work with random forest models, while rsample [43] aids in splitting the dataset into training and testing subsets. The yardstick [44] package is used for evaluating the performance of the model, and ggplot2 is employed for creating visualizations, although the latter is not explicitly mentioned in the code given that it is part of tidyverse [37] that we loaded before. Next, we set the seed function to a number set.seed(256) to ensure that our results would repeat in a similar way when re-running the code, e.g., data splitting would be consistent across different runs of the code.\n\n# Step 1: Load necessary libraries\nlibrary(randomForest)  # For building the Random Forest model\nlibrary(rsample)       # For data splitting\nlibrary(yardstick)     # For model evaluation\n\n# Set seed for reproducibility\nset.seed(256)\n\n\n\n4.2 Splitting the dataset\nThen, in the second step, we split the dataset (student_data_standardized) into training and testing sets (Figure 11.4–C). This is done using the initial_split function from the rsample package, which divides the data into an 80% training set and a 20% testing set. The training function extracts the training subset, while the testing function retrieves the testing subset and they are both assigned to two data frames train_data and test_data respectively.\n\n# Step 2: Split the data into training and testing sets\n# Using initial_split from rsample package for an 80/20 split\ndata_split <- initial_split(student_data_standardized, prop = 0.8)\ntrain_data <- training(data_split)\ntest_data <- testing(data_split)\n\n\n\n4.3 Creating and fitting the model\nIn the third step, we build and fit the random forest model (Figure 11.4–D). The randomForest function is used to create this model, where Final_Grade is predicted based on the engagement predictor variables such as Freq_Course_View, Freq_Lecture_View, and Session_Count. The model is configured to use 1000 trees, which is specified by the ntree parameter. In Random Forest models, specifying a large number of trees enhances performance by reducing variance and improving stability through the aggregation of predictions from 1000 diverse trees. Each tree provides a unique perspective due to its training on random data subsets, and averaging their predictions minimizes overfitting. Yet, whereas more trees may improve model performance, balancing the number of trees is important, as performance gains diminish —or disappear— beyond a certain point. Also, setting a very high number of trees incurs computational costs, increases the risk of overfitting, and training time.\nAfter fitting the model, the code prints a summary of the random forest model using print(rf_model). This summary provides insights into the model’s performance and structure. Additionally, importance(rf_model) is called to extract and display the importance of each variable in the model, helping to identify which predictors contribute most to the model’s predictions.\n\n# Step 3: Create and fit a Random Forest model\n# Building the Random Forest model with 1000 trees\nrf_model <- randomForest(Final_Grade ~ \n                           Freq_Course_View + Freq_Lecture_View + Freq_Forum_Consume +\n                           Freq_Forum_Contribute + Regularity_Course_View +\n                           Regularity_Lecture_View + Regularity_Forum_Consume +\n                           Regularity_Forum_Contribute + Session_Count +\n                           Total_Duration + Active_Days,\n                         data = train_data, ntree = 1000)\n\n# Print model summary and variable importance\nprint(rf_model)\n\n\nCall:\n randomForest(formula = Final_Grade ~ Freq_Course_View + Freq_Lecture_View +      Freq_Forum_Consume + Freq_Forum_Contribute + Regularity_Course_View +      Regularity_Lecture_View + Regularity_Forum_Consume + Regularity_Forum_Contribute +      Session_Count + Total_Duration + Active_Days, data = train_data,      ntree = 1000) \n               Type of random forest: regression\n                     Number of trees: 1000\nNo. of variables tried at each split: 3\n\n          Mean of squared residuals: 0.7\n                    % Var explained: 33\n\n\n\n# Printing the model summary and variable importance\nimportance(rf_model)\n\n                            IncNodePurity\nFreq_Course_View                     17.0\nFreq_Lecture_View                    14.5\nFreq_Forum_Consume                   24.0\nFreq_Forum_Contribute                47.7\nRegularity_Course_View               18.7\nRegularity_Lecture_View              13.1\nRegularity_Forum_Consume             13.7\nRegularity_Forum_Contribute          23.7\nSession_Count                        21.6\nTotal_Duration                       22.0\nActive_Days                           8.9\n\n\n\n\n4.4 Evaluating the model’s performance\nThe next stage in our workflow is to evaluate how well our model predicts performance (Figure 11.4–E). In Step 4, we make predictions on the test data using our trained Random Forest model by using the predict function, which generates a set of predicted values based on the features in test_data and the algorithm estimated before.\n\n# Step 4: Make predictions on the test data\n# Making predictions based on the test data using the trained model\npredictions <- predict(rf_model, newdata = test_data)\n\nIn Step 5, we add these predictions to the test dataset, creating a new data frame called evaluation_data that includes both the actual and predicted grades. We then use the metrics function from the yardstick package to evaluate our model’s performance by comparing the predicted grades with the actual ones, and we print the performance results to obtain performance metrics RMSE, MAE, and R-squared.\n\n# Step 5: Evaluate the model's performance\n# Adding predictions to the test data for evaluation\nevaluation_data <- bind_cols(test_data, Predicted_Grade = predictions)\n\n# Evaluating model performance\nperformance_metrics <- evaluation_data |>\n  metrics(truth = Final_Grade, estimate = Predicted_Grade)\n\n# Print the model performance metrics\nprint(performance_metrics)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       0.631\n2 rsq     standard       0.525\n3 mae     standard       0.480\n\n\nIn step 6, we produce two visualizations. The first visualization plots the actual versus predicted grades (Figure 3.4). Examining the distance between the residual and the fitted line gives an idea about how the model fared. The other plot is an improved plot of the important variables (Figure 4.4).\n\n# Step 6: Visualize predicted vs actual grades\nggplot(evaluation_data, aes(x = Final_Grade, y = Predicted_Grade)) +\n  geom_point(color = \"blue\", alpha = 0.5) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  labs(x = \"Actual Grade\", y = \"Predicted Grade\") +\n  theme_minimal()\n\n\n\n\nFigure 4. Scatter plot comparing predicted vs. actual grades\n\n\n\n\n\n# Enhanced variable importance plot\n# Extracting variable importance from the Random Forest model\nvariable_importance <- importance(rf_model)\nvar_imp_df <- data.frame(Variable = rownames(variable_importance), \n                         Importance = variable_importance[, 1])\n\n# Plotting variable importance\nggplot(var_imp_df, aes(x = reorder(Variable, Importance), y = Importance)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  coord_flip() +\n  labs(x = \"Variable\", y = \"Importance\") +\n  theme_minimal()\n\n\n\n\nFigure 5. Variable importance\n\n\n\n\nThis code snippet demonstrates how to create and interpret residual plots for a random forest model. The process begins by calculating residuals, which are the differences between the actual Final_Grade and the Predicted_Grade. These residuals are then visualized in two separate plots using ggplot2. The first plot is a basic scatter plot of residuals against predicted grades, with each point representing a prediction (Figure 3.6). A horizontal red dashed line at zero helps identify any systematic over- or under-prediction. The second plot builds upon the first by adding a green smoothed trend line, which can reveal non-linear patterns in the residuals. Ideally, the residuals should be randomly scattered around the zero line with no discernible pattern, indicating that the model’s errors are randomly distributed and there are no underlying biases or issues with the model.\n\n# Calculate residuals\nevaluation_data$residuals <- \n  evaluation_data$Final_Grade - evaluation_data$Predicted_Grade\n\n\n# Add a smoothed line to show trends\nggplot(evaluation_data, aes(x = Predicted_Grade, y = residuals)) +\n  geom_point(color = \"blue\", alpha = 0.5) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  geom_smooth(method = \"loess\", color = \"green\", se = FALSE) +\n  labs(x = \"Predicted Grade\", y = \"Residuals\") +\n  theme_minimal()\n\n\n\n\nFigure 6. Residual Plot for Random Forest Model with Trend Line.\n\n\n\n\n\n\n4.5 Other algorithms\nIn the same way, we can execute other algorithms. Here we use a simpler algorithm (linear regression) given that it assumes that the relation between predictors and predicted grades is linear. Also, linear regression model does not need tuning (e.g., number of trees etc.). The way variable importance is calculated also differs: Random forest provides an inherent measure of feature importance, whereas for linear regression, we use the absolute values of the coefficients for importance. The evaluation metrics and visualization steps remain largely the same, although the results and interpretations would likely differ due to the fundamental differences in the models. Not also that the linear regression had lower R² but higher error measures RMSE and MAE. Please also note the differences and similarities in variable importance between the two algorithms.\n\n# Step 1: Create and fit a Linear Regression model\nlm_model <- lm(Final_Grade ~ \n               Freq_Course_View + Freq_Lecture_View + Freq_Forum_Consume +\n               Freq_Forum_Contribute + Regularity_Course_View +\n               Regularity_Lecture_View + Regularity_Forum_Consume +\n               Regularity_Forum_Contribute + Session_Count +\n               Total_Duration + Active_Days,\n               data = student_data_standardized)\n\n# Print model summary\nprint(summary(lm_model))\n\n\nCall:\nlm(formula = Final_Grade ~ Freq_Course_View + Freq_Lecture_View + \n    Freq_Forum_Consume + Freq_Forum_Contribute + Regularity_Course_View + \n    Regularity_Lecture_View + Regularity_Forum_Consume + Regularity_Forum_Contribute + \n    Session_Count + Total_Duration + Active_Days, data = student_data_standardized)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5669 -0.4984  0.0593  0.5060  1.9453 \n\nCoefficients:\n                                         Estimate            Std. Error t value    Pr(>|t|)    \n(Intercept)                  0.000000000000000747  0.046179132911298881    0.00       1.000    \nFreq_Course_View            -0.004596244897394796  0.098861369352034281   -0.05       0.963    \nFreq_Lecture_View           -0.019940653761920547  0.064689500916218223   -0.31       0.758    \nFreq_Forum_Consume          -0.144808972991361057  0.118530889483868279   -1.22       0.223    \nFreq_Forum_Contribute        0.523780479201737648  0.089481942137727782    5.85 0.000000014 ***\nRegularity_Course_View       0.073043787722892314  0.105214018434519427    0.69       0.488    \nRegularity_Lecture_View      0.031349755109250026  0.081356823049423099    0.39       0.700    \nRegularity_Forum_Consume     0.050298824195160151  0.066285623420039699    0.76       0.449    \nRegularity_Forum_Contribute  0.106781491699962761  0.055297248687460540    1.93       0.055 .  \nSession_Count                0.278012448531555212  0.125078957967640347    2.22       0.027 *  \nTotal_Duration              -0.077927196019897907  0.100821687015266614   -0.77       0.440    \nActive_Days                 -0.035067688196140728  0.111265764443180828   -0.32       0.753    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.78 on 273 degrees of freedom\nMultiple R-squared:  0.416, Adjusted R-squared:  0.392 \nF-statistic: 17.7 on 11 and 273 DF,  p-value: <0.0000000000000002\n\n\n\n# Step 2: Make predictions on the data\npredictions_lm <- predict(lm_model, newdata = student_data_standardized)\n\n# Step 3: Evaluate the model's performance\n# Adding predictions to the data for evaluation\nevaluation_data_lm <- bind_cols(student_data_standardized, \n                                Predicted_Grade_lm = predictions_lm)\n\n# Evaluating model performance\nperformance_metrics_lm <- evaluation_data_lm |>\n  metrics(truth = Final_Grade, estimate = Predicted_Grade_lm)\n\n# Print the model performance metrics\nprint(performance_metrics_lm)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       0.763\n2 rsq     standard       0.416\n3 mae     standard       0.602\n\n\n\n# Step 4: Visualize predicted vs actual grades\nggplot(evaluation_data_lm, aes(x = Final_Grade, y = Predicted_Grade_lm)) +\n  geom_point(color = \"blue\", alpha = 0.5) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  labs(x = \"Actual Grade\", y = \"Predicted Grade\") +\n  theme_minimal()\n\n# Step 5: Variable importance plot\n# Extracting variable importance from the Linear Regression model\nvariable_importance_lm <- abs(coef(lm_model)[-1])  # Exclude intercept\nvar_imp_df_lm <- data.frame(Variable = names(variable_importance_lm), \n                            Importance = variable_importance_lm)\n\n# Plotting variable importance\nggplot(var_imp_df_lm, aes(x = reorder(Variable, Importance), y = Importance)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  coord_flip() +\n  labs(x = \"Variable\", y = \"Absolute Coefficient Value\") +\n  theme_minimal()\n\n\n\n\n\n\n\n(a) Scatter plot comparing actual grades to predicted grades.\n\n\n\n\n\n\n\n(b) Variable importance.\n\n\n\n\nFigure 7. Linear regression plots"
  },
  {
    "objectID": "chapters/ch03-prediction/ch03-prediction.html#tutorial-2-a-modern-approach-to-predictive-modelling-using-tidymodels",
    "href": "chapters/ch03-prediction/ch03-prediction.html#tutorial-2-a-modern-approach-to-predictive-modelling-using-tidymodels",
    "title": "3  Artificial Intelligence: Using Machine Learning to Predict Students’ Performance",
    "section": "5 Tutorial 2: A Modern Approach to Predictive Modelling using tidymodels",
    "text": "5 Tutorial 2: A Modern Approach to Predictive Modelling using tidymodels\nHaving used the traditional way, now we introduce a more modern approach of doing machine learning in R using tidymodels. The tidymodels framework provides a consistent interface for modeling in R by integrating several packages that work together seamlessly throughout the modeling process. The tidymodels package eliminates the need to learn and remember the various syntax and functions required by different modeling packages. Instead of having to switch between different paradigms and approaches for each package, you can use the same set of tools and functions. This uniformity speeds up model development and enhances code readability and maintainability.\nAdditionally, tidymodels offers integrated tools for tasks like pre-processing, resampling, and performance evaluation, further simplifying and accelerating the entire modeling workflow. For instance, the parsnip package in tidymodels allows us to define models in a consistent manner, regardless of the underlying algorithm (e.g., randomForest, ranger, xgboost). It provides functions like rand_forest() to specify the model, set_engine() to choose the computational backend, and set_mode() to define whether the model is used for regression or classification.\nThis organization is achieved through the use of workflows, which combines the entire modeling process in a single, manageable object, ensuring that each component is handled systematically. The tidy approach includes several steps: 1) splitting the data into training and testing sets (as done before), 2) defining a model specification (we will do a Random Forest model) using the standard approach of parsnip package, 3) creating a workflow combining the model specifications, 4) fitting the model to the training data, 5) making predictions on the test data, and 6) evaluating the model’s performance and visualizing the results. This tutorial will guide you through these steps, using parsnip to create a random forest model, evaluate its performance, and visualize the results. Below, we describe each step in detail:\n\nData Splitting: This step is done in the same way we did before.\nSelecting the predictor and target variables: The formula specifies the relationship between the target variable and the predictor variables. In regression tasks, this formula includes the dependent variable (e.g., Final_Grade) and the independent variables — in our case, engagement indicators— such as Freq_Course_View and Total_Duration. Defining the formula clearly outlines which variables are used to predict the outcome, ensuring that the model is correctly specified. We can also reuse the formula in other models without having to specify the arguments again.\nModel Specification with parsnip: Using parsnip, we define our random forest model with specific parameters. The rand_forest() function allows us to set the models parameters. The most important parameters are number of trees and number of variables to try in each split. In our model, we set number of trees (trees) to 1000 and the number of variables to sample at each split (mtry) to 3. Setting the engine to \"randomForest\" and the mode to \"regression\", we configure the model to perform regression tasks using the random forest algorithm.\nCreating the Workflow: A workflow is an object that combines together the model specification and the formula. This organized approach helps manage different components of the modeling process efficiently. Using the workflow() function, we add the random forest model specification and formula to create a complete modeling pipeline.\nFitting the Model: The model is fitted to the training data using the fit() function.\nMaking Predictions and Evaluating Performance: After fitting the model, we use it to make predictions on the test data as we did before. Similarly, for evaluation, these predictions are compared to the actual values. We calculate performance metrics such as R-squared (rsq), MAE (mae), and RMSE (rmse) using the metrics() function from the yardstick package. We will visually assess the model’s performance using plots as we have done before.\n\n\n5.1 Load the necessary libraries\nFirst, we load the necessary libraries. The tidymodels package [45] is a meta-package (a collection of packages) that loads several other packages useful for modeling, including parsnip, rsample, recipes, and more. Note that we will not run the data splitting code since it was already split before.\n\n# Load necessary libraries\nlibrary(tidymodels) #Loading tidymodels loads all the necessary packages for estimation.\n# Set seed for reproducibility\nset.seed(256)\n\n# Step 1: Split the data into training and testing sets, this step will not be\n# run as it is already done before. In case you are running this code only,\n# you may need to uncomment it.\n# \n# data_split_tidy <- initial_split(student_data_standardized, prop = 0.8)\n# train_data <- training(data_split_tidy)\n# test_data <- testing(data_split_tidy)\n\n\n\n5.2 Select the predictor and target variables\nIn tidymodels, the models are specified using a formula that defines the target variable, and the predictors following a specific syntax: target_variable \\~ predictor_1 + predictor_2 + ... + predictor_n, where target_variable is the variable we want to predict. In our case, it is Final_Grade. In turn, predictor_1, predictor_2, …, predictor_n are the predictor variables that will be used to make the prediction. In our case, these are the engagement indicators. See below how the formula is defined for our model in the same way.\n\n# Step 2: Define the formula\n# We define the formula to specify the relationship between the target variable \n# 'Final_Grade' and the predictor variables\nformula_tidy <- Final_Grade ~ Freq_Course_View + Freq_Lecture_View + \n                        Freq_Forum_Consume + Freq_Forum_Contribute + \n                        Regularity_Course_View + Regularity_Lecture_View + \n                        Regularity_Forum_Consume + Regularity_Forum_Contribute +\n                        Session_Count + Total_Duration + Active_Days\n\n\n\n5.3 Define the Random Forest Model Specification\nIn this step, we specify the random forest model we plan to estimate by defining the type of model, the engine (the computational back-end), and the hyperparameters (the model configurations). This process enables the creation of straightforward and modular machine learning pipelines (we can change any of these components easily). The package parsnip encompassed within tidymodels ecosystem provides a unified interface for defining and fitting various statistical and machine learning models. To specify a random forest model in parsnip, we use the rand_forest() function. This function allows us to define the type of model and set hyperparameters without immediately fitting the model. This separation of concerns is beneficial for the reusability of the code. The mode argument is set to \"regression\" because we are predicting a continuous outcome (Final_Grade). The trees = 1000 argument specifies the number of trees in the forest. The mtry argument represents the number of predictors in each split. Setting this to tune() indicates that we want to tune this hyperparameter, meaning we will search for the optimal value during the model training process. The set_engine() function specifies the computational backend to use for fitting the model. In this case, we use \"ranger\", which is a fast implementation of random forests suitable for large datasets.\n\n# Step 3: Define a random forest model specification with `ntree` and `mtry` parameters\n# We specify the random Forest model with 1000 trees and 3 variables randomly \n# sampled at each split\nrf_specification_tidy <- rand_forest(trees = 1000, mtry = tune()) |>\n  set_mode(\"regression\") |>\n  set_engine(\"ranger\")\n\nIn the tidymodels framework, we integrate this formula into a workflow to streamline the modeling process. The workflow() function initializes a new workflow. The add_model function allows us to specify the model which is the random forest in our case. And add_formula function allows us to add the formula to the workflow, specifying the relationship between the target variable and predictor variables. The workflow helps ensure that all necessary components for training the model are combined in a single object. This makes it easier to apply or modify the model, and helps maintain a tidy, reproducible code.\n\n# Step 4: Create the workflow. We create a workflow that combines the model \n# specification with the formula\nrf_workflow_tidy <- workflow() |>\n  add_model(rf_specification_tidy) |>\n  add_formula(formula_tidy)\n\nThe next steps are similar to our previous approach with small changes like using the pipe ‘|>’ to enhance model readability. First, using the workflow, we fit the random forest model (corresponding to Figure 11.4–D) using the previously defined workflow and training data (step 5). Using the fitted model, we make predictions on the test data and combines these predictions with the original test data to evaluate the model performance (step 6, corresponding to Figure 11.4–E). Next, we calculate performance metrics (R-squared, MAE, and RMSE) by comparing the predicted values to the actual values and then print the results. The last part of the code visualizes the predicted versus actual grades as before (Figure 3.8).\n\n# Step 5: Fit the random forest model\n# We fit the random forest model to the training data using the workflow\nrf_fitting_tidy <- rf_workflow_tidy |>\n  fit(data = train_data)\n\n# Step 6: Make predictions on the test data and evaluate the model's performance\n# We make predictions on the test data, calculate performance metrics, and \n# visualize the results\npredictions_tidy <- predict(rf_fitting_tidy, new_data = test_data) |>\n  bind_cols(test_data)\n\n# Calculate performance metrics: R-squared, MAE, and RMSE\nperformance_metrics_tidy <- predictions_tidy |>\n  metrics(truth = Final_Grade, estimate = .pred)\n\n# Print the model performance metrics\nprint(performance_metrics_tidy)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       0.705\n2 rsq     standard       0.431\n3 mae     standard       0.531\n\n# Scatter plot comparing actual grades to predicted grades\nggplot(predictions_tidy, aes(x = Final_Grade, y = .pred)) +\n  geom_point(color = \"blue\", alpha = 0.5) +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  labs(x = \"Actual Grade\", y = \"Predicted Grade\") +\n  theme_minimal()\n\n\n\n\nFigure 8. Predicted vs. Actual Grades\n\n\n\n\n\n\n5.4 Multiple algorithms\nThe uniform framework of tidymodels enables easier estimation and comparison of multiple models and in fact several configurations of the same algorithm if needed. The next code uses tidymodels to build and evaluate five machine learning algorithms namely: linear regression, support vector machine (SVM), random forest, K-nearest neighbors (KNN), and a neural network. Using tidymodels saves a lot of time in working with diverse packages and interfaces for each algorithm. Given that we already used similar code before, and we already loaded and split the data, we briefly describe the code for the estimation.\n\n5.4.1 Model specification\nFirst, we define model specifications using the parsnip package. The linear_reg function is used to specify a linear regression model with the lm engine. For SVM , we use the svm_rbf function and set the engine to kernlab, specifying that it will be used for regression. The random forest model is specified using rand_forest, setting the number of trees to 1000 and allowing the number of variables randomly sampled at each split (mtry) to be tuned. The nearest_neighbor function specifies a KNN model with 5 neighbors, and the engine is set to kknn. Finally, for the neural network, the mlp function specifies a model with 10 hidden units and 100 epochs, using the nnet engine, set for regression.\n\n# Linear Regression specification\nlr_specification <- linear_reg() |> set_engine(\"lm\")\n\n# SVM specification\nsvm_specification <- svm_rbf() |> set_engine(\"kernlab\") |>\n  set_mode(\"regression\")\n\n# Random Forest specification with ntree and mtry parameters\nrf_specification <- rand_forest(trees = 1000, mtry = tune()) |>\n  set_engine(\"randomForest\") |>\n  set_mode(\"regression\")\n\n# KNN specification\nknn_specification <- nearest_neighbor(neighbors = 5) |>\n  set_engine(\"kknn\") |>\n  set_mode(\"regression\")\n\n# Neural Network specification\nnn_specification <- mlp(hidden_units = 10, epochs = 100) |>\n  set_engine(\"nnet\") |>\n  set_mode(\"regression\")\n\n\n\n5.4.2 Create workflows\nNext, we create workflows for each model specification using the workflow function. This involves adding the model specification and the formula to the workflow. The formula defines the relationship between the predictors and the target variable Final_Grade.\n\n# Linear Regression workflow\nlr_workflow <- workflow() |>\n  add_model(lr_specification) |>\n  add_formula(formula_tidy)\n\n# SVM workflow\nsvm_workflow <- workflow() |>\n  add_model(svm_specification) |>\n  add_formula(formula_tidy)\n\n# Random Forest workflow\nrf_workflow <- workflow() |>\n  add_model(rf_specification) |>\n  add_formula(formula_tidy)\n\n# KNN workflow\nknn_workflow <- workflow() |>\n  add_model(knn_specification) |>\n  add_formula(formula_tidy)\n\n# Neural Network workflow\nnn_workflow <- workflow() |>\n  add_model(nn_specification) |>\n  add_formula(formula_tidy)\n\n\n\n5.4.3 Fit the models\nWe then fit each model to the training data using the fit function. This step trains the models using the specified workflows and the training dataset.\n\n# Fit the Linear Regression model\nlr_fitting <- lr_workflow |>\n  fit(data = train_data)\n\n# Fit the SVM model\nsvm_fitting <- svm_workflow |>\n  fit(data = train_data)\n\n# Fit the Random Forest model\nrf_fitting <- rf_workflow |>\n  fit(data = train_data)\n\n# Fit the KNN model\nknn_fitting <- knn_workflow |>\n  fit(data = train_data)\n\n# Fit the Neural Network model\nnn_fitting <- nn_workflow |>\n  fit(data = train_data)\n\n\n\n5.4.4 Calculate Fit Indices and Residuals\nLastly, we create calculate_metrics function to assess the performance of each fitted model. It takes the model fit object and test data as inputs, makes predictions on the test data, and binds these predictions to the test data (Table 3.1). Performance metrics such as R-squared, mean absolute error (MAE), and root mean squared error (RMSE) are calculated using the metrics function from the yardstick package.\n\n# Function to calculate metrics and residuals\ncalculate_metrics <- function(model_fitting, test_data, truth_col = \"Final_Grade\") {\n  # Make predictions\n  predictions <- predict(model_fitting, new_data = test_data) |>\n    bind_cols(test_data)\n  \n  # Calculate residuals\n  residuals <- predictions |>\n    mutate(residuals = !!sym(truth_col) - .pred)\n  \n  # Calculate performance metrics\n  performance_metrics <- residuals |>\n    metrics(truth = !!sym(truth_col), estimate = .pred)\n  \n  list(performance_metrics = performance_metrics, residuals = residuals)\n}\n\n# Calculate metrics and residuals for each model\nlr_results <- calculate_metrics(lr_fitting, test_data)\nsvm_results <- calculate_metrics(svm_fitting, test_data)\nrf_results <- calculate_metrics(rf_fitting, test_data)\nknn_results <- calculate_metrics(knn_fitting, test_data)\nnn_results <- calculate_metrics(nn_fitting, test_data)\n\n# Combine predictions and residuals\nlr_residuals <- lr_results$residuals |> mutate(model = \"Linear Regression\")\nsvm_residuals <- svm_results$residuals |> mutate(model = \"SVM\")\nrf_residuals <- rf_results$residuals |> mutate(model = \"Random Forest\")\nknn_residuals <- knn_results$residuals |> mutate(model = \"KNN\")\nnn_residuals <- nn_results$residuals |> mutate(model = \"Neural Network\")\n\ncombined_residuals <- bind_rows(lr_residuals, svm_residuals, \n                                rf_residuals, knn_residuals, nn_residuals)\n\n# Extract and combine performance metrics\nperformance_metrics <- bind_rows(\n  lr_results$performance_metrics |> mutate(model = \"Linear Regression\"),\n  svm_results$performance_metrics |> mutate(model = \"SVM\"),\n  rf_results$performance_metrics |> mutate(model = \"Random Forest\"),\n  knn_results$performance_metrics |> mutate(model = \"KNN\"),\n  nn_results$performance_metrics |> mutate(model = \"Neural Network\")\n) |> arrange(.metric)\n\n# Print performance metrics\nprint(performance_metrics)\n\n\n\n\n\nTable 1. Performance metrics by model\n\n\n.metric\n.estimator\n.estimate\nmodel\n\n\n\n\nmae\nstandard\n0.48\nLinear Regression\n\n\nmae\nstandard\n0.51\nSVM\n\n\nmae\nstandard\n0.53\nRandom Forest\n\n\nmae\nstandard\n0.60\nKNN\n\n\nmae\nstandard\n1.11\nNeural Network\n\n\nrmse\nstandard\n0.62\nLinear Regression\n\n\nrmse\nstandard\n0.67\nSVM\n\n\nrmse\nstandard\n0.70\nRandom Forest\n\n\nrmse\nstandard\n0.75\nKNN\n\n\nrmse\nstandard\n1.35\nNeural Network\n\n\nrsq\nstandard\n0.52\nLinear Regression\n\n\nrsq\nstandard\n0.45\nSVM\n\n\nrsq\nstandard\n0.43\nRandom Forest\n\n\nrsq\nstandard\n0.35\nKNN\n\n\nrsq\nstandard\n0.05\nNeural Network\n\n\n\n\n\n\nWen can plot the performance metrics to compare them across the models (Figure 3.9).\n\n# Plot performance metrics\nperformance_metrics |>\n  ggplot(aes(x = model, y = .estimate, fill = model)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_wrap(~ .metric, scales = \"free\") +\n  labs(x = \"Model\",\n       y = \"Metric Value\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nFigure 9. Comparing performance metrics across models\n\n\n\n\nWe can also create a scatter plot comparing actual grades to predicted grades, providing a visual assessment of the model’s performance (Figure 3.10).\n\ncombined_residuals |>\n  ggplot(aes(x = Final_Grade, y = .pred, color = model)) +\n  geom_point() +\n  # Add a linear model fit line\n  geom_smooth(method = \"lm\", se = FALSE, linetype = \"dashed\", color = \"black\") +  \n  facet_wrap(~ model) +\n  labs(x = \"Actual Final Grade\",\n       y = \"Predicted Final Grade\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 10. Plot actual vs. predicted with a line matching the slope of actual vs. predicted\n\n\n\n\nYou can evaluate the results of each model by comparing it against the other models and choose the best performing model with lowest error rates."
  },
  {
    "objectID": "chapters/ch03-prediction/ch03-prediction.html#discussion-and-conclusions",
    "href": "chapters/ch03-prediction/ch03-prediction.html#discussion-and-conclusions",
    "title": "3  Artificial Intelligence: Using Machine Learning to Predict Students’ Performance",
    "section": "6 Discussion and conclusions",
    "text": "6 Discussion and conclusions\nThis chapter has provided a comprehensive exploration of predictive modeling in the context of learning analytics, where we focus on the practical application of machine learning techniques to forecast student performance. We have systematically mapped the main steps of the predictive modeling process, from data preparation and exploratory analysis to the building, evaluation, and interpretation of various models. The chapter explored different traditions from traditional machine learning, to the modern tidy approach and comparison across several models (linear regression, random forests, support vector machines, K-nearest neighbors, and neural networks). Furthermore, we investigated the use of various evaluation metrics – RMSE, MAE, and R-squared – to estimate model accuracy, which can help us understand the trade-offs associated with each algorithm. We also shown how to visualize model predictions against actual outcomes and examine residual plots to assess the model fit and highlight areas that needed further consideration.\nSeveral important points need to be emphasized here. First, the important of data and variable selection according to a predefined learning theory that can offer a platform for variable inclusion and model results interpretation for more discussion of this issue, please refer to the vast literature on theory and predictive analytics. Second, the significance of careful data preprocessing cannot be overstated; even the most advanced algorithms cannot compensate for poor-quality data. Third, the absence of a single “best” algorithm became apparent, highlighting how algorithm choice is always dataset-specific, goal-oriented, and depends on the desired interpretability of the model. Fourth, we recognized that predictive models are tools to enhance decision-making, but do not replace human judgment and expertise, particularly in educational contexts, which require ethical considerations.\nFinally, this chapter has demonstrated that the practical application of data science techniques can help to inform and improve educational practices. However, it also highlighted the importance of understanding the limitations of predictive models and machine learning in general [46]. Worth also noting that while explanation and transparency of the machine learning models are useful, they still can do deliver results that are more data driven that plausible or aligned with theory [21]. Other Issues like overfitting, data quality, and ethical considerations need to be carefully addressed to ensure responsible and meaningful use of predictive analytics in education. Finally, the chapter emphasized that predictive modeling is not just about achieving high accuracy, but also about gaining actionable insights into factors influencing student success and informing interventions and improving learning outcomes. We also need to emphasize that these results are aggregates of the whole dataset and therefore, it can be misleading and lacks full representation of each and everyone [47, 48]."
  },
  {
    "objectID": "chapters/ch04-classification/ch04-classification.html",
    "href": "chapters/ch04-classification/ch04-classification.html",
    "title": "4  Artificial Intelligence: Using Machine Learning to Classify Students and Predict Low Achievers",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch04-classification/ch04-classification.html#introduction",
    "href": "chapters/ch04-classification/ch04-classification.html#introduction",
    "title": "4  Artificial Intelligence: Using Machine Learning to Classify Students and Predict Low Achievers",
    "section": "1 Introduction",
    "text": "1 Introduction\nIn the previous chapter [1], we implemented machine learning to predict students’ grades as a continuous outcome, i.e., predicted the numerical final grades, which is often referred to as a regression problem. However, more often than not, we are not interested in the specific grade but rather in a broad range or category such as drop-outs (students who abandon the course) or low achievers (students who will score below a certain grade or fail the course). In that, we want to identify whether a student is at risk of dropping out or failing a course and, therefore, may need support. Hence, in this chapter we address this achievement classification problem. The most common achievement classification problem is to classify or in fact predict low achievers [2]. This is commonly done with early course data given that there will be an opportunity for remedial and support before the end of the course. Similar to the previous chapter, we will start by demonstrating the code for a random forest model [1]. Random forest is chosen because it is robust and offers insights (explanation) into the important variables that drove the algorithm to take a decision [3]. The analysis process involves a sequence of steps similar to the previous chapter with two notable differences. First, we have to prepare the data in a slightly different manner and the evaluation and the metrics follow a different approach given that it is a classification problem. Also, similar to the previous chapter, we will begin with the traditional approach and then we will proceed with the tidymodels approach."
  },
  {
    "objectID": "chapters/ch04-classification/ch04-classification.html#previous-research-on-identifying-at-risk-students",
    "href": "chapters/ch04-classification/ch04-classification.html#previous-research-on-identifying-at-risk-students",
    "title": "4  Artificial Intelligence: Using Machine Learning to Classify Students and Predict Low Achievers",
    "section": "2 Previous research on identifying at-risk students",
    "text": "2 Previous research on identifying at-risk students\nThere are several aims in research on the identification of at-risk students: 1) to develop a model to identify the at-risk students, 2) to identify attributes or indicators for the identification of at-risk students, 3) to develop and evaluate intervention methods to prevent student dropout or failure [4–6].\nSeveral studies compared the performance of several algorithms to predict at-risk students. For example, [7] reported that Naïve Bayes (NB) outperformed k-Nearest Neighbors (KNN), Decision Tree (DT), Random Forest (RF), and Support Vector Machines (SVM). [8] developed a predictive model using genetic algorithms that performed better than DT, RF, Multilayer Perceptron, Logistic Regression (LG), and the AdaBoost algorithm (ADA). The Deep Long Short-Term Memory Model by [9] outperformed LG and artificial neural networks. RF was the best-performing algorithm in [10] compared to SVM, KNN, ET, ADA, Gradient boosting classifier, and Deep Feed Forward Neural Network, in [11] compared to SVM, KNN, DT, ADA, Extreme Gradient, Bayesian Classifier, and Linear Discriminant Analysis, and in [12] compared to DT, SVM and NB. [13] reported that KNN had a higher performance than LG, SVM with different kernels, DT, and RF. Finally, instead of comparing different methods, [14] combined five base classifiers (NB, SVM, KNN, Repeated Incremental Pruning to Produce Error Reduction classifier and C4.5 classifier) into the Majority Voting ensemble classifier. As the diversity of potential classification algorithms and the differing performance results indicate, the portability of predictive models from one context to another can be challenging [12, 15]. In addition, model interpretability and transparency are important in the practical implementations of predictive models. However, there is a trade-off in predictive accuracy and the ability to generate generalizable models of low transparency models to take into consideration [15].\nThe selection of data and development of appropriate indicators used in a predictive model is a subject of extensive research. Most studies use student log data to determine student performance [e.g., 8, 16, 17]. Several studies complement student activity data with additional data sources. For example, [18] included not only student log data but also library book loans or card swipes into buildings. [7] used the pre-test scores and formative assessment scores in their predictive model. [19] found that demographic data improved prediction. [20] included a bias analysis in order to include three protected demographic attributes into their predictive model. [21] collected clicker data, as the learning management system was not popular to use by students. It is important to remember that every predictive model needs to be complemented by human judgment and contextual information. The research reported in [22], where the authors focused on examining false positive and false negative errors of a predictive model, showed that there are many events that can influence student behavior and performance that are not captured by the predictive models, such as family or work responsibilities and health issues.\nThe typical definitions of at-risk students include students who are likely to drop out of a class or students who may fail a particular exam. A vast body of research reported indicators of struggling students identified by their predictive models. For example, [14] found that the strongest predictors of performance are connected to the successful completion of a set of course activities, such as watching videos or completing self-assessment quizzes. Similarly, [6] reported that student engagement and the consistency of using online resources were the strongest performance predictors. [10] found that clickstream and assessment variables had the most significant impact on student performance. [12] reported the highest predictive power of formative assessment scores and cumulative GPA. [23] highlighted the importance of the first written assignment and quiz for student attrition. A similar finding was reported in [11], who reported the average grade obtained in the first period of the first semester as the strongest dropout predictor. Other conceptualizations of at-risk students include tracking when a student suddenly changes their typical behavior in an online learning system [19] or defining a specific threshold to consider a student at risk, e.g., scores below 5% over the passing mark [6].\nThe temporal dimension is crucial in developing predictive models. It includes several aspects, such as how often predictions should be calculated, how long the data should be collected to ensure accurate predictions, when predictions should take place in order to be actionable, or how long a student should be inactive to trigger an intervention. For example, [14] used data from the first cohort of students to develop a classifier for another cohort of students. [8] generated a prediction model every two weeks, while [18] generated an alert of no-engagement after a period of two weeks of inactivity. Others calculate predictions every week [e.g., 24, 25] or every four weeks [12] or sometimes even on daily basis [26]. The intervals can also vary depending of the purpose of the predictive model. [13] developed predictions of passing or failing the next formative exam once a week, and, in addition, the classifiers from the first six weeks of classes were trained to predict the outcome of the mid-term exam, and the classifiers from weeks 7 to 12 were used to predict the end-of-semester’s exam outcome. [9] developed a classifier that has shown around 90% accuracy within the first 10 weeks of student interaction in an online learning environment, [23] reported a satisfactory prediction probability after week 5, while [10] showed almost 80% precision and accuracy score at 20% of the course length.\nThe predictions can be displayed directly to a student in the form of a dashboard or an alert. Another option is to inform a teacher about struggling students in a teacher-facing dashboard or through notifications [27]. Learning analytics dashboards that include predictive analytics are not widely implemented [28]. In addition, these dashboards often present only the prediction in the form of a visualization (e.g., signal lights) or percentage score without any explanation [29]. However, there is evidence that awareness of being at risk is not enough to improve student outcomes, and predictions need to be connected to be connected to an intervention [12]. The interventions can have different forms. For example, based on the analytics shown in a teacher-facing dashboard, a teacher can contact at-risk students personally to provide additional support [30]. Automated interventions embedded within the predictive analytics can include an adaptive feedback system that sends students custom messages based on their performance [13] or sending student-specific recommendations for learning materials and additional mentoring [12]. The evaluation of the interventions can be measured in reduced drop-out or increased performance. A comprehensive implementation of predictive analytics includes several iterations of improving the predictive model, adjusting an effective and transparent representation of the predictive results, and evaluating several intervention approaches [31]."
  },
  {
    "objectID": "chapters/ch04-classification/ch04-classification.html#classifying-students-with-r",
    "href": "chapters/ch04-classification/ch04-classification.html#classifying-students-with-r",
    "title": "4  Artificial Intelligence: Using Machine Learning to Classify Students and Predict Low Achievers",
    "section": "3 Classifying students with R",
    "text": "3 Classifying students with R\nIn this section we present two approaches to implement classification with R: one following a more classic procedure (tutorial 1), and the second one following a more updated procedure (tutorial 2) based on tidymodels. Broadly, the workflow followed is depicted in Figure 11.4, aligned with the common steps that are performed in any ML pipeline. First, we explore the data to gain an idea of the magnitude, format and content of the data, explore the relationships between the different variables, etc. This step is called exploratory data analysis. Next, we prepare the data (B) for subsequent steps. This might include removing or fixing incomplete records, converting data to different types, unifying values that are close to one another, or rescaling the data. The transformations that we need to do are commonly dictated by the type of data and the ML models that we will be using in subsequent steps. After data preparation, the next step is data splitting (C). Here, the dataset is divided into training and testing sets. The training set is used to fit the model, while the testing set is reserved for evaluating its performance. It is crucial to ensure that the testing set remains untouched until the evaluation stage to provide an unbiased estimate of the model’s generalization ability. Once the data is split, the process moves to model fitting (D). During this stage, the training data is used to train the selected machine learning models. This involves finding the optimal parameters that minimize error or maximize predictive accuracy for the given task. Depending on the algorithm, the model may identify different patterns, such as decision boundaries or relationships between features and the target variable. Finally, in the model evaluation step (E), the trained model is tested on the unseen testing data. Performance metrics that quantify the accuracy and precision of the model are computed to determine how well the model performs on new data. This step ensures that the model is not overfitting the training data and can generalize well to other datasets. We can repeat this process for several models with different underlying ML algorithms (tutorial 3), or using only a subset of features of the data and compare the performance metrics among different models to select the one with the best fit.\n\n\n\nFigure 1. ML workflow implemented in the tutorial\n\n\n\n3.1 Tutorial 1: A traditional classification approach with Random Forest\nThe first example follows the workflow depicted in Figure 11.4– We will estimate a Random Forest model to classify students into high achievers and low achievers based on their final grades and the engagement indicators as predictors. It is assumed that the reader is familiar with the R programming language. If that is not the case, it is recommended to refer to previous tutorials on the basics of R [32], data cleaning [33], basic statistics [34], and visualization [35]. In this chapter we will use a dataset that was used in the previous chapter [1] and based on this paper [17].\nIn Step 1, we start by loading the necessary R packages. We use randomForest [3] to estimate a random forest model, tidyverse [36] for data manipulation and visualization, rsample [37] for splitting the data, caret [38] for model evaluation, and pROC to visualize this evaluation. We also use the package rio [39] to import the data. To ensure reproducibility, we set the seed to a random number (1410). Setting the seed ensures that the code will produce identical results every time it is run. For instance, the data will be randomly split in the same way each time.\nThe next step would be conducting exploratory data analysis (EDA) to better understand the dataset and identify possible relationships between variables or problems in the data (Figure 11.4–A). We implemented this step in detail with the same dataset in Chapter 3 [1] so we will not repeat it here for the sake of brevity. Therefore, we can move on to the next step, which is data preparation (Figure 11.4–B). We prepare the data by creating a new binary classification target variable named Achievement by categorizing students into High_Achievers and Low_Achievers based on their Final_Grade. Those who are above the median (top 50%) will be classified as high achievers and the others will be classified as lower achievers. This categorical variable is coded as a factor as it is required by several functions. Of course, there are several ways to label students as at risk; for instance, based on a threshold score of 60% or even higher.\nIn Step 2, we split the dataset into training and testing sets using an 80/20 split where we will use 80% of the data to train the model and 20% for evaluation (corresponding to Figure 11.4–C). We use the argument strata = \"Achievement\" to make sure that both the training and testing datasets have balanced classes of low and high achievers.\nHaving prepared the data we will then, define and fit the random forest model (Figure 11.4–D) to predict Achievement based on engagement indicators e.g., lecture views, forum activity, session counts, total duration of activities, and active days.\nLastly, to evaluate the model (Figure 11.4–e), we make predictions on the test dataset using the fitted model. For binary classification models, the evaluation commonly includes a confusion matrix as well as other metrics e.g., accuracy, precision, and recall as well as a visual evaluation through the receiver operating characteristic (ROC) curve, which we will explain in context with the estimation code.\nThe first chunk of code loads the necessary libraries and the data that we will use for building the model and sets a random seed as described earlier.\n\n# Load necessary libraries \nlibrary(randomForest)  # For Random Forest model\nlibrary(tidyverse)     # For data manipulation and visualization\nlibrary(rsample)    # For data splitting and modeling workflow\nlibrary(caret)      # For model evaluation\nlibrary(pROC) # For visualizing receiver operating characteristic (ROC curves) \nlibrary(rio)  # For importing data files \n\n# Set seed for reproducibility\nset.seed(1410)\n\n\n3.1.1 Preparing the data\nAs a first step in our analysis, we prepare the data (Figure 11.4–B) by creating a binary target variable, Achievement, which categorizes students based on their Final_Grade. Specifically, students with grades above the median are labeled as “High Achievers”, and those below are labeled as “Low Achievers”. This transformation is performed using the mutate function, ensuring the target variable is a factor with appropriate levels.\n\n# Step 1: Load and prepare the data\nraw_data <- import(\"https://github.com/lamethods/data2/raw/main/lms/lms.csv\")\n\n# Create a binary classification target variable\nmedian_grade <- median(raw_data$Final_Grade)\nstudent_data <- raw_data |>\n  mutate(Achievement = factor(ifelse(Final_Grade > median_grade, \n                                     \"High_Achievers\", \"Low_Achievers\"), \n                              levels = c(\"Low_Achievers\", \"High_Achievers\")))\n\n\n\n\n3.2 Splitting the data into training and testing sets\nIn the second step, we split the dataset into training and testing sets (Figure 11.4–C) using an 80/20 split. To maintain the class distribution, we use stratified sampling by passing the option strata to the initial_split function with the which takes care of distributing the achievement classes (high and low achievers) in the training and testing sets in a balanced way. In doing so, we ensure that both the training and testing sets are representative of the overall class distribution and we create a more balance model.\n\n# Step 2: Split the data into training and testing sets\ndata_split <- initial_split(student_data, prop = 0.8, strata = \"Achievement\")\ntrain_data <- training(data_split)\ntest_data <- testing(data_split)\n\n\n\n3.3 Creating and training the model\nIn step 3, we create and fit a random forest model using the randomForest function (Figure 11.4–D). The model predicts the Achievement variable based on engagement indicators (e.g., course views, lecture views, forum consumption and contribution), regularity indicators (Regularity Course View, Regularity Lecture_View, Regularity Forum Consume, Regularity Forum Contribute), and time indicators (session counts, total duration of activities, active days) [16]. The model is specified using a formula syntax: target_variable ~ predictor_1 + predictor_2 + ... + predictor_n. To ensure model robustness, we set ntree to 1000 (please refer to the previous chapter for more details on the number of trees [1]). Given that we are also interested in explaining the results, we set the importance argument to TRUE to estimate the importance of each variable.\n\n# Step 3: Create and fit a Random Forest model\nrf_model <- randomForest(\n  Achievement ~ Freq_Course_View + Freq_Lecture_View + Freq_Forum_Consume +\n    Freq_Forum_Contribute + Regularity_Course_View + \n    Regularity_Lecture_View + Regularity_Forum_Consume +\n    Regularity_Forum_Contribute + Session_Count + \n    Total_Duration + Active_Days,\n  data = train_data, \n  ntree = 1000,\n  importance = TRUE)\n\n\n\n3.4 Evaluating the model\nIn step 4, we apply the model that we just trained to a new dataset (the test set) to evaluate its performance on new data (unseen by the model) that we held out. This allows us to evaluate how the model perform, or in other words, how likely the results we obtained to replicate in the future. A good model is expected to perform well on both learning and testing data. For the evaluation purpose, we use the predict function to generate predictions from our trained random forest model, rf_model and the test_data. The predict function generates two types of predictions: the probabilities and the classes.\n1) Probability Predictions: estimate the probability of a student belonging to each achievement category (Low Achievers or High Achievers), we save these probabilities in a predictions_prob variable. To get these probabilities we specify the argument type = \"prob\" argument.\n2) Class Predictions: predict the most likely achievement category for each student i.e., whether the student is predicted as high or low achiever. We save these class predictions in the predictions_class variable.\nIn the last step, and for convenience, we we add these prediction as extra columns within the original test dataset to make it easier for further analysis.\n\n# Step 4: Make predictions on the test set for the probabilities and the classes\npredictions_prob <- predict(rf_model, test_data, type = \"prob\")\npredictions_class <- predict(rf_model, test_data)\n\n# Add predictions and probabilities to the test dataset\ntest_data <- test_data |>\n  mutate(Predicted_Class = predictions_class,\n         Probability_Low_Achievers = predictions_prob[, \"Low_Achievers\"],\n         Probability_High_Achievers = predictions_prob[, \"High_Achievers\"])\n\nIn Step 5, we evaluate the model’s performance (Figure 11.4–E). For this, we use the confusionMatrix function from the caret package, which provides a detailed evaluation, including the confusion matrix and several other metrics. The confusion matrix is a table that summarizes the model’s predictions compared to the actual outcomes. Specifically, it shows the counts of correctly predicted positive cases (true positives), correctly predicted negative cases (true negatives), incorrectly predicted positive cases (false positives), and incorrectly predicted negative cases (false negatives).\nFor example, in our case:\n\nTrue Positives (TP): 20 Low-achieving students were correctly classified as “Low Achievers”.\nTrue Negatives (TN): 23 High-achieving students were correctly classified as “High Achievers”.\nFalse Positives (FP): 9 Low-achieving students were incorrectly classified as “High Achievers”.\nFalse Negatives (FN): 6 High-achieving students were incorrectly classified as “Low Achievers”.\n\n\n3.4.1 Evaluation metrics\nThe confusionMatrix package produces several classification metrics; we will explain them here with equations in light of our results.\n\nAccuracy: Measures the proportion of correct predictions (both true positives and true negatives) out of the total number of cases. In our case, accuracy was 0.7414 indicating that approximately 74.14% of the classifications were correct. The 95% CI (Confidence Interval) indicates that the interval (0.6096, 0.8474) likely contains the true accuracy. The accuracy is calculated according to the following equation:\n\\[\n  \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} = \\frac{20 + 23}{20 + 23 + 9 + 6} = 0.7414\n\\]\nKappa: Cohen’s Kappa is a measure of agreement between the observed accuracy and the accuracy expected by chance. In our case, the kappa value was 0.483 indicating a moderate agreement beyond chance. As a practical rule, Kappa of 0–0.20 is considered slight, 0.21–0.40 as fair, 0.41–0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1 as almost perfect.\n\\[\n\\kappa = \\frac{\\text{Accuracy} - \\text{Expected Accuracy}}{1 - \\text{Expected Accuracy}} = \\frac{0.7414 - 0.5}{1 - 0.5} = 0.4828\n\\]\nMcnemar’s Test P-Value: This test assesses if FP and FN differ significantly, the high p-value (0.789) in our case suggests no significant difference between the error rates for either class.\n\n\n\n3.4.2 Sensitivity and Specificity\n\nSensitivity (True Positive Rate) measures the model’s ability to correctly identify “Low Achievers” (the positive class in this case). A value of 0.6897 indicates that the model were able to correctly identify 68.97% of actual “Low Achievers.”\n\\[\n\\text{Sensitivity} = \\frac{TP}{TP + FN} = \\frac{20}{20 + 9} = 0.6897\n\\]\nSpecificity (True Negative Rate) measures the model’s ability to correctly identify “High Achievers” (the negative class). A value of 0.7931 suggests that the model correctly identified 79.31% of actual “High Achievers.”\n\\[\n  \\text{Specificity} = \\frac{TN}{TN + FP} = \\frac{23}{23 + 6} = 0.7931\n\\]\n\nPlease note that the model was able to identify more of high achievers than low achievers, this is probably due to the fact that high achievers generate more data that can be used to identify them.\n\n3.4.2.1 Predictive Values\n\nPositive Predictive Value (PPV or Precision): The proportion of positive results that are true positives. The PPV of 0.7692 indicates that 76.92% of the predicted “Low Achievers” are actual “Low Achievers”.\n\\[\n\\text{PPV} = \\frac{TP}{TP + FP} = \\frac{20}{20 + 6} = 0.7692\n\\]\nNegative Predictive Value (NPV): The proportion of negative results that are true negatives. The NPV of 0.7188 indicates that 71.88% of the predicted “High Achievers” are actual “High Achievers”.\n\\[\n\\text{NPV} = \\frac{TN}{TN + FN} = \\frac{23}{23 + 9} = 0.7188\n\\]\n\n\n\n3.4.2.2 Prevalence and Detection\nThese metrics provide insights into the distribution of the positive class (Low Achievers) in the actual data and in the model’s predictions.\nPrevalence: The proportion of the total number of cases that are positives. The prevalence of 0.5 indicates that 50% of the students are “Low Achievers”. The detection rate of 0.3448 indicates that 34.48% of the total students were correctly identified as “Low Achievers”. The detection prevalence of 0.4483 indicates that 44.83% of the students were predicted to be “Low Achievers”.\nThe code below computes the confusion_matrix and prints it.\n\n# Step 5: Evaluate the model\n# Create confusion matrix\nconfusion_matrix <- confusionMatrix(test_data$Predicted_Class, test_data$Achievement)\n\n# Print confusion matrix and other metrics\nprint(confusion_matrix)\n\nConfusion Matrix and Statistics\n\n                Reference\nPrediction       Low_Achievers High_Achievers\n  Low_Achievers             21              6\n  High_Achievers             8             23\n                                          \n               Accuracy : 0.7586          \n                 95% CI : (0.6283, 0.8613)\n    No Information Rate : 0.5             \n    P-Value [Acc > NIR] : 5.025e-05       \n                                          \n                  Kappa : 0.5172          \n                                          \n Mcnemar's Test P-Value : 0.7893          \n                                          \n            Sensitivity : 0.7241          \n            Specificity : 0.7931          \n         Pos Pred Value : 0.7778          \n         Neg Pred Value : 0.7419          \n             Prevalence : 0.5000          \n         Detection Rate : 0.3621          \n   Detection Prevalence : 0.4655          \n      Balanced Accuracy : 0.7586          \n                                          \n       'Positive' Class : Low_Achievers   \n                                          \n\n\n\n\n3.4.2.3 Visual evaluation\nIn step 6 we evaluate the model performance visually. We use a fourfold plot to visualize the confusion matrix, in which the circles in each quadrant are proportional to the counts in the matrix (Figure 4.2). The additional circumferences in each quadrant represent the confidence intervals of each count.\n\nlibrary(caret)\n# Step 6: Visualize results\nfourfoldplot(confusion_matrix$table)\nroc_obj <- roc(test_data$Achievement, predictions_prob[, \"Low_Achievers\"])\n\n\n\n\nFigure 2. Confusion matrix\n\n\n\n\nNext we plot the ROC curve which is a graphical representation of model’s performance across all possible threshold values (Figure 4.3). The ROC curve plots the True Positive Rate (Sensitivity) against the False Positive Rate (\\(1 - \\text{Specificity}\\)). In other words, the ROC curve helps visualize the trade-off between sensitivity and specificity. A good model would have higher values of sensitivity and specificity resulting in a curve above the diagonal line while a bad model would produce a diagonal line. Another related value is the area under the curve (AUC). The AUC quantifies the overall ability of the model to discriminate between positive and negative classes, value ranges from 0 to 1 where AUC = 0.5 indicates the model performs no better than random guessing, values below 0.5 indicate that the model performs worse than random guessing and AUC values more than 0.5 indicates the model performs better than guessing (assuming that there are the same number of high achievers than low achievers). In our case, the value was 0.76 which is a moderate performance.\n\n# Plot ROC curve\nplot(roc_obj, asp = NA)\nauc_value <- auc(roc_obj)\nprint(paste(\"AUC:\", auc_value))\n\n[1] \"AUC: 0.762187871581451\"\n\n\n\n\n\nFigure 3. ROC Curve for Random Forest Model\n\n\n\n\n\n\n3.4.2.4 Explainability\nGiven the importance of intepretability of machine learning models, it is important to understand which of the features of our dataset contribute to the final prediction [40, 41]. For this purpose, we can extract the variable importance and visualize them. The code below extracts variable importance values from the fitted random forest model (rf_model). The importance values contain two metrics: MeanDecreaseAccurac and MeanDecreaseGini. The MeanDecreaseAccuracy measures how much the accuracy of the model decreases when a particular variable is excluded. Higher values indicate that the variable is more important for maintaining the accuracy of the model. For instance, Freq_Forum_Contribute has the highest MeanDecreaseAccuracy value of 39.6879041, suggesting it is the most critical variable for the model’s accuracy. In contrast, Freq_Lecture_View has a negative value (\\(-0.2569615\\)), indicating that excluding this variable slightly improves the model’s accuracy. Please note that a more detailed explanation of model explainability is offered in next chapters with more comprehensive methods [42, 43].\nThe MeanDecreaseGini measures the total decrease in node impurity that a variable contributes across all the trees in the forest. Higher values signify greater importance. For example, Freq_Forum_Contribute again shows high importance with a MeanDecreaseGini value of 21.348017, emphasizing its importance for predicting students’ achievement. On the other hand, Freq_Lecture_View has a lower MeanDecreaseGini value of 5.959581, indicating it is less influential in reducing impurity.\n\n# Extract variable importance from the fitted model\nimportance_values <- rf_model$importance\n\n# Convert to a data frame for plotting\nimportance_df <- as.data.frame(importance_values)\nimportance_df$Variable <- rownames(importance_df)\n\n\n# Plot variable importance for MeanDecreaseAccuracy\nggplot(importance_df, aes(x = reorder(Variable, MeanDecreaseAccuracy), \n                          y = MeanDecreaseAccuracy)) +\n  geom_bar(stat = \"identity\", fill = \"turquoise\") +\n  coord_flip() +\n  labs(x = \"Variable\",\n       y = \"Importance\") +\n  theme_minimal()\n\n# Plot variable importance for MeanDecreaseGini\nggplot(importance_df, aes(x = reorder(Variable, MeanDecreaseGini), \n                          y = MeanDecreaseGini)) +\n  geom_bar(stat = \"identity\", fill = \"turquoise\") +\n  coord_flip() +\n  labs(x = \"Variable\",\n       y = \"Importance\") +\n  theme_minimal()\n\n\n\n\n\n\n\n(a) MeanDecreaseAccuracy\n\n\n\n\n\n\n\n(b) MeanDecreaseGini\n\n\n\n\nFigure 4. Variable Importance for the Random Forest Model\n\n\n\n\n\n\n\n3.5 Tutorial 2: An alternative implementation of random forests with tidymodels\nIn this section, we present an alternative to the process described in the previous example using tidymodels [44] instead of the classic approach. The tidymodels framework offers a unified streamlined interface for machine learning in R that makes it easier to specify models, update them, and compare one against another. Instead of working with different packages for different models, each with its own syntax, tidymodels employs a consistent syntax across all stages of estimation. All the more so, when we need to estimate several models as it is the case in our tutorial, tidymodels makes it easier to estimate and maintain the code. The main packages within tidymodels include rsample [37] for data splitting, parsnip for a specifying models [45], recipes for data pre-processing and feature engineering [46], workflows for combining pre-processing, modeling, and yardstick [47] for model evaluation. A more detailed discussion of tidymodels can be found the in the previous chapter.\nPerforming the classification with tidymodels follows a sequence of steps similar to what we had before with some differences. In general, the first three steps (loading packages, preparing the data and splitting) are the same as before but later steps uses the standard approach of defining models, workflows, training and evaluation. Below is a detailed description of the code and estimation steps.\n\n3.5.1 Preparing the data\nThe first steps are similar to the traditional approach: we load the necessary packages tidyverse and tidymodels; then we prepare the data (Figure 11.4–B) by creating a binary target variable, which we call Achievement based on Final_Grade to classify students to low and high achievers.\n\nlibrary(tidymodels)\n\n# Set seed for reproducibility\nset.seed(1410)\n\n# Load and prepare the data\n# Assuming student_data is already loaded into the environment\nstudent_data <- raw_data |>\n  mutate(Achievement = factor(ifelse(Final_Grade > median(Final_Grade), \n                                     \"High_Achievers\", \"Low_Achievers\"),\n                              levels = c(\"Low_Achievers\", \"High_Achievers\")))\n\n\n\n3.5.2 Splitting the data into training and testing sets\nNext, we split the data into training and testing sets (Figure 11.4–C). The training set is then used to make the actual predictions and the testing test is used for model evaluation. We opt again here for stratified sampling strata = Achievement to preserve class distribution (balanced proportion of high and low achievers in the training and testing datasets).\n\n# Step 2: Split the data into training and testing sets\ndata_split <- initial_split(student_data, prop = 0.8, strata = Achievement)\ntrain_data <- training(data_split)\ntest_data <- testing(data_split)\n\n\n\n3.5.3 Creating a recipe\nUnlike the previous chapter [1], where we used a formula, we extend it here by using a recipe. A recipe offers more options like including other data preparation and pre-processing steps. In our case, we define a recipe with a formula and also a function for normalizing the predictors so that all variables are on a similar scale. For that, we use step_normalize(all_predictors()) function which will create new variables with standard deviation of one and a mean of zero.\n\n# Create a recipe\nrf_recipe <- recipe(Achievement ~ Freq_Course_View + Freq_Lecture_View + \n                    Freq_Forum_Consume + Freq_Forum_Contribute + \n                    Regularity_Course_View + Regularity_Lecture_View + \n                    Regularity_Forum_Consume + Regularity_Forum_Contribute + \n                    Session_Count + Total_Duration + Active_Days,\n                    data = train_data) |>\n  step_normalize(all_predictors())\n\n\n\n3.5.4 Creating the model\nNext, we define the random forest model specifications. In this specification, we define the rf_spec object which will store the configuration for our model. The specification sets the trees parameter to 1000 which means the random forest will consist of 1000 decision trees. A larger number of trees generally leads to more stable results. The mtry parameter is set to 5, which means that during each split in the decision trees, the algorithm will randomly select 5 predictors (from the engagement indicators). This randomness helps prevent overfitting and ensures diversity among the trees. Then, we specify the engine for our random forest using the set_engine(\"ranger\")** function. The ranger engine is a fast and efficient engine that can handle large datasets. Most importantly, we specify set_mode(\"classification\"), which means that the random forest model will predict a categorical outcome (high or low achievers), compare this to regression in the previous chapter.\n\n# Create a random forest model specification\nrf_spec <- rand_forest(trees = 1000, mtry = 5) |>\n  set_engine(\"ranger\",  importance = \"impurity\") |>\n  set_mode(\"classification\")\n\n\n\n3.5.5 Creating a workflow\nThe next step is to create a workflow which is a container that holds the recipe and the model specifications that we just defined. Workflows provide a way to chain these components together and execute them in a specific order when fitting the algorithm. To create a workflow for a random forest model, we begin by initializing the workflow object using the function workflow() and adding the recipe rf_recipe that that we just created containing the formula and the pre-processing steps. Next, we specify the model that we just defined rf_spec. In the following step, we fit the model (Figure 11.4–D) to the training data by passing it to the fit function and the workflow and we store the fitted model to the object rf_fit_tidy. rf_fit_tidy contains data about how the model predicts the data.\nTo evaluate the model’s performance (Figure 11.4–E), we generate predictions on the test dataset test_data using the trained model rf_fit_tidy. We enhance our test dataset with these predictions by computing three new columns and adding them to the test_data: Predicted_Class_tidy, which contains the predicted class for each observation; Probability_Low_Achievers_tidy, which gives the probability of an observation being classified as a “Low Achiever”; and Probability_High_Achievers_tidy, which provides the probability of an observation being classified as a “High Achiever”. The resulting augmented dataset combines original features and the predictions and their probabilities which will make it easier to plot and evaluate the data later.\n\n# Create a workflow\nrf_workflow <- workflow() |>\n  add_recipe(rf_recipe) |>\n  add_model(rf_spec)\n\n# Fit the model\nrf_fit_tidy <- rf_workflow |>\n  fit(data = train_data)\n\n# Make predictions on the test set\npredictions_prob_tidy <- predict(rf_fit_tidy, test_data, type = \"prob\")\npredictions_class_tidy <- predict(rf_fit_tidy, test_data)\n\n# Add predictions and probabilities to the test dataset\ntest_data <- test_data |>\n  mutate(\n    Predicted_Class_tidy = predictions_class_tidy$.pred_class,\n    Probability_Low_Achievers_tidy = predictions_prob_tidy$.pred_Low_Achievers,\n    Probability_High_Achievers_tidy = predictions_prob_tidy$.pred_High_Achievers)\n\n\n\n3.5.6 Evaluating the model\nIn the next step, we evaluate the performance of the model. The process involves creating a confusion matrix and computing several evaluation metrics as before. The first part of the code generates a confusion matrix. This matrix provides a summary of how well the model is performing in distinguishing between the classes.\nIn the code below, we create the confusion matrix using the conf_mat() function. Computing the evaluation metrics in yardstick [47] has two steps 1) defining the metrics and then computing them. So, we define the custom set of evaluation metrics using the metric_set function. This set includes the common classification metrics: accuracy, sensitivity, specificity, F1 score, balanced accuracy, positive predictive value, and negative predictive value.\nFinally, we compute these custom metrics and add them the test data. The function calculates all the specified metrics, comparing the true Achievement values against the Predicted_Class_tidy predictions. The event_level = \"first\" parameter specifies which class should be considered the positive class for binary classification metrics, i.e., low achievers. The resulting detailed metrics are stored in ‘detailed_metrics_tidy’ and then printed.\nThe results of the evaluation shows that the accuracy of the model is 0.741 which means that 74.1% of “High Achievers” and “Low Achievers” were correctly classified. The sensitivity of 0.793 means that in 79.3% of the times, the model correctly identified students who are actually “Low Achievers” and labeled them as such. Specificity shows how the model identified the other category of students: “High Achievers” and identifying them which was approximately 69.0%. The F-measure, around 0.754, combines sensitivity and precision into a single metric, balancing the model’s ability to identify “High Achievers” with how accurate those predictions are. This score indicates a good balance between detecting “High Achievers” and avoiding false positives. In case you are interested, the F-measure is computed as the harmonic mean of precision and recall following this formula.\n\\[\n\\text{F1-Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} = 2 \\cdot \\frac{0.7692 \\cdot 0.6897}{0.7692 + 0.6897} = 0.7273\n\\]\nPositive Predictive Value (PPV) of 0.719 indicates how often the model’s prediction of “Low Achievers” is correct which was accurate in 71.9% of the time. Negative Predictive Value (NPV) was 0.769, showing the accuracy of predictions for “High Achievers was correct 76.9% of the time. Balanced Accuracy was 0.741 and reflects the model’s overall performance across both classes by equally weighting the ability to correctly identify”Low Achievers” and “High Achievers.”\n\n# Confusion Matrix\nconf_mat_tidy <- conf_mat(test_data, truth = Achievement, \n                          estimate = Predicted_Class_tidy)\nprint(conf_mat_tidy)\n\n                Truth\nPrediction       Low_Achievers High_Achievers\n  Low_Achievers             20              6\n  High_Achievers             9             23\n\n# Custom metric set\ncustom_metrics_tidy <- metric_set(accuracy, sens, yardstick::spec, \n                                  f_meas, bal_accuracy, ppv, npv)\n\n# Compute all metrics\ndetailed_metrics_tidy <- test_data |>\n  custom_metrics_tidy(\n    truth = Achievement, \n    estimate = Predicted_Class_tidy,\n    event_level = \"first\")\n\nprint(detailed_metrics_tidy)\n\n# A tibble: 7 × 3\n  .metric      .estimator .estimate\n  <chr>        <chr>          <dbl>\n1 accuracy     binary         0.741\n2 sens         binary         0.690\n3 spec         binary         0.793\n4 f_meas       binary         0.727\n5 bal_accuracy binary         0.741\n6 ppv          binary         0.769\n7 npv          binary         0.719\n\n\nIn the last step, we estimate and plot the AUC and ROC curve. The visualization and estimation follow exactly the same way we did before (Figure 4.5). The only difference is that we use roc_curve() function from the yardstick package [47]. The function takes two important arguments, the truth argument which represents the class we predicted and the probability of the class we are focusing on which is lower achievers in our case.\n\n# Compute ROC and AUC using yardstick\nroc_data_tidy <- test_data |>\n  roc_curve(truth = Achievement, Probability_Low_Achievers_tidy)\n\nauc_value_tidy <- test_data |>\n  roc_auc(Achievement, Probability_Low_Achievers_tidy) |>\n  pull(.estimate)\n\n# Plot ROC Curve using ggplot2\nggplot(roc_data_tidy, aes(x = 1 - specificity, y = sensitivity)) +\n  geom_line(color = \"blue\") +\n  geom_abline(linetype = \"dashed\", color = \"red\") +\n  labs(x = \"False Positive Rate\", y = \"True Positive Rate\") + \n  annotate(\"text\", x = 0.75, y = 0.25, \n           label = paste(\"AUC =\", round(auc_value_tidy, 3)), size = 5) + \n  theme_minimal()\n\n\n\n\nFigure 5. ROC Curve for Random Forest Model\n\n\n\n\n\n\n3.5.7 Explainability\nTo extract the variable importance we have to pull the data from the model. In the code below we pull the variable importance data, create a data frame and then use ggplot to visualize it. As you can see, variable importance can be plotted and their interpretation could help guide us understand what actions students take that may be associated with better performance (Figure 4.6).\n\n# Extract the ranger model from the workflow\nimportance_values <- pull_workflow_fit(rf_fit_tidy)$fit$variable.importance\n\n# Convert to a data frame for plotting\nimportance_df <- as.data.frame(importance_values)\nimportance_df$Variable <- rownames(importance_df)\ncolnames(importance_df) <- c(\"Importance\", \"Variable\")\n\n# Plot variable importance\nggplot(importance_df, aes(x = reorder(Variable, Importance), y = Importance)) +\n  geom_bar(stat = \"identity\", fill = \"turquoise\") +\n  coord_flip() +\n  labs(x = \"Variable\", y = \"Importance\") +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_text(color = \"black\"),\n    axis.title = element_text(color = \"black\"),\n    plot.title = element_text(size = 14, face = \"bold\", color = \"black\"))\n\n\n\n\nFigure 6. Variable Importance (tidymodels)\n\n\n\n\n\n\n\n3.6 Tutorial 3: Evaluating multiple models with tidymodels\nIn the next example, we will take advantage of tidymodels framework to fit, evaluate and compare several models. In most practical cases, we will need to estimate several models, evaluate them, compare them and choose the one that performs better.\nIn the next example, we will estimate several machine learning models using the tidymodels ecosystem to streamline the process. The aim —as we did before— is to classify students into high or low achievers based on their engagement indicators. The process will follow a systematic sequence: loading the packages and the data, preparing the data, splitting it into training and testing sets, creating a recipe, defining model specifications, creating workflows, fitting the models, making predictions, evaluating the models, and finally visualizing the results. Given that we are estimating 13 models, we will create functions to save time rather than repeating the same process 13 times in each step, these functions will help automate the repetitive tasks. The following code will explain these steps as follows:\n\n3.6.1 Preparing the data\nThe first step, is similar in each code we discussed before, in which we load the required packages. Also, you may be prompted to install missing packages if some dependencies are not loaded. The required packages here are the packages that provide the engines for the algorithms we are estimating e.g., (ranger, xgboost, kernlab, glmnet, nnet, discrim, naivebayes, randomForest, baguette), and modeling and evaluation tools (yardstick, tidymodels). The engines are the software implementations of the algorithms we are estimating. Then we import the student data which includes the engagement indicators and the final grade and prepare it for analysis (Figure 11.4–B). Given that we are doing a classification task, we need a binary outcome, as before, we add a new column to the data that divides the students into “High_Achievers” or “Low_Achievers” based on whether their final grade is above or below the median (top or bottom 50%).\n\n# Step 1: Load and prepare the data\n# Assuming student_data is already loaded into the environment\nstudent_data <- raw_data |>\n  mutate(Achievement = factor(ifelse(Final_Grade > median(Final_Grade), \n                                     \"High_Achievers\", \"Low_Achievers\"),\n                              levels = c(\"Low_Achievers\", \"High_Achievers\")))\n\n\n\n3.6.2 Splitting the data into training and testing sets\nThen, we split the data into training and testing sets (Figure 11.4–C). We use initial_split from rsample with the option strata to ensure an 80/20 split while stratifying by the Achievement column to maintain the proportion of high and low achievers in both sets. These steps are already familiar by now.\n\n# Step 2: Split the data into training and testing sets\ndata_split <- initial_split(student_data, prop = 0.8, strata = Achievement)\ntrain_data <- training(data_split)\ntest_data <- testing(data_split)\n\n\n\n3.6.3 Creating a recipe\nIn this step, we create a recipe that specifies the predictors (engagement indicators) and the target variable (achievement). The recipe also includes a step to normalize all predictor variables, so that they are measured on the same scale. This normalization step is particularly important for algorithms that are sensitive to the scale of the input features to ensure that features contribute equally to the model. Given that we will use identical recipe for all models, we won’t have to write multiple recipes, this is in fact, one of the powerful features of tidymodels.\n\nmodel_recipe <- recipe(Achievement ~ Freq_Course_View + Freq_Lecture_View + \n                       Freq_Forum_Consume + Freq_Forum_Contribute + \n                       Regularity_Course_View + Regularity_Lecture_View + \n                       Regularity_Forum_Consume + Regularity_Forum_Contribute + \n                       Session_Count + Total_Duration + Active_Days,\n                       data = train_data) |>\n  step_normalize(all_predictors())\n\n\n\n3.6.4 Create all model specifications\nIn this step, we specify the machine learning models through the parsnip package interface [45]. This involves specifying each algorithm and setting the engine and the parameters. We selected some popular algorithms for demonstration purposes. However, for a full list of supported algorithms, please refer to the parsnip [45] documentation. Below is the list of algorithms we will demonstrate, along with their respective engines and packages:\n\nRandom Forest: Random Forest is an ensemble learning method that builds multiple decision trees and merges their predictions by averaging for more accurate and stable results. It is implemented using two engines: ranger from the ranger package [48], known for its speed and performance, and randomForest from the randomForest package, which is the original R implementation [3].\nXGBoost (eXtreme Gradient Boosting): XGBoost is an efficient and scalable implementation of the gradient boosting framework, optimizing performance by iteratively improving model accuracy. It is implemented using the xgboost engine from the xgboost package [49].\nSupport Vector Machines (SVM): SVM is a powerful classification algorithm that finds the best boundary (hyperplane) to separate different classes in the data. It is implemented through the kernlab engine from the kernlab package [50].\nLogistic Regression: Logistic Regression is a well-known classification algorithm that models the probability of a binary outcome. It uses the glm engine from base R.\nK-Nearest Neighbors (KNN): KNN is a simple, non-parametric algorithm that classifies a data point based on the majority class of its nearest neighbors. It is implemented using the kknn engine from the kknn package [51].\nNeural Networks: Neural Networks are computational models inspired by the human brain, capable of modeling complex patterns in data through layers of interconnected nodes. They are implemented using the nnet engine from the nnet package [52].\nDecision Trees: Decision Trees split the data into branches to help make decisions based on the features of the data. They are implemented using the rpart engine from the rpart package [53].\nNaive Bayes: Naive Bayes is a probabilistic algorithm based on Bayes’ theorem, assuming independence between features. It is implemented through the naivebayes engine from the naivebayes package [54].\nLinear Discriminant Analysis (LDA): LDA is used for classification, finding the linear combinations of features that best separate different classes. It uses the MASS engine from the MASS package [55].\nBagged Trees: Bagged Trees is an ensemble method that improves the stability and accuracy of machine learning algorithms by combining multiple models. It uses the rpart engine from the rpart package [53].\nMultivariate Adaptive Regression Splines (MARS): MARS is a non-linear regression technique that models relationships by fitting piecewise linear regressions. It is implemented using the earth engine from the earth package [56].\nBayesian Additive Regression Trees (BART): BART is a Bayesian approach to machine learning that creates a sum-of-trees model for both regression and classification tasks. It is implemented using the dbarts engine from the dbarts package.\n\nThese models cover a wide range of popular approaches, all implemented under the parsnip framework for consistent and streamlined usage.\nBelow is the code that loads the required packags and creates a function specifying each model and its engine, ensuring the full name of the algorithm is used for better display in later steps.\n\nlibrary(tidymodels) # laod the tidymodels framework and its packages\n# Load the required packages for each algorithm\n# Random Forest\nlibrary(ranger) # Engine: ranger, known for its speed and performance\nlibrary(randomForest) # Engine: randomForest, the original R implementation\n# XGBoost (eXtreme Gradient Boosting)\nlibrary(xgboost) # Engine: xgboost, gradient boosting framework\n# Support Vector Machines (SVM)\nlibrary(kernlab) # Engine: kernlab, a powerful classification algorithm\n# Logistic Regression\n# Base R includes glm, no additional package needed\n# K-Nearest Neighbors (KNN)\nlibrary(kknn)  # Engine: kknn, a simple, non-parametric classification algorithm\n# Neural Networks\nlibrary(nnet) # Engine: nnet, inspired by the human brain for complex patterns\n# Decision Trees\nlibrary(rpart) # Engine: rpart, recursive partitioning for classification trees\n# Naive Bayes\nlibrary(discrim) # Engine: naivebayes, based on Bayes' theorem\n# Linear Discriminant Analysis (LDA)\nlibrary(MASS)  # Engine: MASS, classification via linear combinations of features\n# Bagged Trees\n# Uses the same package as Decision Trees: rpart\n# Multivariate Adaptive Regression Splines (MARS)\nlibrary(earth) # Engine: earth, non-linear regression with piecewise linear fits\n# Bayesian Additive Regression Trees (BART)\nlibrary(dbarts) # Engine: dbarts, Bayesian approach creating a sum-of-trees model\nlibrary(baguette)  # Engine: rpart, an ensemble of decision trees\nlibrary(parsnip) # Interface for `tidymodels`\n\nset_classification <- function(x, engine) {\n  x |> set_engine(engine) |> set_mode(\"classification\")\n}\n\n# Function to specify each model and its engine\ncreate_model_specs <- function() {\n  list(\n    \"Random Forest (ranger)\" = rand_forest() |> set_classification(\"ranger\"),\n    \"XGBoost\" = boost_tree() |> set_classification(\"xgboost\"),\n    \"SVM (RBF)\" = svm_rbf() |> set_classification(\"kernlab\"),\n    \"Logistic Regression\" = logistic_reg() |> set_classification(\"glm\"),\n    \"K-Nearest Neighbors\" = nearest_neighbor() |> set_classification(\"kknn\"),\n    \"Neural Network\" = mlp() |> set_classification(\"nnet\"),\n    \"Decision Tree\" = decision_tree() |> set_classification(\"rpart\"),\n    \"Naive Bayes\" = naive_Bayes() |> set_classification(\"naivebayes\"),\n    \"Linear Discriminant Analysis\" = discrim_linear() |> set_classification(\"MASS\"),\n    \"Bagged Tree\" = bag_tree() |> set_classification(\"rpart\"),\n    \"Random Forest (randomForest)\" = rand_forest() |> set_classification(\"randomForest\"),\n    \"MARS\" = mars() |> set_classification(\"earth\"),\n    \"BART\" = parsnip::bart() |> set_classification(\"dbarts\")\n  )\n}\n\n# Create the model specifications\nmodel_specs <- create_model_specs()\n\n\n\n3.6.5 Creating the workflows and fitting the data\nRather than creating a list of 13 workflows, we define a function that automates the process and we use lapply to apply this function to all list items. The function helps easy specification of the models with their respective engines and parameters and the results will be a list which we will also use in further analysis in the same way.\nThe code below has two key functions: create_workflows and fit_models. The create_workflows function builds a workflow for each model specification in the provided model_specs list, using the specified model recipe and lapply to iterate over the list items. This results in a list of workflows. Then, the fit_models function is used to fit each of these workflows to the training data (Figure 11.4–D), again using lapply to apply the function to every workflow in the list.\n\ncreate_workflows <-  function(spec) {\n  workflow() |> add_recipe(model_recipe) |> add_model(spec)\n}\n\nmodel_workflows <- lapply(model_specs, create_workflows)\n\n### Fitting the models\n\nfit_model <- function(workflow) {\n  fit(workflow, data = train_data)\n}\n\nmodel_fits <- lapply(model_workflows, fit_model)\n\n\n\n3.6.6 Evaluating the models\nOnce the models are trained, we can make predictions on the test data. Similar to the previous steps, we create a make_predictions function, and then apply this function to each element of the model_fits list (created in the previous step). The function uses predict to get both class predictions and probabilities, and bind them with the test data for late steps in the evaluation evaluation.\nWe then, evaluate the models (Figure 11.4–E) with a custom function evaluate_model using several metrics as before. Our evaluation will includes computing a confusion matrix, the evaluation metrics (accuracy, sensitivity, specificity, F1 score, balanced accuracy, positive predictive value (PPV), and negative predictive value (NPV)) as well as ROC curve for each algorithm. Given that the function creates several objects for all the model, the function is a but more sophisticated, and uses list lists.\n\nmake_predictions <- function(fit) {\n  predict(fit, test_data) |>\n    bind_cols(predict(fit, test_data, type = \"prob\")) |>\n    bind_cols(test_data)\n}\n\nmodel_preds <- lapply(model_fits, make_predictions)\n\nevaluate_model <- function(pred, model_name) {\n  # Confusion Matrix\n  conf_mat <- conf_mat(pred, truth = Achievement, estimate = .pred_class)\n  \n  # Other metrics (Class metrics only)\n  metrics <- metric_set(accuracy, sens, yardstick::specificity, \n                        f_meas, bal_accuracy, ppv, npv)\n  model_performance <- metrics(pred, truth = Achievement, \n                               estimate = .pred_class)\n  \n  # ROC Curve\n  roc_curve <- roc_curve(pred, truth = Achievement, .pred_Low_Achievers) |>\n    mutate(model = model_name)\n  \n  list(conf_mat = conf_mat, performance = model_performance, roc = roc_curve)\n}\n\n# Evaluate each model and store results\nevaluate_all_models <- function(preds, model_names) {\n  mapply(evaluate_model, preds, model_names, SIMPLIFY = FALSE)\n}\n\nevaluation_results <- evaluate_all_models(model_preds, names(model_preds))\n\n\n3.6.6.1 Combining and Plotting Results\nWe then combine the performance metrics and ROC curve data for all models into single data frames. These data frames will be used to generate visualizations and for comparison if needed.\n\ncombine_performance_metrics <- function(evaluation_results) {\n  performance_df <- do.call(rbind, lapply(evaluation_results, function(res) {\n    res$performance\n  }))\n  performance_df$model <- rep(names(evaluation_results), \n                              times = sapply(evaluation_results, function(res) {\n    nrow(res$performance)\n  }))\n  performance_df\n}\n\nperformance_df <- combine_performance_metrics(evaluation_results)\n\ncombine_roc_curves <- function(evaluation_results) {\n  do.call(rbind, lapply(evaluation_results, function(res) {\n    res$roc\n  }))\n}\n\nroc_df <- combine_roc_curves(evaluation_results)\n\nextract_confusion_matrices <- function(evaluation_results) {\n  lapply(evaluation_results, function(res) {\n    res$conf_mat\n  })\n}\n\nconf_mat_list <- extract_confusion_matrices(evaluation_results)\n\nWe define a color palette for plotting and then create plots for performance metrics and ROC curves.\n\npalette <- c(\"darkgreen\", \"green\", \"cyan\", \"blue\", \"purple\", \"magenta\", \"pink\",\n             \"red\", \"orange\", \"yellow\", \"darkgoldenrod4\", \"grey\", \"black\" )\n\nperformance_df |>\n  dplyr::select(model, .metric, .estimate) |>\n  pivot_longer(cols = .estimate, names_to = \"metric\", values_to = \"value\") |>\n  ggplot(aes(y = .metric, x = value, fill = model)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  scale_fill_manual(values = palette, name = \"Metric\") +\n  theme_minimal() +\n  labs(x = \"Value\", y = \"Model\") \n\n\n\n\nFigure 7. Model Performance Metrics\n\n\n\n\n\nggplot(roc_df, aes(x = 1 - specificity, y = sensitivity, color = model)) +\n  geom_line(size = 0.5) +\n  geom_abline(linetype = \"dashed\") +\n  scale_color_manual(values = palette, name = \"Metric\") +\n  theme_minimal() +\n  labs(x = \"1 - Specificity\", y = \"Sensitivity\") +\n  theme(legend.position = \"right\")\n\n\n\n\nFigure 8. ROC Curves for Different Models\n\n\n\n\nFinally, we plot the confusion matrices for each model using a heatmap.\n\nplot_confusion_matrix <- function(conf_mat, model_name) {\n  autoplot(conf_mat, type = \"heatmap\") +\n    scale_fill_gradient(low = \"white\", high = \"blue\") +\n    theme_minimal() +\n    labs(title = paste(model_name), fill = \"Count\")\n}\n\nlibrary(gridExtra)\n\n# List to store ggplot objects\nplot_list <- list()\n\n# Generate and store each confusion matrix plot\nfor (model_name in names(conf_mat_list)) {\n  conf_mat_plot <- plot_confusion_matrix(conf_mat_list[[model_name]], model_name)\n  plot_list[[model_name]] <- conf_mat_plot\n}\n\n# Combine all plots into one grid\n# Adjust ncol based on how many columns you want\ngrid.arrange(grobs = plot_list, ncol = 3, nrow = 5) \n\n\n\n\nFigure 9. Confusion matrices of each model\n\n\n\n\nYou can compare the algorithms and evaluate which algorithm has better performance and choose the best model based on this analysis."
  },
  {
    "objectID": "chapters/ch04-classification/ch04-classification.html#discussion",
    "href": "chapters/ch04-classification/ch04-classification.html#discussion",
    "title": "4  Artificial Intelligence: Using Machine Learning to Classify Students and Predict Low Achievers",
    "section": "4 Discussion",
    "text": "4 Discussion\nThis chapter explained the application of machine learning, specifically classification techniques, to identify at-risk students in educational settings. We transitioned from the regression-based prediction of student grades discussed in the previous chapter [1], to a focus on identifying risky categories of academic performance namely low achievers.\nThe chapter demonstrated a detailed tutorial on using the Random Forest algorithm for classification. It covered the entire workflow, including data preparation—creating a binary target variable and splitting data into training and testing sets—model training, applying the model to new data, and evaluation using metrics like accuracy, precision, recall and sensitivity. Additionally, the importance of model interpretability was highlighted by showcasing how to extract and visualize variable importance in a Random Forest model. This helps in understanding which factors contribute most significantly to the classification, offering insights into student behaviors that correlate with success or failure. An alternative implementation using the tidymodels framework was presented. This modern approach provides a streamlined and consistent syntax for various machine learning tasks, making it easier to build, compare, and maintain models. Using tidymodels, we demonstrated how to automate the process of fitting, predicting, and evaluating thirteen different classification algorithms, including Random Forest, XGBoost, SVM, Logistic Regression, KNN, Neural Networks, Decision Trees, Naive Bayes, LDA, Bagged Trees, MARS, and BART.\nIf we can accurately early predict low achievers, we can identify those who are struggling early in the course. This allows for proactive interventions, preventing students from falling too far behind and potentially dropping out [17, 57]. Understanding the factors that contribute to a student being classified as “at-risk” can inform personalized support strategies [42, 43, 58]. For example, if low engagement with forum discussions is a significant predictor, educators can encourage participation or encourage students to interact with their peers. For teachers, they can use the classification models to identify struggling students before they fail a course, allowing them to reach out, offer additional support, and address any underlying issues that may be hindering progress. The insights gained from variable importance analysis can help teachers tailor their feedback and resources to address specific student needs. Furthermore, these models may help teachers allocate their time and resources more efficiently by focusing on students who need the most support—particularly valuable in large classes where providing individualized attention can be challenging. also, educators can use the data to initiate conversations about academic progress, identify areas where students are struggling, and collaboratively develop strategies for improvement.\nHowever, it is important to acknowledge potential drawbacks associated with predictive modeling in education. One significant concern is the risk of profiling students and inaccurately labeling them as low achievers when they are not. Misclassifications can have detrimental effects on students’ self-esteem, motivation, and overall educational experience. False positives—students incorrectly identified as at-risk—may receive unnecessary interventions, which can lead to stigmatization or decreased expectations from teachers and peers.\nTherefore, it is important to use predictive models as supportive tools rather than judgments. Integrating human insight with predictive analytics ensures a more nuanced understanding of each student’s needs. We can’t stress enoguh how ethical considerations must be at the forefront when implementing predictive analytics where transparency about how predictions are made, ensuring data privacy, and actively working to mitigate biases in the models are essential steps."
  },
  {
    "objectID": "chapters/ch05-regularization/ch05-regularization.html",
    "href": "chapters/ch05-regularization/ch05-regularization.html",
    "title": "5  Comparative Analysis of Regularization Methods for Predicting Student Certification",
    "section": "",
    "text": "Forthcoming\n\n\n\n\n\nReferences\n\n\n1. Papadakis S (2023) MOOCs 2012-2022: An overview. Advances in Mobile Learning Educational Research 3:682–693\n\n\n2. Deng R, Benckendorff P, Gannaway D (2019) Progress and new directions for teaching and learning in MOOCs. Computers & Education 129:48–60\n\n\n3. Knowles MS (1970) The Modern Practice of Adult Education: Andragogy versus Pedagogy. Englewood Cliffs, NJ: Cambridge Adult Education\n\n\n4. Brooker A, Corrin L, De Barba P, Lodge J, Kennedy G (2018) A tale of two MOOCs: How student motivation and participation predict learning outcomes in different MOOCs. Australasian Journal of Educational Technology 34:\n\n\n5. Spector JM, Johnson TE, Young PA (2014) An editorial on research and development in and with educational technology. Educational Technology Research and Development 62:1–12\n\n\n6. Bingol I, Kursun E, Kayaduman H (2020) Factors for success and course completion in massive open online courses through the lens of participant types. Open Praxis 12:223–239\n\n\n7. Xiong Y, Li H, Kornhaber ML, Suen HK, Pursel B, Goins DD (2015) Examining the relations among student motivation, engagement, and retention in a MOOC: A structural equation modeling approach. Global Education Review 2:23–33\n\n\n8. Han F, Ellis RA (2023) Self-reported and digital-trace measures of computer science students’ self-regulated learning in blended course designs. Education and Information Technologies 28:13253–13268\n\n\n9. Han F, Ellis RA, Pardo A (2022) The descriptive features and quantitative aspects of students’ observed online learning: How are they related to self-reported perceptions and learning outcomes? IEEE Transactions on Learning Technologies 15:32–41\n\n\n10. Siemens G, Gasevic D (2012) Guest editorial-learning and knowledge analytics. Journal of Educational Technology & Society 15:1–2\n\n\n11. Jovanovic J, López-Pernas S, Saqr M (2024) Predictive modelling in learning analytics: A machine learning approach in R. In: Learning analytics methods and tutorials: A practical guide using r. Springer Nature Switzerland Cham, pp 197–229\n\n\n12. Gray CC, Perkins D (2019) Utilizing early engagement and machine learning to predict student outcomes. Computers & Education 131:22–32\n\n\n13. Tibshirani R (1996) Regression shrinkage and selection via the LASSO. Journal of the Royal Statistical Society Series B: Statistical Methodology 58:267–288\n\n\n14. Fan J, Li R (2001) Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association 96:1348–1360\n\n\n15. Zhang C-H (2010) Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics 894–942\n\n\n16. Hastie T, Tibshirani R, Wainwright M (2015) Statistical learning with sparsity. Monographs on Statistics and Applied Probability 143:8\n\n\n17. Fu L, Wang Y-G, Wu J (2024) Recent advances in longitudinal data analysis. Modeling and Analysis of Longitudinal Data 50:173\n\n\n18. Wang Y-G, Wu J, Hu Z-H, McLachlan GJ (2023) A new algorithm for support vector regression with automatic selection of hyperparameters. Pattern Recognition 133:108989\n\n\n19. Huber PJ (1973) Robust regression: Asymptotics, conjectures and Monte Carlo. The Annals of Statistics 799–821\n\n\n20. Jiang Y, Wang Y-G, Fu L, Wang X (2019) Robust estimation using modified Huber’s functions with new tails. Technometrics 61:111–122\n\n\n21. Zou H, Li R (2008) One-step sparse estimates in nonconcave penalized likelihood models. The Annals of Statistics 36:1509\n\n\n22. Hastie T, Tibshirani R, Friedman JH, Friedman JH (2009) The Elements of Statistical Learning: Data mining, Inference, and Prediction. Springer\n\n\n23. Bergstra J, Bengio Y (2012) Random search for hyper-parameter optimization. Journal of Machine Learning Research 13:\n\n\n24. Snoek J, Larochelle H, Adams RP (2012) Practical Bayesian optimization of machine learning algorithms. Advances in Neural Information Processing Systems 25:\n\n\n25. Hand DJ (2012) Assessing the performance of classification methods. International Statistical Review 80:400–414\n\n\n26. Yuan J, Qiu X, Wu J, Guo J, Li W, Wang Y-G (2024) Integrating behavior analysis with machine learning to predict online learning performance: A scientometric review and empirical study. arXiv preprint arXiv:240611847\n\n\n27. Wickham H (2011) ggplot2. Wiley Interdisciplinary Reviews: Computational Statistics 3:180–185\n\n\n28. Pedersen TL (2019) Package “patchwork.” R package http://CRAN R-project org/package= patchwork Cran\n\n\n29. Breheny P, Breheny MP (2024) Package “ncvreg”\n\n\n30. Chans GM, Portuguez Castro M (2021) Gamification as a strategy to increase motivation and engagement in higher education chemistry students. Computers 10:132\n\n\n31. Looyestyn J, Kernot J, Boshoff K, Ryan J, Edney S, Maher C (2017) Does gamification increase engagement with online programs? A systematic review. PloS One 12:e0173403"
  },
  {
    "objectID": "chapters/ch06-xai-global/ch06-xai-global.html",
    "href": "chapters/ch06-xai-global/ch06-xai-global.html",
    "title": "6  Explainable Artificial Intelligence in Education: A Tutorial for Identifying the Variables that Matter",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch06-xai-global/ch06-xai-global.html#introduction",
    "href": "chapters/ch06-xai-global/ch06-xai-global.html#introduction",
    "title": "6  Explainable Artificial Intelligence in Education: A Tutorial for Identifying the Variables that Matter",
    "section": "1 Introduction",
    "text": "1 Introduction\nMachine learning (ML) models and artificial intelligence (AI) applications are increasingly improving their performance and efficiency, often at the expense of becoming more complex [1]. A common drawback of complex models is that they often operate as “opaque boxes,” making it difficult to understand the grounds behind their decisions or predictions[1, 2]. This lack of transparency can be problematic, especially in critical applications where trust, fairness, and accountability are indispensable. In the context of education, we must understand how models predict student performance, recommend learning resources, or profile students according to their behavior [2–4]. Otherwise, without model transparency, there is a risk of reinforcing biases, misinterpreting student actions and abilities, or making unfair decisions about placements and interventions [1]. Explainability ensures that researchers, educators and administrators can trust AI outcomes and make informed decisions [2, 5].\nVariable-importance measures hold significant value in ML explainability. They can transform models from opaque to white-box ones where the inner-workings are understandable, i.e., we can understand how variables explain the model results [6]. In our case that entails knowing which variables are more associated with students’ performance; in other words, what were the learning activities that students did so that they may score higher grades. Further, if we identify the variables that have little or no influence on model predictions, we can simplify our models by excluding irrelevant features. Moreover, model explainability allows us to assess the validity of a model against a theory or hypothesis or discover the factors that may be involved in affecting our outcome beyond our theoretical model. In doing so, explainability helps us to get actionable insights from the data, build informed decisions and expand our theoretical knowledge [6–8].\nIn this chapter, we provide a tutorial on ML explainability in the context of education using two examples. First, we describe a case study on regression using a well-known example: predicting students’ grades. Secondly, we describe the same procedure for classification. It is assumed that the reader is familiar with the R programming language. If that is not the case, it is recommended to refer to previous tutorials on the basics of R [9], data cleaning [10], basic statistics [11], visualization [12], and prediction [15]."
  },
  {
    "objectID": "chapters/ch06-xai-global/ch06-xai-global.html#explainable-ai-in-education",
    "href": "chapters/ch06-xai-global/ch06-xai-global.html#explainable-ai-in-education",
    "title": "6  Explainable Artificial Intelligence in Education: A Tutorial for Identifying the Variables that Matter",
    "section": "2 Explainable AI in education",
    "text": "2 Explainable AI in education\nThe use of explainable AI in educational research is not widespread but its is becoming increasingly prevalent. Studies often employ inherently interpretable algorithms, such as decision trees, to provide global explanations [6]. An example is the work by [16], who used logistic regression to predict factors that would allow to early predict dropout. Since this is not possible for less interpretable algorithms —which often yield higher accuracy— other studies rely on explainable AI to interpret the results of predictive models. For instance, Pellagatti et al. [17] used random forests to predict student dropout. The authors used variable importance to identify which features contributed the most to the predictions, and partial dependence plots to assess how variations in these features influenced the results. Saqr & López-Pernas [2] predicted students’ grades using several algorithms and examined the explainability of each model, the most important variables (variable importance) and how they contributed to the predicted grade (partial dependence plot). The authors also used Shapley Additive explanations to visualize the distribution of contribution for each feature to the predictions across students. In a similar study, used Tiukhova et al. [18] Shapley Additive explanations to explore the stability of the most important features that predict student success in a Naïve Bayes predictive model. Please, refer to Chapter 2 in this book for an in-depth explanation of xAI [1]."
  },
  {
    "objectID": "chapters/ch06-xai-global/ch06-xai-global.html#a-tutorial-on-global-xai-explainability-using-dalex",
    "href": "chapters/ch06-xai-global/ch06-xai-global.html#a-tutorial-on-global-xai-explainability-using-dalex",
    "title": "6  Explainable Artificial Intelligence in Education: A Tutorial for Identifying the Variables that Matter",
    "section": "3 A tutorial on global xAI explainability using DALEX",
    "text": "3 A tutorial on global xAI explainability using DALEX\nIn this section, we provide a tutorial on ML explainability in the context of education using two examples. First, we describe a case study on regression using a well-known example: predicting students’ grades. Secondly, we describe the same procedure for classification. The general workflow followed in each of the examples is the usual ML pipeline, depicted in Figure 11.4, enhanced with the xAI techniques.\n\n\n\nFigure 1. xAI workflow implemented in the tutorial\n\n\n\n3.1 DALEX for student grade prediction\nFor our first example, we use a opaque box model and we show how it can be explained (i.e., white-boxed) using the DALEX (Descriptive mAchine Learning EXplanations) approach [19]. We demonstrate the process of building and evaluating a Support Vector Machine (SVM) regression model using the kernlab engine and analyzing its performance. The estimation follows what we have learned in the previous chapters e.g., [14, 15] to create a predictive model. Briefly, we start by loading the necessary libraries and importing the student data as we mentioned before, the data is based on the study [20] and have been briefly explained in [14] and represent students’ engagement indicators e.g., reading lectures, time spent online etc. This corresponds to the exploratory data analysis (EDA) described in Figure 11.4–A.\nIn the following code, we performed the necessary steps for data preparation (Figure 11.4–B). We beging by standardizing the numeric columns to ensure all features are on the same scale. Then, we split the data into training and testing sets for model evaluation (Figure 11.4–C). Then, define the formula that specifies the relationship between the target variable (Final_Grade) and the predictor variables (engagement indicators). Finally, we fit the SVM model using the radial kernel on the training data (Figure 11.4–D).\n\nset.seed(50)\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(rsample)\nlibrary(e1071) \nlibrary(DALEX)\nlibrary(rio)\n\n# Import the data\nstudent_data <- import(\"https://github.com/lamethods/data2/raw/main/lms/lms.csv\")\n\n# Standardize the numeric columns\nstudent_data_standardized <- student_data |>\n  mutate(across(where(is.numeric), ~scale(.) |> as.vector()))\n\n# Split the data into training and testing sets\ndata_split <- initial_split(student_data_standardized, prop = 0.8)\ntrain_data <- training(data_split)\ntest_data <- testing(data_split)\n\n# Define the formula to specify the relationship between the target variable \n# and the predictor variables\nformula <- Final_Grade ~ Freq_Course_View + Freq_Lecture_View + \n  Freq_Forum_Consume + Freq_Forum_Contribute + Regularity_Course_View +\n  Regularity_Lecture_View + Regularity_Forum_Consume + \n  Regularity_Forum_Contribute + Session_Count + Total_Duration + Active_Days\n\n# Fit the SVM model\nsvm_fit <- svm(formula,\n               data = train_data,\n               kernel = \"radial\")\n\nIn this time, we will not proceed to evaluate the model (Figure 11.4–E) using the traditional methods discussed in the previous chapters [14, 15], because DALEX has its own evaluation and explanation approach that makes this process easy and straightforward. Instead, we will create an explainer. An explainer is the first step in evaluating and explaining the model. Given that models create different output formats, an explainer standardizes model outputs making them uniform across different algorithms. In other words, the explainer object combines all the necessary components for evaluating a ML model (i.e., the model, the data, and the target variable) into a single uniform object. Whether comparing multiple models or analyzing a single model over different datasets, the explainer provides a standardized way to approach these tasks. Further, when we create the explainer object, it gives us access to the full potential of the DALEX capabilities and access to an extensive suite of diagnostic tools. These tools include variable importance measures, partial dependence plots, accumulated local effects plots, break down plots, SHAP values, ceteris paribus profiles, and more.\nCreating an explainer is a straightforward process. We use the explain function with a few arguments. The first argument is the model, where we pass the svm_fit object representing the trained SVM regression model. The data argument specifies the holdout dataset which will be used for evaluating the model (test_data) excluding the target variable Final_Grade. The y argument tells DALEX what is the target variable that we are predicting and we specify that by passing the target variable (test_data$Final_Grade). The next arguments are optional, the label argument assigns a readable label to the explainer object which will be helpful in identifying the model in plots and summaries, especially when working with multiple models. The verbose argument specifies the level of details of the function’s output. When verbose = FALSE, the function suppresses output messages.\n\nset.seed(50)\n\nlibrary(DALEX)\n# Create an explainer with DALEX\nexplainer_svm <- explain(\n  svm_fit,\n  data = test_data |> dplyr::select(-Final_Grade),\n  y = test_data$Final_Grade,\n  label = \"SVM\",\n  verbose = FALSE\n)\nprint(explainer_svm)\n\nModel label:  SVM \nModel class:  svm.formula,svm \nData head  :\n  Freq_Course_View Freq_Forum_Consume Freq_Forum_Contribute Freq_Lecture_View\n1       -1.4795970         -1.6037982             -1.476917        -0.1035753\n2        0.3237727          0.9709293              2.284040         0.9374399\n  Regularity_Course_View Regularity_Lecture_View Regularity_Forum_Consume\n1            -1.04549094               0.0204884               -0.7421165\n2             0.08799424              -0.2511021                0.8384102\n  Regularity_Forum_Contribute Session_Count Total_Duration Active_Days\n1                  -1.3787562    -1.4299170     -1.8650048  -0.7372311\n2                   0.7258669     0.7089812      0.8587155   0.2254762\n\n\nAlternatively (in the alternative code chunk below), given that DALEX works also with tidymodels. The same result can be also achieved using this package, where we can specify the model, create a workflow and then, fit the model as we have seen in the previous chapters [14] and then create an explainer.\n\n# Alternative code chunk\nset.seed(50)\n\n#Alternative model estimation for SVM\n\nlibrary(tidymodels)\n# Define SVM model specification with a radial basis function kernel\nsvm_specification <- svm_rbf() |>\n  set_mode(\"regression\") |>\n  set_engine(\"kernlab\")\n\n# Create the workflow. Combine the model specification with the formula\nsvm_workflow <- workflow() |>\n  add_model(svm_specification) |>\n  add_formula(formula)\n\n# Fit the SVM model\nsvm_fit_tidy <- svm_workflow |>\n  parsnip::fit(data = train_data)\n\n# Create an explainer with DALEX\nexplainer_svm_tidy <- explain(\n  svm_fit_tidy,\n  data = test_data |> dplyr::select(-Final_Grade),\n  y = test_data$Final_Grade,\n  label = \"SVM\",\n  verbose = FALSE)\n\n# print(explainer_svm_tidy) # Uncomment to see the output\n\nHaving created the explainer, we can use it with DALEX for a range of uses and, most importantly, for model explaining.\n\n3.1.1 Model evaluation\nDALEX offers an easy interface for calculating model performance. To evaluate a model, we simply call the function model_performance() which takes one argument, the explainer object (explainer_svm) which we created earlier. The model_performance() function calculates the performance metrics for the regression task: Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared (R²), and Mean Absolute Error (MAE). The result of this function call is assigned to a new object called model_performance_svm which we print to display the calculated performance metrics.\nAs the results show, The model’s performance is moderate, with an R² of about 0.41 meaning it captures some of the patterns in the data, but there is still a significant amount of unexplained variance. The RMSE of about 0.71 gives us an idea of the typical prediction errors (remember that the grades were scaled). The distribution of residuals suggests that the model’s errors are fairly balanced between over-predictions and under-predictions, which is good. However, there are some larger errors at the extremes (as seen in the 0% and 100% percentiles of the residuals).\n\nset.seed(50)\n\n# Calculate and print model performance metrics for the SVM model\nmodel_performance_svm <- model_performance(explainer_svm)\nprint(model_performance_svm)\n\nMeasures for:  regression\nmse        : 0.5556946 \nrmse       : 0.7454493 \nr2         : 0.4063128 \nmad        : 0.4442718\n\nResiduals:\n         0%         10%         20%         30%         40%         50% \n-1.64093297 -0.58085583 -0.41026001 -0.07698939  0.07855870  0.28133263 \n        60%         70%         80%         90%        100% \n 0.34303022  0.51622444  0.76114088  1.06787159  1.81745294 \n\n\nTo visualize these metrics, we use DALEX to create several types of performance plots (Figure 6.2).\nThe Lift Curve demonstrates how well the model ranks predictions compared to random guessing; a higher curve indicates better performance. The curve starts with a brief dip near zero positive rate, indicating a small initial under-performance then rises sharply to a peak lift of about 4.5 at around 10% positive rate. This suggests the model is particularly effective at identifying the top 10% of most likely positive instances. From this peak, the curve gradually declines as the positive rate increases, maintaining a lift above 1 (random selection) for most of its range (i.e., continues to outperform random selection until very high positive rates.). The curve’s shape, starting high and slowly descending, is characteristic of a well-performing predictive model. This lift curve suggests that the SVM model has good discriminating power and could be particularly useful in scenarios where identifying the most likely positive cases is important.\nThe Residuals Boxplot visualizes the spread and outliers of residuals, ideally showing minimal outliers and a symmetric distribution around zero. The red dot represents the RMSE, a measure of the model’s average prediction error. Its position around 0.7-0.8, suggesting moderate prediction accuracy. Overall, this boxplot indicates that while the SVM model’s residuals are somewhat symmetrically distributed around zero, there is a slight tendency for the model to underpredict (positive residuals) more often than overpredict.\nThe Residuals Histogram provides a frequency distribution of residuals, ideally normal around zero. The residuals histogram reveals that most residuals are not very centered around zero. The histogram shows an asymmetrical distribution with more residuals on the positive side, suggesting a potential bias in the model where it tends to underpredict more frequently than overpredicting. The concentration is noticeable most at around 1, suggests a tendency for the model to underpredict grades.\nThe Cumulative Gain Chart shows the cumulative gain achieved by the model across percentiles, with curves above the diagonal suggesting better predictive power. Finally, the Empirical Cumulative Distribution Function (ECDF) plot illustrates the cumulative probability of residuals, helping to understand their distribution across values.\nWhile visualizing model performance using plots can be beneficial, especially for comparing multiple models, interpreting single-model plots might be challenging. However, performance plots may not provide actionable insights to help tune the model or adjust data.\n\nset.seed(50)\n\n# Generate various model performance plots\nplot(model_performance_svm, geom = \"lift\") \nplot(model_performance_svm, geom = \"gain\") \nplot(model_performance_svm, geom = \"boxplot\") \nplot(model_performance_svm, geom = \"histogram\")\nplot(model_performance_svm, geom = \"ecdf\")\n\n\n\n\n\n\n\n(a) Lift Curve - SVM Model\n\n\n\n\n\n\n\n(b) Cumulative Gain Chart - SVM Model\n\n\n\n\n\n\n\n\n\n(c) Residuals Boxplot - SVM Model\n\n\n\n\n\n\n\n(d) Residuals Histogram - SVM Model\n\n\n\n\n\n\n\n\n\n(e) Empirical Cumulative Distribution Function - SVM Model\n\n\n\n\nFigure 2. Performance plots\n\n\n\n\n\n3.1.2 Model explainability\nThe DALEX way of explaining models is built on Leo Breiman’s variable-importance measure for random forests [21]. The method uses permutation to assess the significance of explanatory variables. The algorithm changes a variable’s values several times and assesses the resulting impact on model performance. The greater the degradation in performance, the more important the variable is to the prediction task. These methods —while straightforward— offers an effective and robust method for model-agnostic explanations. In doing so, it can be applied to explain any model regardless of the model explainability.\nTo compute and visualize the importance of our SVM model, we use the model_parts function. model_parts is the main function for generating global (i.e., model level) model explanations (contrast those to local explanation in [22] ). The function accepts several arguments, but only one is required: the explainer object we previously created (explainer_svm). In its simplest form, you can call model_parts(explainer_svm), with all other options set to their default values. For instance, loss_function, is set by default to loss_root_mean_square indicating that the drop in RMSE will be used to measure the model’s performance.\nWe store the results of the model_parts in the vi_svm object which contains the calculated variable importance and then pass it to the plot function. Other arguments may need to be specified, for instance, the B argument (defaults to 10) controls the number of permutations used in the calculation, higher numbers (e.g., 500) are always recommended to increase the robustness of the results but may be computationally demanding. Also, the type argument can be specified to change the type of feature importance. By default (\"raw\"), provides raw drop loss values or \"ratio\" to return the ratio of drop loss to the full loss in model, while \"difference\" returns drop loss in the full model. As the results show (Figure 6.3), the most important variables are contributing to forums, regularity in course view, and in contributing to forums as well as session count and duration. These variables are consistent across all methods and they represent cognitive engagement (contributing to problem solving through forums) and time investment in the course, all are consistent with the hypothesis that cognitive engagement is more likely to result in higher achievement.\n\nset.seed(50)\n\n# Default model\nvi_svm <- model_parts(explainer_svm)\nplot(vi_svm)\n\n# Same as before with all arguments specified \nvi_svm <- model_parts(explainer_svm, loss_function = loss_root_mean_square, \n                      B = 100)\nplot(vi_svm)  \n\n# other variations: difference and ratio \nvi_svm_difference <- model_parts(explainer_svm, loss_function = loss_root_mean_square, \n                                 B = 100, type = \"difference\")\nplot(vi_svm_difference)  \n\nvi_svm_ratio <- model_parts(explainer_svm, loss_function = loss_root_mean_square, \n                            B = 100, type = \"ratio\")\nplot(vi_svm_ratio)\n\n\n\n\n\n\n\n(a) Default model\n\n\n\n\n\n\n\n(b) All arguments specified\n\n\n\n\n\n\n\n\n\n(c) Difference\n\n\n\n\n\n\n\n(d) Ratio\n\n\n\n\nFigure 3. Global explanations\n\n\n\n\n3.1.2.1 Creating Custom Loss Functions for DALEX\nWhereas the default is loss function in DALEX is based on RMSE, we can also use different metrics to assess the model loss. In the next code chunk, we show how to build two custom functions. These custom functions are used to compute variable importance for the SVM model. In particular, we build two custom loss functions: one for MAE, which calculates the mean of absolute differences between observed and predicted values, and another for MAD, which computes the median of these absolute differences. The results are visualized by creating plots for the MAE-based and MAD-based variable importance (Figure 6.4). The choice of loss function can significantly impact the interpretation of variable importance, so researchers need to select a function that aligns with the data and analysis goals.\n\nset.seed(50)\n\n# Load necessary libraries\nlibrary(DALEX)\nlibrary(ggplot2)\n\n# Define the custom MAE loss function\nloss_mae <- function(observed, predicted) {\n  # Compute the Mean Absolute Error\n  mean(abs(observed - predicted))\n}\n\n# Define the custom MAD loss function\nloss_mad <- function(observed, predicted) {\n  # Compute the Median Absolute Deviation\n  median(abs(observed - predicted))\n  #mean(abs(predicted - median(predicted))) another version\n}\n\n# Compute variable importance using the custom MAE loss function\nvi_svm_mae <- model_parts(explainer_svm, loss_function = loss_mae)\n\n# Compute variable importance using the custom MAD loss function\nvi_svm_mad <- model_parts(explainer_svm, loss_function = loss_mad)\n\n# Plot the results for MAE\nplot(vi_svm_mae) \n\n# Plot the results for MAD\nplot(vi_svm_mad)\n\n\n\n\n\n\n\n(a) MAE\n\n\n\n\n\n\n\n(b) MAD\n\n\n\n\nFigure 4. Variable Importance with Custom loss functions\n\n\n\n\n\n\n3.1.3 Model selection: creating and evaluating several ML models\nWe have previously explored how to estimate, evaluate, and explain a single ML model. However, explainability techniques allow us to make an informed decision about the most suitable model beyond simple performance metrics. To showcase this potential, we will estimate several ML models, evaluate their performance, and then select and explain the best-performing model. Since we have already covered the detailed process of estimating multiple ML models in the previous chapters [14, 15], we will not revisit that process in depth. The following code estimates seven models:\n\nLinear Regression: Fits a linear model to our data using the lm function available in base R.\nDecision Tree: Implements a decision tree model suitable for regression tasks with the rpart package [23].\nRandom Forest: Builds a forest model using the randomForest package [24].\nSupport Vector Machine: Estimates a support vector machine model using the svm function of the e1071 package [25].\nK-Nearest Neighbors: Estimates a k-nearest neighbors model via the kNN function [26].\nGradient Boosting: Introduces gradient boosting with the gbm package [27].\nXGBoost: Trains an efficient and scalable version of gradient boosting with the xgboost package [28].\n\nFor the XGBoost model, a special approach is required. First, we convert our data into matrix format using model.matrix, excluding the first column to avoid the dummy variable. We then create train_matrix and test_matrix for our training and test sets and extract the target variable into train_label. Finally, the xgboost function is used to train the model.\n\nset.seed(50)\n\n# Load necessary packages\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(rpart)\nlibrary(randomForest)\nlibrary(e1071)\nlibrary(kknn)\nlibrary(gbm)\nlibrary(xgboost)\nlibrary(DALEX)\n\n# Linear Regression\nlinear_model <- lm(formula, data = train_data)\n\n# Decision Tree\ndecision_tree_model <- rpart(formula, data = train_data, method = \"anova\")\n\n# Random Forest\nrandom_forest_model <- randomForest(formula, data = train_data, ntree = 100)\n\n# Support Vector Machine\nsvm_model <- svm(formula, data = train_data)\n\n# k-Nearest Neighbors\nknn_model <- train(formula, data = train_data, method = \"kknn\", tuneLength = 5)\n\n# Gradient Boosting Machine\ngbm_model <- gbm(formula, data = train_data, distribution = \"gaussian\", \n                 n.trees = 100, interaction.depth = 3, shrinkage = 0.1, cv.folds = 5)\n\n# XGBoost\ntrain_matrix <- model.matrix(formula, data = train_data)[,c(-1)]\ntest_matrix <- model.matrix(formula, data = test_data)[,c(-1)]\n\ntrain_label <- train_data$Final_Grade\nxgboost_model <- xgboost(data = train_matrix, label = train_label, nrounds = 100, \n                         objective = \"reg:squarederror\", verbose = 0)\n\n\n\n3.1.4 Multiple models\n\n3.1.4.1 Creating multiple explainers\nIn our previous step, we constructed several ML models. To understand how each model makes predictions, we need to create an explainer for each model. We do so using the explain function to generate these explainers in the same way we did before. We begin by specifying the model, the test data (excluding the target variable Final_Grade), and a descriptive label to identify the model when multiple models are visualized or compared.\nThe below code creates an explainer_linear for the linear regression model using the test data without the Final_Grade column, and set a label indicating \"Linear Regression\". We then replicate this process for the decision tree, random forest, SVM, k-nearest neighbors, gradient boosting, and XGBoost models, creating respective explainers for each. For XGBoost, we use the pre-created test data matrix, the XGBoost model, test data matrix. Finally, we use the print function to provide summary information about the explainer and allow us to verify its correct configuration.\n\nset.seed(50)\n\n# Create `DALEX`explainers\nexplainer_linear <- explain(\n  model = linear_model, y = test_data$Final_Grade,\n  data = dplyr::select(test_data, -Final_Grade),\n  label = \"Linear Regression\", verbose = FALSE\n)\n\nexplainer_decision_tree <- explain(\n  model = decision_tree_model, y = test_data$Final_Grade,\n  data = dplyr::select(test_data, -Final_Grade),\n  label = \"Decision Tree\", verbose = FALSE\n)\n\nexplainer_random_forest <- explain(\n  model = random_forest_model, y = test_data$Final_Grade,\n  data = dplyr::select(test_data, -Final_Grade),\n  label = \"Random Forest\", verbose = FALSE\n)\n\nexplainer_svm <- explain(\n  model = svm_model, y = test_data$Final_Grade,\n  data = dplyr::select(test_data, -Final_Grade),\n  label = \"SVM\", verbose = FALSE\n)\n\nexplainer_knn <- explain(\n  model = knn_model$finalModel, y = test_data$Final_Grade,\n  data = dplyr::select(test_data, -Final_Grade),\n  label = \"k-NN\", verbose = FALSE\n)\n\nexplainer_gbm <- explain(\n  model = gbm_model, y = test_data$Final_Grade,\n  data = dplyr::select(test_data, -Final_Grade),\n  label = \"GBM\", verbose = FALSE\n)\n\nexplainer_xgboost <- explain(\n  model = xgboost_model, y = test_data$Final_Grade,\n  data = test_matrix, label = \"XGBoost\", verbose = FALSE\n)\n\n\n\n3.1.4.2 Evaluation of multiple models\nIn the next step, we evaluate the performance of the multiple ML models (Figure 11.4–E). For each model of our models (linear regression, decision tree, random forest, support vector machine, k-nearest neighbors, gradient boosting machine, and XGBoost), the code calculates performance metrics using the model_performance function. The process is simply a replication of what we have seen for the single model. The model_performance function then generates the performance metrics for each model. The performance metrics are then stored in separate variables for each model, allowing for easy comparison and analysis of how well each model performs on the given dataset. We extract and combine these metrics in a table for comparison (Table 6.1).\n\nset.seed(50)\n\n# Calculate performance metrics for each model\nperformance_linear <- model_performance(explainer_linear)\nperformance_decision_tree <- model_performance(explainer_decision_tree)\nperformance_random_forest <- model_performance(explainer_random_forest)\nperformance_svm <- model_performance(explainer_svm)\nperformance_knn <- model_performance(explainer_knn)\nperformance_gbm <- model_performance(explainer_gbm)\nperformance_xgboost <- model_performance(explainer_xgboost)\n\n# Combine performance metrics into a single table\nperformance_metrics <- rbind(\n  `Linear Regression` = as.array(performance_linear$measure),\n  `Decision Tree` = performance_decision_tree$measures, \n  `Random Forest` = performance_random_forest$measures,\n  `SVM` = performance_svm$measures$r2, \n  `KNN` = performance_knn$measures,\n  `GBM` = performance_gbm$measures,\n  `XGBoost` = performance_xgboost$measures)  \n\n# Print the combined dataframe\nprint(performance_metrics)\n\n\n\n\n\nTable 1. Performance metrics of each model\n\n\nModel\nmse\nrmse\nr2\nmad\n\n\n\n\nLinear Regression\n0.56\n0.75\n0.41\n0.48\n\n\nDecision Tree\n0.63\n0.79\n0.33\n0.48\n\n\nRandom Forest\n0.51\n0.71\n0.46\n0.42\n\n\nSVM\n0.41\n0.41\n0.41\n0.41\n\n\nKNN\n0.57\n0.75\n0.39\n0.54\n\n\nGBM\n0.49\n0.70\n0.47\n0.42\n\n\nXGBoost\n0.50\n0.71\n0.46\n0.39\n\n\n\n\n\n\nThe performance metrics for these models reveal interesting insights into their predictive capabilities. XGBoost emerges as the top performer, closely followed by Gradient Boosting Machine (GBM) and Random Forest. This pattern suggests that ensemble methods are good for this dataset, likely due to their ability to capture complex relationships and reduce overfitting. Interestingly, Linear Regression model performs rather well compared to the more complex algorithms like SVM and K-Nearest Neighbors (KNN). This suggests that while there are certainly non-linear aspects to the data, there are also significant linear relationships. The KNN model’s performance is somewhat moderate, with a high Mean Absolute Deviation (MAD). This could indicate that the model is sensitive to outliers although we performed feature scaling. It is clear that no single model dominates across all metrics. Moreover, the respectable performance of Linear Regression reminds us that simpler models shouldn’t be discounted, especially when interpretability is a concern. The plots of the model’s performance confirm these findings (Figure 6.5).\n\n\n\n\nset.seed(50)\n\n# Create combined plots\n# Lift plot\nplot(performance_linear, performance_decision_tree, performance_random_forest,\n     performance_svm, performance_knn, performance_gbm, performance_xgboost,\n    geom = \"lift\") \n\n# Gain plot\nplot(performance_linear, performance_decision_tree, performance_random_forest,\n     performance_svm, performance_knn, performance_gbm, performance_xgboost,\n     geom = \"gain\") \n\n# Residuals plot\nplot(performance_linear, performance_decision_tree, performance_random_forest,\n     performance_svm, performance_knn, performance_gbm, performance_xgboost,\n     geom = \"boxplot\"\n)\n\n# ecdf curve\nplot(performance_linear, performance_decision_tree, performance_random_forest,\n     performance_svm, performance_knn, performance_gbm, performance_xgboost,\n     geom = \"ecdf\"\n)\n\n# histogram \nplot(performance_linear, performance_decision_tree, performance_random_forest,\n     performance_svm, performance_knn, performance_gbm, performance_xgboost,\n     geom = \"histogram\" \n) \n\n\n\n\n\n\n\n(a) Lift Curves\n\n\n\n\n\n\n\n(b) Cumulative Gain Charts\n\n\n\n\n\n\n\n\n\n(c) Residuals\n\n\n\n\n\n\n\n(d) Reverse cumulative distribution\n\n\n\n\n\n\n\n\n\n(e) Histogram plot\n\n\n\n\nFigure 5. All models\n\n\n\nThe lift plot shows a lot of overlap, same with the cumulative gain chart. Of course, we see some models perform well across some ranges but nothing consistent. The box plot of residuals show that XGBoost, GBM, and Random Forest have the best performance, with the smallest and most compact boxes, indicating lower and more consistent residuals. Their median lines (the vertical line inside each box) are closest to zero, suggesting more accurate predictions overall. The red dots represent the root mean square of residuals for each model, providing a single metric for comparison. These dots align well with the overall box plot distributions, confirming the relative performance of the models. Overall, this visualization reinforces the strengths of ensemble methods like XGBoost and GBM for this particular dataset. The reverse cumulative distribution plot offers a visual comparison of residual performance across the ML models. The graph’s leftward shift indicates better model performance, with XGBoost, GBM, and Random Forest as top performers, aligning with earlier metric analyses. Linear Regression and SVM show similar, middling performance, while k-NN and Decision Tree lag behind. The y-axis represents the percentage of predictions with residuals exceeding the x-axis value which gives an idea about the error distribution.\nTo conclude, it seems that XGBoost is the best performing model, and so we will demonstrate how to explain it using the same methods we learnt before.\n\n\n3.1.4.3 Explaining the XGBoost model\nThe next step replicates what we have already done for the SVM model. The code computes and visualizes the important variables using the default and the custom loss functions. The variable importance is calculated three times: for RMSE (the default function), MAD, and MAE (the custom loss functions). We use the model_parts function with the explainer_xgboost to generate variable importance scores. The parameter B=500 indicates that 500 permutations are performed to assess importance. The results are stored in vi_xgboost, vi_xgboost_mad, and vi_xgboost_mae, corresponding to the different loss functions. Finally, these results are plotted with titles indicating the loss function used (Figure 6.6).\n\nset.seed(50)\n\n# Compute and plot variable importance using the custom RMSE loss function\nvi_xgboost <- model_parts(explainer_xgboost, loss_function = loss_root_mean_square,\n                          B=500, type = \"ratio\")\nplot(vi_xgboost)  \n\n# Compute and plot variable importance using the custom MAD loss function\nvi_xgboost_mad <- model_parts(explainer_xgboost, loss_function = loss_mad,  B=500)\nplot(vi_xgboost_mad)\n\n# Compute and plot variable importance using the custom MAE loss function\nvi_xgboost_mae <- model_parts(explainer_xgboost, loss_function = loss_mae,B=500)\nplot(vi_xgboost_mae) \n\n\n\n\n\n\n\n(a) RMSE\n\n\n\n\n\n\n\n(b) MAD\n\n\n\n\n\n\n\n(c) MAE\n\n\n\n\nFigure 6. XGBoost Variable importance using custom loss function\n\n\n\n\n\n\n3.1.5 Partial-dependence Profiles\nTwo types of plots can help explain impact of different engagement variables on students’ grades (Figure 11.4–F): Partial Dependence Plots (PDPs) and Accumulated Local Effects (ALE) plots. Both plots aim to illustrate how the predicted outcome (grades) changes in response to variations in specific predictor variables (such as engagement indicators), but they do so in slightly different ways, each with its own strengths.\n\n3.1.5.1 Partial Dependence Plots (PDPs)\nPDPs are designed to show the average effect of a single feature on the predicted outcome. This is useful for understanding the overall trend or relationship between a specific feature (e.g., frequency of forum contributions) and the outcome (grades). For instance, in the generated PDPs, you might observe that increasing the frequency of forum contributions has a positive impact on the predicted grades, indicating that students who participate more in forums tend to achieve higher grades. Conversely, a flat line in a PDP, such as the one you might see for the “active days” variable, suggests that changes in this feature do not significantly affect the outcome. In general, the regularity of activity often seems more impactful than raw frequency or duration. For instance, regularity of course and lecture viewing show positive associations with grade predictions. The PDPs can be generated by the code using model_profile and plot(pdp_xgboost) with the test data excluding the outcome variable.\n\n\n3.1.5.2 Accumulated Local Effects (ALE) Plots\nALE plots offer a more refined alternative to PDPs, particularly when dealing with correlated features (Figure 6.7). Unlike PDPs, which show the average effect of a feature, ALE plots focus on the local changes in predictions as the feature varies. This approach accounts for the potential interactions between features and provides a more accurate interpretation when features are correlated. ALE can be created in the same way, but with the option type = \"accumulated\". This makes ALE plots particularly useful in complex educational datasets where many engagement variables might be interrelated.\n\n# Partial Dependence Plots\npdp_xgboost <- model_profile(explainer_xgboost, \n                variables = colnames(test_data)[colnames(test_data) != \"Final_Grade\"])\nplot(pdp_xgboost) + labs(subtitle = \"\")\n\n# Accumulated Local Effects (ALE) Plots\nale_xgboost <- model_profile(explainer_xgboost, type = \"accumulated\", \n                variables = colnames(test_data)[colnames(test_data) != \"Final_Grade\"])\nplot(ale_xgboost) + labs(subtitle = \"\")\n\n\n\n\n\n\n\n(a) Partial Dependence Plots\n\n\n\n\n\n\n\n(b) Accumulated Local Effects (ALE) Plots\n\n\n\n\nFigure 7. PDPs vs. ALE plots\n\n\n\nThe ALE plots might show that, even after accounting for other correlated behaviors, frequent contributions to forums still have a significant positive effect on grades, reinforcing the importance of this type of engagement. PDPs and ALEs can also be plotted while clustering the data (Figure 6.8). For instance the k=3 argument in the code specifies that the feature values should be divided into three clusters for calculating Accumulated Local Effects (ALE). This means that the ALE plots will show how the predicted outcome changes as the feature varies within each of these three clusters, providing a view of the heterogeneity of predictor outcome effect.\n\n# Accumulated Local Effects (ALE) Plots\nale_xgboost <- model_profile(explainer_xgboost, type = \"accumulated\", \n                variables = colnames(test_data)[colnames(test_data) != \"Final_Grade\"], \n                k = 3)\nplot(ale_xgboost) + labs(subtitle = \"\")\n\n\n\n\nFigure 8. ALE plot with 3 clusters\n\n\n\n\n\n\n\n\n3.2 Explaining a classification model\nExplaining a classification model goes almost the same way as regression with the necessary modifications, e.g., specifying the correct target. The next code estimates a Gradient Boost Model (GBM) in the same way we explained before in the classification chapter. The code begins by loading the necessary libraries, setting a random seed is set to ensure consistent results. Data is imported from a CSV file, and a new binary classification variable, Achievement, is created based on whether a student’s Final_Grade is above or below the median (top 50% of achievement level). The Achievement variable is then converted to numeric format to be suitable for model training (Figure 11.4–B). The dataset is split into training and testing sets (Figure 11.4–C), with 80% allocated for training while ensuring class distribution is preserved. Finally, a GBM model is fitted using the engagement predictors to estimate the binary outcome (Figure 11.4–D). The model is configured with a Bernoulli distribution for binary classification, 1000 trees, a tree depth of 3, a learning rate of 0.01, and a bagging fraction of 0.7. These values are chosen for demonstration.\n\n# Load necessary libraries\nlibrary(gbm)\nlibrary(rio)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(DALEX)\nlibrary(rsample)\n\n# Set seed for reproducibility\nset.seed(50)\n\n# Load data\nstudent_data <- import(\"../student_data.csv\")\n\n# Create binary classification target variable\nstudent_data <- student_data |>\n  mutate(Achievement = ifelse(Final_Grade > median(Final_Grade), \n                              \"High_Achievers\", \"Low_Achievers\")) |>\n  mutate(Achievement = factor(Achievement, \n                              levels = c(\"High_Achievers\", \"Low_Achievers\")))\n\n# Convert target to numeric for model fitting\nstudent_data$Achievement_numeric <- as.numeric(student_data$Achievement) - 1\n\n# Split data into training and testing sets\ndata_split <- initial_split(student_data, prop = 0.8, strata = Achievement_numeric)\ntrain_data <- training(data_split)\ntest_data <- testing(data_split)\n\n# Fit GBM model\ngbm_model <- gbm(\n  Achievement_numeric ~  Freq_Course_View + Freq_Lecture_View + Freq_Forum_Consume + \n    Freq_Forum_Contribute + Regularity_Course_View + Regularity_Lecture_View + \n    Regularity_Forum_Consume + Regularity_Forum_Contribute +\n    Session_Count + Total_Duration + Active_Days,\n  data = train_data,\n  distribution = \"bernoulli\",  # For binary classification\n  n.trees = 1000,\n  interaction.depth = 3,\n  shrinkage = 0.01,\n  bag.fraction = 0.7\n)\n\nSimilar to what we did before, we create an explainer for the fitted Gradient Boosting Model (GBM). The explain function takes the estimated gbm_model and the test data (excluding the target variable and the final grade). The explainer is labeled as “GBM Model” for identification. Subsequently, the model’s performance is assessed using the model_performance function, which computes the classification performance metrics (Figure 11.4–E) based on the explainer object. The results are then printed out to provide insights into how well the GBM model performs on the test data.\n\n# Create `DALEX` explainer\nexplainer_gbm <- explain(\n  model = gbm_model,\n  data = test_data[, 1:11],\n  y = test_data$Achievement_numeric,\n  label = \"GBM Model\"\n)\n\nPreparation of a new explainer is initiated\n  -> model label       :  GBM Model \n  -> data              :  58  rows  11  cols \n  -> target variable   :  58  values \n  -> predict function  :  yhat.gbm  will be used (  default  )\n  -> predicted values  :  No value for predict function target column. (  default  )\n  -> model_info        :  package gbm , ver. 2.2.2 , task classification (  default  ) \n  -> predicted values  :  numerical, min =  0.01791613 , mean =  0.5135735 , max =  0.9899749  \n  -> residual function :  difference between y and yhat (  default  )\n  -> residuals         :  numerical, min =  -0.9362661 , mean =  -0.01357354 , max =  0.9794141  \n  A new explainer has been created!  \n\n# Model performance\nmodel_performance_gbm <- model_performance(explainer_gbm)\nprint(model_performance_gbm)\n\nMeasures for:  classification\nrecall     : 0.7931034 \nprecision  : 0.7666667 \nf1         : 0.779661 \naccuracy   : 0.7758621 \nauc        : 0.8133175\n\nResiduals:\n          0%          10%          20%          30%          40%          50% \n-0.936266132 -0.537579796 -0.400499445 -0.149711621 -0.054921553 -0.003945525 \n         60%          70%          80%          90%         100% \n 0.051416919  0.101556426  0.251119244  0.505825858  0.979414109 \n\n\n\nThe recall of 79.31% means that the model correctly identifies 79.31% of the low achievers, showing strong sensitivity in detecting students who might struggle.\nThe precision of 76.67% shows that, out of all the students predicted to be low achievers, 76.67% were correctly classified, highlighting the model’s reliability.\nThe F1-score of 77.97%, which balances both recall and precision, suggests the model performs well in identifying low achievers overall.\nThe accuracy of 77.59% indicates that the model correctly classifies 77.59% of all students, whether they are high or low achievers.\nThe AUC of 81.33% further confirms the model’s ability to effectively distinguish between low and high achievers, with a score over 80% generally indicating strong performance.\n\nWe can also visualize the results by using the plot function with the model_performance_gbm object and specifying the desired geometry (geom). The resulting plots (Figure 6.9) show that the ROC curve is well above the diagonal (the dashed line), indicating good model performance with high true positive rates even at low false positive rates, suggesting strong discriminating power. The residuals boxplot is fairly symmetrical around the zero line, which is favorable. The lift curve starts high (around 2) and gradually decreases, remaining above 1 throughout, meaning the model consistently outperforms random selection in identifying positive cases. The cumulative gain chart rises steeply at first and then flattens, significantly outperforming the diagonal baseline, indicating that the model effectively identifies a large proportion of positive cases early in its ranked predictions. The precision-recall curve shows high precision at low recall values, followed by a gradual decline as recall increases, suggesting a trade-off between precision and recall. The residuals histogram distribution is roughly symmetrical and centered near zero, which is desirable, with some outliers. Lastly, the empirical cumulative distribution curve indicates that a large proportion of residuals are concentrated around zero, with relatively few extreme values.\n\n# Create individual plots\nplot(model_performance_gbm, geom = \"roc\")\nplot(model_performance_gbm, geom = \"boxplot\")\nplot(model_performance_gbm, geom = \"lift\")\nplot(model_performance_gbm, geom = \"gain\")\nplot(model_performance_gbm, geom = \"prc\")\nplot(model_performance_gbm, geom = \"histogram\")\nplot(model_performance_gbm, geom = \"ecdf\")\n\n\n\n\n\n\n\n(a) ROC Curve - GBM Model\n\n\n\n\n\n\n\n(b) Residuals Boxplot - GBM Model\n\n\n\n\n\n\n\n\n\n(c) Lift Curve - GBM Model\n\n\n\n\n\n\n\n(d) Cumulative Gain Chart - GBM Model\n\n\n\n\n\n\n\n\n\n(e) Precision-Recall Curve - GBM Model\n\n\n\n\n\n\n\n(f) Residuals Histogram - GBM Model\n\n\n\n\n\n\n\n\n\n(g) Empirical Cumulative Distribution Function - GBM Model\n\n\n\n\nFigure 9. GBM plots\n\n\n\nExplaining the model (Figure 11.4–F) is performed in the same way we did before, we use the model_parts function with the explainer object explainer_gbm. The only parameter we pass is the number of permutations B = 500 to make the results more robust. The default method computes variable importance using the AUC loss function which measures how much the AUC would decrease if that variable were removed from the model. We see that the forum contribute variables (frequency and regularity) as well as session count are the variables with most importance (Figure 6.10).\n\n# Variable importance with AUC loss\nvi_gbm <- model_parts(explainer_gbm, B = 500)\nplot(vi_gbm)\n\n\n\n\nFigure 10. Variable importance with AUC loss\n\n\n\n\nWhile standard metrics like AUC are useful, sometimes you may want to evaluate the model using custom loss functions that better align with your specific goals (Figure 6.11). Here, we define two custom loss functions: loss_logloss, which computes the logarithmic loss (or logloss), and loss_f1_score, which is based on the F1 score.\n\nLogarithmic Loss (Logloss): This function measures how well the predicted probabilities match the actual outcomes. Lower values indicate better performance.\nF1 Score: The F1 score balances precision and recall, especially important when dealing with imbalanced datasets. Here, we invert the F1 score so that lower values represent better performance, aligning with the convention used by model_parts.\n\n\n# Custom loss functions (same as in the previous code)}\nloss_logloss <- function(observed, predicted) {\n  -mean(observed * log(predicted) + (1 - observed) * log(1 - predicted))\n}\n\nloss_f1_score <- function(observed, predicted) {\n  predicted_class <- ifelse(predicted > 0.5, 1, 0)\n  TP <- sum(observed == 1 & predicted_class == 1)\n  FP <- sum(observed == 0 & predicted_class == 1)\n  FN <- sum(observed == 1 & predicted_class == 0)\n  F1 <- 2 * TP / (2 * TP + FP + FN)\n  return(1 - F1)  # return 1 - F1 to keep lower values better\n}\n\n# Variable importance with custom loss functions\nvi_gbm_logloss <- model_parts(explainer_gbm, loss_function = loss_logloss, \n                              B = 500, type = \"ratio\")\nplot(vi_gbm_logloss)\n\nvi_gbm_f1 <- model_parts(explainer_gbm, loss_function = loss_f1_score, B = 500)\nplot(vi_gbm_f1)\n\nvi_gbm_default <- model_parts(explainer_gbm, B = 500)\nplot(vi_gbm_default)\n\n\n\n\n\n\n\n(a) Custom Logloss Loss\n\n\n\n\n\n\n\n(b) F1 Score Loss\n\n\n\n\n\n\n\n(c) Default Loss (Logloss)\n\n\n\n\nFigure 11. GBM Variable Importance with Custom Loss functions\n\n\n\nWe can also generate the PDP and ALE plots for the GBM model (Figure 6.12)\n\n# Partial Dependence Plots (PDP)\npdp_gbm <- model_profile(\n  explainer_gbm,\n  variables = colnames(test_data[, 1:11])\n)\nplot(pdp_gbm) + labs(subtitle = \"\")\n\n\n# Accumulated Local Effects (ALE) Plots\nale_gbm <- model_profile(\n  explainer_gbm,\n  type = \"accumulated\",\n  variables = colnames(test_data[, 1:11])\n)\nplot(ale_gbm) + labs(subtitle = \"\")\n\n\n\n\n\n\n\n(a) PDP\n\n\n\n\n\n\n\n(b) ALE Plot\n\n\n\n\nFigure 12. GBM Partial dependence and ALE plots"
  },
  {
    "objectID": "chapters/ch06-xai-global/ch06-xai-global.html#discussion-and-conclusions",
    "href": "chapters/ch06-xai-global/ch06-xai-global.html#discussion-and-conclusions",
    "title": "6  Explainable Artificial Intelligence in Education: A Tutorial for Identifying the Variables that Matter",
    "section": "4 Discussion and conclusions",
    "text": "4 Discussion and conclusions\nIn this chapter, we went through a tutorial of xAI. In particular, we demonstrated various techniques for making the inner workings of machine learning models clear and transparent. In doing so, we can understand and reveal the variables that significantly influence student outcomes, whether predicting grades or identifying at-risk learners. We demonstrated several variable importance measures, partial dependence plots, and accumulated local effects. This transparency is very important for ensuring fairness, accountability, and the effective implementation of AI in educational settings, empowering stakeholders to make informed decisions and build a deeper understanding of the underlying learning processes. Most importantly, with transparency comes trust and reliance. If the stake holders understand the inner workings of an algorithm, they will be fast to implement it and adopt such methods in their institutions [29].\nHowever, while xAI provides a powerful lens for understanding model behavior, it is essential to acknowledge its inherent limitations and the important role of human interpretation [2, 5, 30]. As our results suggest, even detailed explanations of individual algorithmic decisions might fall short when it comes to achieving genuine personalization or providing truly individualized support. Furthermore, instances where algorithms rely on seemingly “wrong predictors” to generate explanations highlight the potential pitfalls of a purely data-driven approach [2]. This underscores that while AI can offer valuable insights and identify patterns, it cannot and must not operate in isolation.\nAs such we can say that the effectiveness of AI in education hinges on a collaborative approach where human expertise and contextual understanding complement the analytical power of algorithms [2]. The ability to interpret explanations, identify potential biases or limitations, and integrate domain knowledge remains paramount, for a more detailed discussion please refer to the second chapter of this book[1]."
  },
  {
    "objectID": "chapters/ch07-xai-local/ch07-xai-local.html",
    "href": "chapters/ch07-xai-local/ch07-xai-local.html",
    "title": "7  Individualized Explainable Artificial Intelligence: A Tutorial For Identifying Local and individual Predictions",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch07-xai-local/ch07-xai-local.html#introduction-to-local-explanations",
    "href": "chapters/ch07-xai-local/ch07-xai-local.html#introduction-to-local-explanations",
    "title": "7  Individualized Explainable Artificial Intelligence: A Tutorial For Identifying Local and individual Predictions",
    "section": "1 Introduction to Local Explanations",
    "text": "1 Introduction to Local Explanations\nOne of the key objectives of learning analytics is to predict —and understand— students’ performance to identify which indicators of students’ behavior can be used to predict student success [1]. Understanding these indicators not only helps in making accurate predictions but, more importantly, allows educators and administrators to explain the decision-making process of predictive algorithms. For instance, if a model predicts a student’s final grade, we need to know which factors (such as frequency of course views, participation in forums, or regularity of lecture viewing) influenced that prediction. If a model predicts a low grade for a student despite high engagement, we need to understand if the timing or irregularity of their engagement might be the reason. This transparency is a pre-requisite for trust and for making informed interventions to support students [2–4] .\nTransparency and trust are even more important in scenarios where decisions need to be justifiable on a case-by-case basis or where model predictions have consequences such as issuing an intervention or changing a curriculum [2, 4]. To have these case-by-case explanations, we would need local explanations (single instance explanations). Unlike global explanations, which provide an aggregate overview of the model across the entire dataset (see Chapter [3]), local explanations explain why the model made a specific prediction for a particular student.\nIn the previous chapter [3], we have seen how we can use explainable artificial intelligence (xAI) to obtain global explanations from predictive models using the DALEX R package. Though global explanations give an idea about what works for most students (i.e., feature importance), they do not accurately capture the individual differences or the variability among students [5]. In the present chapter, we go one step further and leverage the same technology to obtain local explanations. To be able to follow the tutorial presented in this chapter, it is assumed that the reader is familiar with the R programming language. If that is not the case, it is recommended to refer to previous tutorials on the basics of R [6], data cleaning [7], basic statistics [8], visualization [9], and prediction [10–12]."
  },
  {
    "objectID": "chapters/ch07-xai-local/ch07-xai-local.html#main-methods-for-local-explanations",
    "href": "chapters/ch07-xai-local/ch07-xai-local.html#main-methods-for-local-explanations",
    "title": "7  Individualized Explainable Artificial Intelligence: A Tutorial For Identifying Local and individual Predictions",
    "section": "2 Main Methods for Local Explanations",
    "text": "2 Main Methods for Local Explanations\nSeveral methods exist for local explanations, each offering a different perspective on how the model arrives at a particular prediction. We will focus here on the options offered by the DALEX package due to its simplicity and our previous experience with it. More importantly, DALEX combines several methods and offers them within its unified framework. Further, DALEX is model-agnostic, meaning it can be applied to any type of predictive model. We will discuss three methods here [13], namely, Break Down, SHAP (SHapley Additive exPlanations), and LIME (Local Interpretable Model-agnostic Explanations). We will go over the three methods briefly here, but further detailed explanation is given later in context. Also, refer to [2] for an overview of xAI.\nBreak Down Method decomposes a prediction into contributions from individual features, starting from a baseline (often the average prediction). Break Down is simple to interpret, making it easy for educators who need to quickly understand why a particular grade was predicted. However, its simplicity comes with some downsides. Most importantly, it assumes feature contributions are independent, which may not always be the case, especially when student behaviors are interrelated in complex ways.\nSHAP values offer a more robust approach that is insensitive to variable order and accounts for variable interactions. Based on game theory, SHAP values fairly distribute the prediction’s deviation from the baseline across all features [14]. This is particularly useful when indicators interact in ways that significantly impact the prediction. For example, the combination of frequent forum contributions and regular lecture viewing might have a different impact than these behaviors considered independently. However, SHAP is computationally intensive, which can be a drawback for large datasets or complex models.\nLIME is another popular method that approximates the examined models with a simpler, interpretable model to decipher their complex decisions [15]. LIME does so by perturbing the features slightly and observing the changes in predictions. However, LIME’s explanations can be sensitive to the choice of parameters, and the simple model it uses might not fully capture the complexity of the original model."
  },
  {
    "objectID": "chapters/ch07-xai-local/ch07-xai-local.html#local-explanations-in-the-existing-research",
    "href": "chapters/ch07-xai-local/ch07-xai-local.html#local-explanations-in-the-existing-research",
    "title": "7  Individualized Explainable Artificial Intelligence: A Tutorial For Identifying Local and individual Predictions",
    "section": "3 Local explanations in the existing research",
    "text": "3 Local explanations in the existing research\nThough local explainability is a powerful tool to make ML predictions understandable, this technique has not been yet capitalized on in educational research and practice. Very few instances have been reported in which local explainability has been operationalized. For instance, Jang et al. [16] analyzed data from seven Korean courses and employed SHapley Additive exPlanation (SHAP) to visualize two instances where students were identified as at-risk. Their findings highlighted low engagement with homework as the most critical factor in both cases. Similarly, Nagy and Molontay [17] leveraged SHAP to explain instances of student dropout through detailed case studies. Other researchers have used local explanations to focus on specific subsets of students. For example, Lin et al. [18] examined students who improved versus those who did not, while Adnan et al. [19] concentrated on students who failed. Saqr & López-Pernas [4] used local explanability to diagnose why students that have been erroneously predicted as low achievers or high achievers were mispredicted. These studies demonstrate the utility of local explanations but remain limited to isolated cases of explainability. Notably, the full potential of single-instance explanations has yet to be realized. Current research focuses primarily on identifying factors associated with specific outcomes but stops short of addressing mispredictions or offering actionable insights for improvement. Developing methods that use algorithms to suggest interventions to help students achieve their desired outcomes represents a critical gap in the field."
  },
  {
    "objectID": "chapters/ch07-xai-local/ch07-xai-local.html#local-explanations",
    "href": "chapters/ch07-xai-local/ch07-xai-local.html#local-explanations",
    "title": "7  Individualized Explainable Artificial Intelligence: A Tutorial For Identifying Local and individual Predictions",
    "section": "4 Local Explanations",
    "text": "4 Local Explanations\nIn this section, we provide a tutorial on local explainability in the context of education using two examples. First, we describe a case study on regression using a well-known example: predicting students’ grades. Secondly, we describe the same procedure for classification. The general workflow followed in each of the examples is the usual ML pipeline, depicted in Figure 11.4, that we have followed in previous chapters [11, 12, 20] enhanced with the local xAI techniques.\n\n\n\nFigure 1. xAI workflow implemented in the tutorial\n\n\nFirst, we load the necessary libraries and we import the data. The data contains engagement indicators based on the data in this paper [21] and discussed briefly in [11].\n\n4.1 Building a machine learning model for explanation\nThe initial steps involve data importing, preparation and creating a machine learning model. In this example we will build a Support Vector Machine (SVM) model and explain a single instance using different methods. First, to ensure that the analysis is reproducible, we start by setting a seed using set.seed(265), which guarantees that the random processes, such as data splitting, produce the same results every time the code is run.\n\n# Set a seed for reproducibility of results\nset.seed(265)\n\nNext, the dataset is pre-processed by centering the predictor variables (Figure 11.4–B). Centering involves subtracting the mean from each value so that the variables have a mean of zero. In this case, variables (e.g., Freq_Course_View, Freq_Lecture_View, and Active_Days) so the mean=0 corresponding to the average of all students, negative values indicate below the mean (lower activity) and positive values (higher activities). Thus, centering will make it easier for us to interpret the values by comparing to the reference average.\n\n# Center the selected predictor variables in the dataset. This involves \n# subtracting the mean from each value, making the mean of each variable zero\nstudent_data_centered <- student_data |>\n  mutate(across(Freq_Course_View:Active_Days, ~scale(., scale = FALSE) |> as.vector()))\n\nAfter preparing the data, we split it into training and testing (Figure 11.4–C) sets using initial_split. The dataset is divided into two parts: 80% of the data (train_data) will be used to train the model, and the remaining 20% (test_data) will be reserved for testing and validation.\n\n# Split the centered dataset into training (80%) and testing (20%) sets\ndata_split <- initial_split(student_data_centered, prop = 0.8)\ntrain_data <- training(data_split)  # Extract the training set\ntest_data <- testing(data_split)    # Extract the testing set\n\nWe then define the model formula where Final_Grade is the target variable to be predicted, and the predictors e.g., Freq_Course_View, Freq_Lecture_View, Session_Count, and Active_Days. Finally, we fit the SVM model (Figure 11.4–D) using the radial kernel, which is a popular choice for capturing non-linear relationships in the data. We use the svm() function from the e1071 package to train the model using the specified formula.\n\n# Define the formula specifying the relationship between the target variable\n#  (Final_Grade) and the predictors (Freq_Course_View, Freq_Lecture_View, etc.)\nformula <- Final_Grade ~ Freq_Course_View + Freq_Lecture_View + Freq_Forum_Consume +\n  Freq_Forum_Contribute + Regularity_Course_View +\n  Regularity_Lecture_View + Regularity_Forum_Consume +\n  Regularity_Forum_Contribute + Session_Count +\n  Total_Duration + Active_Days\n\n# Fit a Support Vector Machine (SVM) model using the radial kernel\n# The model is trained on the training dataset with the specified formula\nsvm_fit_radial <- svm(formula, data = train_data, kernel = \"radial\")\n\nAfter fitting the SVM model (svm_fit_radial) on the training data, the next step would be to evaluate its performance (Figure 11.4–E) and use global explainability techniques to understand the model’s general functioning. Since we covered that in Chapter 5 [20], we will move on to using local explainability techniques (Figure 11.4–F) to interpret and understand the model’s predictions. The DALEX package provides tools for model explanation, and to use DALEX [22], we need to create an explainer. As we have learned in the previous chapter [3], the explainer object encapsulates the model, the data it was trained on, and the corresponding target variable. Once the explainer is created, we can use it to generate local explanations with the Break Down method, SHAP values, or LIME.\nThe explainer is created using the explain() function, which takes several arguments: the model argument is set to svm_fit_radial, which is the SVM model trained using the radial kernel; the data argument is the test dataset, excluding the target variable Final_Grade (achieved by using test_data[,-12]); the y argument specifies the true values of the target variable Final_Grade, allowing DALEX to compare the model’s predictions with the actual outcomes, and the label argument provides a name for the model in the explainer, useful for comparing multiple models, in this case labeled as “SVM Radial”. With the explainer set up, we can now use various DALEX functions to assess the behavior of the SVM model, such as understanding which features are most important or how the model behaves on individual predictions.\n\n# Create an explainer for the SVM model using the DALEX package\nexplainer_svm <- explain(\n  model = svm_fit_radial,    # The trained SVM model with a radial kernel\n  data = test_data[,-12],    # Test data excluding the target variable (Final_Grade)\n  y = test_data$Final_Grade, # True values of the target variable for comparison\n  label = \"SVM Radial\"       # Label for the model in the explainer\n)\n\nPreparation of a new explainer is initiated\n  -> model label       :  SVM Radial \n  -> data              :  57  rows  11  cols \n  -> target variable   :  57  values \n  -> predict function  :  yhat.svm  will be used (  default  )\n  -> predicted values  :  No value for predict function target column. (  default  )\n  -> model_info        :  package e1071 , ver. 1.7.16 , task regression (  default  ) \n  -> predicted values  :  numerical, min =  55.05479 , mean =  68.49092 , max =  80.54996  \n  -> residual function :  difference between y and yhat (  default  )\n  -> residuals         :  numerical, min =  -21.71365 , mean =  -1.440045 , max =  13.08022  \n  A new explainer has been created!  \n\n# Check the explainer\nprint(explainer_svm)\n\nModel label:  SVM Radial \nModel class:  svm.formula,svm \nData head  :\n  Freq_Course_View Freq_Forum_Consume Freq_Forum_Contribute Freq_Lecture_View\n1         8.210526           5.449123              8.926316          28.12982\n2        35.210526          99.449123            -15.073684          63.12982\n  Regularity_Course_View Regularity_Lecture_View Regularity_Forum_Consume\n1           -0.007578947              0.08301754              -0.09329825\n2            0.112421053              0.21301754               0.04670175\n  Regularity_Forum_Contribute Session_Count Total_Duration Active_Days\n1                 -0.02273684      16.45614       8545.021   0.9368421\n2                 -0.08273684      60.45614       9076.021   5.9368421\n\n\n\n\n4.2 Breakdown profiles\nBreakdown profiles provide a clear and simple way to examine how each feature contributes to a model’s prediction. The process begins with a baseline value (often the mean prediction of the average outcome, which serves as a reference point). The contribution of each feature is then calculated by assessing how much the prediction changes when that feature is added to the baseline, assuming all other features remain constant. For example, the plot starts with the average grade and then shows how much a variable —such as the frequency of course views or the number of active days—affects the prediction to be higher or lower. Since the process is sequential, the order in which features affects the results. If the model first adds the contribution of one feature, then another, it could lead to overestimation or underestimation of the impact of the feature that follows. Therefore, it is important to interpret the breakdown results with this in mind, especially in models where features might interact.\nWhereas breakdown profiles are simple and easy to understand, they have some disadvantages. Breakdown methods typically assume that features contribute independently to the prediction, which might not hold true in complex models where interactions between features are common. It is not hard to assume that session count and active days are related or have some interactions. This simplification can lead to erroneous interpretations as the combined effect of interacting features may not be accurately represented by summing individual contributions. Another disadvantage is the issue of order dependence which we already mentioned. Different orders of predictors can produce different breakdowns, making it challenging to ascertain the true impact of each feature. This can complicate the interpretation, especially in models with highly correlated features, where the order in which features are considered may significantly alter the results presented by the plot.\nTo visualize the results of the breakdown process, we will use waterfall plots. In waterfall plots, each bar represents the contribution of a feature showing how it increases or decreases the predicted value as we move from the baseline (usually the average prediction) to the final prediction. The bars can be green (positive) or red (negative) contribution and is proportional to the magnitude of contribution of the feature, see Figure 7.2 for an example.\n\n4.2.1 Creating and visualizing breakdown results through waterfall plots\nTo demonstrate how the Breakdown method and waterfall plots explain the contribution of different features to a model’s prediction for a specific student, we will select a single instance from the test dataset. Here, we choose the fifth row from test_data, which we obtain it by subsetting the 5th row, and storing it in the Instance5 object.\n\nInstance5 <- test_data[5, ]  # Select any other observation from `test_data` if needed\n\nTo decompose the prediction for Instance5, we use the predict_parts() function, which provides a detailed, instance-specific explanation of how each feature affects the predicted grade. To do so, we call predict_parts() with the following arguments: explainer = explainer_svm which contains the SVM model and necessary data; new_observation = Instance5, specifying that we want to explain the predictions for Instance5; and type = \"break_down\", indicating that we want to use the Break Down method for the explanation. The predict_parts()function returns a detailed breakdown of the model’s prediction.\n\n# Set a seed for reproducibility\nset.seed(265)\n\nbreakdown_explanation_5 <- predict_parts(\n  explainer = explainer_svm,\n  new_observation = Instance5,\n  type = \"break_down\"\n)\n\nTo print the breakdown results, we use print(breakdown_explanation_5), where positive values indicate that a feature increases the prediction, and negative values indicate a downward effect.\n\nprint(breakdown_explanation_5) # Print the breakdown explanation to the console\n\n                                                  contribution\nSVM Radial: intercept                                   68.491\nSVM Radial: Freq_Forum_Contribute = -46.07              -2.767\nSVM Radial: Regularity_Forum_Contribute = -0.1327       -2.324\nSVM Radial: Freq_Course_View = 132.2                     0.435\nSVM Radial: Session_Count = 107.5                        1.101\nSVM Radial: Total_Duration = -13480                      0.015\nSVM Radial: Regularity_Lecture_View = 0.07302           -0.054\nSVM Radial: Active_Days = -2.063                        -0.201\nSVM Radial: Freq_Forum_Consume = -84.55                  0.442\nSVM Radial: Regularity_Course_View = -0.007579          -0.238\nSVM Radial: Regularity_Forum_Consume = -0.0133          -0.026\nSVM Radial: Freq_Lecture_View = 34.13                   -0.008\nSVM Radial: prediction                                  64.868\n\n\nWe can also create a visual representation of these contributions using plot(breakdown_explanation_5), making it easier to interpret how each feature influences the prediction.\n\n# Plot the breakdown explanation to visualize the contribution of each feature\nplot(breakdown_explanation_5)\n\n\n\n\n\n\nFigure 2. Breakdown explanation to visualize the contribution of each feature to the prediction of instance 5.\n\n\n\n\nFigure 7.2 above illustrates the breakdown plot for Instance5 predicted grade. The bars represent the contribution of each predictor to the final grade prediction. Red bars indicate factors that decreased the prediction, while green bars show factors that increased it. The purple bar on the far right reflects the final predicted grade, which is lower than the baseline due to the cumulative negative impact of several factors, particularly related to forum participation and activity levels.\nLet us go throughout the results of breakdown explanation of (Instance5). The baseline prediction (the intercept) is 68.491, which represents the average grade prediction across all students in the dataset. For Instance5, the final predicted grade is 64.868, a notable decrease from the baseline prediction. The breakdown plot helps us understand why and how the predicted grades deviated below this average.\nWe see that several negative indicators impacted the student’s predicted grade. The largest decrease came from low frequency of forum contributions (-46.07), which resulted in a -2.767 decrease in the predicted grades. Furthermore, the student’s irregularity of forum contributions (-0.1327) contributed to a further reduction of -2.324 grades. Another negative predictor was the student’s low number of active days (-2.063), which negatively affected the grade prediction by -0.201 points. Interestingly, despite the low score for low frequency of forum consumption (-84.55), it contributed positively to the predicted grade by 0.442 points. This finding suggests that values below the average can still be a positive indicator for predicting better grades. In other words, the average may be higher than what the algorithm considers “enough”. There were also several positive engagement behaviors that increased the student’s predicted grade. The frequency of course views (132.2) positively influenced the prediction by 0.435 grades, and the session count (107.5) contributed even more with an increase of 1.101 grades. The impact of other engagement indicators was minimal or close to 0. For instance, the total duration of engagement (-13,480) had a negligible effect on the prediction, adding only 0.015 grades. Additionally, indicators such as regularity of lecture viewing (0.07302), frequency of lecture viewing (34.13), regularity in course viewing (-0.007579), and regularity in forum consumption (-0.0133) showed minor effects with contributions.\nThe following equation shows how each feature adds to or subtracts from the initial baseline of 68.491, resulting in the final predicted grade of 64.868.\n\\[\n\\begin{aligned}\n- 2.767 \\, (\\text{Freq\\_Forum\\_Contribute}) \\\\\n- 2.324 \\, (\\text{Regularity\\_Forum\\_Contribute}) \\\\\n+ 0.435 \\, (\\text{Freq\\_Course\\_View}) \\\\\n+ 1.101 \\, (\\text{Session\\_Count}) \\\\\n+ 0.015 \\, (\\text{Total\\_Duration}) \\\\\n- 0.054 \\, (\\text{Regularity\\_Lecture\\_View}) \\\\\n- 0.201 \\, (\\text{Active\\_Days}) \\\\\n+ 0.442 \\, (\\text{Freq\\_Forum\\_Consume}) \\\\\n- 0.238 \\, (\\text{Regularity\\_Course\\_View}) \\\\\n- 0.026 \\, (\\text{Regularity\\_Forum\\_Consume}) \\\\\n- 0.008 \\, (\\text{Freq\\_Lecture\\_View}) \\\\\n= \\mathbf{64.868}\n\\end{aligned}\n\\]\n\n\n4.2.2 Order of variables\nOne of the problems of breakdown plots is that they are influenced by the order different variables where different orders may result in different results. To demonstrate these changes, let us change the order of the variables that we have. For that, we will shuffle the regularity features to come first, then followed by time and later comes the frequency features. We do so, by specifying a new object and we store the order to it Custom_order.\n\nset.seed(265)\n\n# Define a custom order for the features in the breakdown analysis\nCustom_order <- c(\"Regularity_Course_View\", \"Regularity_Lecture_View\", \n                  \"Regularity_Forum_Consume\", \"Regularity_Forum_Contribute\", \n                  \"Session_Count\", \"Total_Duration\", \"Active_Days\", \n                  \"Freq_Course_View\", \"Freq_Lecture_View\",\n                  \"Freq_Forum_Consume\", \"Freq_Forum_Contribute\")\n\n# Select the fifth instance from the test data for local explanation\nInstance5 <- test_data[5, ]\n\n# Generate a breakdown explanation using the custom order of features\nbreakdown_explanation_5_ordered <- predict_parts(\n  explainer = explainer_svm,           # The model explainer\n  new_observation = Instance5,         # The instance to explain\n  order = Custom_order,                # Apply the custom feature order\n  type = \"break_down\"                  # Use the breakdown method\n)\n\n# Display and plot the breakdown explanation\nprint(breakdown_explanation_5_ordered)\n\n                                                  contribution\nSVM Radial: intercept                                   68.491\nSVM Radial: Regularity_Course_View = -0.007579          -0.122\nSVM Radial: Regularity_Lecture_View = 0.07302            0.209\nSVM Radial: Regularity_Forum_Consume = -0.0133          -0.036\nSVM Radial: Regularity_Forum_Contribute = -0.1327       -2.040\nSVM Radial: Session_Count = 107.5                        0.758\nSVM Radial: Total_Duration = -13480                     -0.728\nSVM Radial: Active_Days = -2.063                        -0.184\nSVM Radial: Freq_Course_View = 132.2                    -0.355\nSVM Radial: Freq_Lecture_View = 34.13                    0.117\nSVM Radial: Freq_Forum_Consume = -84.55                  0.122\nSVM Radial: Freq_Forum_Contribute = -46.07              -1.364\nSVM Radial: prediction                                  64.868\n\n\n\n\n\n\n\nFigure 3. Breakdown explanation to visualize the contribution of each feature to the prediction of instance 5 providing the features in a custom order\n\n\n\n\nWe can see there are differences between our original case, and the shuffled example (Figure 7.3). The most significant difference comes from Freq_Forum_Contribute where it reduces the prediction by 2.767 in the original example while in the shuffled order example, it resulted in 1.364 grade drop. Similarly, Regularity_Forum_Contribute showed a more negative impact in the original example (-2.324) than in the shuffled ordered example (-2.040). Whereas Total_Duration has almost no effect in the first example (0.015), it turned negative in the shuffled example reducing the grade by -0.728. Session_Count contributes positively in both cases but to different extents, adding 1.101 in the first example and 0.758 in the second. Freq_Course_View also shifts from a small positive contribution in the first example (0.435) to a slight negative impact in the second (-0.355). Finally, Active_Days remains consistently negative but with similar small effects in both cases. Overall, while we saw differences as well as consistency, the fact that the importance and contribution of variables differ with changing order of variables makes some of our conjectures unreliable. To further show the magnitude of the problem we have plotted different variations of the variable order in Figure 7.4. Therefore, more consistent methods in the next sections.\n\nset.seed(265)\n\nlibrary(patchwork)  # For combining plots\n\n# Generate 12 different plots with different feature orders\nplots <- list()\nfor (i in 1:length(Custom_order)) {\n  random_order <- sample(Custom_order)\n  # Compute breakdown explanation with the current order\n  breakdown_explanation_ordered <- predict_parts(\n    explainer = explainer_svm,\n    new_observation = Instance5,\n    order = random_order,  # Apply the random feature order\n    type = \"break_down\"\n  )\n  \n  # Plot the breakdown explanation\n  plots[[i]] <- plot(breakdown_explanation_ordered) +\n    ggtitle(paste(\"Breakdown Explanation - Order\", i))\n}\n\n# Combine all plots into a single image\ncombined_plot <- wrap_plots(plots, ncol = 3)  # Adjust ncol as needed\nprint(combined_plot)\n\n\n\n\nFigure 4. Breakdown explanation to visualize the contribution of each feature for different feature orders\n\n\n\n\n\n\n\n4.3 SHAP (Shapley Additive Explanations) Values: A Detailed Explanation\nSHAP (Shapley Additive Explanations) is a popular and more robust method for explaining model predictions with roots coming from game theory principles. By design, SHAP allocates “credit” or “blame” to each feature in a model’s prediction in a balanced manner. By doing so, SHAP ensures that the allocation of importance to each feature reflects their true impact on the model’s output. The contribution of each feature is considered by treating the prediction process like a game, where the features are like players cooperating or working together to make a prediction. Further, in order to ensure that the SHAP results are robust, the SHAP values are calculated by considering all possible combinations (subsets) of features and measuring how the inclusion of the feature changes the prediction in each combination. For example: In our model, the SHAP value for lecture viewing frequency would measure how much this feature changes the prediction compared to when it is not considered, across all possible feature combinations. If lecture viewing frequency has a SHAP value of +2, this means that the lecture viewing frequency contributes 2 points above the base prediction towards the final predicted grade.\n\n4.3.1 Advantages of SHAP Values\nUnlike methods like the breakdown profile, SHAP does not depend on the order in which features are evaluated. SHAP achieves so by considering all possible permutations of feature order and their interactions. This makes SHAP more robust, and replicable, i.e., one is more likely to obtain the same results when repeating the same procedure and with even a different order. Furthermore, SHAP values ensure that every feature is given “credit” (or “blame”) for its contribution in a fair way. This fairness comes the way SHAP credit is distributed evenly across all features. Another advantage of SHAP is that their capability to capture complex variable interactions. SHAP values can capture interaction effects between features. If two features are jointly influencing a prediction in a significant way, SHAP can model this interaction effectively by considering all feature combinations. The last but not least, it that SHAP values can offer both global and local interpretations. In local Interpretation (such as our single instance here), SHAP values explains exactly why the model in this particular instance made a certain prediction, showing the contribution of each feature to that prediction. Aggregating SHAP values across multiple instances (Global Interpretation) gives the overall importance of features for the a dataset or a group of data-points.\n\n\n4.3.2 Using SHAP for Single Instance Explanation\nEstimating SHAP values and plotting them is similar to the same method we used before for estimating breakdown values with one difference that we need to tell predict_parts() function to use SHAP for calculating the variable contributions. As the code shows below, we start by selecting Instance5 for explanation. We use the same function predict_parts() and we set the (type = \"shap\"). In the same way, we can use the print(shap_explanation) to print the SHAP values to the console, and use the plot(shap_explanation) to visualize the SHAP values (Figure 7.5).\n\n# Compute SHAP explanation for a single instance\n# 'type = \"shap\"' specifies that we want to use SHAP (SHapley Additive \n# exPlanations) to explain the prediction.\nshap_explanation <- predict_parts(\n  explainer = explainer_svm,    # The explainer object for the SVM model\n  new_observation = Instance5,  # The specific instance that we want to explain\n  type = \"shap\"                 # Use the SHAP method for generating the explanation\n)\n\n# Print the SHAP explanation\nprint(shap_explanation) \n\n                                                          min          q1\nSVM Radial: Active_Days = -2.063                  -0.49658324 -0.22311978\nSVM Radial: Freq_Course_View = 132.2              -1.48162457 -1.20526531\nSVM Radial: Freq_Forum_Consume = -84.55           -0.28813326  0.09521635\nSVM Radial: Freq_Forum_Contribute = -46.07        -3.10980972 -2.65792505\nSVM Radial: Freq_Lecture_View = 34.13             -0.25630966 -0.07190942\nSVM Radial: Regularity_Course_View = -0.007579    -0.59193116 -0.34560552\nSVM Radial: Regularity_Forum_Consume = -0.0133    -0.28839371 -0.13128944\nSVM Radial: Regularity_Forum_Contribute = -0.1327 -2.20487383 -1.63159583\nSVM Radial: Regularity_Lecture_View = 0.07302     -0.25722592  0.04465720\nSVM Radial: Session_Count = 107.5                 -0.09696409  0.71374494\nSVM Radial: Total_Duration = -13480               -1.12235678 -0.35860487\n                                                        median        mean\nSVM Radial: Active_Days = -2.063                  -0.204843035 -0.20141599\nSVM Radial: Freq_Course_View = 132.2              -0.540784779 -0.47298234\nSVM Radial: Freq_Forum_Consume = -84.55            0.147215782  0.15824248\nSVM Radial: Freq_Forum_Contribute = -46.07        -1.987895533 -2.06071066\nSVM Radial: Freq_Lecture_View = 34.13             -0.008078213 -0.01132575\nSVM Radial: Regularity_Course_View = -0.007579    -0.247471470 -0.28872377\nSVM Radial: Regularity_Forum_Consume = -0.0133    -0.096102784 -0.07505243\nSVM Radial: Regularity_Forum_Contribute = -0.1327 -1.344400605 -1.36909015\nSVM Radial: Regularity_Lecture_View = 0.07302      0.144839790  0.14101142\nSVM Radial: Session_Count = 107.5                  0.780290982  0.77841235\nSVM Radial: Total_Duration = -13480               -0.181893922 -0.22175830\n                                                           q3          max\nSVM Radial: Active_Days = -2.063                  -0.15432837  0.168528921\nSVM Radial: Freq_Course_View = 132.2               0.30866996  0.638062414\nSVM Radial: Freq_Forum_Consume = -84.55            0.28148119  0.436235637\nSVM Radial: Freq_Forum_Contribute = -46.07        -1.51178694 -1.200990928\nSVM Radial: Freq_Lecture_View = 34.13              0.05137132  0.276856860\nSVM Radial: Regularity_Course_View = -0.007579    -0.23689879  0.004032344\nSVM Radial: Regularity_Forum_Consume = -0.0133    -0.02484703  0.331331795\nSVM Radial: Regularity_Forum_Contribute = -0.1327 -1.08818167 -0.568598896\nSVM Radial: Regularity_Lecture_View = 0.07302      0.26508558  0.426892538\nSVM Radial: Session_Count = 107.5                  1.02310034  1.260721365\nSVM Radial: Total_Duration = -13480                0.04578698  0.144467705\n\n\n\n# Plot the SHAP explanation\nplot(shap_explanation) \n\n\n\n\nFigure 5. SHAP Explanation for Instance 5\n\n\n\n\nLet us decompose the SHAP values in our final example to examine the individual contributions of each variable. As the figure shows, the largest negative contribution comes from Freq_Forum_Contribute, which resulted in a drop of -2.77 points from the baseline of 68.49 shown as a red bar. Following this, Regularity_Forum_Contribute reduced the predicted grade by -2.32 points, also shown as a red bar on the negative side of the plot (left of the 0). Both indicators reflect that low frequency and irregular engagement in collaborative learning activities may decrease the student’s predicted achievement. This is of course because, the model has learnt that students who contribute to the forums always score better grades. In contrast, Session_Count increases the predicted grade by 1.10 points, Freq_Course_View by 0.44 points. Regularity_Course_View reduces the prediction by -0.12 points. Additionally, low Total_Duration contributes a small negative value of -0.73 points, while low values of Active_Days decreases -0.20 points. Regarding lecture engagement, Regularity_Lecture_View has a low positive contribution of 0.21 points, while Freq_Lecture_View slightly increases the prediction by 0.12 points. These findings suggest that regularity in engaging with lectures is a stronger predictor of performance than the sheer number of views. Lastly, forum consumption shows an interesting dynamic: although Freq_Forum_Consume is low, it adds 0.44 points to the prediction, whereas Regularity_Forum_Consume slightly decreases the prediction by -0.04 points. To put this in an equation, we start by the average model prediction of 68.49 and add or substract each variable contribution. Prediction = 68.49 - 2.77 - 2.32 + 1.10 + 0.44 - 0.12 - 0.73 - 0.20 + 0.21 + 0.12 + 0.44 - 0.04 = 64.87\n\n\n\n4.4 LIME (Local Interpretable Model-Agnostic Explanations) explanations\nLIME works in a different way than the previous methods. In that, LIME creates an approximation of the model’s behavior by creating a “simpler model” [15]. LIME does so by generating perturbations i.e., slight modifications to the variables of the instance being explained and then analyze how these modifications affect the model’s predictions. In doing so, LIME constructs a simple, interpretable model (such as linear regression) that mimics the behavior of the more complex model. This approach allows LIME to estimate the contribution of each feature. The contributions are then ranked and visualized. Unlike SHAP, which provides globally consistent feature contributions (i.e., the contributions sum up to the prediction), LIME focuses more on local behavior and might provide different interpretations.\nThe code is different here, since we use a different function, the predict_surrogate from the DALEXtra package to generate a LIME explanation and the surrogate model. But before using predict_surrogate, we need to ensure the necessary DALEXtra methods are loaded as shown below. Then we pass explainer_svm object, which contains the SVM model and the instance (Instance5), the n_features argument specifying the number of features contributing to the prediction will be considered, the n_permutations sets the number of perturbations around the instance to 1000 to accurately approximate the model’s local behavior. The type = \"lime\" argument specifies that LIME is the method used for generating the explanation. Finally, the plot function visualizes the results.\n\n# Ensure the necessary DALEXtra methods are loaded\nmodel_type.dalex_explainer <- model_type.dalex_explainer\npredict_model.dalex_explainer <-  predict_model.dalex_explainer\n\n# Generate LIME explanation using DALEXtra's predict_surrogate function\nLime_Explainer <- predict_surrogate(\n  explainer = explainer_svm,    # The explainer object for the SVM model\n  new_observation = Instance5,  # The specific instance (Instance5) to be explained\n  n_features = 12,              # Number of features to include in the explanation\n  n_permutations = 1000,        # Number of permutations for the surrogate model\n  type = \"lime\"                 # Specify that we want a LIME explanation\n)\n\n# Plot the LIME explanation\nplot(Lime_Explainer) \n\n\n\n\nFigure 6. LIME Explanation for Instance 5\n\n\n\n\nThe plot shown in Figure 7.6 provides LIME explanations of how the model estimated the final grade (64.87) of instance 5. First, the explanation fit is 0.41 indicating how well the surrogate model approximates the original model’s decision. We can see that the student low Freq_Forum_Contribute have the highest negative influence on the predicted grade reducing it by nearly 3 points. So did Regularity_Forum_Contribute which reduced slightly less than 3 points. The positive contributions are indicated by blue bars. We can see that the Session_Count is the strongest positive variable, adding about 1.5 points to the prediction. Other features like Freq_Forum_Consume and Active_Days had smaller but still notable positive contributions.\n\n4.4.1 Multiple instances\nSimilar to single instances, we can estimate the SHAP values for multiple instances. In the next example the code proceeds to making predictions on the test data. In this examples, we zoom down to students who were mis-predicted and differ from their actual grades by more than 10 points. Once these students are identified, we separate them into two groups: those whose grades were over-predicted and those whose grades were under-predicted. We then calculate the Aggregated SHAP values for both groups of students. These values can help us identify which features most significantly contributed to the over-prediction or under-prediction of grades. You can examine both charts and you will see, for instance, that students who were over predicted were highly active.\n\n# Predict the Final_Grade on the test dataset\npredictions <- predict(svm_fit_radial, newdata = test_data)\n\n# Calculate residuals (the difference between actual grades and predicted grades)\nresiduals <- test_data$Final_Grade - predictions\n\n# Define the threshold for significant prediction errors (10 points)\nthreshold <- 10\n\n# Identify students whose predicted grades were increased by more than the threshold\nincreased_grades <- test_data |>\n  mutate(Residual = residuals) |>\n  filter(Residual <= -threshold)\n\n# Identify students whose predicted grades were decreased by more than the threshold\ndecreased_grades <- test_data |>\n  mutate(Residual = residuals) |>\n  filter(Residual >= threshold)\n\n\n# Aggregate SHAP values for students with increased grades\nshap_increased <- predict_parts(\n  explainer = explainer_svm,\n  new_observation = increased_grades |> dplyr::select(-Final_Grade, -Residual),\n  type = \"shap\",\n  B = 50  # Number of Monte Carlo simulations for SHAP values\n)\n\n# Plot the aggregated SHAP values for increased grades\nplot(shap_increased)  \n\n\n\n\nFigure 7. SHAP For Increased Grades\n\n\n\n\n\n# Aggregate SHAP values for students with decreased grades\nshap_decreased <- predict_parts(\n  explainer = explainer_svm,\n  new_observation = decreased_grades |> dplyr::select(-Final_Grade, -Residual),\n  type = \"shap\",\n  B = 50  # Number of Monte Carlo simulations for SHAP values\n)\n\n# Plot the aggregated SHAP values for decreased grades\nplot(shap_decreased)\n\n\n\n\nFigure 8. SHAP For Decreased Grades\n\n\n\n\n\n\n\n4.5 Explanation of a classifier model\nThe process of explaining a single instance in classifier models is almost the same. The below code creates a classification model for explaining student achievement (high or low achievement) and then creates an explanation of instance 10. In the code below, we start by centering the data as we did before (step 1). Then, we create a binary classification target, Achievement from Final_Grade where “Low_Achievers” are considered if they score below the median grade (step 2). Then, we split the data into training and testing sets, ensuring stratification by Achievement to maintain consistent proportions of high and low achievers in both subsets (step 3). Then, we estimate an SVM model using the e1071 package, with probability = TRUE to produce probability estimates for each prediction to help us in evaluating the model (step 4). Lastly, we create the explainer using explain() function, which prepares the model and test data for generating local explanations (step 5).\n\n# Step 1: Center the numeric variables in the dataset\nstudent_data_centered <- student_data |>\n  mutate(across(Freq_Course_View:Active_Days, ~scale(., scale = FALSE) |> as.vector()))\n\n# Step 2: Create a binary classification target variable\nmedian_grade <- median(student_data_centered$Final_Grade)\nstudent_data_centered <- student_data_centered |>\n  mutate(Achievement = ifelse(Final_Grade < median_grade, \n                              \"Low_Achievers\", \"High_Achievers\")) |>\n  mutate(Achievement = factor(Achievement, \n                              levels = c(\"High_Achievers\",\"Low_Achievers\")))\n\n# Step 3: Split the data into training and testing, stratifying by the target variable\ndata_split <- initial_split(student_data_centered, prop = 0.8, strata = Achievement)\ntrain_data <- training(data_split)\ntest_data <- testing(data_split)\n\n# Step 4: Train an SVM model using e1071\nsvm_model_Classify <- svm(\n  Achievement ~ Freq_Course_View + Freq_Lecture_View + Freq_Forum_Consume +\n    Freq_Forum_Contribute + Regularity_Course_View + \n    Regularity_Lecture_View + Regularity_Forum_Consume +\n    Regularity_Forum_Contribute + Session_Count + \n    Total_Duration + Active_Days,\n  data = train_data,\n  probability = TRUE)\n\n# Step 5: Create explainer\nexplainer_svm_Classify <- explain(\n  svm_model_Classify,\n  data = test_data |> dplyr::select(-Final_Grade, -Achievement),\n  y = test_data$Achievement,\n  label = \"SVM Classification\",\n  verbose = FALSE\n)\n\nUsing the same code as earlier, we create the local explanation for a single student.\n\n# Select a single instance for local explanation\nInstance10 <- test_data[10, ]  # You can select any observation from test_data\n\n# Local interpretation using Break Down method\nbreakdown_Classify <- predict_parts(\n  explainer = explainer_svm_Classify,\n  new_observation = Instance10 |> dplyr::select(-Final_Grade, -Achievement),\n  type = \"break_down\")\n\n# Display and plot the breakdown explanation\nplot(breakdown_Classify)\n\n\n# Local interpretation using SHAP method\nShap_Classify <- predict_parts(\n  explainer = explainer_svm_Classify,\n  new_observation = Instance10 |> dplyr::select(-Final_Grade, -Achievement),\n  type = \"shap\")\n\n# Display and plot the SHAP explanation\nplot(Shap_Classify)\n\n\n\n\n\n\n\n(a) Breakdown\n\n\n\n\n\n\n\n(b) SHAP\n\n\n\n\nFigure 9. Local explanations for the classification results of student 10"
  },
  {
    "objectID": "chapters/ch07-xai-local/ch07-xai-local.html#conclusions",
    "href": "chapters/ch07-xai-local/ch07-xai-local.html#conclusions",
    "title": "7  Individualized Explainable Artificial Intelligence: A Tutorial For Identifying Local and individual Predictions",
    "section": "5 Conclusions",
    "text": "5 Conclusions\nIn conclusion, this chapter has demonstrated the role of instance-level explanations in moving beyond aggregate insights to understand the unique factors influencing individual predictions within machine learning models. Unlike global explanations that offer a broad overview, these local techniques, exemplified by Break Down plots, SHAP values, and LIME enable educators and stakeholders to dissect the specific reasons behind a particular outcome for a given student which helps enhance transparency and trust.\nHowever, while instance-level predictions allow us to understand parts or instances of an aggregated model they should not be thought of as idiographic models. For idiographic modeling of individual students, we need to implement models that uses the student as a unit of analysis [23]. Put another way, the whole model data comes from the same student which will be domonstrated in a separate chapter [24]. For more reading about idiographic modeling and what they mean, readers are advised to refer to the idiographic chapter [25]."
  },
  {
    "objectID": "chapters/ch08-llms/ch08-llms.html",
    "href": "chapters/ch08-llms/ch08-llms.html",
    "title": "8  An Introduction to Large Language Models in Education",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch08-llms/ch08-llms.html#introduction",
    "href": "chapters/ch08-llms/ch08-llms.html#introduction",
    "title": "8  An Introduction to Large Language Models in Education",
    "section": "1 Introduction",
    "text": "1 Introduction\nLarge Language Models (LLMs) are advanced AI systems primarily designed to process and generate human-like text, but their capabilities extend far beyond natural language tasks, enabling transformative applications across diverse domains, including education, research, software development, and more [1, 2]. Taking advantage of massive datasets and neural network architectures —such as the transformer mechanism [2]— LLMs can analyse context, predict sequential elements (e.g., the next word or token), and produce coherent, contextually appropriate outputs.\nWhile text generation and understanding are at the core of LLMs, their versatility stems from their ability to generalize patterns in data. This enables them to tackle tasks like code generation [3], and even multimodal applications that integrate text with other data types, such as images and videos [4]. For instance, specialized models like Claude Sonnet excel in programming tasks, while multimodal extensions of GPT-4 demonstrate the ability to describe images and interpret visual data alongside textual inputs.\nLLMs are opening new possibilities in education by transforming how students learn and how educators teach and research [5]. These models can dynamically generate test questions tailored to diverse learning levels [6], translate educational materials to improve accessibility [7], summarize complex concepts for clearer understanding [8], and simulate conversational practice to enhance language skills [9]. The personalization of learning experiences through LLMs can support students in mastering content at their own pace. In research, they enable the creation of synthetic datasets [10], simulate experimental conditions for pedagogical studies [11], and analyse vast corpora of text and structured data to uncover actionable insights.\nThe scalability of LLMs, exemplified by models like GPT-3 with 175 billion parameters (i.e., the weights and biases of the different layers), has accelerated their adoption across fields. However, they are not without limitations. Issues such as biased, inaccurate, or fabricated outputs highlight the need for critical human oversight [12]. A human-in-the-loop approach is crucial to ensuring that LLMs align with educational goals and ethical standards, serving as tools to enhance learning rather than replace human expertise [13, 14]. Despite these challenges, LLMs are transforming how information is delivered, processed, and utilized. Their ability to integrate language understanding with broader data-processing capabilities positions them as invaluable tools for solving complex problems and enhancing accessibility across disciplines. The following sections explore the transformative applications of LLMs in education research and practice."
  },
  {
    "objectID": "chapters/ch08-llms/ch08-llms.html#behind-the-scenes-of-llms",
    "href": "chapters/ch08-llms/ch08-llms.html#behind-the-scenes-of-llms",
    "title": "8  An Introduction to Large Language Models in Education",
    "section": "2 Behind the scenes of LLMs",
    "text": "2 Behind the scenes of LLMs\nTo better understand how large language models function, it is helpful to break down their core components and processes into three interrelated aspects based on the acronym “GPT”: the Transformer architecture (T), which underpins their computational ability; the Pre-training phase (P), where the model learns patterns and knowledge from vast datasets; and the Generative abilities (G), which showcase their practical applications. The sequence T → P → G provides a logical progression from the foundational mechanics of LLMs to their training processes and, finally, to their real-world outputs. This structure offers a holistic view of these transformative technologies: first understanding the architecture that powers LLMs, then exploring how they are trained to predict and fine-tune their outputs, and ultimately seeing what they can generate. In the following sections, we explore each of these phases in detail, shedding light on their individual contributions to the overall capabilities of LLMs.\n\n2.1 How LLMs Work: The “Transformer” Architecture\nThe “Transformer” in GPT refers to the architectural framework that powers modern LLMs, enabling their remarkable fluency and adaptability. This architecture revolutionized natural language processing by introducing the attention mechanism [2], which allows the model to understand relationships within a sequence and process context effectively. Without the “T,” LLMs could not achieve the coherence and precision that define their generative abilities.\nAt its core, a LLM is a sophisticated function designed to predict the next element in a sequence. For example, given the input “The sun is shining”, the model might predict “brightly” or “today”. While this prediction task may sound straightforward, this isn’t a simple function like y = 2x + 1, in which the parameters are the coefficient of the linear function 2 and 1, and input of x=1 would give an output of 3. The underlying computations in LLMs are vastly more intricate. Instead of a handful of parameters, LLMs employ billions of them within multi-layered neural networks. Each layer refines the model’s understanding of the input, performing complex transformations based on the outputs of previous layers.\nPrior to the attention mechanism, models like recurrent neural networks (RNNs) [15] and convolutional neural networks (CNNs) [16] struggled to effectively process relationships in long or complex sequences, often treating input elements in isolation or with limited context [17]. The attention mechanism transformed this by allowing the model to weigh the importance of different parts of the input sequence based on their relevance to the task [2]. For example, consider the sentences “There’s a bat on the tree” and “He swings the bat”. In the first, “bat” refers to an animal, while in the second, it refers to a sports tool. The attention mechanism enables the model to focus on surrounding words like “tree” or “swings” to deduce the correct meaning. This ability to dynamically assign importance to specific words based on context is the key to LLMs’ accuracy and coherence.\nThe attention mechanism calculates attention weights, assigning varying levels of importance to each word based on its relevance to the task at hand. For instance, in the sentence “The sun is shining”, the word “sun” is given more weight than less significant words like “the”. These weighted representations of words are passed through multiple layers of the neural network, with each layer building on the contextual understanding established by the previous one. Early layers might identify simple word relationships, like “sun” being associated with “shining”, while later layers incorporate broader knowledge, such as recognizing weather-related descriptions that often lead to words like “today” or “brightly”.\nIn summary, the Transformer architecture is highly effective because it dynamically focuses attention on the most relevant parts of an input sequence at any given moment. This mechanism mirrors human behavior, where we concentrate on the information most pertinent to the task at hand while filtering out distractions. This synergy between attention and neural network computation forms the foundation of the architecture, making it the core of modern generative AI.\n\n\n2.2 Why LLMs Work: The Power of Being “Pre-trained”\nPreviously, we discussed that LLMs are predictive models with billions of parameters, enabling them to perform complex calculations. But how are these parameters determined? That’s where the “P” in GPT - Pre-training - comes in.\nIn any prediction task, the goal is to learn from existing data to make accurate predictions for new inputs. For example, imagine you’re predicting the next number in a sequence like 2, 4, 6, 8. Based on the pattern, you might predict 10 as the next number. Similarly, LLMs predict the next word in a sentence based on patterns they’ve learned from vast amounts of text. For instance, given the sentence “The sun is shining”, the model might predict “brightly” as the next word. Before training begins, the structure of the model must be defined. In our example, this would mean deciding the type of rule used to predict the sequence, like identifying it as “add 2 to the previous number”. For LLMs, this involves defining the number of layers in the model, the capacity of the attention mechanism to focus on different parts of the input, and the dimensions of its internal representations and computations. These decisions shape how well the model can understand and generate language.\nOnce the model structure is determined, the training process begins. In the numerical example, if we assume that we have an arithmetic sequence (that is, xt = 1 * xt-1 + 2), the process involves identifying the parameters a=1 and b=2, which defines the rule for predicting the next number in the sequence. For LLMs, determining parameter values is far more complex, involving billions of parameters and advanced optimisation techniques. The training process is generally divided into two main stages: pre-training and fine-tuning, with reinforcement learning with human feedback (RLHF) often added for further refinement. Let us explore how these processes work in more detail in the following sections.\n\n2.2.1 Pre-training and Fine Tuning\nPre-training is the initial phase [18] where the model is exposed to a vast corpus of text data, such as books, articles, and websites. The size and diversity of this dataset are crucial for the model to learn general patterns of language, including grammar, semantics, and common word associations. Importantly, this process is self-supervised [19], meaning the model learns from the structure of the data itself without requiring manually labelled examples. For instance, given the input “The sun is shining”, the model predicts the next word based on patterns it has seen in similar contexts during pre-training. In this case, it might predict words like “brightly” or “today” depending on the associations it has learned from the training data.\nOn the technical side, the accuracy of these predictions is measured using a loss function, which quantifies how far the predicted word is from the actual next word in the sequence. For example, if the model predicts “cloudy” instead of “brightly,” the loss function assigns a higher value, indicating a larger error. These errors are minimized through a process called backpropagation, which calculates how each parameter in the model contributes to the error. Optimisation algorithms then adjust the parameters to reduce the loss, gradually improving the model’s ability to make accurate predictions.\nAfter pre-training, the model undergoes fine-tuning on smaller, domain-specific datasets. For example, if the model is to generate weather reports, it might be fine-tuned on a specialized corpus of meteorological data. This step builds on the general language patterns learned during pre-training, refining the model to perform specialized tasks with high accuracy. Returning to the earlier example, fine-tuning might teach the model to complete “The sun is shining” with specific terms like “in the afternoon” or “through the clouds”, based on its specialized training data.\n\n\n2.2.2 Reinforcement Learning with Human Feedback (RLHF)\nTo further enhance the model’s alignment with human expectations, a process called reinforcement learning with human feedback (RLHF) [20] is often applied. This technique fine-tunes the model beyond its technical accuracy, ensuring it generates outputs that are clear, relevant, and aligned with human preferences. The approach was popularized by the work of Christiano et al. [2017], which demonstrated how human preferences could be used to guide models in learning complex tasks where clear evaluation criteria are difficult to define.\nIn RLHF, human evaluators review the model’s outputs and rank them based on criteria like clarity, appropriateness, and usefulness. For instance, if the model is tasked with completing the phrase “The sun is shining”, it might produce options like “brightly”, “on the horizon”, or “through the clouds”. Evaluators would rank these outputs, perhaps preferring “brightly” for its clarity and generality over “on the horizon”, which might seem overly specific. These rankings are then used to train a reward model, which predicts scores for outputs based on their alignment with human preferences.\nDuring the reinforcement learning phase, the LLM generates new predictions, and the reward model assigns scores to these predictions. Using reinforcement learning algorithms, such as Proximal Policy Optimisation (PPO) [21], the LLM updates its parameters to maximize the reward from the reward model. During this process, desirable outputs are assigned higher scores, while less preferred options are penalized. Through this iterative process, the model improves its ability to align with human-provided feedback, producing outputs that better meet expectations for clarity, relevance, and user-friendliness.\n\n\n\n2.3 What LLMs Do: The “Generative” Core of GPT\nAt the heart of large language models lies their generative ability - the “G” in GPT - which enables them to produce coherent, contextually relevant, and often human-like text. This generative capability transforms LLMs from passive tools into active collaborators across a wide range of applications, from creative writing and summarizing complex documents to coding and conversational AI.\nThe generative process begins with a prompt, which serves as the input to the model. For instance, given the phrase “The sun is shining”, the model predicts likely continuations one word after another based on patterns it has learned during training. It might generate “brightly through the clouds” or “today after a long storm,” depending on its understanding of the context and relationships within the data it was trained on. This prediction is not random but calculated from probabilities assigned to thousands of potential next words, allowing the model to choose the most appropriate continuation.\nLLMs excel in generating text that reflects the tone, style, and intent of the input. For example, when prompted with “Write a formal letter about sunny weather”, the model might begin: “Dear Sir/Madam, I am writing to express my appreciation for the delightful sunny weather we have been experiencing”. Conversely, a casual prompt like “Tell me something cool about sunny days” could result in: “Did you know sunshine boosts your serotonin levels, making you feel happier?”. These examples demonstrate the model’s ability to adapt to the user’s intent and produce contextually appropriate outputs.\nBeyond generating text, LLMs can perform tasks such as aiding in language learning [9], creating summaries [8], and writing code [3]. This flexibility stems from their pre-training on diverse datasets that include examples across various domains, enabling them to handle a broad range of tasks. Despite its versatility, the generative process is not without challenges. LLMs may occasionally produce outputs that are factually inaccurate, contextually inappropriate, or overly verbose—a phenomenon known as “hallucination”. This highlights the importance of human oversight to ensure the quality and reliability of their outputs. Table 1 summarizes the main uses of LLMs and how they can be applied to different tasks in education and learning analytics.\n\nSummary of main LLM tasks\n\n\n\n\n\n\n\n\n\nTask\nDescription\nExample Models\nHow It Works Technically\nApplication in education\n\n\n\n\nText Completion\nPredicting and generating text to complete a prompt.\nGPT-4, GPT-o1, Claude, LLaMA\nUtilizes autoregressive transformers to predict the next token based on preceding tokens, employing learned probability distributions.\nAuto-generating personalized feedback for students [22] .\n\n\nText Generation (Creative Writing)\nGenerating creative and coherent long-form text.\nGPT-4, GPT-o1, Claude, LLaMA\nUses autoregressive token prediction to produce semantically coherent, fluent, and diverse text.\nSupporting creative writing assignments, generating essay ideas, or storytelling exercises for students [23].\n\n\nQuestion Answering\nProviding answers to factual or contextual questions.\nGPT-4, Claude, Gemini, RoBERTa\nEmploys attention mechanisms to focus on relevant parts of the input context, generating precise answers. In extractive QA, identifies and extracts specific text spans.\nAutomated Q&A systems for course material, providing quick answers to student queries [24]\n\n\nQuestion Generation\nGenerating questions based on input content.\nGPT-4, T5, BERT-QG\nSequence-to-sequence models generate question tokens conditioned on input context.\nAuto-generating quiz questions or comprehension tests from textbook material or lecture notes [25].\n\n\nText Summarization\nGenerating concise summaries from longer texts.\nGPT-4, Gemini, Claude, BART, T5\nApplies sequence-to-sequence transformer architectures to condense input text into coherent summaries, preserving essential information.\nSummarizing lecture notes, research papers, or learning materials for quicker understanding [26].\n\n\nTranslation\nTranslating text between languages.\nGPT-4, Gemini, mT5, LLaMA\nUses encoder-decoder transformer models to map input tokens from the source language to the target language, aligning syntactic and semantic structures.\nSupporting multilingual learners by translating course materials, assignments, or instructions [27].\n\n\nText Classification\nAssigning labels or categories to text.\nBERT, RoBERTa, DistilBERT\nTokenizes input text and processes it through transformer layers to produce embeddings, which are then classified into categories using a task-specific head.\nAnalyzing student responses for sentiment (e.g., identifying frustration) [28], or automatic discourse coding [29].\n\n\nChat and Dialogue\nGenerating conversational responses for chatbots.\nClaude, GPT-4, Gemini, LLaMA\nMaintains dialogue context by incorporating previous conversation turns, generating contextually relevant responses through autoregressive token prediction.\nPowering tutoring chatbots that assist students with real-time explanations and guidance [30].\n\n\nCode Generation\nWriting or completing code snippets.\nGPT-o1, Codex, Gemini\nPredicts subsequent code tokens, based on previous training on a large corpora of code, ensuring syntactic and semantic correctness.\nAssisting students in programming courses by auto-generating or debugging code snippets [31].\n\n\nParaphrasing and Rewriting\nGenerating rephrased or simplified versions of input text.\nGPT-4, Claude, T5\nFine-tuned models rephrase input text while preserving semantic meaning, often using reinforcement learning.\nHelping students rephrase ideas to avoid plagiarism, simplifying content, or generating alternative explanations of concepts [32, 33].\n\n\nStyle Transfer\nRewriting text with a different style or tone.\nGPT-4, CTRL, Claude, LLaMA\nGenerates text conditioned on desired stylistic attributes, often fine-tuned on datasets exemplifying target styles to learn appropriate transformations.\nSimplifying advanced academic content for younger students or adapting tone for academic writing [34].\n\n\nKnowledge Extraction\nExtracting entities or relationships from text.\nGPT-4, BERT, Gemini\nUtilizes attention layers to identify and classify entities and their relationships within text, enabling structured information extraction.\nExtracting key concepts, relationships, or named entities from lecture transcripts and research articles.\n\n\nTask automation\nPerforming tasks with minimal or no labeled examples.\nClaude, GPT-4, GPT-o1, BART\nTakes advantage of in-context learning by interpreting task instructions and examples provided within the input, adapting to new tasks without explicit retraining.\nAutomating grading of open-ended responses with minimal training or applying rubrics [35], performing tedious technical tasks such as converting a screenshot of a table into LATEX code.\n\n\nData-to-Text Generation\nGenerating natural language summaries or explanations from structured data.\nGPT-4, T5\nEncoder-decoder architectures map structured inputs (e.g., tables) into coherent text.\nGenerating textual explanations of student performance data, or dashboards [36].\n\n\nMultimodal Integration\nCombining text with images, audio, or other modalities.\nGemini, GPT-4V, CLIP\nAligns representations from different modalities using combined encoder architectures, facilitating tasks that require understanding across multiple data types (image, audio, video, etc.).\nAnalyzing student engagement via multimodal data (e.g., combining video, text, and audio) [37].\n\n\nText-to-Speech\nConverting written text into spoken language.\nTacotron, Gemini, WaveNet, VALL-E\nDeep learning architectures generate natural-sounding speech from tokenized text inputs.\nAssisting visually impaired students, enabling spoken lecture notes, teaching young children who cannot read yet, or improving pronunciation for language learners [38, 39].\n\n\nText-to-Video\nConverting written text into video.\nSora, Runway Gen 2\nMultimodal transformer architectures process text as input and generate video sequences using latent diffusion or frame interpolation.\nCreating engaging educational videos from course materials [40]\n\n\nReasoning & Problem Solving\nSolving logical, mathematical, or structured reasoning tasks.\nClaude, GPT-4, GPT-o1, Gemini\nEmploys chain-of-thought prompting and step-by-step token generation to tackle complex reasoning tasks, enhancing problem-solving capabilities\nAssisting students with step-by-step solutions in math or logical reasoning exercises, enhancing comprehension [41].\n\n\n\n\n\n2.4 Using LLMs: From Web Interfaces to Advanced Frameworks\nAfter understanding how LLMs work and what they can be used for, the next step is exploring how to use them in practical scenarios. Whether you are a beginner looking to interact with LLMs through simple web interfaces or an advanced user building custom applications using APIs and frameworks, there are many ways to harness their capabilities.\nFor beginners, web interfaces like ChatGPT, Claude (by Anthropic), and Perplexity AI offer user-friendly ways to explore LLM capabilities without requiring technical expertise. These platforms can assist with summarizing academic papers, generating insights from student feedback, generating or assessing code, brainstorming interventions for struggling learners, and more. These tools support uploading files, such as PDFs, or accessing content from open-access URLs, making them versatile for analyzing publicly available academic content.\nSuch tools demonstrate the transformative power of LLMs across diverse domains, enabling students to engage more deeply with complex concepts while allowing educators to focus on higher-level instruction and personalized support. Whether applied to programming, language learning, or data analysis, these tools foster iterative learning: if the initial output does not fully meet the user’s needs, prompts and inputs can be refined to generate more tailored and meaningful results. This interaction not only enhances understanding but also encourages critical thinking and adaptability in both learners and educators.\nThe second way to interact with large language models is through an Application Programming Interface (API). An API acts as a bridge, allowing software applications to communicate with the LLM programmatically. This makes APIs essential for developers seeking to integrate LLMs into custom applications, websites, or tools. Using APIs offers several significant advantages, particularly in education and learning analytics. One of the key benefits is integration, as APIs can seamlessly connect LLM functionalities to various platforms, enabling educators and developers to embed advanced capabilities into their own applications or systems. This flexibility allows for the customization of LLM features to suit specific educational needs, such as automating feedback, analyzing student performance data, or generating personalized learning materials tailored to individual progress. APIs also enhance efficiency by automating repetitive tasks, reducing the need for manual interaction with web interfaces, and enabling large-scale operations. However, while APIs provide powerful tools for enhancing educational workflows, they require basic programming knowledge for implementation and careful management of API keys to ensure security and prevent unauthorized access. Additionally, API usage may incur costs, which could pose a limitation for projects with tight budgets.\nAPI implementations are available in many programming languages, being Python the most common one. In the R programming language —the main language used in this book—, the elmer package [42] provides wrappers to interact with the APIs of the most common LLMs. In Chapter [43] of this book, we provide a tutorial on how to use LLMs via API to obtain personalized feedback.\nWhile APIs like OpenAI’s are excellent for quick, scalable interactions with powerful pre-trained language models, they operate as black boxes, where you rely on the provider to host and manage the models. This simplicity is a significant strength, but it may not meet the needs of users seeking greater control for performing context-specific tasks. This is where frameworks like Hugging Face’s Transformers come in. While transformers are not the only option within the Hugging Face ecosystem, they are among the most powerful and widely used tools for leveraging state-of-the-art language models. Hugging Face provides an open-source library —with over 1 million models (Dec, 2024)— that allows users to download and run these models locally or on the cloud infrastructure you control. A comprehensive list of pre-trained models is available on the Hugging Face Models Hub.\nIn the context of education and learning analytics, transformers are capable of making sense of complex data, such as analyzing student feedback, extracting themes from surveys, or identifying trends in course engagement. Moreover, users can further customize these models and fine-tune them to address their specific education needs. By offering flexibility, greater control, and adaptability, transformers expand the potential of LLMs beyond the simplicity of many API-based interactions.\nConsider a scenario where an educator needs to analyse student feedback to understand sentiments and identify areas for improvement. A pre-trained sentiment analysis model from Hugging Face can quickly classify feedback as positive, neutral, or negative, offering actionable insights for educators. For example, a model such as bert-base-multilingual-uncased can be used, which has been fine-tuned specifically for sentiment analysis of product reviews in six languages. It is designed for immediate use in analyzing sentiment across multilingual product reviews and can also serve as a robust starting point for further fine-tuning on related sentiment analysis tasks, such as analyzing students’ course feedback or collaborative discourse. A few LLMs that have been specifically trained for education purposes exist, such as EduBERT [44] or K-12BERT [45]. In Chapter 10 [46] of this book, we provide a tutorial on how to use language models locally to automatically classify students’ discourse in collaborative problem-solving."
  },
  {
    "objectID": "chapters/ch08-llms/ch08-llms.html#conclusion",
    "href": "chapters/ch08-llms/ch08-llms.html#conclusion",
    "title": "8  An Introduction to Large Language Models in Education",
    "section": "3 Conclusion",
    "text": "3 Conclusion\nThis chapter provided an introduction to LLMs, exploring their foundational components —transformer architecture, pre-training, and generative abilities—- while demonstrating their transformative potential across applications in education. Building on this, the chapter also explored tools for interacting with LLMs, from beginner-friendly web interfaces to more advanced tools like APIs and frameworks such as OpenAI API and Hugging Face’s Transformers. These approaches enable users to operationalize LLMs for tasks such as summarizing academic papers, automatic text classification, and generating of learning materials.\nHowever, as the integration of LLMs in education becomes increasingly widespread, it is crucial to critically examine their limitations. The potential for biases in their outputs, issues of factual inaccuracies, and the challenges of ensuring transparency and interpretability must be addressed to harness their capabilities effectively. A careful balance between taking advantage of LLMs’ strengths and mitigating their shortcomings is necessary to ensure they serve as tools that improve, rather than hinder, educational and research practices [47].\nAn important avenue for addressing these challenges is the integration of explainable AI (XAI) techniques with LLMs. XAI methods aim to make the predictions and operations of LLMs more transparent [48, 49], helping users understand the factors driving their outputs. In educational contexts, for example, XAI can clarify why specific feedback was generated for a student or reveal how particular patterns in data influence predictions, such as grading or performance analytics. Transparency is needed to build trust by enabling users to identify potential biases or inaccuracies in model outputs. As the adoption of LLMs continues, embedding XAI into their workflows will be critical for ensuring ethical and equitable outcomes."
  },
  {
    "objectID": "chapters/ch09-nlp/ch09-nlp.html",
    "href": "chapters/ch09-nlp/ch09-nlp.html",
    "title": "9  The Use of Natural Language Processing in Learning Analytics",
    "section": "",
    "text": "Forthcoming"
  },
  {
    "objectID": "chapters/ch10-bert/ch10-bert.html",
    "href": "chapters/ch10-bert/ch10-bert.html",
    "title": "10  Using BERT-like Language Models for Automated Discourse Coding: A Primer and Tutorial",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch10-bert/ch10-bert.html#introduction",
    "href": "chapters/ch10-bert/ch10-bert.html#introduction",
    "title": "10  Using BERT-like Language Models for Automated Discourse Coding: A Primer and Tutorial",
    "section": "1 Introduction",
    "text": "1 Introduction\nCoding text data is an effortful and time-consuming process and constitutes one of the major challenges faced by qualitative researchers. Coding refers to the process of “essentially indexing or mapping data, to provide an overview of disparate data that allows the researcher to make sense of them in relation to their research questions” [1]. Coding requires defining codes, i.e. the concepts of interest, and localizing places in text where codes occur [2]. There are two main approaches of deriving at the codes: inductive coding, where codes emerge from the data, and deductive coding, which uses a set of predetermined codes to classify the data [3].\nThe coding process requires close inspection of data before it can be analyzed. Researchers often have to scroll through large volumes of text, identify the main concepts and themes, and assign codes that best describe them. This process requires a high level of attention, as every line of text must be carefully examined. Furthermore, maintaining consistency in coding across different parts of the data is difficult, especially if more than one researcher is involved, often requiring multiple rounds of review, discussion, and revision [1]. Coding of text is vital in qualitative research; however, it is also an important part of many quantitative or mixed-methods studies, where text data coding is often delegated as a part of data preprocessing before it can be analyzed by other, often quantitative, methods. In terms of machine learning (ML), deriving codes inductively can be similar to text summarization tasks, where an algorithm discovers topics within a text, while localizing a presence of a specific code in text can match annotation or classification tasks, where an algorithm assigns a class label to a given data point based on its characteristics.\nLarge Language Models (LLMs) offer a promising solution to ease some of the challenges related to coding of text data. LLMs are trained on vast amounts of text data and have the ability to process and generate human-like language [4]. When presented with the task of coding text data, LLMs can significantly reduce the time and effort required by automating —at least part of— the process, as this technology can quickly scan large volumes of text, as well as identify the main underlying concepts. The introduction of the transformer architecture enabled the development of of different types of LLMS such as Google’s Bidirectional Encoder Representations from Transformers (BERT) trained using masked language modeling and open AI’s Generative Pre-trained Transformers (GPT) trained using causal language modeling (see [5], [6] for more details). Although both models can be adapted to a variety of tasks, BERT models are typically recommended for text classification, while GPT models for text summarization [7].\nLLMs are capable of performing zero-shot classification, which means that without the need for specifically trained data for certain categories [4, 8], the model can classify text based on its broad understanding of language and knowledge. This capability stems from the extensive and diverse textual data encountered during the pretraining phase, enabling the model to make reasonable inferences and predictions for categories it has not been explicitly trained on through contextual understanding and knowledge transfer.\nWhile zero-shot classification significantly reduces both the technical and intellectual effort required from the researchers, it often lacks the precision and reliability that might be required for domain-specific tasks. A possible middle ground solution that achieves a good trade-off between manual coding and completely automated coding, is the combination of word embeddings —implemented in LLMs— with supervised ML. Word embeddings capture the semantic meaning of text in a way that is informed by the context in which words appear [9] (the concept is explained more thoroughly in Section 10.3.5). Generating these embeddings allows us to use the textual information as features in a classification model (such as Random Forest or Support Vector Machine) [10]. This approach requires that researchers to manually code part of the data, while the major part of the process is delegated to the LLM, achieving —allegedly— more accurate and more consistent classifications, as the model is explicitly trained on data that reflects the task at hand.\nIn this tutorial, we will learn how to implement automated discourse classification using word embeddings. We will rely on the R package text and BERT-like models. We will train a classification ML model with a small percentage of a dataset on collaborative problem solving, and we will automatically classify the remaining text. We will evaluate the classification performance and compare the agreement of the human-coded and LLM-coded text using Cohen’s kappa, Gwet’s AC1 and Matthews Correlation Coefficient. We will also compare the performance of different BERT-like algorithms using traditional metrics. Lastly, we will investigate how the insights gathered from applying different learning analytics techniques to the automatically coded data differ from those obtained from human-coded data."
  },
  {
    "objectID": "chapters/ch10-bert/ch10-bert.html#related-work-on-using-llms-to-support-text-coding",
    "href": "chapters/ch10-bert/ch10-bert.html#related-work-on-using-llms-to-support-text-coding",
    "title": "10  Using BERT-like Language Models for Automated Discourse Coding: A Primer and Tutorial",
    "section": "2 Related work on using LLMs to support text coding",
    "text": "2 Related work on using LLMs to support text coding\nThe rise in popularity of LLMs has caught the attention of learning analytics researchers for their potential to assist researchers at all stages of the learning analytics cycle including synthetic data generation and automatic feedback generation, as well as automatic coding, which is the goal of our tutorial [11]. LLMs can be used to support developing code definitions, i.e. a codebook, and identifying said codes in the text segments.\nEmerging research shows that LLMs has the potential to support developing a codebook. In the context of qualitative coding, a codebook is a structured document that outlines the codes, categories, and their definitions, serving as a guide for systematically categorizing, analyzing and interpreting qualitative data. The authors of [12] compared three codebook development settings: 1) fully automated approach with ChatGPT, 2) hybrid ChatGPT-human approach, and 3) fully human approach. The hybrid approach resulted in best utility ratings, highest conceptual overlap and higher inter-rater reliability than other approaches. Interestingly, both hybrid and human approaches developed similar codebooks, while the fully automated approach generated a codebook less aligned with the study goals and was an outlier in terms of conceptual similarity. Similarly, the hybrid approach has yielded positive results to code a small sample of interviews in a study by [13]. The authors of [14] reported moderate to substantial similarity of themes generated by an LLM and humans. However, there are still open questions about scalability and about whether the use of AI for that task will not lead to superficial understanding of the data [15, 16]. The increasing familiarity with the data is expected to lead changes in the codebook over time; the coding process is supposed to be iterative and may lead to revalidation of earlier coding [1]. Hence, implementation of LLMs at this stage of text data coding should be carefully considered.\nEarly studies using LLMs for identifying codes in text segments showed mixed results. A recent work [17] evaluated the capability of generative artificial intelligence (AI), particularly GPT models, to automatically code discourse within a learning analytics study, employing various prompting and training strategies. Their findings indicated that fine-tuning methods (refer to [4] to learn more about this concept) yielded the most favorable outcomes, though none of the results met the standard reliability thresholds typically expected in the field. [18] investigated the reliability of automatic discourse coding by applying a range of LLMs from the GPT family, along with various prompting techniques, to an asynchronous online discussion dataset. The authors determined that “the agreement with human coding was substantial at best”, and do not recommend that AI replaces humans in coding, at least not yet in a reliable way. Another study by [19] operationalized BERT models to create word embeddings, and used them to train a Random Forest classifier to automatically code collaborative problem-solving conversations. Although findings are not yet beyond ‘substantial’ accuracy, the rapid advance of LLMs suggests that these models could soon achieve higher levels of reliability and accuracy in automatic coding tasks. Also, the selection of a specific LLM may affect the performance. [20] reported that ChatGPT and Bard performed similarly across both types inductive and deductive thematic analysis. Higher performance of BERT-like models for text classification tasks than the GPT models was reported in several studies [e.g., 21, 22]. Finally, using LLM models for text coding may require decomposing this complex task into multiple simple binary classifications (i.e., processing each code at a time and classifying each text unit regarding whether the code is included or not). Complex codes are more challenging for LLMs to codes [18, 22, 23].\nAdjusting the temperature setting or conducting multiple iterations of coding are some of the strategies used to improve LLM’s performance in classification tasks [18, 24]. Another strategy is to improve the LLM prompt. For instance, [25] tested codebook-centered or example-centered prompts, as well as experimented with the number of examples used in a prompt. The results showed that the codebook-centered design performed better than the example-centered designs, while adding even one example of a code to a prompt increased the performance notably. Wording of a prompt is crucial. A recent work [26] demonstrated that using negated instructions (e.g., “Generate the incorrect solution..”) instead of positive instructions (e.g., “Generate the correct solution..”) decreases the performance of all LLMs tested. Decreased performance could be also caused by changes in the LLM model itself [27]. This is particularly important in case of proprietary models (i.e., any LLMs that cannot be downloaded, run offline and shared [28]). For example, the majority of BERT models are an open-source, while most GPT models are not publicly available [6]. Using proprietary models requires researchers to concede a degree of control to developers. Changes made to proprietary models are not transparent and may effect the text coding results and, in effect, the final analysis.\nAs LLMs may be prone to hallucinations, it is critical to evaluate model’s performance before relying on its results for analysis. One method to do it, is to use human-coded data as a baseline to assess performance [29]. However, this method requires available data that was collected in a similar context, which may be unavailable in educational settings. Another approach is to manually check the LLM coding [30], which may be applicable for smaller dataset; however, it may as effortful as manually coding the dataset instead of using an LLM. Furthermore, there are many discussions about the best metrics that can be used to assess the performance of LLMs in text coding tasks. Metrics such as accuracy, precision or recall are typical measurements used in machine learning for classification tasks (see [31] for an overview). At the same time, other metrics, such as Cohen’s kappa, are applied in the qualitative research to ensure the level of agreement among multiple coders [2]. Although Cohen’s kappa is often used to evaluate text classification task [32], it has been critiqued as not an appropriate approach to this task. Instead metrics like Matthews’ correlation coefficient [33, 34] or Gwet AC1 are suggested [35, 36].\nFinally, ethical issues are important to consider in case of coding education text data using LLMs. We addressed some of these issues throughout this chapter (for a comprehensive review see [37]). There are two ethical issues that need to be highlighted in case of using LLMs for text coding. First, educational data may include sensitive information and feeding it to a proprietary LLM can be a breach student privacy rights depending on local regulations. Second, inherent bias in the LLM from its pre-training data was proven to affect classification tasks [38]. There are several techniques and recommendations that can be considered to address bias in LLMs (see [39] for more details)."
  },
  {
    "objectID": "chapters/ch10-bert/ch10-bert.html#a-case-study-on-collaborative-problem-solving",
    "href": "chapters/ch10-bert/ch10-bert.html#a-case-study-on-collaborative-problem-solving",
    "title": "10  Using BERT-like Language Models for Automated Discourse Coding: A Primer and Tutorial",
    "section": "3 A case study on collaborative problem solving",
    "text": "3 A case study on collaborative problem solving\n\n3.1 The dataset\nThe dataset that we use in this tutorial is based on an experiment in which small groups participated in a group discussion. In this discussion, the groups aimed to solve the Wason card selection task: a logic puzzle used to study reasoning and decision-making [40]. The discussion data has been qualitatively coded according to the nature of participants’ contributions Specifically, 500 dialogues, containing 14,000 utterances were coded using a novel annotation schema designed to capture deliberation cues. For more details about the experiment, refer to the original article [40].\n\n\n3.2 Automated discourse coding\nThe process followed in the tutorial is depicted in Figure 10.1. We will first divide our dataset in training and testing sets (I). Contrary to other ML tasks, we use only a small part of the data for training (15%), and the rest for testing. The goal is to minimize the number of pieces of text that we have to manually code. Then, we will create word embeddings from the text data (II). Word embeddings are numerical representations of words that capture their semantic meaning, enabling the model to understand and process the text in a way that reflects the relationships between words. We will try different existing LLMs to create the embeddings. We then use the word embeddings corresponding to the training data and their manual codes as an input to train our classifier ML model (III). Lastly, we use the word embeddings of the test dataset to predict their codes using the trained model and evaluate the results (IV). We compare the predicted codes with the actual ones using both traditional ML performance metrics based on the confusion matrix, as well as several measures of interrater reliability to assess the agreement between human and AI coding. We will compare the performance of the different BERT-like language models to assess which one is more suitable for the classification task. For a beginners’ tutorial on ML classification tasks, refer to Chapter 4 [41] in this book.\n\n\n\nFigure 1. Process followed in the tutorial\n\n\n\n\n3.3 Setting up the enviroment\nBefore starting our task, we need to install and load the necessary packages, which are described below:\n\ndplyr: A powerful and popular R package used for data manipulation and transformation [42]. It provides a set of intuitive functions that make it easy to filter, select, mutate, arrange, and summarize data within a data frame. It is especially useful for cleaning and preparing data before analysis.\ntidyr: An essential R package for tidying data, tidyr helps reshape and clean data sets so they can be analyzed more effectively [43]. It provides functions to pivot data between wide and long formats, split or combine columns, and ensure that data is in a consistent, tidy format suitable for analysis.\ntibble: A modern take on data frames in R, tibble [44] provides a lightweight and faster alternative to the traditional data.frame object.\nreticulate: A package that provides an interface between R and Python [45]. It allows users to run Python code within R and seamlessly exchange data between the two languages. This is particularly useful when combining the strengths of R and Python in a single analysis pipeline, taking advantage of Python’s ML libraries, such as TensorFlow or Scikit-learn, while still working within an R environment.\ncaret: A package used for building and evaluating ML models in R [46]. It simplifies the process of training models, performing cross-validation, and optimizing model parameters. The package supports a wide range of algorithms and provides tools for data splitting, pre-processing, feature selection, and model evaluation.\ntext: The text package is designed for text analysis in R [47]. It provides tools for processing and analyzing textual data, such as tokenization, vectorization, sentiment analysis, and topic modeling. It uses transformers, natural language processing (NLP) and ML methods to examine text and numerical variables.\nmltools: The mltools package in R [48] provides a collection of machine learning tools and utilities designed to simplify common preprocessing and data transformation tasks for machine learning workflows. We will use it for some of its evaluation functions.\n\nBelow is the code to load the libraries. Install them first if you do not have them already using the commented code.\n\nlibrary(dplyr) # install.packages(\"dplyr\")\nlibrary(tidyr) # install.packages(\"tidyr\")\nlibrary(tibble) # install.packages(\"tibble\")\nlibrary(reticulate) # install.packages(\"reticulate\")\nlibrary(text) # install.packages(\"text\")\nlibrary(caret) # install.packages(\"caret\")\nlibrary(mltools) # install.packages(\"mltools\")\n\n\n\n\nIn addition to the previous libraries, since the text library relies on Python underneath, we need to make sure we have Python installed (preferrably together with the conda environment). If you do not have it and you do not wish to install the complete Python environment, you can install miniconda through the package reticulate as follows (uncomment to run):\n\n# reticulate::install_miniconda()\n\nOnce done, you need to initialize the Python session as follows. This is an interactive command so you will need to enter “yes” in the terminal to advance.\n\ntextrpp_initialize(save_profile = TRUE)\n\nNow that we have all the R packages, we will download our data. The dataset described earlier is available in Hugging Face and therefore we can download it using the datasets package in Python. We need to install it before using it for the first time with the commented command. After importing datasets, we download the dataset that we will use in the tutorial: \"gkaradzhov/DeliData\". The cache_dir argument indicates where the data will be stored in your computer; feel free to change the name from \"cache\" to something else.\n\n# reticulate::py_install(\"datasets\")\n\ndatasets <- import(\"datasets\") \n\ndataset <- datasets$load_dataset(\"gkaradzhov/DeliData\", cache_dir = \"cache\")\n\nWe can now extract the data in an R dataframe as follows:\n\ndf_raw <- dataset$train$to_pandas()\n\nIf we inspect the dataframe, we see that the it contains a column to identify the team (group_id) and the message (message_id), a column containing the message original text (original_text), and a cleaned one (clean_text). It also contains the code used to annotate the message (annotation_target), and two other annotation fields. In addition, it contains several other columns related to the solution of the task and the team’s performance. Let us focus on the column that contains the manual code (annotation_target). We can inspect it to see the different codes that have been used to classify the discussion utterances:\n\ntable(df_raw$annotation_target)\n\n\n         0      Agree   Disagree Moderation  Reasoning   Solution \n      3274       2047        111        785       4382       3403 \n\n\nWe see that there are many rows that have not been assigned a code. They correspond to messages that do not fall under any of the deliberation codes. We can remove them from our dataset to work only with the coded text as follows:\n\nclassification <- c(\"Solution\", \"Reasoning\", \"Agree\", \"Disagree\", \"Moderation\")\ndf <- df_raw |> filter(annotation_target %in% classification) \n\nNow we are ready to start working with the data.\n\n\n3.4 Splitting the dataset\nAs is common in most ML tasks, we split our data into a training dataset and a testing dataset. We will use the training dataset to train an ML model to classify text data into one of the five codes and we will use the test dataset to evaluate how accurately our model classifies “unseen” text. This strategy is designed to mimic a realistic coding scenario where the objective is to reduce the amount of text that needs to be manually coded. We use the function createDataPartition from the caret package to split our data into a training dataset (consisting of 15% of the data) and a testing dataset (the remaining 85% of the data). We do so in a way that the two partitions are balanced in term of proportion of codes.\n\nseed = 123\nset.seed(seed) # for reproducibility\ntrainIndex <- createDataPartition(df[[\"annotation_target\"]], p = 0.15, \n                                  list = FALSE, \n                                  times = 1)\n\n# Creating training and testing datasets\ndf_train <- df[trainIndex, ]\ndf_test <- df[-trainIndex, ]\n\n\n\n3.5 Word embeddings\nOur next task is to convert our text data into numeric representations (i.e., word embeddings) to be able to apply common ML techniques. Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation. Word embeddings can be defined as dense vectors of real numbers that represent words in a way in which words that share similar contexts are placed closer together. Unlike traditional one-hot encoding, where each word is represented as a unique binary vector, word embeddings capture semantic relationships between words, making them powerful for various NLP tasks. In simpler terms, word embeddings are a way to represent words as numbers so that words with similar meanings end up having similar numerical patterns.\nIn simple models like Word2Vec, word embeddings are created by training on large text corpora. These models learn to map each word to a vector in such a way that words used in similar contexts have similar vectors. For example, the vectors for “king” and “queen” might be close to each other because these words often appear in similar contexts (e.g., both might appear near words like “monarch,” “crown,” and “royalty”). BERT models take the concept of word embeddings one step further. BERT is a transformer-based model that understands the context of a word in a sentence by looking at both its left and right sides simultaneously (bidirectional). This is in contrast to earlier models like Word2Vec, which only considered the context in a single direction.\nBERT starts by tokenizing the input text. This process breaks the text into smaller units called tokens. BERT uses a technique called WordPiece tokenization, where words are broken down into subword units. This helps BERT handle out-of-vocabulary words by representing them as combinations of known subwords. BERT has an embedding layer that converts these tokens into dense vectors. However, unlike static word embeddings (e.g., Word2Vec, where each word has a single vector), BERT’s embeddings are dynamic. The embedding for a word depends not just on the word itself but also on the entire context of the sentence. In addition to token embeddings, BERT includes positional embeddings, which provide the model with information about the position of each token in the sentence. This is crucial because, unlike recurrent neural networks for example, transformers do not inherently handle the order of words. BERT then passes these embeddings through multiple layers of transformers, where self-attention mechanisms allow the model to focus on different parts of the sentence. This enables BERT to capture complex dependencies and relationships between words in a sentence. The output from the final transformer layer provides the contextualized word embeddings. These embeddings are rich in context and meaning because they take into account the entire sentence. For example, the word “bank” will have different embeddings in the sentences “He sat on the river bank” and “He went to the bank to withdraw money,” reflecting the different meanings of the word in these contexts.\nSeveral variations and enhancements have been created based on BERT. For instance, RoBERTa (A Robustly Optimized BERT Pretraining Approach) [49] builds on BERT’s architecture by enhancing the pretraining process with more data, among other improvements. These modifications, along with larger batch sizes and learning rates, as well as the introduction of dynamic masking, make RoBERTa more robust and better performing than BERT on various natural language processing tasks. Another example is XLNet [50], which improves upon BERT by using permutation language modeling, allowing it to capture bidirectional context without the need for masking. It builds on the Transformer-XL architecture, enabling the handling of longer sequences and dependencies beyond the sentence level. XLNet’s approach combines autoregressive training with a permutation-based technique, making it particularly effective for tasks requiring complex context understanding, such as question answering and language modeling.\nWe will create word embeddings for the training and testing datasets using different variations of BERT in order to compare performance between models. For this purpose, we will use the textEmbed function from the text package. By default, this function uses the \"bert-base-uncased\" language model to create the word embeddings. We will use this model as a baseline, and compare it with two other models: \"roberta-base\" and \"xlnet-base-cased\". You can try other language models available in Hugging Face that support the task of creating word embeddings (text classification category). You should take into account that this process takes a considerable amount of run time. As an input to the function we provide the \"clean_text\" column of our training and testing data, which contains the text to be coded.\n\n# Creating embeddings for the training data\n## BERT\nword_embeddings_bert <- textEmbed(df_train[,\"clean_text\"], \n                            model = \"bert-base-uncased\",\n                            aggregation_from_tokens_to_word_types = \"mean\",\n                            keep_token_embeddings = FALSE)\n## RoBERTa\nword_embeddings_roberta <- textEmbed(df_train[,\"clean_text\"], \n                            model = \"roberta-base\",\n                            aggregation_from_tokens_to_word_types = \"mean\",\n                            keep_token_embeddings = FALSE)\n## XLNet\nword_embeddings_xlnet <- textEmbed(df_train[,\"clean_text\"], \n                            model = \"xlnet-base-cased\",\n                            aggregation_from_tokens_to_word_types = \"mean\",\n                            keep_token_embeddings = FALSE)\n# Creating embeddings for the testing data\n## BERT\nword_embeddings_bert_test <- textEmbed(df_test[,\"clean_text\"], \n                            model = \"bert-base-uncased\",\n                            aggregation_from_tokens_to_word_types = \"mean\",\n                            keep_token_embeddings = FALSE)\n## RoBERTa\nword_embeddings_roberta_test <- textEmbed(df_test[,\"clean_text\"], \n                            model = \"roberta-base\",\n                            aggregation_from_tokens_to_word_types = \"mean\",\n                            keep_token_embeddings = FALSE)\n## XLNet\nword_embeddings_xlnet_test <- textEmbed(df_test[,\"clean_text\"], \n                            model = \"xlnet-base-cased\",\n                            aggregation_from_tokens_to_word_types = \"mean\",\n                            keep_token_embeddings = FALSE)\n\nIf we inspect the generated embeddings, we see that there are 1,612 rows in the texts of the training embeddings: one per row of the training dataset. However, instead of one column with the text, we have a large number of numerical columns that represent the text. For example, in BERT, we have 768, which corresponds to the size of the BERT token embedding vector.\n\nas_tibble(word_embeddings_bert$texts$texts)\n\n# A tibble: 1,612 × 768\n   Dim1_texts Dim2_texts Dim3_texts Dim4_texts Dim5_texts Dim6_texts Dim7_texts\n        <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl>      <dbl>\n 1      0.528    -0.0955     0.0600     0.124      0.533      -0.420     0.0723\n 2      0.507    -0.0987     0.0220    -0.188      0.181      -0.655     0.356 \n 3      0.448    -0.0161     0.257     -0.485     -0.310      -0.194     0.190 \n 4     -0.117    -0.318      0.418     -0.119      0.216      -0.511     0.287 \n 5     -0.629    -0.0899     0.0623    -0.0132     0.143       0.292    -0.210 \n 6      0.421    -0.211      0.318      0.104     -0.0157     -0.366     0.231 \n 7      0.615     0.0502     0.561     -0.406      0.0821     -0.309    -0.0774\n 8      0.126     0.0952     0.146      0.113     -0.318      -0.142    -0.350 \n 9      0.287     0.0764     0.261     -0.0609     0.0596     -0.345     0.0266\n10      0.547    -0.272      0.163      0.0564    -0.197      -0.391     0.0832\n# ℹ 1,602 more rows\n# ℹ 761 more variables: Dim8_texts <dbl>, Dim9_texts <dbl>, Dim10_texts <dbl>,\n#   Dim11_texts <dbl>, Dim12_texts <dbl>, Dim13_texts <dbl>, Dim14_texts <dbl>,\n#   Dim15_texts <dbl>, Dim16_texts <dbl>, Dim17_texts <dbl>, Dim18_texts <dbl>,\n#   Dim19_texts <dbl>, Dim20_texts <dbl>, Dim21_texts <dbl>, Dim22_texts <dbl>,\n#   Dim23_texts <dbl>, Dim24_texts <dbl>, Dim25_texts <dbl>, Dim26_texts <dbl>,\n#   Dim27_texts <dbl>, Dim28_texts <dbl>, Dim29_texts <dbl>, …\n\n\n\n\n3.6 Training the model\nNow that we have converted our text data into numeric variables through word embeddings, we are ready to train our ML model. Specifically, we will train a Random Forest classifier algorithm. We will rely on the textTrainRandomForest function from the text package for this purpose. As training data, we will enter the embeddings from the previous step (for each of the three BERT-like models). As an output, we will enter the column that contains the codes in the original training data: \"annotation_target\". We will use 5-fold cross validation to train the model (outside_folds argument). For more information about the training function, refer to the text package documentation.\n\ntrained_model_bert <- textTrainRandomForest(\n  x = word_embeddings_bert$texts,\n  y = data.frame(as.factor(df_train[[\"annotation_target\"]])),\n  outside_folds = 5,\n  simulate.p.value = TRUE,\n  append_first = TRUE,\n  multi_cores = TRUE,\n  seed = seed\n)\n\ntrained_model_roberta <- textTrainRandomForest(\n  x = word_embeddings_roberta$texts,\n  y = data.frame(as.factor(df_train[[\"annotation_target\"]])),\n  outside_folds = 5,\n  simulate.p.value = TRUE,\n  append_first = TRUE,\n  multi_cores = TRUE,\n  seed = seed\n)\n\ntrained_model_xlnet <- textTrainRandomForest(\n  x = word_embeddings_xlnet$texts,\n  y = data.frame(as.factor(df_train[[\"annotation_target\"]])),\n  outside_folds = 5,\n  simulate.p.value = TRUE,\n  append_first = TRUE,\n  multi_cores = TRUE,\n  seed = seed\n)\n\n\n\n3.7 Using the model to predict\nWe can now use the trained models to code the remainder of the data (i.e., the word embeddings of the test dataset).\n\npredicted_bert <- textPredict(\n  model_info = trained_model_bert,\n  word_embeddings = word_embeddings_bert_test$texts\n)\npredicted_roberta <- textPredict(\n  model_info = trained_model_roberta,\n  word_embeddings = word_embeddings_roberta_test$texts\n)\npredicted_xlnet <- textPredict(\n  model_info = trained_model_xlnet,\n  word_embeddings = word_embeddings_xlnet_test$texts\n)\n\n\n\n3.8 Evaluating the model\nAs a first measure of the performance of our models, we can use the results of the training data. To ease the comparison, let us first combine all the results in a single dataframe (rbind) and adding the name of the model mutate to be able to identify each set of results. We then pivot our data (pivot_wider) to show all the metrics of each model as columns, and each model as a row.\nAs we can see in Table 10.1, RoBERTa consistently outperforms BERT and XLNet across most key metrics, showcasing its superior performance. For accuracy, RoBERTa achieves the highest value (0.788), followed closely by BERT (0.770), while XLNet (0.734) lags behind. This indicates RoBERTa’s ability to make correct predictions more consistently across the dataset. When considering balanced accuracy, which accounts for both sensitivity and specificity, RoBERTa (0.769) again leads, demonstrating its effectiveness at handling imbalanced classes. BERT performs respectably (0.748), while XLNet (0.717) shows the weakest balance between true positive and true negative rates. In terms of sensitivity (recall), RoBERTa (0.605) outperforms BERT (0.568) and XLNet (0.517), highlighting its ability to correctly identify positive instances. Meanwhile, for specificity, RoBERTa (0.934) slightly surpasses BERT (0.928) and XLNet (0.917), reinforcing its capacity to avoid false positives. For precision, RoBERTa (0.857) maintains its advantage over BERT (0.843) and XLNet (0.809), reflecting fewer false positives. The F1 score, which balances precision and recall, also favors RoBERTa (0.645), with BERT (0.610) trailing and XLNet (0.547) performing the weakest. Looking at kappa, a measure of agreement between predicted and actual labels beyond chance, RoBERTa (0.685) once more outperforms BERT (0.654) and XLNet (0.600), showcasing its robustness in classification consistency. Finally, ROC-AUC values indicate RoBERTa (0.910) as the strongest model, effectively balancing sensitivity and specificity, followed by BERT (0.889) and XLNet (0.843). Overall, the results strongly favor RoBERTa across all metrics, making it the most suitable model for text dataset coding in this scenario. BERT provides solid performance but falls slightly behind in several areas, while XLNet struggles particularly with sensitivity, F1 score, and class balance.\n\nrbind( # Combine the performance results of each model\n  trained_model_bert$results |> mutate(Model = \"BERT\"),\n  trained_model_roberta$results |> mutate(Model = \"RoBERTa\"),\n  trained_model_xlnet$results |> mutate(Model = \"XLNet\")) |>\n  select(-.estimator) |> \n  pivot_wider(names_from = \".metric\", values_from = \".estimate\") \n\n\n\n\n\nTable 1. Performance of each model\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\naccuracy\nbal_accuracy\nsens\nspec\nprecision\nkap\nf_meas\nroc_auc\n\n\n\n\nBERT\n0.770\n0.748\n0.568\n0.928\n0.843\n0.654\n0.610\n0.889\n\n\nRoBERTa\n0.788\n0.769\n0.605\n0.934\n0.857\n0.685\n0.645\n0.910\n\n\nXLNet\n0.734\n0.717\n0.517\n0.917\n0.809\n0.600\n0.547\n0.843\n\n\n\n\n\n\nIn a real setting, if we were to choose one of the three models to code our text dataset based solely on the performance metrics obtained from the training process, we would choose RoBERTa. Since in this case we have access to the fully coded dataset, let us verify if RoBERTa is still the best choice when faced with the remainder of the dataset —still unseen by any model. For this purpose, we will compare the manual codes with the automatic codes classified by our models. First, let us combine all codes into a single dataframe. We obtain the manually coded data from the test data (df_test), and the automatically coded data from the prediction results of each model (e.g., predicted_bert). We also need to convert the five possible codes to factors to be able to operate with them as “categories”.\n\nresults <- data.frame(\n  manual = as.factor(df_test$annotation_target),\n  bert = as.factor(predicted_bert$`texts__cv_method=\"validation_split\"pred`),\n  roberta = as.factor(predicted_roberta$`texts__cv_method=\"validation_split\"pred`),\n  xlnet = as.factor(predicted_xlnet$`texts__cv_method=\"validation_split\"pred`)\n)\n\nTo evaluate the results, we can use the confusionMatrix function from the caret package, which takes as an input the results of the automatic coding and the manual codes. We can combine again all the results together using rbind.\n\ncm_bert <- confusionMatrix(results$bert, results$manual)\ncm_roberta <- confusionMatrix(results$roberta, results$manual)\ncm_xlnet <- confusionMatrix(results$xlnet, results$manual)\n\nrbind(Bert = cm_bert$overall, \n      RoBERTa = cm_roberta$overall, \n      XLnet = cm_xlnet$overall)\n\nWhen comparing the accuracy and Cohen’s Kappa, after predicting the code of the test data, RoBERTa is still ahead of the two other models. The accuracy is quite high and the Cohen’s Kappa is substantial (> 0.6). Therefore, our initial choice was correct.\n\n\n\n\nTable 2. Performance of each model calculated using the test data\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy\nKappa\nAccuracyLower\nAccuracyUpper\nAccuracyNull\nAccuracyPValue\n\n\n\n\nBert\n0.737\n0.599\n0.728\n0.746\n0.409\n0\n\n\nRoBERTa\n0.770\n0.654\n0.761\n0.779\n0.409\n0\n\n\nXLnet\n0.744\n0.616\n0.734\n0.752\n0.409\n0\n\n\n\n\n\n\nIn addition, we calculate Gwet’s AC1 and Matthews Correlation Coefficient (MCC). Gwet’s AC1 provides a robust measure of inter-rater agreement for multiclass settings, addressing the limitations of traditional kappa statistics. MCC, extended to handle multiclass classifications, assesses the quality of the classifier by considering the entire confusion matrix, providing a balanced evaluation of performance across all classes.\n\n# Gwet AC1\ncalculate_gwets_ac1 <- function(table) {\n  n <- sum(table)\n  po <- sum(diag(table)) / n\n  pe <- sum(pmax(rowSums(table), colSums(table))) / n^2\n  (po - pe) / (1 - pe)\n}\n\nextra_results <- data.frame(\n  gwets_ac1 = c(calculate_gwets_ac1(cm_bert$table),\n                calculate_gwets_ac1(cm_roberta$table),\n                calculate_gwets_ac1(cm_xlnet$table)),\n  mcc = c(mcc(results$bert, results$manual),\n          mcc(results$roberta, results$manual),\n          mcc(results$xlnet, results$manual)\n  )\n)\n\nrownames(extra_results) <- c(\"Bert\", \"RoBERTa\", \"XLnet\")\nextra_results\n\n\n\n\n\nTable 3. Additional performance-metrics\n\n\n\ngwets_ac1\nmcc\n\n\n\n\nBert\n0.737\n0.611\n\n\nRoBERTa\n0.770\n0.664\n\n\nXLnet\n0.743\n0.620\n\n\n\n\n\n\nAll three models demonstrate substantial inter-rater reliability (Table 10.3) as per the benchmarks commonly used for interpreting AC1 (values above 0.70 are typically considered strong). RoBERTa is the best-performing model in this evaluation metric, followed by XLNet, with BERT slightly trailing, where the results are almost exactly matching the accuracy metric. Regarding MCC, all values here fall between 0.6 and 0.7, indicating moderate to strong correlations between the model predictions and the manual labels. Similar to Gwet’s AC1 results, RoBERTa achieves the highest MCC score, indicating the best performance in terms of balanced prediction quality.\nWe can also extract the results by class to gain more knowledge about the potential classification errors. Some models might be better at capturing some types of discussion contributions than others. The code below extracts the performance metrics by class and creates a long data frame containing the model name, the code (agree, disagree, etc.) the metric name (sensitivity, precision, etc.), and the value of the metric. We can combine the results for each model in a single dataframe using rbind.\n\n# BERT\nbyclass_bert <- cm_bert$byClass |> data.frame() |> \n  rownames_to_column() |> # Convert rowname (code) to new column\n  pivot_longer(2:12) |> # Convert to a long format where each metric is a new row\n  mutate(Model = \"BERT\") # Add the model name as a column\n\n# RoBERTa\nbyclass_roberta <- cm_roberta$byClass |> data.frame() |> \n  rownames_to_column() |>\n  pivot_longer(2:12) |> \n  mutate(Model = \"RoBERTa\")\n\n# XLNet\nbyclass_xlnet <- cm_xlnet$byClass |> data.frame() |> \n  rownames_to_column() |>\n  pivot_longer(2:12) |> \n  mutate(Model = \"XLNet\")\n\n# Combine all results together\nbyclass_all <- rbind(byclass_bert, byclass_roberta, byclass_xlnet)\n\nWe can also plot the results using ggplot2. We use a bar plot (geom_col) to plot all the metrics grouped by model. We use facet_wrap to separate the plot by coding class (agree, disagree, moderation, etc.).\n\nggplot(byclass_all, aes(x = name, y = value, group = Model, fill = Model)) + \n  geom_col(position = \"dodge2\", width = 0.9) +\n  facet_wrap(\"rowname\", ncol = 1) + \n  theme_minimal() +\n  scale_fill_manual(values = c(\"#76B7B2\", \"#B07AA1\", \"#FF9DA7\")) +\n  theme(legend.position = \"bottom\",\n        axis.text.x = element_text(angle = 45, hjust = 1)) + \n  labs(fill = \"Model\") + xlab(\"\") + ylab(\"\") \n\n\n\n\nFigure 2. Model performance comparison by class\n\n\n\n\nWhen we visualize the different performance metrics of each model per class (Figure 10.2), we can see that RoBERTa outperforms or equals the other model in 31 of the 55 comparisons, XLNet 21, and BERT 16. These findings further confirm that RoBERTa is the most suitable model for the classification task.\nIn addition, we can also test whether the combination of all three models yields better results. First we create an option to obtain the vote from all three algorithms (i.e., the statistical mode):\n\n# Calculates with class is selected by most models\ngetmode <- function(v) {\n   uniqv <- unique(v)\n   uniqv[which.max(tabulate(match(v, uniqv)))]\n}\n\nNow, we iterate through our results and, for each coded, we gather the voting of all three models. We input RoBERTa in the first position so that in case all models have come up with a different code, the one created by RoBERTa is chosen:\n\nresults_with_vote <- results |> \n  rowwise() |> \n  mutate(vote = getmode(c(roberta, xlnet, bert))) |> \n  ungroup()\n\nWe can now create the confusion matrix for the voted code. We can see that the results are slightly below those achieved by RoBERTa alone, so we are not getting any advantage by voting.\n\ncm_vote <- confusionMatrix(results_with_vote$vote, results_with_vote$manual)\ncm_vote$overall\n\n      Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull \n     0.7646994      0.6455472      0.7558531      0.7733763      0.4085125 \nAccuracyPValue  McnemarPValue \n     0.0000000            NaN \n\n\n\n\n3.9 Impact on findings\nTo complement the evaluation of our models, we assess the impact of using an automatically coded dataset on the findings derived from applying common learning analytics methods to the data. We choose RoBERTa, as it was our best performing model, to compare the results with those obtained from manually coded data.\nFirst, we need to construct the complete AI-generated dataset which contains all of the data classified by RoBERTa and the small percentage classified manually used as training. We store it in the complete_ai variable.\n\ndf_test_recoded <- df_test\ndf_test_recoded$annotation_target <- results$roberta\ncomplete_ai <- rbind(df_train, df_test_recoded)\n\nWe start by analyzing our data using sequence analysis, which is a method that is commonly used to study temporally-ordered data, and specifically collaborative learning (e.g., [51]). For a full tutorial about sequence analysis consult [52]. The first step is to order the messages in each group and to transpose the data so that is in a wide format (i.e., each column represents one message order: message 1, message 2, etc.). Then, we use seqdef from the TraMineR package [53,], to construct a sequence object, where each group is represented as a sequence of all its contributions. We use seqdplot to plot the sequence distribution plot (Figure 10.3). From visual inspection, we can see that the sequence profiles are similar, although the RoBERTa side has a lower presence of the minority codes.\n\nlibrary(TraMineR)\n\nwider_human <- df |> group_by(group_id) |> \n  mutate(order = seq_along(message_id)) |> \n  pivot_wider(id_cols=\"group_id\", names_from=\"order\",values_from = \"annotation_target\")\n\nwider_ai <- complete_ai |> group_by(group_id) |> \n  mutate(order = seq_along(message_id)) |> \n  pivot_wider(id_cols=\"group_id\", names_from=\"order\",values_from = \"annotation_target\")\n\nseq_human <- seqdef(wider_human, 2:ncol(wider_human))\nseq_ai <- seqdef(wider_ai, 2:ncol(wider_ai))\n\nseqdplot(seq_human) \nseqdplot(seq_ai)\n\n\n\n\n\n\nFigure 3. Comparing the sequence distribution plots between the AI-coded and human-coded datasets\n\n\n\n\nNext, we use transition network analysis, a novel method that implements stochastic process mining through Markov models [54], to model the transitions between codes and assess whether there are differences between the two datasets. For a full tutorial, refer to [55]. We use the tna function to generate the Markov model containing the probability of transitioning between each pair of codes. We do so for the human coded and for the data coded with the RoBERTa model. We can visualize each model using the plot function. We can also plot the difference between the transitions using plot_compare. However, this is only a visual comparison. We can run a permutation test (permutation_test) to statistically compare the two models and visualize only the statistically significant differences (Figure 10.4).\n\nlibrary(tna)\n# Compute transition probabilities\ntransitions_ai <-  tna(seq_ai)\ntransitions_human <-  tna(seq_human)\n\n# Plot human coded transition network\nplot(transitions_human)\n\n# Plot RoBERTa transition network\nplot(transitions_ai)\n\n# Plot difference network\nplot_compare(transitions_human, transitions_ai)\n\n# Run a permutation test to assess which differences are statistically significant\npermutation <- permutation_test(transitions_human, transitions_ai, it = 1000)\n\n# Plot the significant differences identified in the permutation test\nplot(permutation, minimum = 0.01)\n\n\n\n\n\n\n\n(a) Human-coded\n\n\n\n\n\n\n\n(b) RoBERTa\n\n\n\n\n\n\n\n\n\n(c) Difference network\n\n\n\n\n\n\n\n(d) Permutation test\n\n\n\n\nFigure 4. Comparing the transition plots between the AI-coded and human-coded datasets\n\n\n\nIn the transition network created from human codes, Solution and Agree are central nodes with strong self-loops and frequent transitions between them. Moderation shows significant connections from Agree and Solution, while Disagree is weakly integrated. In the RoBERTa network, Solution and Agree remain central, and Disagree still has low connections. The difference network highlights that Reasoning plays a stronger role in RoBERTa’s coding, especially in its links from Agree and Solution. Disagree is less integrated, and some transitions, like from Moderation to Reasoning, are significantly reduced.\nWe could continue with other relevant methods, like clustering participants according to their argumentation profiles or role in the discussion, but we have already demonstrated that the results somewhat differ between the two sets of codes. However, the overall takeaway for both versions is that Reasoning is the driver of the conversation. It is not expected that two human-coded datasets yielded more similar results."
  },
  {
    "objectID": "chapters/ch10-bert/ch10-bert.html#conclusion",
    "href": "chapters/ch10-bert/ch10-bert.html#conclusion",
    "title": "10  Using BERT-like Language Models for Automated Discourse Coding: A Primer and Tutorial",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nThis tutorial examined the use of BERT-like language models for automating discourse coding in R, showcasing how these models can expedite qualitative text classification while maintaining an acceptable level of accuracy. We illustrated this with a case study on collaborative problem-solving. Among the models tested, RoBERTa demonstrated better performance compared to BERT and XLNet, making it the most effective model for the classification task. We exemplified how applying different analytical techniques from the learning analytics repertoire, including sequence analysis and transition network analysis, revealed somewhat different insights from the human coded data than from the automatically coded data. However, there was an overall alignment between both in capturing the broad patterns of discourse and the disagreement was comparable to the one expected between two humans. As large language models continue to advance, we anticipate even greater accuracy in automated discourse coding, making automated coding an increasingly reliable tool for qualitative research.\nAlthough automated coding with large language models offers promising efficiency and consistency, caution is warranted when interpreting the results. As pointed out earlier in this chapter, the reliability of automated methods can vary based on the context, dataset characteristics, and the specific model used. Differences in coding accuracy may introduce biases or overlook human judgment, which could impact downstream analyses. We must critically evaluate the output of automated coding by comparing it against human-coded benchmarks and incorporating rigorous metrics like Gwet’s AC1 or Matthews Correlation Coefficient. Additionally, researchers should remain mindful of the iterative and reflexive nature of qualitative coding, which automated systems cannot fully replicate. Therefore, automated methods should be viewed as complementary to, rather than replacements for, human expertise in the coding process."
  },
  {
    "objectID": "chapters/ch11-llmsxai/ch11-llmsxai.html",
    "href": "chapters/ch11-llmsxai/ch11-llmsxai.html",
    "title": "11  LLMs for Explainable Artificial Intelligence: Automating Natural Language Explanations of Predictive Analytics Models",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch11-llmsxai/ch11-llmsxai.html#introduction",
    "href": "chapters/ch11-llmsxai/ch11-llmsxai.html#introduction",
    "title": "11  LLMs for Explainable Artificial Intelligence: Automating Natural Language Explanations of Predictive Analytics Models",
    "section": "1 Introduction",
    "text": "1 Introduction\nLarge language models (LLMs) have recently become a transformative technology thanks to their ability to process natural language and generate close-to-human quality text [1]. Before LLMs, generating human-like text automatically was often a slow and manual process, requiring a lot of time and expertise. Since LLMs have been trained on vast amounts of data, they enable the automation of text generation, with little to no human intervention. Recently, the field of learning analytics has recognized the potential of generative artificial intelligence (AI) as a whole, and LLMs in particular, in various stages of the learning analytics cycle such as generating synthetic data, automated scoring and coding or feedback generation [2].\nIn this tutorial, we focus on the last phase of the learning analytics cycle: reporting the results of learning analytics applications and turning them into actionable insights [3]. Specifically, we will focus on making the results of predictive models understandable for teachers and students to aid decision making. LLMs have emerged as a powerful tool in this domain, particularly in enhancing the explainability of AI models, also known as Explainable AI (XAI). XAI refers to techniques and methods that enable humans to understand, trust, and make the right decisions based on the outputs generated by AI systems [4]. In learning analytics, XAI has the potential to become an extremely central tool because it allows teachers, students, parents and administrators to understand the rationale behind the predictions made by AI models.\nTraditionally, explaining the results of complex predictive models required specialized knowledge in AI. However, LLMs have the potential to bridge this gap by generating natural language explanations that are both accurate and accessible to non-experts. For instance, when a predictive model identifies that a student is at risk of not completing a course, an LLM can be used to generate a human-readable explanation of why the model reached that conclusion. This explanation might include an overview of the key factors that influenced the prediction, such as the student’s engagement with course materials, performance on assessments, and participation in class discussions. The LLM can then articulate this information in a way that is tailored to the teacher’s or student’s specific context, providing clear guidance on potential interventions or areas for improvement rather than an endless list of numbers, tables and graphs.\nThe application of generative AI in general, and LLMs in particular, to learning analytics and education research and practice is still in its early stages [5]. One could say that the potential of LLMs to make learning analytics insights more accessible has been barely explored. Still, a few studies demonstrating LLMs’ ability to simplify very technical information and generate personalized, human-readable feedback for learners, teachers and other stakeholders can be mentioned here. One of the few existing examples is the study by Pinargote et al. [6], in which an LLM was used to transform insights shown in a learning analytics dashboard into textual explanations. While students appreciated the metrics and feedback for understanding group contributions, opinions varied on the fairness and necessity of detailed feedback. The study by Susnjak [7] proposed a framework that combines XAI with LLMs —similar to what we are doing in this tutorial— to offer prescriptive analytics to students at risk. A formal evaluation of the framework is still needed to validate it empirically.\nIn this tutorial, we will explore how to use LLMs to enhance the explainability of predictive models. Specifically, we will demonstrate how to generate natural language explanations of XAI results using LLMs. The reader will understand how to embed LLMs in the XAI pipeline to create personalized and contextualized explanations that make predictive model outputs understandable, helping teachers and students make informed decisions based on AI-generated insights."
  },
  {
    "objectID": "chapters/ch11-llmsxai/ch11-llmsxai.html#motivation-and-relevant-work-for-using-llms-on-xai-output-in-learning-analytics-research",
    "href": "chapters/ch11-llmsxai/ch11-llmsxai.html#motivation-and-relevant-work-for-using-llms-on-xai-output-in-learning-analytics-research",
    "title": "11  LLMs for Explainable Artificial Intelligence: Automating Natural Language Explanations of Predictive Analytics Models",
    "section": "2 Motivation and Relevant Work for using LLMs on XAI Output in Learning Analytics Research",
    "text": "2 Motivation and Relevant Work for using LLMs on XAI Output in Learning Analytics Research\nPrevious chapters of this book have introduced and discussed XAI in detail [4, 8, 9]. To summarise, XAI techniques, such as feature importance, help explain the outputs of black-box machine learning models. For example, in authorship detection [10], XAI algorithms identify and quantify key features in text that distinguish students’ writing profiles. Similarly, in predicting student outcomes [11], XAI highlights how different aspects of students’ online learning activities contribute to the model’s prediction of their results. For instance, an XAI result might show a feature importance rating of 0.8 for starting assessments early and 0.6 for forum participation when predicting a “distinction” grade. This indicates that starting assessments early has a greater influence on the prediction than forum participation. In such cases, XAI provides actionable feedback by not only identifying “who is at risk” but also implying “how to improve”.\nHowever, despite their utility, XAI outputs can be challenging for students and educators to interpret due to two key reasons. First, XAI results are typically numerical or visual representations (e.g., feature importance graphs) that may not intuitively convey actionable insights. For example, understanding that a “0.8 feature importance” correlates with a behaviour requires familiarity with both the AI model’s purpose and the assumptions underlying its predictions. Expecting students and educators to possess such domain knowledge in XAI and data literacy is often unrealistic.\nSecond, even when XAI outputs are understood, human biases can influence interpretation and decision-making. For instance, over-reliance on XAI outputs may lead to uncritical trust in predictions, while under-reliance due to distrust or skepticism may result in the outputs being ignored [12, 13]. This highlights the need for additional support mechanisms to help interpret and act on XAI results effectively.\n\n2.1 Addressing the Challenge with LLMs\nLLMs, with their inherent ability to process and present information in natural language, can bridge the gap between XAI outputs and human understanding. Thanks to their ability to translating complex, technical insights into accessible, human-readable language, LLMs have the potential to enhance the interpretability and actionable value of XAI systems. For instance, instead of presenting raw feature importance values, an LLM can generate contextualised feedback such as: “The prediction is largely influenced by your consistent forum participation and early completion of assessments, which have shown a strong correlation with achieving higher grades in this course.”\nThis capability aligns with research exploring the use of LLMs in combination with XAI across diverse domains. For example, Khediri et al. [14] used XAI to identify critical network features that contributed to intrusion detection and employed LLMs to generate user-friendly explanations. These explanations improved decision-making by helping users understand the nature of detected anomalies. Mekrache et al. [15] applied XAI to anomaly detection in network operations and used LLMs to transform technical results into actionable insights for operators, enhancing both trust and operational efficiency. In the field of learning analytics, Swamy et al. [16] introduced the iLLuMinaTE framework, which combines XAI and LLMs to extract and interpret feature importance from students’ online behaviours. Tested on three MOOCs, this framework provided explanations that 89.52% of students found more actionable and trustworthy than traditional XAI tools. However, it struggled in contexts requiring deeper contextual understanding, highlighting an area for improvement.\nBuilding on these efforts, Susnjak [7] developed a comprehensive framework that integrates predictive analytics (identifying at-risk students) with prescriptive analytics (recommending actionable interventions). This framework operates in three stages. First, ML models (e.g., Random Forest) predict student success and use XAI techniques like SHAP to identify key behavioural features influencing these predictions. Then, using counterfactual modeling, the framework generates “what-if” scenarios to determine the minimal changes needed for a positive outcome. In the last phase, LLMs translate predictive and counterfactual insights into personalised, actionable messages. For example, a message generated by Susnjak’s [7] framework might read: “You’re currently a full-time student studying towards a 200-credit degree with an average grade of 53%. To maximise your success, consider engaging more with online materials and quizzes.” In this framework, the integration of XAI and LLMs ensures that XAI identifies the “why” behind predictions. LLMs convey the “what next” in terms of practical guidance.\nIn addition to frameworks like Susnjak’s, researchers are exploring alternative applications of LLMs in learning analytics that bypass traditional XAI techniques. For instance, Pinargote et al. [6] investigated the potential of LLMs to directly process contextual information about students’ learning environments and behavioral data. After embedding these details into carefully designed prompts, LLMs were tasked with generating comprehensive summaries of learning analytics dashboards. These dashboards were not only data-rich but also included natural language narratives that translated raw data into actionable insights for educators and students.\nUnlike traditional XAI approaches, which rely on detailed feature importance metrics to explain predictions, Pinargote’s approach makes use of LLMs to combine data interpretation with contextual knowledge. This approach highlights the capability of LLMs to humanize data insights, making them more accessible to non-technical users. Additionally, the incorporation of contextual cues (e.g., the type of course, common behaviours of top-performing students, or seasonal trends in engagement) allows LLMs tp generate recommendations that are relevant to specific learning environments. This customization enhances the relevance and usability of the feedback, addressing one of the primary challenges in learning analytics: translating technical results into actionable strategies for improvement.\nWhile promising, these alternative approaches also raise critical questions about accuracy and generalisability. For instance, relying solely on LLMs to interpret behavioral data may introduce biases or hallucinations in the generated insights. Moreover, the extent to which these narratives align with actual student needs and institutional goals requires further investigation. Despite these challenges, the potential of LLMs to complement traditional XAI methods in specific contexts demonstrates their versatility in advancing learning analytics research.\nAs we explore these innovative uses of LLMs, it becomes clear that their integration into learning analytics frameworks is not merely a tool for explanation but a transformative step towards making learning analytics truly human-centered. LLMs’ ability to simplify complex data and align insights with users’ contexts opens new pathways for actionable, personalized, and impactful feedback in education."
  },
  {
    "objectID": "chapters/ch11-llmsxai/ch11-llmsxai.html#choosing-an-llm-and-interacting-with-it-via-api",
    "href": "chapters/ch11-llmsxai/ch11-llmsxai.html#choosing-an-llm-and-interacting-with-it-via-api",
    "title": "11  LLMs for Explainable Artificial Intelligence: Automating Natural Language Explanations of Predictive Analytics Models",
    "section": "3 Choosing an LLM and interacting with it via API",
    "text": "3 Choosing an LLM and interacting with it via API\nMany commercial LLMs offer both web-based and programmatic interfaces to interact with their models. These are undoubtedly an easy solution to access AI in its advanced form. However, there are two major drawbacks of relying on commercial LLMs. The first one is that commercial solutions often require a paid subscription which may not be a possibility for all researchers. The second and most important one is that relying on commercial products raises concerns about the privacy of students’ data, especially if they are used to train future models.\nOpen-source LLMs are a viable alternative that can be deployed locally in a way that students’ personal or sensitive data is kept to the organization. Furthermore, open-source LLMs are becoming increasingly capable. Although open-source alternatives often require more technical knowledge than commercial ones to be deployed and secured, there is an increasing number of off-the-shelf solutions that address this difficulty and make installing and using local LLMs far much easier. For instance, an application that is becoming increasingly popular is LM Studio, a desktop platform that allows users to run almost any LLM available in Hugging Face1 that has been converted to gguf, including Llama 3, Phi 3, Falcon, Mistral, StarCoder and Gemma. LM Studio deploys a local server that can be interfaced through a programmable application interface (API), more specifically a REST API, and therefore be used from any program (e.g., an R script). Moreover, many existing applications use OpenAI API standards which makes them compatible with most existing libraries that implement OpenAI clients.\nFor our tutorial, we are going to use LM Studio as our backend. To install it, follow the instructions on their website2 for your operating system and run the app (Figure 11.1).\n\n\n\nFigure 1. LM Studio interface\n\n\nThe next step would be to download one of the existing models. For the sake of this tutorial we are going to use a Gemma model (lmstudio-ai/gemma-2b-it-GGUF)3. Gemma has been developed based on the principles of Gemini (Google’s frontier model) and publicly and freely available at Hugging Face. This LLM is quite small (1.50GB on disk) but still achieves good performance. Feel free to choose a different model, depending on your device’s capabilities and the availability of more recent releases at the time you are reading this chapter. Search for the chosen model in the LM Studio app and download it by clicking on the “Download” button next to its name (Figure 11.2).\n\n\n\nFigure 2. Downloading a model in LM studio\n\n\nOnce you have downloaded the model, you need to initialize the server4. First, click on the Local Server tab (the <-> icon on the left sidebar). On the top bar, select the LLM you have downloaded from the dropdown list and then start the server by clicking on the “Start Server” button. By default, your server will run on the port 1234 and will have the default API key “lm-studio”, but you can change this configuration if you so wish. Once the server has started, it will be ready to receive and reply to API requests (Figure 11.3).\n\n\n\nFigure 3. Downloading a model in LM studio\n\n\nTo interact with the API from an R script, we will use the ellmer R package [17]. This R package offers a wrapper function for interacting with many well-known LLM platforms. The package enables users to send prompts, retrieve the generated responses, and customize API interactions directly within R. It is created and maintained by the tidyverse [18] community, which is one of the most active in R. Below we import the library:\n\nlibrary(ellmer) # install.packages(\"ellmer\")\n\nThe next step after installing the library is creating a client to interact with the LLM API. The ellmer package offers a variety of clients including Anthropic’s Claude, AWS Bedrock, Azure OpenAI, Databricks, GitHub model marketplace, Google Gemini, Groq, Ollama, OpenAI, perplexity.ai, Snowflake Cortex, and VLLM. In our case, we are going to choose the OpenAI client which is compatible with our “lmstudio-ai/gemma-2b-it-GGUF” model in LM studio.\nThe client will include the connection information to access LM studio. Change the code accordingly if you have configured a different port and API key in LM Studio. If instead of using LM studio you prefer to use OpenAI, simply omit the base_url argument and provide the right OpenAI API key in the api_key argument. If you are using another LLM backend, check the ellmer documentation to choose the right client instead of chat_openai. Specify the model name according to the model you have downloaded in LM studio (“lmstudio-ai/gemma-2b-it-GGUF” in our case), or the model of your choice within OpenAI (e.g., gpt-3.5-turbo) if you chose that option. We can also specify a system prompt (system_prompt) if we so wish [19].\n\nclient <- chat_openai(\n    base_url = \"http://localhost:1234/v1\",\n    model = \"lmstudio-ai/gemma-2b-it-GGUF\",\n    api_key = 'lm-studio',\n    system_prompt = \"You are an assistant that is expert in explainable AI for\n    providing learning analytics recommendations.\",\n)\n\nYou can check whether you are able to connect to your local LM studio by sending a simple prompt. We will use the client that we have just created and enter the content of our prompt as an argument to the chat function. For example:\n\nclient$chat(\"Who are you?\")\n\n\n\n\n\n\n\nI am a language model designed to assist with learning analytics and explainable \nAI (XAI) tasks. I have been trained on a massive  dataset of text and code and \nam able to provide insights and recommendations related to XAI.\n\n**Here are some of the specific ways I can assist you with learning analytics \nand XAI:**\n\n* **Identifying patterns and relationships in data:** I can analyze data to \nidentify patterns, trends, and outliers that could be  indicative of important \ninsights or anomalies.\n* **Ranking and filtering data points:** I can rank data points based on their \nrelevance to a specific query or hypothesis,allowing you to focus on the most \nimportant information.\n* **Visualizing data and insights:** I can generate visualizations, charts, and \nother representations of data and insights to help  you understand them more \neasily.\n* **Providing explanations for AI models:** I can explain the decisions made by \nAI models, including the data they consider, the  algorithms used, and the \nweightings assigned to different features.\n* **Recommending learning algorithms and techniques:** Based on the data and \ninsights I identify, I can recommend appropriate \nlearning algorithms and techniques for further analysis.\n\n**I am here to assist you in any way that I can, so please do not hesitate to \nask me any questions you may have.**\n\n\n\nIf you get an error, there could be several causes: that your server is not running, that you are running it on a different port than the default (1234), or that you provided the wrong model name, among other possibilities. If everything is in order, you will see the output of the prompt as shown above.\nNow that we have our infrastructure working, we can learn how to use it to enhance our learning analytics pipeline. In the next section, we present a case study about explaining and providing recommendations based on the results of an XAI predictive model. We will use the Gemma LLM through the LM studio API using the ellmer package just like we have learned in the previous steps."
  },
  {
    "objectID": "chapters/ch11-llmsxai/ch11-llmsxai.html#a-case-study-about-predictive-modeling",
    "href": "chapters/ch11-llmsxai/ch11-llmsxai.html#a-case-study-about-predictive-modeling",
    "title": "11  LLMs for Explainable Artificial Intelligence: Automating Natural Language Explanations of Predictive Analytics Models",
    "section": "4 A case study about predictive modeling",
    "text": "4 A case study about predictive modeling\n\n4.1 Introduction\nIn previous chapters of this book, we learned about XAI [4, 8, 20] and how to implement a pipeline for predicting students’ performance based on their online engagement [21, 22], which has been an established goal of learning analytics and AI in education [5]. The overall workflow is depicted in Figure 11.4. It is common to start with an exploratory data analysis (EDA) (A), as depicted in [21] followed by data preparation (B). Then we split the data into a training set and a testing set (C) and we use the training set to train an ML model (D) and evaluate its performance (E). We then use XAI techniques (F) to understand the model in general (i.e., global explainability, [20]) or specific predictions (i.e., local explainability, [8]). In this chapter, we go one step further and use LLMs (G) to explain the XAI outcomes using natural language, honoring one of the main uses of LLMs, which is to provide automated personalized feedback [1]. Please, consult the previous chapters to learn more about XAI and LLMs.\n\n\n\nFigure 4. XAI + LLM workflow implemented in the tutorial\n\n\n\n\n4.2 Model fitting\nFirst, we are going to load all the necessary libraries, in addition to ellmer [17], which we have already described in the previous section:\n\nrio: rio stands for “R Input/Output” and is a package used for importing and exporting data files [23]. It supports a wide variety of file formats like CSV, Excel, JSON, and more. We will use it to load our dataset from an external file.\ntidyverse: tidyverse is a collection of R packages designed for data science [18]. It includes packages like dplyr for data manipulation, tidyr for tidying data, and several others. In the tutorial, we will use it to manipulate and explore our data by filtering, selecting, or transforming data.\nrsample: The rsample package provides functions to create resampling objects that can be used to evaluate and tune models [24]. It includes methods like bootstrapping, cross-validation, and splitting data into training and test sets. In this tutorial, it is used to split the dataset into training and testing sets. This is a crucial step in building and evaluating machine learning models.\ne1071: The e1071 package includes functions for various machine learning algorithms [25], especially support vector machines (SVMs), which is the family of models that we are using in this tutorial.\nDALEX: The package DALEX (Descriptive mAchine Learning EXplanations) is used for model interpretation [26]. It provides tools to explain and visualize how machine learning models make predictions, making them more transparent and understandable. In the tutorial, DALEX is the engine that performs XAI.\n\n\nset.seed(50)\n# Load necessary libraries\nlibrary(rio)\nlibrary(tidyverse)\nlibrary(rsample)\nlibrary(e1071) \nlibrary(DALEX)\n\n\n\n\nThe next step is importing our data, which is available on a Github repository along with most of the datasets used in the book, and assign it to the student_data variable. The dataset we are using is a synthetic dataset based on the study by Jovanović et al [27] and consists mostly of behavioral engagement indicators that can be obtained from the logs captured by the learning management system (LMS). Refer to Chapter 3 for a more detailed descritpion and EDA.\n\n# Import the data\nstudent_data <- \n      import(\"https://raw.githubusercontent.com/lamethods/data2/main/lms/lms.csv\")\n\nThen, we follow the typical machine learning pipeline (Figure 11.4). The EDA of this dataset is presented in Chapter 3 so we omit it here for brevity. We prepare the data by standardizing all numeric columns in (i.e., transforming them to have a mean of 0 and a standard deviation of 1) (Figure 11.4–B). This is important for many machine learning models that are sensitive to the scale of features. Then, we split our data into training (80%) and testing (20%) using the rsample package functions (Figure 11.4–C). The training set will be used to fit the model (Figure 11.4–D), while the test set is used to evaluate its performance (Figure 11.4–E). Please, bear in mind that in a real setting more data cleaning and pre-processing operations are likely to be needed before being able to utilize the data.\n\n# Standardize the numeric columns\nstudent_data_standardized <- student_data |>\n  mutate(across(where(is.numeric), ~scale(.) |> as.vector()))\n\n# Split the data into training and testing sets\ndata_split <- initial_split(student_data_standardized, prop = 0.8)\ntrain_data <- training(data_split)\ntest_data <- testing(data_split)\n\nWe then specify the formula of our model. The formula defines the relationship between the target variable (Final_Grade) and the predictor variables (engagement indicators like course views, forum contributions, etc.). We use the formula and the training data to fit an SVM model (Figure 11.4–D).\n\n# Define the formula to specify the relationship between the target variable \n# and the predictor variables\nformula <- Final_Grade ~ Freq_Course_View + Freq_Lecture_View + \n  Freq_Forum_Consume + Freq_Forum_Contribute + Regularity_Course_View +\n  Regularity_Lecture_View + Regularity_Forum_Consume + \n  Regularity_Forum_Contribute + Session_Count + Total_Duration + Active_Days\n\n# Fit the SVM model\nsvm_fit <- svm(formula, data = train_data, kernel = \"radial\")\n\n\n\n4.3 Model performance\nNow that we have created and fitted our ML model, we can evaluate how well the model performed (Figure 11.4–E) and use XAI to understand how it made decisions. The explain() function from DALEX creates an explainer object for the SVM model. This explainer is used to assess and visualize the model’s performance.\n\nset.seed(50)\n\n# Create an explainer with DALEX\nexplainer_svm <- explain(\n  svm_fit,\n  data = dplyr::select(test_data, -Final_Grade),\n  y = test_data$Final_Grade,\n  label = \"SVM\",\n  verbose = FALSE\n)\n\nFor instance, the model_performance function calculates various performance metrics for the model using the test data. It return a performance object (mp_svm) that contains statistics and diagnostic information about the model’s predictive accuracy. Some of the common metrics provided are:\n\nRMSE (Root Mean Square Error): Measures the average error between the predicted values and the actual values. Lower RMSE values indicate better model performance.\nMAE (Mean Absolute Error): Another measure of the average magnitude of prediction errors, which is less sensitive to outliers compared to RMSE.\nR-squared: The proportion of the variance in the dependent variable that is predictable from the independent variables.\nResiduals: The difference between the observed target values and the predicted values for each test observation.\n\n\nmp_svm <- model_performance(explainer_svm)\nmp_svm\n\nMeasures for:  regression\nmse        : 0.5556946 \nrmse       : 0.7454493 \nr2         : 0.4063128 \nmad        : 0.4442718\n\nResiduals:\n         0%         10%         20%         30%         40%         50% \n-1.64093297 -0.58085583 -0.41026001 -0.07698939  0.07855870  0.28133263 \n        60%         70%         80%         90%        100% \n 0.34303022  0.51622444  0.76114088  1.06787159  1.81745294 \n\n\nMeasures for:  regression\nmse        : 0.56 \nrmse       : 0.75\nr2         : 0.41\nmad        : 0.44\n\nResiduals:\n0%    10%   20%   30%    40%   50%   60%   70%   80%   90%   100% \n-1.64 -0.58 -0.41 -0.08  0.08  0.28  0.34  0.52  0.76  1.07  1.82 \nLet us now see how an LLM can be used to explain the performance results to a practitioner (Figure 11.4–G). For that purpose, we use the client$chat function and we create an appropriate prompt. According to the DAIR.AI’s Prompt Engineering Guide [28], the main elements of a prompt are: instruction, input data, output format, and context information. We follow these guidelines and the work by Zytek et al. [29] to draft the instruction, context, and desired output format of our prompt, and we add the input data from the model performance object. To make sure we pass the text version of the results rather than the R object itself, we use the capture.output function to capture the output of calling the command print on the object and we assign it to a variable named mp_svm_output. The output is collected as a vector of lines of text, so we need to combine all the lines together into a single textual variable using paste(mp_svm_output, collapse = \"\\n\"). We then combine the prompt with the results using the same technique. Lastly, we use the client$chat function to send the prompt to the LLM.\n\n\n# Capture print output of the model performance\nmp_svm_output <- capture.output(mp_svm) \n\n# Put it together into a single string of text\nmp_svm_print <- paste(mp_svm_output, collapse = \"\\n\")\n\n# Combine generic prompt with our model performance\nmv_svm_prompt <- paste(\"You are helping teachers understand an ML model’s \nprediction. The model predicts students' performance based on engagement measures. \nI will give you the model performance evaluation. Come up  with a textual \nnon-technical explanation  of how well the model performs that I could include in \na dashboard. Take into account that the data has been standardized:\",\nmp_svm_print,  collapse = \"\\n\")\n\n# Send the prompt to the LLM and print the results\nclient$chat(mv_svm_prompt)\n\n\n\n\n\n\n\nThe model successfully predicts students' performance based on engagement \nmeasures, achieving a mean squared error (MSE) of 0.4504274, a root mean \nsquare error (RMSE) of 0.6711389, and an r² value of 0.4785088. These indicate \nthat the model is performing well in predicting performance based on engagement \nmeasures.\n\nThe model also achieves a median Absolute Deviation (MAD) of 0.4217903, which \nsuggests that the model can effectively capture the typical difference between \nindividual student scores on the engagement measures. The model also has a high \npercentage of residuals within the range of -0.1 to 0.1, indicating that the \nmodel is able to make accurate predictions while considering the effect of \nengagement  measures on student performance.\n\n\n\n\n\n4.4 Feature importance\nThe LLM has offered an explanation for the overall performance results, but still we would need to know what are the variables that are influencing the model’s decisions. A common explainable XAI technique that we can use for this purpose is feature importance (Figure 11.4–F). We can use this technique (implemented in the feature_importance function of DALEX) to calculate how much each feature contributes to the prediction. Features with high importance have a bigger influence on the final grade predictions. From the results we can see that the number of contributions to the forum is the most predictive feature, followed by the regularity thereof (Figure 11.5). Next were the session count, the total time spent online and the regularity of visiting the course main page.\n\nvi_svm <- feature_importance(explainer_svm)\nvi_plot <- plot(vi_svm)\nvi_plot \n\n\n\n\nFigure 5. Feature importance\n\n\n\n\nWe can ask the LLM to come up with an explanation of these results (Figure 11.4–G). First, we extract the statistics from the feature importance. The results of the feature imporatance function returns the raw data after each permutation. If we want to get the final results, the easiest way is to obtain them from the plot (vi_plot$data). We order the results from most important to least important, and we display them as pairs (Variable, loss), as suggested by [29].\n\nvi_svm_text <- vi_plot$data |> # Extract data from the plot\n  arrange(desc(median)) |> # Order by median value\n  mutate(Features = # Create pairs (feature, median feature value)\n           paste0(\"(\",variable,\", \", round(median, 2), \")\")) |> \n  pull(Features) |> # Extract the column that contains the pairs\n  paste(collapse = \"\\n\") # Create a single text variable that contains all pairs\ncat(vi_svm_text) # Print output\n\n(Freq_Forum_Contribute, 0.91)\n(Regularity_Forum_Contribute, 0.8)\n(Session_Count, 0.79)\n(Regularity_Course_View, 0.78)\n(Active_Days, 0.78)\n(Total_Duration, 0.77)\n(Freq_Course_View, 0.77)\n(Regularity_Lecture_View, 0.76)\n(Freq_Lecture_View, 0.75)\n(Freq_Forum_Consume, 0.75)\n(Regularity_Forum_Consume, 0.75)\n\n\nWe now follow the same prompt guidelines as before [28, 29] to construct our prompt. We combine it with the feature importance output using paste and we send it to the LLM with client$chat.\n\nclient$chat(paste(\"I will give you the results of permutation based \nfeature importance of a model computed with DALEX in the form \n(feature, median_dropout_loss). Please provide a clear, concise and understandable \nnarrative  for teachers that explains how these contributions influence students' \npredicted grades. Ensure the explanation highlights the significance of each \nfeature's contribution to the overall prediction in order\",\nvi_svm_text, collapse = \"\\n\"))\n\n\n\n\n\n\n\nThe results of the permutation-based feature importance computed with DALEX \nprovide a clear view of how various factors influence students' predicted \ngrades. Here’s a  concise explanation of each feature's contribution to the \nmodel’s predictions:\n\n    *Frequent Forum Contributions* (0.91): The most important factor. Active \n    participation in forums signals higher engagement with the material and \n    likely leads to better understanding and academic performance.\n    \n    *Session Count* (0.73): The number of sessions reflects how often students \n    interact with the platform. More sessions indicate consistent effort, which \n    improves learning outcomes.\n    \n    *Regular Forum Contributions* (0.71): Consistent engagement in forums over \n    time  is crucial. Regular interaction helps students stay engaged, \n    enhancing  their grasp of the subject.\n    \n    *Regular Course Views* (0.71): Regularly viewing course materials helps \n    students keep up with the content, reinforcing their knowledge and boosting \n    their grades.\n    \n    *Total Engagement Duration* (0.71): The total time spent engaging with \n    coursecontent reflects dedication. More time on task generally correlates \n    with better academic outcomes.\n    \n    *Regular Forum Consumption* (0.7): Regularly reading forum posts without \n    contributing also has a significant impact. This allows students to learn \n    from others’ insights and discussions.\n    \n    *Active Days* (0.7): The number of days a student actively participates in \n    the course is another key indicator of consistent engagement, which is \n    closely tied to higher performance.\n    \n    *Frequent Forum Consumption* (0.69): Reading forums frequently helps \n    students  stay informed about discussions, reinforcing their learning.\n    \n    *Regular Lecture Views* (0.69): Watching lectures on a regular basis is \n    important for knowledge retention and consistent progress through the course.\n    \n    *Frequent Course Views* (0.68): Students who frequently revisit course \n    materials are more likely to master the content, contributing to better \n    academic results.\n    \n    *Frequent Lecture Views* (0.66): Watching lectures often helps students \n    review  and understand complex concepts, leading to improved performance.\n\nIn conclusion, the most influential factors driving student performance are \nactive and regular engagement, particularly through forums and course \ninteractions. These behaviors reflect deeper learning and a steady approach to \nstudying, leading to higher predicted grades.\n\n\n\nThe LLM correctly identifies the important features and how important they are in the final decision. However, the LLM seems to be making some inferences about whether the variables positively or negatively predict the final grade, which is not something that we can obtain from the feature importance solely. It also makes some strange interpretations, like reading posts without contributing has a significant impact inference, which is not very plausible.\n\n\n4.5 Partial Dependence Profile\nThe Partial Dependence Profile (PDP) is another XAI technique, which used to explain the relationship between a specific feature and the predicted outcome of a machine learning model. It helps visualize and understand how changes in a feature affect the model’s predictions, while keeping all other features constant. Contrary to feature importance which calculates to which extent a feature affects the prediction accuracy, the PDP helps us understand how increasing or decreasing the value of a certain feature affects the final outcome. For example, if a student increases the number of forum contributions, will they be predicted as having a higher or lower final grade? To obtain this information we use the model_profile function from DALEX and plot the results (Figure 11.6).\n\npdp_svm_fcontrib <- model_profile(explainer_svm, variables = \"Freq_Forum_Contribute\")\nplot(pdp_svm_fcontrib) + theme(plot.subtitle = element_blank())\n\n\n\n\nFigure 6. PDP of Freq_Forum_Contribute\n\n\n\n\nAccording to the PDP, an increase in forum contributions is expected to result in a higher grade prediction. Let’s see if the LLM can obtain a similar interpretation. Since the number of data points used to create the previous graph is too high to pass to the LLM, we can provide only a few values of the curve (i.e., a few x-y pairs) and try to recreate the same profile. For example, we can only estimate the profile for all integer values of x (from -3 to 3). The plot generated is very similar to the previous one (Figure 11.7):\n\npdp_svm_fcontrib3 <- model_profile(explainer_svm, variables = \"Freq_Forum_Contribute\", \n              variable_splits = list(Freq_Forum_Contribute = -3:3)) \nplot(pdp_svm_fcontrib3) + theme(plot.subtitle = element_blank())\n\n\n\n\nFigure 7. PDP of Freq_Forum_Contribute with limited x-axis values\n\n\n\n\nWe can now extract the x-y pairs from the results as follows:\n\npdp_svm_fcontrib3xy <- pdp_svm_fcontrib3$agr_profiles |> # Extract the data\n   mutate(Pair = # Create (x,y) pairs\n            paste0(\"(x = \", `_x_`, \", y = \", round(`_yhat_`, 2), \")\")) |> \n   pull(Pair) |> # Extract the x,y pairs\n   paste(collapse = \"\\n\") # Join the pairs together in a single text variable\n   \ncat(pdp_svm_fcontrib3xy) # Print the (x,y) pairs\n\n(x = -3, y = -0.9)\n(x = -2, y = -0.82)\n(x = -1, y = -0.52)\n(x = 0, y = -0.05)\n(x = 1, y = 0.37)\n(x = 2, y = 0.57)\n(x = 3, y = 0.48)\n\n\nWe create the prompt following the guidelines by [29] and send it to the LLM to obtain the explanation in natural language using client$chat:\n\nclient$chat(paste(\"I will give you the PDP x-y pairs of \nthe feature Freq_Forum_Contribute. Please provide a concise one-sentence\nexplanation for teachers about how this feature influences students' predicted \ngrades Ensure the explanation highlights how changes in the feature affect the \ngrade prediction.\", pdp_svm_fcontrib3xy, collapse =\"\\n\"))\n\n\n\n\n\n\n\nAs students increase their forum contributions, their predicted grades improve \nsignificantly, peaking with moderate to high contributions, though excessive \nparticipation shows slightly diminishing returns.\n\n\n\nIt seems that the LLM can accurately capture the direction of the effect of this feature. We can also automate this process for all the features iterating through all the features and storing the response of the LLMs in a list. To avoid printing the LLM response every time, we can add the argument echo = FALSE to the client$chat function call.\n\n# Get the feature names from the model\nfeatures <- names(explainer_svm$data)\n\n# Initialize an empty list to store the results\nall_pdp_results <- list()\n# Loop through each feature in the model\n\nfor (feature in features) {\n  # Create PDP for the current feature\n  variable_splits <- list()\n  variable_splits[[feature]] <- -3:3\n  pdp_svm <- model_profile(explainer_svm, variables = feature, \n                           variable_splits = variable_splits)\n  \n  # Extract x-y pairs from the PDP\n  pdp_svm_xy <- pdp_svm$agr_profiles |>\n    mutate(Pair  = paste0(\"(x = \", `_x_`, \", y = \", round(`_yhat_`, 2), \")\")) |> \n    pull(Pair) |> \n    paste(collapse = \"\\n\")\n  \n  prompt <- paste(\"I will give you the PDP x-y pairs of the feature\", \n  feature, \".\", \"Please provide a concise, non-technical one-sentence explanation \n  for teachers about how this feature influences students' predicted grades. \",\n  \"Ensure the explanation highlights how changes in the feature affect the grade \n  prediction.\", pdp_svm_xy, collapse = \"\\n\")\n  # Generate the response for the current feature\n  pdp_svm_res <- client$chat(prompt, echo = FALSE)\n  \n  # Store the result in the list\n  all_pdp_results[[feature]] <- pdp_svm_res\n  \n  # Print result\n  cat(\"\\n\\n\", feature, \"\\n\", sep = \"\")\n  \n  cat(all_pdp_results[[feature]]) \n}\n\n\n\n\n\n\n\nFreq_Course_View\nAs students increase their course views from very low to moderate, their \npredicted grades improve slightly, but viewing too much beyond that point can \nresult in lower predicted grades.\n\nFreq_Forum_Consume\nAs students increase their frequency of consuming forum content from very low \nto moderate levels, their predicted grades improve slightly, but consuming too \nmuch forum content beyond that point may result in lower predicted grades.\n\nFreq_Forum_Contribute\nAs students increase their forum contributions, their predicted grades rise \nsignificantly, peaking at moderate levels, but very high contributions show a \nslight decline in impact.\n\nFreq_Lecture_View\nWatching lectures more frequently improves predicted grades up to a point, \nbut excessive viewing results in a slight decrease in the benefit.\n\nRegularity_Course_View\nConsistently viewing course content improves predicted grades, with the benefit \nleveling off at higher regularity.\n\nRegularity_Lecture_View\nRegular lecture viewing has a small positive impact on predicted grades, \nespecially as students maintain a steady viewing habit.\n\nRegularity_Forum_Consume\nRegularly reading forum content gradually improves predicted grades, showing a \nsteady increase with more consistent engagement.\n\nRegularity_Forum_Contribute\nRegular forum contributions improve predicted grades, but excessive regularity \ncan slightly lower the benefit.\n\nSession_Count\nMore sessions improve predicted grades steadily, with the largest gains seen at \nmoderate session counts.\n\nTotal_Duration\nThe longer students engage with the course, the better their predicted grades, \nthough the impact flattens with very high durations\n\n\n\nWe can check if the generated explanations are correct by looking at the PDP plots of all the features (Figure 11.8):\n\npdp_svm <- model_profile(explainer_svm)\nplot(pdp_svm) + theme(plot.subtitle = element_blank(), text = element_text(size = 6))\n\n\n\n\nFigure 8. PDP plots of all features\n\n\n\n\n\n\n4.6 Local explanations\nThe previous XAI techniques are useful to understand the model’s performance and overall decision-making process. However, they do not help us understand how specific decisions have been made for specific students. For that, we need local explanations. We can explain a specific student’s prediction (e.g., number 2) using the predict_parts function from DALEX. In Figure 11.9, we see that this student has been predicted to have a grade of 0.979 (almost one SD above the mean), and therefore they are a very high achieving student. We see how the most contributing factor has been the frequency of contribution to the forum, followed by the duration of their online time.\n\n# Row 2 is student 2. \n# We remove column 12 since it contains the grade that we want to predict\nstudent2 <- test_data[2, -12] \nexplanation2 <- predict_parts(explainer_svm, new_observation = student2)\nplot(explanation2)\n\n\n\n\nFigure 9. SHAP explanations for student 2\n\n\n\n\nWe can try to see if the LLM can come up with a similar description. Let us extract the variable values and contributions from the results in the format (Feature name, Feature value, Feature contribution).\n\nlocal2 <- explanation2 |> data.frame() |> head(-1) |> # Extract explanation data\n  mutate(Tuple = # Create tuples (Feature name, Feature value, Feature contribution)\n           paste0(\"(\",variable_name,\", \", \n                  round(as.numeric(variable_value), 2), \", \", \n                  round(contribution, 2), \")\")) |> \n  pull(Tuple) |> # Extract tuples\n  paste(collapse = \"\\n\") # Join tuples together in a single numeric variable\ncat(local2)\n\n(intercept, 1, -0.01)\n(Freq_Forum_Contribute, 2.28, 0.58)\n(Regularity_Lecture_View, -0.25, -0.02)\n(Regularity_Forum_Contribute, 0.73, 0.08)\n(Regularity_Forum_Consume, 0.84, -0.02)\n(Session_Count, 0.71, 0.03)\n(Freq_Course_View, 0.32, -0.02)\n(Freq_Lecture_View, 0.94, 0.06)\n(Regularity_Course_View, 0.09, -0.01)\n(Active_Days, 0.23, 0.04)\n(Total_Duration, 0.86, 0.19)\n(Freq_Forum_Consume, 0.97, 0.06)\n\n\nNow let us create the prompt following the guidelines by [29], and send it to the LLM to obtain the explanations using send_prompt.\n\nlocal_prompt <- paste(\"I will give a SHAP feature contribution explanation in the \nformat (feature_name, feature_value, contribution). Please provide a concise, \nclear and understandable narrative for a teacher that explains how these \ncontributions influence students' predicted grades (standardized). Ensure the \nexplanation highlights the significance of each feature's contribution to the \noverall prediction.\\n\", local2, collapse = \"\\n\")\n\nclient$chat(local_prompt)\n\n\n\n\n\n\n\nBased on the SHAP feature contributions, here's a clear and concise explanation for \nteachers about how each feature influences the predicted grade (standardized):\n\nIntercept (1, -0.01): The baseline predicted grade is slightly adjusted downward by the \nintercept, but the overall impact is minimal.\n\nFreq_Forum_Contribute (2.28, 0.58): This is the most significant positive contributor to \nthe predicted grade. Frequent forum contributions strongly boost the predicted grade, \nhighlighting the importance of active student engagement in discussions.\n\nRegularity_Lecture_View (-0.25, -0.02): Inconsistent lecture viewing has a small negative \nimpact on the predicted grade, suggesting that irregular engagement can slightly lower \nstudent performance.\n\nRegularity_Forum_Contribute (0.73, 0.08): Regular contributions to forum discussions \npositively influence the predicted grade, though not as strongly as frequency, \nreinforcing  the value of steady participation.\n\nRegularity_Forum_Consume (0.84, -0.02): Regularly reading forums without contributing \nhas a slight negative effect, implying that passive consumption of forum content alone \nmay not benefit grades as much as active participation.\n\nSession_Count (0.71, 0.03): Engaging in more sessions slightly raises the predicted \ngrade, reflecting the importance of consistent interaction with the course platform.\n\nFreq_Course_View (0.32, -0.02): Viewing course materials frequently has a small negative\ncontribution, suggesting that excessive course views may not significantly boost \nperformance.\n\nFreq_Lecture_View (0.94, 0.06): Frequent lecture viewing positively influences the \npredicted grade, showing that regularly revisiting lectures helps students perform better.\n\nRegularity_Course_View (0.09, -0.01): Regular course viewing has a minor negative \ncontribution, indicating that irregular viewing habits may slightly hinder grade outcomes.\n\nActive_Days (0.23, 0.04): Engaging on more active days has a modest positive effect on \nthe predicted grade, emphasizing the value of consistency in student engagement.\n\nTotal_Duration (0.86, 0.19): Spending more time engaging with the course content \nsignificantly boosts the predicted grade, reflecting the importance of overall study time.\n\nFreq_Forum_Consume (0.97, 0.06): Frequently reading forum discussions without \ncontributing  provides a small positive contribution, showing that staying informed \nthrough forum activity still benefits performance, though less than active participation.\n\nSummary:\nActive participation in forums and time spent engaging with course content are the \nstrongest drivers of predicted grades, while passive or inconsistent behaviors tend to \nhave smaller or negative impacts. This highlights the importance of both frequency and \nregularity of engagement across different types of activities.\n\n\n\nWe can try different versions of our prompt until we obtain satisfactory results. Now, following our initial goal, we need to automate the automated feedback generation for all students. We can create a function to get the explanation for any student as follows.\n\nlocal_explanation <- function(student) {\n  # Select a student to explain\n  instance <- test_data[student, -12]\n  # Get local explanations\n  predict_parts(explainer_svm, new_observation = instance)\n}\n\nWe do the same to generate the prompt based on each individual students’ results:\n\nexplanation_to_text <- function(student = 1) {\n  current <- local_explanation(student) \n  current_numbers <- current |> data.frame() |> head(-1) |>\n  mutate(Tuple = paste0(\"(\",variable_name,\", \", \n                        round(as.numeric(variable_value), 2), \", \", \n                        round(contribution, 2), \")\")) |> \n  pull(Tuple) |> paste(collapse = \"\\n\") \n  \n  prompt <- paste(\"I will give a SHAP feature contribution explanation in the \n  format (feature_name, feature_value, contribution). Please provide a concise, \n  clear and understandable narrative for a teacher that explains how these \n  contributions  influence the predicted price of the grade (standardized). \n  Ensure the explanation highlights the significance of each feature's \n  contribution to the overall prediction.\\n\", current_numbers, collapse = \"\\n\")\n  \n  return (prompt)\n}\n\nNow we can iterate through our complete dataset and generate a customized prompt per student, and get the personalized feedback from the LLM. The res dataframe will contain the student’s prompt (in the column named prompt) and the response from the LLM (in the explanation column).\n\nres <- test_data |> \n  mutate(order = row_number()) |> # Get each student number\n  rowwise() |>  \n  mutate(prompt = explanation_to_text(order), # Generate each student's prompt\n         explanation = client$chat(prompt, echo = FALSE)) # Send the prompt to the LLM\n\n\n\n\n\n\n\nWe can print the explanation of each particular student (e.g., number 2) as follows (we have omitted the result to avoid repetition)\n\ncat(res[2 ,]$explanation)"
  },
  {
    "objectID": "chapters/ch11-llmsxai/ch11-llmsxai.html#conclusion",
    "href": "chapters/ch11-llmsxai/ch11-llmsxai.html#conclusion",
    "title": "11  LLMs for Explainable Artificial Intelligence: Automating Natural Language Explanations of Predictive Analytics Models",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nIn this tutorial, we have explored the application of LLMs in generating natural language explanations for predictive models in learning analytics. Throughout the tutorial, we demonstrated how LLMs can be integrated into an XAI pipeline to explain the decisions made by predictive models in an accessible manner. As such, we highlight the potential of LLMs in bridging the gap between technical model outputs and actionable insights for different stakeholders.\nIt is important to mention that the insights generated by the LLM used during in this tutorial are specific to the Gemma model chosen to illustrate the integration of LLMs with XAI. Any future replication of this tutorial may almost certainly bring different outputs. This is because LLMs are by definition stochastic, and their results are not deterministic (consistent between runs). It is also expected that, as research in LLMs advances, new models arise with higher accuracy and lower size, and therefore the insights generated by them will be more relevant and precise.\nSimilarly, although there are some guidelines and recommendations on how to prompt LLMs to achieve the best results, “prompt engineering” lies outside of the scope of our tutorial. Depending on the data, intended audience and platform for delivery of the insights, the prompts need to be modified accordingly. In the tutorial, we merely illustrate how to integrate custom prompts with the output from XAI tools to generate the corresponding insights. Specifically, extracting the relevant part from the XAI results in a way that it can be easily streamlined to the LLM is perhaps the most notable contribution of the tutorial. A lot of testing is needed to find a prompt that brings consistently relevant results which is a requirement to be able to implement the pipeline illustrated in this tutorial in a completely automated way (no human-in-the-loop). Indeed, the pipeline showcased in this chapter is easy to implement manually using web-based tools such as ChatGPT; however, relying on the API allows to upscale the process so it can be automated for a large number of students.\nAlternatively, rather than creating a static explanation of the ML models and their predictions, an interactive process through which the different stakeholders could ask questions to understand the results and obtain explanatinos and recommendations would be of great value. The library ellmer [17] presented in this chapter allows to conduct interactive conversations between the user and the LLM in the console or in the browser. This interaction would be more in line with recent research threads in XAI research, such as evaluative AI: a new perspective on XAI for decision-making in which evidence is provided against and in favor of human-made hypotheses, rather than providing explanations for the single most-likely AI outcome [4, 13]."
  },
  {
    "objectID": "chapters/ch12-cds/ch12-cds.html",
    "href": "chapters/ch12-cds/ch12-cds.html",
    "title": "12  Complex Dynamic Systems in Education: Beyond the Static, the Linear and the Causal Reductionism",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch12-cds/ch12-cds.html#introduction",
    "href": "chapters/ch12-cds/ch12-cds.html#introduction",
    "title": "12  Complex Dynamic Systems in Education: Beyond the Static, the Linear and the Causal Reductionism",
    "section": "1 Introduction",
    "text": "1 Introduction\nLearning —like most psychological phenomena— is far from being simple or trivial. It involves multiple cognitive processes, contexts and dimensions with complex and dynamic interplay between them [1–3]. Therefore, most learning theories and frameworks describe learning as a multifaceted process that evolves over time. For example, most theorists depict self-regulated learning as having several phases or processes [4] that influence each other resulting in the emergence of certain behaviors, e.g., learning strategies. Similarly, engagement is commonly conceptualized as a multidimensional process that unfolds in time with a significant interplay between the dimensions across contexts, tasks and cultures [3, 5]. These complexity features apply to most learning theories, frameworks, and constructs and also extends to interpersonal processes, groups, classrooms, and organizations [6]. All of the aforementioned examples, and many others, exhibit features typical of complex systems. Traditional methods often fail to adequately capture their complexity and dynamics, and therefore, complex system methods are required to capture their unique features [3, 7].\nBut, before moving on, we must articulate what we mean by a complex system. The word complexity signifies multiple components that interact with each other in intricate ways [8]. Thus, a complex system is a collection of components that interact together in unique ways that lead to the emergent behavior of the system as a whole and cannot be understood by nor reduced to its individual components. The term dynamic indicates that the system changes and evolves over time exhibiting different states and conditions [9, 10].\nResearchers vary in their definitions of complex dynamic systems and in the level of detail by which they describe these systems. However, most researchers can agree on some basic requirements for identifying a system as complex: the presence of intricate properties, chaotic unpredictable behavior or outcomes, nonlinear dynamics, and interactions between the system components [11, 12]. Furthermore, these properties exist in different mixtures exhibiting different dynamics and interactions and due to this variability, complex systems and their associated behaviors/properties are not perfectly replicable [9, 10]."
  },
  {
    "objectID": "chapters/ch12-cds/ch12-cds.html#why-complex-systems-in-education",
    "href": "chapters/ch12-cds/ch12-cds.html#why-complex-systems-in-education",
    "title": "12  Complex Dynamic Systems in Education: Beyond the Static, the Linear and the Causal Reductionism",
    "section": "2 Why complex systems in education?",
    "text": "2 Why complex systems in education?\nAs we have mentioned before, humans, their behavior, and their social structures (e.g., groups, institutions, societies) manifest complex and dynamic behavior, making conceptualizing them as complex dynamic systems an apt epistemological endeavor [1]. Viewing human phenomena as complex dynamic systems allows us to capture their complex organization, dynamics, and behavior [13]. It stands to reason that while traditional statistics and inference methods may be useful in capturing some characteristics of complex systems or offer a summary of a system’s state at a single time, such methods are incompatible and would fail in fully capturing the complexity of these systems [11].\nTraditional statistics were designed to capture what is commonly known as a box-and-arrow structure, where A and B leads to C; what is known as component-dominant structure. In component-dominant structures, the components themselves in their accumulation, are assumed to define the behavior of the system [13]. Therefore, the focus in complex systems shifts from analyzing components in isolation to the whole system and to understanding the interactions between them [1, 3]. The effects of interactions—rather than the individual components themselves—become more significant in determining the system’s behavior. These interactions can result in emergent behavior that cannot be fully explained by simply summing the effects of individual components [1, 3].\nLet us take an example. In a box-and-arrow system, a researcher may study variables that influence students’ achievement with data that includes the constructs of student engagement, teacher competence, and home support. The researcher uses regression analysis and computes a coefficient for the relations of each of these variables with achievement to estimate their distinct contributions to predicting achievement [1, 5]. In a regression analysis, the assumption is that each of the three variables relates to achievement in a way that, at least in part, is independent from the other variables. By the statistical assumptions governing the regression analysis, the variables are required to have no significant relationship with each other (often referred to as non-collinearity). Clearly, such assumptions constitute an oversimplification of the interrelations of these variables: they influence each other and interact with each other in non-trivial ways. A supportive home would boost engagement, as would a competent teacher, and, in turn, engagement could facilitate familial support of the student [3, 14]. As such, assuming that variables are independent and unrelated does not reflect real-life social-psychological processes that underlie learning and achievement. Conceptualizing learning and achievement as a complex dynamic system, in which the three variables interdepend and mutually impact each other, would better correspond to reality [3, 13]. A complex dynamic systems approach would allow conceptualizing, investigating, and describing the interactions between these variables, and the mutual influence they exert on each other in a way that reflects the multifaceted nature of learning and achievement. That, however, requires appropriate methods that are able to capture the dynamics, processes, interactions, and intricacies among these variables, and the way they give rise to the global behavior of the system. Such research would lead to a theory of the emergence patterns of the learning and achievement system, how the system’s components reflect a process of self-organization, and how the system’s global behavior constrains the state of the system’s components and their interactions [2, 3, 10, 15, 16].\nThus, in contrast to box-and-arrow or component-dominant systems, complex dynamic systems are interaction-dominant - the interactions and the interdependencies between the components define system behavior. Hence, system behavior cannot be reduced to any of the components or their distinct interactions. As the system’s components interact, new behavior, and behavior patterns emerge. Notably, the relationship between the system’s behavior and the states of its components is not proportional, which manifests in nonlinear system behavior. Rather than an additive process, a complex system’s components and their interactions give rise to behavior that is greater than the sum of its parts (e.g., planning + metacognition \\(\\neq\\) self-regulation). Complex systems are also not strictly predictable [17, 18].\nAdopting a CDS lens in education presents an opportunity to advance our existing theories and provide a deeper understanding of change, stability, and resilience in educational contexts. An opportunity to understand the dynamics and temporal aspects of how a learning process progresses, regresses or interacts with the context and environment. These dynamical aspects, while prevalent in our theories and practice, have remained hardly straightforward to capture with existing methods or interpret with existing theoretical frameworks. Recently, serious work has been done to renew theories within CDS see for instance [5].\nUsing a CDS lens has resulted in significant breakthroughs in other fields e.g., Nobel prize in physics [19] and holds the promise for novel and meaningful insights to unravel the structure and temporal dynamics of learning. These opportunities not only promise a richer understanding but also offer practical implications for designing interventions that are adaptive, resilient, and responsive to the nuances of educational systems. To further explain the complex systems and their dynamics we will discuss the main characteristics and concepts of complex systems in the remainder of this section.\n\n2.1 Nonlinearity\nNonlinearity is a defining feature of complex systems, describing relationships between system components as not proportional or additive and highly sensitive to initial conditions [20]. Within a nonlinear system, the output (or behavior) of the system is not directly proportional to the input, where small changes can lead to disproportionately large outcomes, and vice versa, also described as a “butterfly effect” [21]. This concept directly contrasts with linear systems in which the system’s output is easily predicted by the inputs, such as represented by the equation y = mx + b and similar variations. In complex systems, nonlinear relationships can manifest in ways that defy conventional intuition (e.g., [22]) where a small environmental disturbance, such as the interruption of a classmate, can trigger a cascade of effects that can dramatically and irrevocably alter learning processes, e.g., [22] disruption of cognitive engagement. A typical example of non-linearity in educational psychology is the Yerkes-Dodson law, which describes the relationship between arousal and performance [23]. The Yerkes-Dodson Law states that performance improves as physiological or mental arousal increases, but only to an optimal level. Beyond this point, excessive arousal leads to a decline in performance (see Figure 12.1).\n\n\n\nFigure 1. The Yerkes-Dodson Law [23] illustrates the relationship between arousal and performance. Performance increases with arousal up to an optimal point, beyond which excessive arousal impairs performance. The graph highlights three key zones: increasing performance due to heightened focus and motivation at low-to-moderate arousal (left), optimal performance at moderate arousal levels (peak), and declining performance caused by excessive stress and cognitive overload at high arousal levels (right).\n\n\nSimilarly within learning processes, initial conditions (e.g., the age of a pedagogical agent) can significantly and disproportionately affect students’ learning outcomes. Nonlinearity in complex systems highlights that systems are not merely the sum of their parts; rather, the interactions between components of a system and the elicitation of these interactions by initial condition can create emergent properties, leading to unforeseeable outcomes by considering and examining individual components alone.\n\n\n2.2 Emergence and Self-organization\nIn complex systems, emergence refers to how new properties, patterns, and behaviors arise from the interactions between a system’s individual components, although these behaviors are not present within the components themselves [24, 25]. In other words, the behavior of the whole system cannot be attributed to a single component and the contribution of any component cannot be isolated, nor can it be added to any other putative distinct contributions. It is the interaction that elicits these behaviors. For example, in psychological phenomena such as self-regulated learning, the process of self-regulation cannot occur by just enacting individual components, or strategies. Instead, it is the interaction between these strategies, such as engaging in information processing while evaluating content as (ir)relevant, from which this process emerges [26]. The emergence concept illustrates how the whole system becomes more than just the sum of its parts, exhibiting capabilities that transcend the faculties of individual components.\nSelf-organization, closely related to the concept of emergence, is the process by which order spontaneously arises from disorder without the oversight of a central controller [27, 28]. Self-organization can be explained by the presence of simple local rules that lead to global-scale patterns of behaviors. Continuing our example in applying these concepts to self-regulated learning from above, students’ self-regulation can follow a set of simple rules resulting in self-organization. For example, a simple rule could be that after defining a task, a student should engage in planning procedures. This rule can trigger a set of several cognitive and metacognitive processes (e.g., prior knowledge activation, planning, goal-setting) that occurs without the dictation of a central controller, representing self-organization in learning processes and other educational phenomena. Within psychological phenomena, this can be seen in group dynamics in which social norms, cultures, or identities are formed within groups and result in synchronized behavior, such as that seen within amusement parks or vehicular traffic. The self-organization of these systems is driven by feedback mechanisms that constrain and reinforce patterns from which behavior can demonstrate stability and adaptability (see Figure 12.2).\n\n\n\nFigure 2. Individual interactions (bottom) collectively give rise to emergent properties and patterns (middle), which form higher-order structures through self-organization (top). These emergent structures, in turn, influence and shape the underlying interactions, creating a continuous feedback loop that drives the dynamic behavior of the system.\n\n\n\n\n2.3 Feedback Loops\nA fundamental concept within complex systems, feedback loops occur when the output of a system feeds back into itself to serve as inputs. These feedback loops serve as the way in which a system amplifies its behavior (positive feedback loop) or stabilizes changes within the system (negative feedback loop), shaping the system’s adaptability to environmental changes [29]. Positive feedback loops accelerate change wherein the system moves away from its initial state and experiences exponential growth. We can think of positive feedback loops in terms of the engagement of cognitive and metacognitive strategies during learning. As students become increasingly engaged with learning materials, develop conceptual understanding, and mature in their abilities to adequately deploy learning processes, the student’s cognition and behavior can move away from its initial state, destabilizing to further mature and develop in their application of learning processes.\nNegative feedback loops moderate the change within the system to stabilize behaviors, leading to systems that demonstrate equilibrium. For example, stabilization of cognitive processes, such as in the repetition of learning strategies over a short period of time, can result in the equilibrium of a system in which the learning establishes a new “initial” state. In complex systems, the presence of both positive and negative feedback loops is essential for the balance and functionality of a system wherein excessive change can lead to unsustainable growth and subsequent collapse but a substantive lack of change indicates stagnation.\nWith psychological phenomena, such as emotion regulation, the balance of activating and deactivating states are essential to the stability and health of a system. In this context, an example of a positive feedback loop is the interaction between anxiety and procrastination (Figure 12.3- left): anxiety about a task leads to procrastination, which in turn increases anxiety as deadlines approach, creating a self-reinforcing cycle that can spiral into overwhelming anxiety (instability). On the other hand, a negative feedback loop can involve anxiety and self-regulated time-management strategies (Figure 12.3 - right), where anxiety triggers the use of time-management techniques, such as breaking tasks into smaller steps or scheduling work sessions. These strategies reduce anxiety by promoting a sense of control and progress, gradually stabilizing emotional fluctuations and converging toward a balanced state of productivity.\n\n\n\nFigure 3. An example of a positive feedback loop (left) shows how anxiety and procrastination reinforce each other, amplifying anxiety and potentially leading to overwhelming stress (instability). In contrast, the negative feedback loop (right) demonstrates how anxiety triggers self-regulated time-management strategies, which reduce anxiety and stabilize emotional fluctuations over time, converging toward a balanced and productive state (equilibrium). Adapted from Rosnay [30].\n\n\n\n\n2.4 Adaptation and Evolution\nFeedback loops serve as the mechanisms by which a complex system can adapt and evolve. Through these mechanisms, a system can respond to both internal and external conditions wherein the components of a system can make decisions based on local information, self-organize into new configurations that respond to added conditions, and maintain functionality within dynamic environments [31, 32]. However, there is a balance that must be struck in the adaptability of a system in which a system that is too rigid or too flexible can result in counterintuitive behaviors that result in the instability of a system.\nRigidity, broadly, refers to the inability for system behaviors to adapt to the external pressures and influences of the surrounding environment. In being too flexible, however, system behaviors do not adhere to the rules set to determine how the individual components react to environmental changes and in response to other system changes. For example, if a learner’s self-regulation system is too rigid, this may be characterized by the increased repetition of minimal learning strategies such that a learner may only apply summarizing and note-taking to the learning materials. This presents a potential issue in that if the learning material no longer supports these two strategies (such as seen within non-traditional lectures and materials), the learner may not be able to apply more novel learning strategies. Conversely, if the learner’s behaviors are too flexible and do not adhere to the internal rules of the system, the learner does not demonstrate structure which is imperative for learning.\nThe adaptivity of a system can result in the evolution of the same system where, because of the changes the system was required to undergo as a reaction to internal or environmental changes, the emergent properties and patterns of system dynamics no longer replicate those present within the original system [33]. This is modeled in systems that display increasingly more sophisticated and complex behaviors as time progresses. For example, evolution within educational psychological systems is determined by the emergence of new behaviors or abilities within students. In reference to psychological phenomena, educational psychologists have coined the term learning wherein the understanding of an individual changes as a result of new information present within the environment.\n\n\n2.5 Hierarchies, Scales, and Network Structures\nComplex systems are characterized by their organization and interactions across organizational levels. This organization allows us to understand system structures and the influence each structure and connection has on each other for informing adaptive and evolutionary behaviors. Hierarchical, or multi-level structures, refer to a system’s organization of its components in layers where each level consists of components that can be considered sub-components of a higher-level system [34]. Each of these levels can function on different spatial and temporal scales wherein the behavior of an individual component can affect a larger system. When examining complex systems, it is essential for scales to be considered as observing only one level may not allow for the patterns of the full system to be observed. For example, self-regulated learning consists of several macro-processes which each contain several micro-processes. Examining only a singular macro-process (e.g., planning) —while providing valuable insights about planning, a specific aspect of the self-regulatory process– narrows the scope in which the self-regulatory behavior cannot be fully examined. Networks are the frameworks that connect each component of a system which shapes how information is conveyed across the different components, whether the components are different levels or scales [32]. Within a networked system, components are connected through relationships that dictate how components interact with each other. These relationships determine the speed at which information spreads, the resilience of a system to failures, the uniformity of system components in relation to each other, and the elicitation of emergent behaviors [35]. Because of these hierarchies, scales, and networks, it is difficult to determine the underlying causes of behaviors and predict future system behaviors but is vital for the adaptability and evolution of the complex system (Figure 12.4).\n\n\n\nFigure 4. This network visualization illustrates the hierarchical or multi-level structure of a complex system, with each colored cluster representing a distinct domain of interconnected components. For example, one cluster could represent Self-Regulated Learning (SRL), including components such as goal-setting, self-monitoring, and reflection. Other clusters represent related domains like Engagement and Motivation, each with their own interconnected sub-components. The links between clusters represent the interactions between these domains, demonstrating how a change in one area, such as SRL, can cascade and influence others, like Motivation or Engagement. The thickness in the links between clusters may indicate the strength of the relationship between clusters where colors (e.g., green versus red) may indicate the positive or negative relationship between the clusters.\n\n\n\n\n2.6 Dynamics and Sensitivity to Initial Conditions\nComplex systems exhibit dynamic behaviors that arise from the interactions between system components. System dynamics refer to the changes exhibited by the system over time due to the evolution of component interactions and the influence of external conditions from the environment. The dynamics of a system can demonstrate cycles of repeated behaviors, chaotic behaviors that appear random, or emergent in which unforeseen patterns suddenly or gradually arise from local interactions [20]. However, these dynamics are greatly sensitive to initial conditions in which small variations in the starting point of a process can have a multiplicative effect on system behaviors later on. This ties into the aforementioned “butterfly effect” concept mentioned earlier in this section. In all, the sensitivity to initial conditions makes it nearly impossible to employ predictive modeling when encountering complex systems where short-term behavior may be relatively predictable in contrast to long-term behaviors due to the amplification over a temporal scaling. Attractor states serve as the stable configuration of relationships and behaviors that a complex system tends to evolve toward [36]. This can be thought of as the ‘destination’ of the system in which a mathematical model aims to represent all possible states that a complex system occupies, including the intermediary states, on its course towards a convergence point (see Figure 12.5). An example is found in the study by Gao et al. [37] exploring students’ motivation, where the authors found that several initial conditions interact closely with academic affordances, steering the system toward attractor states primarily influenced by career planning.\n\n\n\nFigure 5. Complex systems tend to evolve toward attractor states—stable patterns or behaviors that the system naturally gravitates toward. These attractors can reflect positive stability, where the system maintains a desirable equilibrium (2A), or, on the other hand, they can signify being “stuck”, where the system is locked into a maladaptive or undesirable state that resists change (2B). The system’s initial conditions (1A vs. 1B) significantly influences its future behavior; tiny differences at the start can result in vastly different outcomes and lead to evolve towards a positive attractor state (1A to 2A) or to a negative one (1B to 2B)."
  },
  {
    "objectID": "chapters/ch12-cds/ch12-cds.html#analytical-approaches",
    "href": "chapters/ch12-cds/ch12-cds.html#analytical-approaches",
    "title": "12  Complex Dynamic Systems in Education: Beyond the Static, the Linear and the Causal Reductionism",
    "section": "3 Analytical approaches",
    "text": "3 Analytical approaches\nScientific paradigms shape the nature of research questions and the methods used to answer them [38]. The complex systems approach uses certain methods of investigation and has developed its own toolbox of analysis and computer modeling.\n\n3.1 Network analysis: Capturing the interactions and dynamics\nNetwork analysis is a quantitative method to identify the structure and interactions between components of a complex system [39, 40]. Networks are represented as nodes (i.e., vertices) and edges (i.e., links) where nodes typically represent the component of a complex system and the edges represent the relationships between these components. These nodes and edges serve as the fundamental building blocks of networks, identifying intersecting relationships between system components. For example, if we take self-regulated learning (see Figure 12.6) as a complex system, a specific strategy (such as goals setting) will be represented as a node where edges will be visible to show how strategies connect with each other, e.g., regulation, or help-seeking [41]. This analytical approach is extremely valuable for capturing and examining interdependent relationships wherein it is essential to dissect the degree to which nodes are related to each other.\n\n\n\nFigure 6. Partial correlation network between SRL components [41]. The thickness of each edge is proportional to the strength of partial correlation. Higher values indicate stronger partial correlation and therefore interdependence.\n\n\nFrom this analytic method, researchers can capture the number of connections each node has which quantifies the degree of influence a particular node has on network dynamics (i.e., strength or expected influence), if a node acts as a critical connector for other parts of a system or other networks which reveals its role in transferring information within a system (i.e., betweenness centrality), the tendency of a system to form tight sub-groups which reveals system hierarchies and interconnectedness (i.e., clustering), and the distance between nodes which reveals the efficiency of information spread across a network (i.e., path length; [42]). These network analytics can allow researchers to visualize system connectedness, patterns, and outliers, analyze how these relationships change over time, and identify the complexity of the system structure (e.g., nested, hierarchical, multilayered) [43, 44].\nThere are several examples in the literature of the use of psychological networks to model psychological or education processes. SRL, as exemplified before, has been one of the phenomena most commonly modeled through complexity methods. For instance, López-Pernas et al. [45] used partial correlation networks [44] to model the relationship between students’ SRL components. The authors found that the SRL dimensions with the most influence in the SRL process differ from the beginning (when metacognition is more influential) to the end of the course (motivation), and therefore might be more amenable to intervention. Saqr & López-Pernas [41] used different temporal and contemporaneous networks to study the differences between group-based (depicted in Figure 12.6) and individual SRL dynamics. The authors concluded that the average SRL process is not representative of the individual SRL processes of each student, advocating for the study of idiographic analysis to gather personalized insights. Using a similar approach, Saqr [46] studied the interplay of engagement dimensions using log data from a learning management system and found marked differences between group-level and within-person variance. These highlight that complex systems behave differently for different individuals, depending on the initial conditions, the interdependence between the system’s components, and the influence of external variables.\nThrough a transition network analysis approach [47], López-Pernas and Saqr [13] studied the longitudinal dynamics of online engagement using sequence analysis and transition networks (Figure 12.7 - left) and found that engagement patterns remain relatively stable over time, aligning with the typical behaviors observed in complex systems (i.e., attractor states, Figure 12.7 - right). Significant changes in these patterns are rare, and when they do occur, they are generally brief, with students often reverting to their original engagement levels. This suggests that a student’s initial engagement state heavily influences their long-term trajectory, effectively determining their likely outcomes.\n\n\n\nFigure 7. Transition network of students in the mostly disengaged trajectory identified by [13]. Students in this trajectory tend to reach a disengaged state and remain disengaged.\n\n\nLastly, it is worth mentioning that, although SRL and engagement can be studied as complex systems of their own, they can be also modeled as part of a macro complex system where they interact with one another and other components (e.g., motivation), as proposed by [44] (Figure 12.8).\n\n\n\nFigure 8. Psychological network illustrating the complex system formed by SRL and other constructs [44]\n\n\n\n\n3.2 Recurrence Quantification Analysis\nRecurrence Quantification Analysis (RQA) is a statistical technique that reveals the nonlinearity and dynamic properties of time series data [48, 49]. This method detects and quantifies how systems shift between repetitive and novel sequences of behaviors across a temporal space. Three types of RQA methods exist: auto-RQA (aRQA), cross-RQA (cRQA), and multidimensional-RQA (mdRQA). aRQA examines the dynamics within a singular time series, cRQA identifies shared dynamics of two time series, and mdRQA identifies the repeating patterns across more than two time series [50]. These analyses can be completed with either categorical or continuous time series data which are mapped against each other on a matrix where Time Series 1 is mapped on the x-axis and Time Series 2 is mapped on the y-axis. On the matrix, the intersection of time series values are highlighted black if the values are considered the same, pre-determined using radial metrics (this is explained in the tutorial down below).\nTaking categorical aRQA as an example, Time Series 1 [A, B, X, Y, A, S,X, Y] will be the same dataset as Time Series 2 [A, B, X, Y, A, S, X, Y]. Every time on the matrix time points match in value, such as that with time points t1 and t5 as well as t3 and t7, the intersection on the matrix is shaded black (see Figure 12.9). The diagonal line represents the line of identity (LOI) where the time series will always be recurrent with itself at Lag 0. Lags are defined by the distance between time points. For example, Lag 1 represents time points that are removed by one time space (e.g., Time 1 and Time 2, Time 2 and Time 3); Lag 2 represents time points removed by two time spaces (e.g., Time 1 and Time 3). By incorporating lags into subsequent analyses, RQA considers the temporal relationships between events such that patterns in data can emerge across large time differences.\n\n\n\nFigure 9. Example of aRQA recurrence matrix.\n\n\nRQA outputs several metrics, including recurrence rate (proportion of repetition), percent determinism (proportion of patterns on diagonals), average diagonal line length (average length of diagonal line structures), laminarity (proportion of repetitions on vertical line structures), and trapping time (average length of vertical lines). This is not an exhaustive list, but it represents the vast amount of information that can be extracted through this analysis that can explain the degree to which a system displays dynamism, how events that occur within the environment impact the dynamics of the system, and the degree to which a system demonstrates adaptivity. For example, a study by Dever et al. [26] applied this analytical methodology to self-regulated learning strategies during game-based learning. In identifying significant differences in the metrics output by aRQA analyses according to learning outcomes and agency experienced by students in the game, Dever et al. [26] established how students demonstrated functionality in their self-regulation during learning and were able to identify instances in which students demonstrated dysfunctional self-regulation. This provides interesting insights into how students should be supported and scaffolded within their self-regulated learning behaviors. For example, can we identify when a student demonstrates dysfunctional behaviors and how can we design interventions which help the student to correct this dysfunction to demonstrate functional self-regulation?\n\n\n3.3 Simulation\nNowadays, computers make it possible to implement complex systems of equations of change. But for those who shy away from mathematical formalization, there are user-friendly computer programs that make it very easy to simulate the complexity of interactions at the origin of self-organization processes and attractor dynamics [17, 51, 52]. Among the most popular computer simulation tools are dynamic networks, agent-based models, and cellular automata. As explained earlier in this chapter, a dynamic network models a complex system as a network of connected nodes (the system’s components). This type of computational tool has been used to model and simulate the dynamics of several phenomena of interest to the educational field, including intelligence development [53], language development [54], parent-child interactions [55], and goal-directed motivation [56]. One of the most user-friendly open-source software packages for creating dynamic network models is Insight Maker1. Agent-based models simulate the behavior of virtual agents that represent the components of a system. These agents interact with each other and with their virtual environment based on iteratively applied evolution rules. Agent-based models have thus made it possible to model the dynamics of phenomena important to the educational context, such as motivation [57, 58], language development [59], and parent-child interactions [60]. One of the most widely used open-source, user-friendly programming platforms for agent-based modeling is NetLogo2. Finally, cellular automata are simplified forms of agent-based models, where agents are the cells of a two-dimensional lattice or a line. The state of each cell at time t+1 depends, through the application of an evolution rule, on the state of the cells in its neighborhood at time t. The same rule is then applied to the new generation of cells at t+1 to shape generation t+2, and so forth. One of the most insightful applications of cellular automata to psychological development processes concerns the self-organizing dynamics of the self-concept into differentiated structures [51], that is, into distinct clusters that may virtually represent different dimensions of the self, such as the academic self, the athletic self, the relational self, etc. Among the open-source platforms for user-friendly implementation of cellular automata we might mention Golly3.\nUnlike the component-dominant mainstream approach, which aims to identify causal factors, the complex dynamic systems paradigm focuses on the identification of interaction-dominant processes. This is what the research methods we have just briefly reviewed contribute to, by aiming to experimentally test the volatility and stability of systems’ behavior, to identify statistical signatures of complexity and non-linearity in the time series of these systems’ states, to formally and/or computationally model the self-organization processes at work within them, and, for the sake of validating the models built, to retrieve the same statistical signatures in the time series generated by their computer simulation. Once validated, these dynamic models are invaluable tools for testing the effects of interventions, such as pedagogical ones, on the dynamics of important educational variables."
  },
  {
    "objectID": "chapters/ch12-cds/ch12-cds.html#data-collection",
    "href": "chapters/ch12-cds/ch12-cds.html#data-collection",
    "title": "12  Complex Dynamic Systems in Education: Beyond the Static, the Linear and the Causal Reductionism",
    "section": "4 Data collection",
    "text": "4 Data collection\n\n4.1 Capturing a system’s state through rich description: interviews and written reflections\nUnderstanding complex phenomena, such as individuals’ learning, motivation, identities, and actions, requires methods that capitalize on the richness and depth of personal experiences. Narrative interviews and written reflections are markedly effective for capturing the state of a complex system at specific points in time. These methods provide insight into how individuals interpret and enact their roles, goals, beliefs, perceptions, and learning strategies within their unique lived contexts [61].\nNarrative interviews serve as an effective tool for exploring individuals’ experiences and meaning-making processes. Interviewers prompt participants to share personal stories, revealing the interplay between system elements such as beliefs, goals, self-perceptions, emotions, and actions. For example, research on teachers’ professional development has used interviews conducted before, during, and after the program to trace changes in participants’ motivations and instructional practices [16]. These interviews allowed participants to reflect on their teaching journey, the influence of prior experiences, and how professional development shaped their beliefs about student learning and instructional strategies.\nWritten reflections provide another method for capturing detailed and introspective data. These reflections can be gathered over time, such as through daily or weekly diary entries, offering snapshots of participants’ evolving thoughts and decisions. For instance, teachers reflecting on their experiences with inquiry-based learning might describe how their beliefs about teaching shifted during the professional development program. Such reflections illuminate the dynamic relationships among the person’s system components, revealing how changes in context or new experiences challenge or reinforce existing beliefs, goals, and strategies.\nThe analysis of these rich data sources involves identifying key elements of the individual’s complex system, such as their beliefs, values, goals, emotions, and actions, as well as the relationships among them [43, 62]. Researchers synthesize these findings to create a holistic understanding of the individual’s system at specific points in time, highlighting patterns and tensions within and across system elements. Additionally, comparing data across participants allows for researchers to identify shared patterns and generate theoretical principles that explain how identities and actions develop in different contexts.\nThese methods emphasize the importance of capturing the nuanced and context-dependent nature of complex systems. Narrative interviews and written reflections provide a deep and detailed understanding of individuals’ experiences, enabling researchers to document the states of their systems at key moments and track their evolution over time.\n\n\n4.2 Capturing real-time emergence through observations and stimulated recall\nStudying real-time emergence in complex systems requires methods that capture dynamic processes as they unfold. Observations and stimulated recall interviews are particularly effective for examining how engagement emerges within specific contexts, providing detailed accounts of participants’ experiences, interactions, decisions, and actions [61].\nDetailed observations allow researchers to document participants’ behaviors and interactions in naturalistic or designed settings. For example, in a study of visitors to a museum exhibition designed to promote inventiveness, the researchers used observations to capture participants’ engagement with the exhibition’s physical and conceptual features. Observational data revealed how visitors interacted with the content, expressed their perceptions, and responded to the contextual elements of the exhibition. This real-time data provided insight into how participants’ identities and goals were activated and shaped by the environment [63].\nStimulated recall interviews build on observational data by eliciting participants’ reflections on their thoughts, emotions, and decisions during specific moments of activity. Conducted shortly after the observed session, these interviews prompt participants to describe their experiences in detail, often using notes or video footage as memory aids. For example, in the museum study described above, following an experience with an artifact, participants were asked to recount their thought processes during the visit, providing insights into how they connected the exhibition content to their roles and goals, such as seeing themselves as creative or inventive in other areas of life.\nCombining observations with stimulated recall interviews allows researchers to construct a comprehensive picture of how engagement emerges in real-time through participants’ experiences in an authentic educational context. Observations provide a foundation for understanding what participants do and say within the context, while stimulated recall interviews add depth by uncovering the subjective meanings behind these actions. Analyzing these data sources together allows researchers to identify patterns of interaction between contextual features and individual dispositions, highlighting the dynamic and context-dependent nature of engagement emergence.\nThese methods are particularly valuable for capturing the temporal and situational aspects of complex systems. Observations provide a detailed record of real-time activity, while stimulated recall interviews offer access to participants’ reflective processes. Together, these approaches enable researchers to study the dynamic interplay between individuals and their environments, shedding light on how identities and actions develop and transform within specific contexts."
  },
  {
    "objectID": "chapters/ch12-cds/ch12-cds.html#final-remarks",
    "href": "chapters/ch12-cds/ch12-cds.html#final-remarks",
    "title": "12  Complex Dynamic Systems in Education: Beyond the Static, the Linear and the Causal Reductionism",
    "section": "5 Final remarks",
    "text": "5 Final remarks\nThe complex dynamic systems approach lends itself to empirical observation, but in a different way to traditional scientific approaches. While the latter aim to discover —by isolating— causal relationships between independent and dependent variables, the complex dynamic systems approach investigates the way in which the system’s global behavioral patterns, or order parameters, change or stabilize under the effect of a parameter external to the system, or control parameter, which constrains the system’s internal interaction dynamics [64]. For example, with regard to motor learning, Zanone and Kelso [65] showed how an oscillatory synchronization motor skill to be acquired can emerge and then stabilize by training it to resist increasing perturbations. Here, perturbations play the role of a control parameter that is experimentally manipulated —by gradually increasing the required frequency of limbs’ oscillation— in order to push the system of the emergent synchronization into disorganization. The resulting learning is thus defined in dynamic terms, that is, as the stabilization of the system’s new behavior in the form of its resistance to perturbation.\nThe states of complex dynamic systems evolve over time while exhibiting typical statistical properties. Recurrence Quantification Analysis (RQA) can detect regularities and forms of stability (attractors) in the dynamics of the states of a complex system. The temporal distribution of these states also reveals a long-term memory property that translates into a particular autocorrelation structure that can be found whatever the time scale on which these states are observed. This statistical structure, which reveals dynamics based on the traces of its own past history, is a power-law temporal distribution known as 1/f noise or pink noise. Its detection is enabled by time-series analysis of the states of the system under study, particularly through Detrended Fluctuation Analysis [DFA; [66]] and the various adaptations of this technique that have been created to overcome its limitations (see, for example, the chapter by Altamore et al., this book). In the educational field, distributions in the form of 1/f noise have been identified in language learning [67] and motor learning [68], attesting that learning is indeed a dynamic phenomenon in the sense of the theory of complex dynamic systems. Interestingly, 1/f distributions are ubiquitous in nature [69], and their replacement by other types of distribution often reflects anomalies [70]. Consequently, from an applied perspective, such replacements can be used as warning signals of a risk of deteriorating behavior, for example, a risk of discouragement towards academic studies or even academic failure. Detecting such a signal can then be very useful in deciding whether to implement individualized support procedures for students in difficulty, before it is too late.\nVariations in the states of a complex dynamic system are the result of iterative processes that can be modeled mathematically. The momentary state of a system depends to a large extent on its previous state, which in turn depended on an earlier state, and so forth. When time is expressed in continuous form, this dependence of a system on its own history can be accounted for by differential equations such as dx/dt = f(x), where d represents the variation in the system’s state x or the variation of time t. When time is discretized, history dependence is expressed by different equations such as xt+1 = f(xt). However, state variations of complex dynamic systems also result from iterative interactions (often reciprocal in the form of couplings) between their components. The above-mentioned equations of change are then enriched by the influences of changes in the variables interacting within the system, taking the form of systems of differential or difference equations. All these equations are capable of producing monostable (one attractor), multistable (several attractors that can be visited more or less cyclically), or even chaotic behaviors of the system. In the educational field, such equations of change have been used to model the dynamics of cognitive development [71], language development [54], social development [72], and teaching-learning interactions [73]."
  },
  {
    "objectID": "chapters/ch13-ega/ch13-ega.html",
    "href": "chapters/ch13-ega/ch13-ega.html",
    "title": "13  The Advanced Applications of Psychological Networks with Exploratory Graph Analysis",
    "section": "",
    "text": "Forthcoming"
  },
  {
    "objectID": "chapters/ch14-rqa/ch14-rqa.html",
    "href": "chapters/ch14-rqa/ch14-rqa.html",
    "title": "14  Detecting Nonlinear Patterns in Education Research: A tutorial on Recurrence Quantification Analysis",
    "section": "",
    "text": "Forthcoming"
  },
  {
    "objectID": "chapters/ch15-tna/ch15-tna.html",
    "href": "chapters/ch15-tna/ch15-tna.html",
    "title": "15  Mapping Relational Dynamics with Transition Network Analysis: A Primer and Tutorial",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch15-tna/ch15-tna.html#introduction",
    "href": "chapters/ch15-tna/ch15-tna.html#introduction",
    "title": "15  Mapping Relational Dynamics with Transition Network Analysis: A Primer and Tutorial",
    "section": "1 Introduction",
    "text": "1 Introduction\nAs a concept, transition networks were proposed in the seventies by the pioneering work of Schnakenberg [1]. More recently, they have become more recognized across several fields to harness their potentials for capturing the relational and temporal dynamic systems [3]. Nevertheless, despite the needs and potential, they have not been visible in education. In this chapter, we introduce transition network analysis (TNA) as a novel method that captures the temporal and relational aspects of an unfolding process. The main principle of TNA is representing the transition matrix between events as a graph to take full advantage of graph theory potentials and the wealth of potential of network analysis [4, 5].\nThink of it as a combination of process mining and network analysis where a process map is represented and analyzed by network analysis—but of course, TNA is far richer than this. In TNA, network analysis is applied to capture the structure, time and relationships in a holistic way. For process mining models, TNA adds network measures at the node, edge and graph level which tells us what events were important (centrality measures), what transitions were central (edge centralities), and which processes are more connected (e.g., graph density) [6]. TNA also adds community finding as well as pattern mining e.g., triads and dyads to capture patterns within the process. For network analysis, TNA brings clustering of sub-networks or different network constellations of typical temporal events which is commonly referred to as tactics. Furthermore, TNA can model covariates to explain certain patterns which is not a common method in network analysis (note that co-variates are not covered in this chapter) [6]. Furthermore, TNA offers far more than a combination of either methods and includes several innovative techniques of its own which include edge verification using bootstrapping and network comparison with permutation, as well as centrality verification through case-dropping. These statistical techniques bring rigor and validation at each edge level, so we can now be sure which edge is likely to replicate in the future and that our inferences and models are valid and not just a chance [4, 5]."
  },
  {
    "objectID": "chapters/ch15-tna/ch15-tna.html#why-tna",
    "href": "chapters/ch15-tna/ch15-tna.html#why-tna",
    "title": "15  Mapping Relational Dynamics with Transition Network Analysis: A Primer and Tutorial",
    "section": "2 Why TNA?",
    "text": "2 Why TNA?\nLearning is a process that can be best conceptualized as a complex dynamic system [7–9]. Such a system is a collection of interconnected components that interact over time, where interactions can enhance, impede, amplify, or reinforce each other [10]. These dynamic interactions lead to the emergence of behaviors that cannot be fully understood by analyzing individual components in isolation. Often, the interactions result in processes that are more than the simple sum of their parts which is referred to as non-linear dynamics [7, 8] (see Figure 15.1). For instance, motivation catalyzes achievement which in turn catalyzes more engagement, enjoyment and motivation. These interdependence, feedback loops and non-linear dynamics are inherently complex and therefore, require a modeling method that goes beyond the traditional linear models which assume that relationships between processes are stable and predictable [7]. TNA as a dynamic probabilistic model, offers a solution that captures the uncertainties through directional probabilities between learning events and accommodates the non-linear, evolving nature of learning processes. It also captures the constellations or the emergent patterns that define or shape the learning process [3].\n\n\n\nFigure 1. Interaction dominant versus component dominant dynamics\n\n\nProbabilistic processes are commonly —and indeed best— represented mathematically as matrices, where rows represent nodes and columns denote direct probabilistic interactions between them [11, 12]. Moreover, matrices are the mathematical foundation of networks [11]. Therefore, matrices and networks, particularly probabilistic networks, are a natural fit that are conceptually and mathematically aligned. As such, using network models to represent probabilistic processes has become a very common practice [12]. For instance, several probabilistic network disciplines have recently become popular e.g., psychological networks. Also, similar to TNA, similar probabilistic directed transition models have been proposed and empirically validated in various fields such as Markovian network models and dynamic network models from Markov process data [1–3].\nDespite TNA demonstrated fit and alignment, an integrated framework that takes advantage of transition data, their relational and temporal interactions and the mathematical potential of matrix representations—as TNA does—has not been fully embraced or extended within educational research. TNA as an approach, not only aligns with established practices in other disciplines but also enhances the analytics repertoire for researchers by giving the power and interpretability to complex educational data and contributing to theory and rigor [4]."
  },
  {
    "objectID": "chapters/ch15-tna/ch15-tna.html#the-theoretical-grounding-of-tna",
    "href": "chapters/ch15-tna/ch15-tna.html#the-theoretical-grounding-of-tna",
    "title": "15  Mapping Relational Dynamics with Transition Network Analysis: A Primer and Tutorial",
    "section": "3 The theoretical grounding of TNA",
    "text": "3 The theoretical grounding of TNA\nTNA uses stochastic process modeling to capture the dynamics of the learning process, namely, Markov models [2]. Markov models align well with the view that the learning process is an outcome of a stochastic data generating process that led to the overall configuration of the process that we observe [13–15]. The data-generating process is what produces various network configurations or patterns based on rules, constraints, or guiding principles—for instance, self-regulation. This means that while the system operates within certain boundaries defined by the framework or principle, the specific ways in which it changes or evolves is rather random and therefore can’t be strictly determined [6, 14]. That is, the transitions are governed by a stochastic process. The stochastic nature of changes introduces variability, making the system’s precise future states unpredictable but statistically describable [14].\nTherefore, to capture the data generating process and given that the data generating process is partly random (i.e., stochastic), it can be statistically modeled using probability theory [3, 12, 14]. That is, a method that assumes that events are probabilistically dependent on the preceding ones like Markov models. This view has been entertained by several theorists across the years who described such stochastic process e.g,.[13–15]. For instance, Winne and Perry conceptualized the dynamics of self-regulation through three interconnected levels: occurrence, contingency, and patterns—that together depict how the learning process unfolds [15].\n\nOccurrences are states, events or actions, such as discussion moves or parts of learning tasks or any learning activity a student makes during learning. Occurrences are the building blocks of the learning process, the events and the elements that make up the full structure. Occurrences are represented as nodes in TNA.\nContingencies represent the sequential dependency or possibility of transitions between events and how certain actions lead to or follow others. For example, after a learner reads a note on a particular topic, she reviews those notes (another occurrences). Contingency is the recording of the transition event or the transition probability when the student moves between two events. This contingency is represented as edges in TNA.\nPatterns are the recurring combinations of occurrences and contingencies that emerge throughout the learning process. Patterns represent consistent behaviors that typify a learner’s approach to studying or learning. For instance, a pattern might show that a student repeatedly engages in self-evaluation after studying a lesson (contingency). According to Winne [16], “researchers assume mental operations generate behavior, and those cognitive and meta cognitive events are what theories seek to account for” (p. 272). In other words, patterns are the hall mark of the data generation process and the backbone of the structure. After all, a learning process is made of several units, some of which are prominent and repetitive giving it the shape and form it is."
  },
  {
    "objectID": "chapters/ch15-tna/ch15-tna.html#the-building-blocks-of-tna",
    "href": "chapters/ch15-tna/ch15-tna.html#the-building-blocks-of-tna",
    "title": "15  Mapping Relational Dynamics with Transition Network Analysis: A Primer and Tutorial",
    "section": "4 The building blocks of TNA",
    "text": "4 The building blocks of TNA\nThe building blocks of TNA are the transitions between events that makes up the transition process. A transition is a conditional relationship between an occurrence and another occurrence, from A \\(\\rightarrow\\) B (or a contingency) [3, 15]. TNA models these transition in sequential data to compute the transition probability between events [2, 3].The result is a transition matrix that is represented as a weighted directed network where the weights are the transition probabilities from an event to the other and the direction represents the direction of these transitions [2, 3].\nLet us see an example of an imaginary transition matrix representing students learning process. We have five learning activities or occurrences (e.g., Watch Video, Quiz, Read Materials, Submit Assignment, and Feedback). The contingencies are the transitions between these occurrences or the probability of a student moving from one activity to another. We can see that transitioning from Watch Video to Quiz occurred 60 out of 100 times which gives us an estimated probability of 0.60 to move from Watch Video to Quiz (or contingency) (Table 15.1). We also see each time a student starts a quiz, they did Submit Assignment which represents an estimated probability of 1 (Figure 15.2).\n\n\n\nTable 1. Frequency of transitions between occurrences\n\n\nFrom\nTo\nFrequency\n\n\n\n\nWatch Video\nQuiz\n60\n\n\nWatch Video\nRead Materials\n10\n\n\nQuiz\nSubmit Assignment\n100\n\n\nRead Materials\nSubmit Assignment\n20\n\n\nRead Materials\nFeedback\n30\n\n\nRead Materials\nQuiz\n50\n\n\nSubmit Assignment\nFeedback\n100\n\n\nWatch Video\nFeedback\n30\n\n\n\n\n\n\n\nFigure 2. A visualization of the transition network of the imaginary example in Table1\n\n\nIn the example, we can see patterns as they emerge when students consistently repeat a behavior, these patterns typify their approach to studying or managing learning tasks. For instance, a repeated pattern of moving from Read Materials to Quiz reveals a strategy of preparing with taking quizzes before attempting assignments. We also see another pattern of transitioning from Submit assignment to Feedback which reflects meta-cognitive self-evaluation. It is also evident that Feedback receives most transitions (has high in-strength centrality) which makes it the central event in the learning process (sum of estimated transition probabilities = \\(1 + 0.3 + 0.3\\)) and that Submit assignment bridges connections to other events (has high betweenness centrality).\n\n4.1 Network Representation\nIn TNA, the learning process is represented as a directed weighted network. This network can be described in the following way:\n\nNodes (\\(V\\)): These represent the different learning events, such as watching a video, taking a quiz, or submitting an assignment. It can also be states, dialogue moves, roles in collaborative activity, motivation states or any event that can be represented as a sequence unit (occurrences).\nEdges (\\(E\\)): These represent the transitions between activities, showing the direction of transitioning from one activity to the next.\nWeights (\\(W\\)): Weights represent the probability of transitioning from an event or state to another (contingencies).\n\nThe building blocks of TNA are weighted edges that under-grid the structure and dynamics of the transition processes. The edges weight indicates the likelihood (possibility) of transition and the arrows reflects the direction of the transition. Let’s take some examples from our example network.\n\nFrom Watch Video to Quiz (\\(v_1 \\to v_2\\), (\\(W\\) = 0.6): means that there is a directed edge from Watch Video (\\(v_1\\)) to Quiz (\\(v_2\\)), with a weight of 0.6. This means there is a 60% probability that students proceed to take the quiz after watching the video, making it a common next step.\nFrom Quiz to Submit Assignment (\\(v_2 \\to v_4\\), (\\(W\\) = 1): means that there is a directed edge from Quiz (\\(v_2\\)) to Submit Assignment (\\(v_4\\)) with a weight of 1. This indicates that all students (\\(100\\%\\)) are likely to move from completing a quiz directly to submitting an assignment. It may be because this is a mandatory progression, meaning every student follows this path.\nFrom Read Materials to Quiz (\\(v_3 \\to v_2\\), (\\(W\\) = 0.5): means there is an edge from Read Materials (\\(v_3\\)) to Quiz (\\(v_2\\)) with a weight of 0.5. This signifies that there is 50% probability that students choose to take a quiz after reading materials, indicating that it is relatively common.\n\nThe TNA network \\(G = (V, E, W)\\) is thus composed of the nodes (\\(V\\)), the edges (\\(E\\)), and weights (\\(W\\)). The resulting directed weighted network \\(G\\) provides a complete view of the learning process, and shows how students navigate through different activities, which transitions are most probable, and which activities are important within the overall learning process.\n\n\n4.2 Mathematical grounding of TNA\nGiven the stochastic nature of the learning process —as we assume it is the generating mechanism of the data—, TNA uses stochastic modelling (i.e., Markov models) [6, 12, 14]. Markov models mine the transition probabilities from sequences of activities by capturing the likelihood of moving from one state to the next as a probability [6, 11, 12]. For a given sequence of events, the transition probability \\(P(v_j \\mid v_i)\\) is estimated from data as:\n\\[\n\\widehat{P(v_j \\mid v_i)} = \\frac{n(v_i \\to v_j)}{\\sum_{k=1}^S n(v_i \\to v_k)}\n\\]\nwhere \\(n(v_i \\to v_j)\\) is the count of observed transitions from state \\(v_i\\) to \\(v_j\\), and the denominator represents the total outgoing transitions from state \\(v_i\\) with \\(S\\) denoting the total number of states. If there are no transitions from \\(v_i\\), then the probability cannot be estimated. These estimated probabilities are assembled into a transition matrix \\(T\\), where each element \\(T_{ij}\\) represents the estimated probability of transitioning from \\(v_i\\) to \\(v_j\\). For example, the matrix of our example looks like this.\n\\[\nT = \\begin{pmatrix}\n0 & 0.6 & 0.4 & 0 \\\\\n0 & 0 & 0.2 & 0.8 \\\\\n0.3 & 0.5 & 0 & 0.2 \\\\\n0 & 0 & 0 & 1\n\\end{pmatrix}\n\\]\nMatrices and networks together offer a seamless representation of probabilistic models, where transition matrices underpin the mathematical structure and graph theory methods provide the tools and foundations for the analysis.\n\n\n4.3 TNA features\n\n4.3.1 Visualization\nVisualization of the TNA models allows researchers to get a bird’s-eye view of the learning process, capture the essence of the full structure, the connectivity of the events, identify important patterns, central learning events as well as how they are temporally related. Comparative plots (either by subtraction or permutation) allows researchers to understand the differences between groups of students—such as high-achieving versus low-achieving students or those in different contexts that may influence strategies or influence behavior [6, 11, 12, 17]\nTNA offers powerful visualization features that rely on the qgraph package with several enhancements [18]. First, TNA graphs capture the full transition process in an intuitive network visualization with optimized layouts. Also, the ability to fix the layout across several plots allowing the comparison across several transition processes. Further, the visualization in TNA can accept other layouts from, e.g., the igraph package as well as any other custom designed layout (see qgraph manual for this) [19]. Furthermore, the versatility of qgraph allows researchers to add more information to the plot e.g., confidence intervals, thresholds and themes etc. The flexible and customization visualization options —as we will see later— offers researchers several possibilities of visualizing patterns (e.g., dyads and triads), communities and any custom configurations.\n\n\n4.3.2 Network measures\nNetwork measures provide important insights into the overall structure of the process (graph-level measures), the importance or the roles of the events (centrality measures) in the process as well as the centrality of transitions (edge centrality) [4, 11, 20]. TNA estimates many of these measures that are appropriately situated for the nature of the network (a probabilistic directed and weighted network).\n\n4.3.2.1 Network-level measures\nTNA offers several basic network measurements that start with the simple measures like node count which indicates the number of unique events or activities in the network and edge count which represent the number of unique directed transitions between these nodes. However, given that TNA networks may be fully connected or saturated (each node is connected to all other nodes), some measures may not be meaningful in such scenarios For instance, network density reflects how completely connected the network is, or the proportion of actual connections to all possible ones. In cases of fully connected networks, density is 1 and therefore, does not offer much information. However there are several other situations (non-connected networks, pruned networks, or bootstrapped models which will be discussed later) that network density will be useful in showing the network structure [4, 11]. Similarly, measures like the mean out-strength and in-strength which refer to the sum of transition probabilities of outgoing or incoming interactions to each node, respectively may need to be understood according to the network connectivity (fully saturated or not) [11].\nCentralization (both out-degree and in-degree) measures how centralized the network is around particular nodes showing a dominant state, or event. These measures will show which events are at the center of the process, or whether an event or a state dominates the process [21]. Reciprocity measures the likelihood that two nodes have mutual connections, reflecting the mutuality of interactions. It may be important in some processes to examine their reciprocal interactions such as collaborative processes where we expect some stronger transitions between, e.g., argumentation and knowledge building [11, 22].\nThese measures together provide a concise overview of network connectivity, distribution, and flow of dynamics in the transition process. Notable to say that no existing process mining model includes or allows or operationalizes network measures to understand the learning process and therefore, all of such measures are among the strengths of TNA.\n\n\n\n4.3.3 Node-level measures\nCentrality measures provide a quantification of the role or importance of a state or an event in the process [21]. With centrality measures, researchers can rank events according to their value in, for instance, bridging the interactions (betweenness centrality) or receiving most transitions (in-strength centrality) [23]. In doing so, centrality measures can reveal which behaviors or cognitive states are central to the learning process, either as destinations of frequent transitions, as starting points for various actions, or as bridges between different learning activities or as a key to a spreading phenomenon. Using centrality measures, researchers can identify important events that can be targets for intervention or improvement [4, 11].\nThe choice of what centrality measure to apply depends heavily on the specific research [21]. From a probabilistic perspective, certain centrality measures reflect the inherent properties of how transitions are structured, specifically, out-strength centrality is consistently equal to 1 across all nodes, it means that the transition probabilities from each event always sum 1 [6]. Therefore, it is meaningless in some situations. However, in TNA networks where we delete insignificant edges and self-transitions, out-strength becomes useful and in that case reflects stability, lack of change (being stuck in a state) or repetitive actions [12].\nIt is also important to mention that in TNA, the raw or absolute values of centrality measures are not inherently meaningful on their own. It is the relative values of these measures that matter, allowing researchers to rank nodes and identify their relative importance or influence within the network [11]. This comparison helps determine which events are central, which nodes serve as key bridges, and which activities are most likely to receive transitions in the learning process. Last, it is important to note that not all centrality measures are directly transferable to TNA and researchers have to think how and why a measure makes sense in their own context [24, 25].\n\n\n4.3.4 Edge-level measures\nIn TNA, edge centrality measures quantify the importance of transitions between events, rather than the importance of the events themselves, they provide insights into how critical a particular transition is in to the flow of transitions or interactions in the process [19]. In particular, betweenness centrality of an edge, for example, measures how often a transition bridges other transitions. In doing so, edge centrality measures help researchers understand not just which nodes are important, but which transitions are important in guiding the learning process and maintaining the integrity of the sequence of activities.\n\n\n4.3.5 Patterns\nPatterns represent the fundamental building blocks of the structure and dynamics of the learning process. As we discussed before, patterns provide insights into, e.g., the behavior and strategies that learners use while studying or interacting with learning materials [4, 16, 20]. Furthermore, capturing repeated consistent patterns allows us to build theories and generalizable inferences [15]. For instance, if we repeatedly found a strong pattern of transition between planning and task enactment in project work and this pattern is associated with project success, we can therefore recommend such approach [26]. We can also design to strengthen such pattern and monitor students when doing it or support them. On the functionality side, TNA allows the identification of several patterns: cliques (dyads and triads and other cliques) and communities. A network clique is a subset of nodes in a graph where every pair of nodes is directly connected to each other by and edge. In network terms, cliques represent tightly-knit communities or groups, closely related entities or systems or interdependent nodes that shape how learning unfolds over time [4] .\n\n\n4.3.6 Cliques: Dyads\nA dyad is the simplest pattern in TNA representing a transition between two nodes. A dyad becomes interesting when it is a mutual dyad (bidirectional) and when the edge weights are high, e.g., more than 0.15 in either direction. Strong mutual dyads (henceforth dyads for brevity) indicate the strong interdependence between states or events that they are recurrently occurring. Analyzing dyads helps to identify important sequences, recurrent or consistent transitional patterns of learning behavior or strategies. For instance, consistently moving from reading materials to taking a quiz indicating a strong self-evaluative strategy. A student commonly transitioning between disengagement and engagement indicates instability and problematic approach. On the overall picture (Figure 15.3 -left), dyads are the backbone of the whole process and the constellation of dyads help characterize and describe the full process.\n\n\n\nFigure 3. Dyads (left) and triads (right)\n\n\n\n\n4.3.7 Cliques: Triads\nTriads represent connections between three nodes capturing more complex relationships that go beyond direct transitions. In graph theory, the constellation of three nodes carries a very central position and hence multiple names and configurations (triples, triads, triangles). In TNA, cliques of three nodes where each node is connected to the others in either direction indicates a strong interdependent subgroup of nodes that form a strong core of the process. Triads represent higher-order dependencies in learning behavior where one activity not only follows another but influence or depends subsequent events. For instance, the triad in the example above (Figure 15.3 - right) shows strong temporal interdependence between studying, revising and planning reflecting a well-managed approached to learning. The analysis of triads is particularly important in TNA, as it can highlight the presence of reciprocal or reinforcing relationships, where learning activities form a cohesive structure that encourages repeated engagement [4] .\n\n\n4.3.8 Communities\nCommunities are groups of nodes that are more closely related or densely interconnected together more than to other nodes in the network. Specifically, in TNA, communities are groups of states or events that frequently transition between one-another or share similar dynamics [27]. Also, communities are cohesive sequences or succession of activities that are more likely to co-occur together, has a typical pathways or recurring behaviors. Unlike cliques, which have a fixed or predefined structure (2-clique or 3-clique), communities are data-driven based on the connectivity patterns in the data which makes them more descriptive of real-world structure [11]. In doing so, communities reveal existing structures, unravel typical configurations or constellations or actually mine them from the learning process. Communities are not strictly fully connected but show higher-than-average connectivity where the grouping indicates a higher level of association and interaction compared to the broader network [27].\nIn the context of TNA, communities can be groups of dialogue moves, utterances, or states, or learning events that are more densely connected with each other or follow each other. Identifying communities help uncover these —the latent or hidden— clusters of related interactions or behaviors during the learning process showing e.g., how learners collaborate or self-regulate or approach their learning. For instance, in a collaborative learning scenario, communities might form around self-evaluative utterances, feedback exchanges, or task-enactment dialogue moves. Identifying these clusters provides insight into common regulatory practices or patterns of interaction that contribute to effective collaboration and learning [24].\nFurthermore, identifying communities of behavior or events in TNA can contribute or advance our theory building and consequently understanding of learning. Think of identifying communities of behavior as uncovering latent variables. These communities represent underlying patterns of interaction inferred from densely connected behaviors into a simplified, meaningful structure suggesting the presence of an underlying construct or a behavioral mechanism that generated such a pattern. This data-driven approach helps provide evidence of existing constructs, concepts or validating existing ones. As such, identifying communities can help refine theoretical models or develop new ones or help discover and describe the dynamics of behavior. Using communities of inter-related events or factors to infer constructs or theoretical models has been an established tradition in several fields, e.g., factor analysis, network modeling or semantic networks [4, 21, 24, 28].\n\n\n4.3.9 Clusters\nClusters represent typical transition networks that recur together across time [24, 29]. Unlike communities, clusters involve the entire network where groups of states or events are similarly interconnected and each exhibit a distinct transition pattern (latent or hidden) with its own set of transition probabilities [6]. This means that clusters capture the dynamics in learning behaviors, revealing typical relations that learners frequently adopt as units across different instances. Identifying clusters of behavior is a very common approach in learning analytics under several names or labels, e.g., tactics, clusters, or strategies. Here, clusters are similar to those in the sequence mining literature where temporal patterns are inferred from the data to reveal typical behavioral approaches [30–32]. Clusters will be discussed in a devoted chapter [24].\n\n\n4.3.10 Sub-networks or network comparison\nOftentimes, we encounter two predefined conditions, such as high versus low achievers, or different course types (e.g., practical versus theoretical, or easy task versus demanding task). In that case, we have two different processes that are defined by their context (not clustered or inferred). Comparing such groups has been commonly performed visually, e.g., comparing two process models, sequence models, or epistemic networks [30, 33]. While visual comparison may show and inform about differences, it does not tell us about how statistically significant these different are. Where exactly the differences are statistically significant and where they are not. TNA offers a rigorous systematic method for process comparison based on permutation. In doing so, we can compare models visually, plot the differences and estimate the significance of each and every edge. Having such a rigorous comparison opens the door for researchers to draw meaningful inferences, identify theoretically significant differences, and refine our understanding of learning processes in various contexts. This statistical rigor enhances the validity of our findings and contributes to the development of more nuanced theories and conceptual frameworks [34].\n\n\n4.3.11 Bootstrapping and model validation\nMost research on networks or process mining uses descriptive methods. The validation or the statistical significance of such models are almost absent in the literature. Having validated models allows us to assess the robustness and reproducibility of our models to ensure that the insights we get are not merely a product of chance and are therefore generalizable.\nTNA offers an important methodological advancement by incorporating bootstrap methods to identify significant edges within the TNA networks [20]. Bootstrapping is a re-sampling technique that entails repeatedly —usually hundreds if not thousands of times— drawing samples from the original dataset with replacement to estimate the model for each of these samples [35, 36]. Bootstrapping does not require strong assumptions regarding the distribution of the data which makes it suitable for analyzing process data which often do not adhere to specific distributions [37]. Given that bootstrapping entails replacement, each bootstrap sample may include multiple copies of some observations while excluding others to assess variability in the estimated parameters. When edges consistently appear across the majority of the estimated models, they are considered stable and significant [37]. In doing so, bootstrapping helps effectively filters out small, negligible or spurious edges resulting in a stable model and valid model [35, 36].\nAnother advantage of bootstrapping is that it can effectively prune dense networks. This because one of the challenges of most probabilistic networks like TNA is that they are commonly fully connected and lack sparsity (i.e., meaning that every possible connection between nodes (events or states) is present to some degree [12, 35]. This is a common issue in most probabilistic networks [35]. Bootstrapping can help mitigate this issue through identifying and eliminating small and uncertain edges to effectively retrieve the backbone of the network. The resulting simplified network is easier to interpret and more likely to be generalizable. The integration of bootstrap methods into TNA represents a significant advancement in the validation of process models that has the potential to improve the quality of the models but also contributes to more effective data-driven theory building in education.\nThe bootstrap function in TNA allows for the calculation of confidence intervals and p-values for each edge weight. This level of statistical rigor provides a quantifiable measure of uncertainty and robustness for each transition in the network. Bootstrapping also help exclude transitions that may not represent true relationships within the data to include only the most meaningful interactions. The resulting process models may offer an empirical assessment for existing constructs and concepts, as well as in validating or refining hypothesis. A systematically rigorous model also allows researchers to uncover patterns and relationships that either support current theoretical frameworks or advance others ."
  },
  {
    "objectID": "chapters/ch15-tna/ch15-tna.html#tutorial-of-tna-with-r",
    "href": "chapters/ch15-tna/ch15-tna.html#tutorial-of-tna-with-r",
    "title": "15  Mapping Relational Dynamics with Transition Network Analysis: A Primer and Tutorial",
    "section": "5 Tutorial of TNA with R",
    "text": "5 Tutorial of TNA with R\nTNA can analyse any data that can be represented as a sequence which has transitions or changes across time. In other words, TNA accepts any categorically ordered event data, e.g., sequence of learning events, states, phases, roles, dialogue moves or interactions to mention a few. This data can come from time-stamped learning management system data, coded interactions data, event-log data or order event data. The data can be in stslist format (sequence object), which is typically created using the seqdef() function from the TraMineR package. In addition, tna accepts wide data format data where each row represents sequence of data and each column is a time point.\nThe analysis in this chapter uses the tna R package and other packages that are necessary for data manipulation and plotting [38]. It is assumed that the reader is familiar with the R programming language. If that is not the case, it is recommended to refer to previous tutorials on the basics of R [39], data cleaning [40], basic statistics [41], and visualization [42]. It is also recommended to have previous knowledge about Markov models [6].\nIn the first step of the analysis, we begin by loading the necessary packages. We will use tidyverse for data manipulation, wrangling, and ensuring the data is clean and organized for analysis [43]. Most importantly, we load the tna package which provides a wide array of functions for estimating, visualizing and manipulating tna models [38]. Besides loading the required libraries, the next code chunk also loads the data that we are going to use throughout the tutorial, which is a built-in dataset in the tna package (group_regulation) and represents interactions between groups of learners trying to regulate their learning based on the data in this study [4].\n\n5.1 Building tna Model\nTNA analysis starts by building the main TNA object which is called model. A model contains all the information necessary for further analysis e.g., plotting, estimation of centralities, or comparison. To estimate a TNA model we use the function tna() with one argument which is a stslist object (sequence object) usually created by the seqdef() function of TraMiner. However, the tna package can also process event data with the prepare_data() function to first convert such data into a proper format. Furthermore, tna() also accepts standard data frames in wide format (to learn how to create a sequence object you may need to refer to this chapter [29]). The function estimates a Markov model from the data where the initial and transition probabilities are estimated directly from the observed initial state probabilities and transition frequencies. The TNA model is structured as a list containing the elements that facilitate the analysis of transition networks, these elements are described below:\n\nInitial Probabilities (inits): These define the likelihood of starting in a particular state at the beginning of the process (at the first time point before any transitions happen). In an educational context, an initial probability represents the probability that a student begins in a specific state (such as “engaged” or “motivated”) before any activities or interventions occur. These probabilities provide a snapshot of where students start within the process and help frame the subsequent transitions.\nTransition Probabilities (weights): These describe the likelihood of moving from one state to another at each step in the process. Transition probabilities capture how students transitions, moves or follows between different learning states (engaged, motivated or achiever) or events (e.g., assessment, reading or submission).\nLabels (labels): The descriptive names for each node in the network and is included to enhance the interpretability of the analysis. The labels are automatically retrieved from the alphabet of the sequence object or the categories in the data frame.\nData (data): This is a version of the sequence data (or data frame) that contains all the necessary sequence data, converted into an internal format used by the tna package for further analysis (permutation, bootstrapping, etc.).\n\nTogether these elements for the basis for analysis and visualization. In the example above we see the TNA network plot with transition probabilities as directed edge weights and the initial probability as the pie (the thin ring around the nodes). In particular, we see video has an initial probability of 1 and all other events have an initial probability of 0. You can use print(model) to get a feel of what it has and how is it structured.\n\nmodel <- tna(group_regulation)\nprint(model)\n\n\n\nState Labels\n\nadapt, cohesion, consensus, coregulate, discuss, emotion, monitor, plan, synthesis \n\nTransition Probability Matrix\n\n             adapt cohesion consensus coregulate discuss emotion monitor  plan synthesis\nadapt      0.00000    0.273     0.477      0.022   0.059   0.120   0.033 0.016    0.0000\ncohesion   0.00295    0.027     0.498      0.119   0.060   0.116   0.033 0.141    0.0035\nconsensus  0.00474    0.015     0.082      0.188   0.188   0.073   0.047 0.396    0.0076\n [ reached getOption(\"max.print\") -- omitted 6 rows ]\n\nInitial Probabilities\n\n     adapt   cohesion  consensus coregulate    discuss    emotion    monitor       plan  synthesis \n     0.011      0.060      0.214      0.019      0.175      0.151      0.144      0.204      0.019 \n\n\n\n\n5.2 Plotting and interpreting tna models\nThe next step is to plot the model, and in TNA it is as easy as creating the model. We can just call the function plot and pass the model argument to it plot(model). The plot is a directed weighted network where nodes take the colors and the labels retrieved from the sequence object (if available). Each node (state or event or a learning activity) is represented as a circle. For instance, we see nodes for adapt, cohesion, coregulate in the network as well as six other nodes. Around each node, there is a thin rim (pie) that represents the initial probability (inits) of starting in this state.\nFor instance, students are more likely to start in planning more than any other state (initial probability of 0.21) and showing as a pie proportional to 0.21 of the circle. We also see, that adapt has very small pie (initial probability of 0.01) which makes sense that adapt follows other actions.\nThe arrows from each node to the other node represent the weighted transition probability and the direction of the transition. For instance, we see that there is a strong transition from adapt to understand with a probability of 0.51 and from adapt to cohesion with a probability of 0.27. We can also see loops, which represent the probability of staying in the same state, or repeating the same event again in the next time point.\n\n5.2.1 Interpretation of the model\nThe model plot reveals the dynamics of self-regulated learning and co-regulation among students. We start with the initial events, where students usually begin with planning (0.20), consensus (0.21), or discussing (0.18). Students are less likely to start with some states such as coregulate (0.019), synthesis (0.02), and adapt (0.012). These smaller initial probabilities suggest that these activities are less common starting points and shows students priorities in their regulation.\nTask-related processes such as planning and discussing are central in the network. The transition probabilities reveal that these processes are interconnected in various ways, reflecting the configuration of the self-regulated group interactions. Notably, transitions to consensus are particularly strong, with significant transitions from adapt (0.48) and cohesion (0.50). This indicates that adaptability and social cohesion are crucial for reaching consensus. The transition from consensus to coregulate (0.19) suggests that achieving agreement often leads to collaborative regulation.\nEmotional engagement plays a significant role students’ bonding and collaboration, as evidenced by the strong transition from emotion to cohesion (0.33). This highlights how emotions can facilitate social interactions and collaborative dynamics. The transition from cohesion to adapt (0.27) shows the role of social interaction in adaptability. Monitoring is another important regulatory process with notable transitions to plan (0.22) and discuss (0.38). This suggests that self-assessment prompts both further planning and discussion, essential for effective self-regulation. The transition from discussing to co-regulate (0.27) indicates that discussions often lead to collaborative regulation, further supported by the transition from co-regulate to monitor (0.09), demonstrating the cyclical nature of self-regulated learning.\nOverall, the network reveals a rich interplay between cognitive and emotional processes, emphasizing task enactment and socio-emotional regulation. Understanding these transitions can help educators develop strategies to support learners.\nThe code below visualizes the network model (Figure 15.4). Given that using plot(model) produces a very dense and hard to read model we will set two parameters:\n\nMinimum Edge Weight (minimum = 0.05): Only edges with a weight of 0.05 or higher will be included in the plot.\nCut-Off Threshold (cut = 0.1): Edges with a weight below 0.1 are entirely shown with lighter color.\n\n\nplot(model)\nplot(model, minimum = 0.05, cut = 0.1)\n\n\n\n\n\n\n\n(a) Default\n\n\n\n\n\n\n\n(b) Minimum = 0.05, Cut = 0.1\n\n\n\n\nFigure 4. TNA visualization\n\n\n\n\n\n5.2.2 Pruning and retaining edges that “matter”\nTransition networks are commonly fully connected —or saturated— where almost all nodes have connections to all other nodes with some probability. Therefore, mechanisms are needed to retrieve the core or backbone structure of the network. In other words, to make the network sparse. Network sparsity enhances interpretability by removing the overly complex structures, making it easier to identify the important components and relationships. It also isolates the signal from the noise by removing the small noisy edges that may obscure meaningful patterns allowing researchers to focus on the important interactions. We can use the minimum argument in TNA plots to remove small edges e.g., below 0.05 (visually) for easier reading and interpretability as we did before. Worth noting that these small probabilities are retained in the model for all further computations. Researchers may be interested in removing edges that are small and negligible in weight or pruning the model. Removing these small edges keep only the strong connections or transitions that are worthy and meaningful. In a network like ours, we have 81 edges and it is hard to take into account all of them.\nIn TNA, small edges can be removed using the prune() function which removes edges using a user-specified method via the method argument. With the \"threshold\" method, edges are removed below a certain threshold value (the default is 0.05 which removes edges below 0.05). The \"lowest\" method deletes the lowest percentile of edges for instance, lowest = 0.1 removes all edges in the lower 10th percentile in terms of edge weights. When using these methods, the prune() function makes sure that it does not disconnect the network, if it finds that the removal of one edge disconnects the network, it retains it to keep the network connected.\nAnother theoretically sound method is to remove edges using a network null model. While several null models exists for non-weighted networks, only a few can be reliable in weighted networks like transition networks. TNA implements the Disparity Filter which is a robust network sparsification algorithm that helps extracts the backbone structure of weighted networks [44]. The disparity filter simplifies the network by discarding weak edge that are lower than a pre-defined alpha level. The values of alpha ranges from 0.05 (roughly statistically significant compared to a random model), to 0.5 (more likely to than chance). Values of 0.05 are very restrictive and removes most edges in the network, therefore, for the case of pruning TNA models, it is advisable to use the default 0.5 levels which simply means that these edges are more likely to have resulted from a learning process rather than by chance. Values of 0.05 maybe used in larger networks or in cases of theory building and could possibly be interpreted that these structure are likely to be repeated or reproducible in future scenarios. Applying the disparity filter to the TNA model is simple and can be applied using the \"disparity\" method in prune() with the model as argument. The code in the next chunk does three all types of pruning in TNA (Figure 15.5):\n\nThreshold: Directly removes edges with weights below 0.15, offering a straightforward cutoff approach.\nLowest Percentage: Removes the bottom 15% of edges when sorted by weight, focusing on relative edge importance.\nDisparity Filter: Uses a different statistical approach (significance level \\(\\alpha\\) = 0.5) to identify significant edges based on the network’s topology.\n\nPruning with tna can also be carried out by bootstrapping, which we will demonstrate later.\n\n# Pruning with different methods (using comparable parameters)\npruned_threshold <- prune(model, method = \"threshold\", threshold = 0.15)\npruned_lowest <- prune(model, method = \"lowest\", lowest = 0.15)\npruned_disparity <- prune(model, method = \"disparity\", level = 0.5)\n\n# Plotting for comparison\nplot(pruned_threshold)\nplot(pruned_lowest)\nplot(pruned_disparity)\nplot(model, minimum = 0.05, cut = 0.1)\n\n\n\n\n\n\n\n(a) Threshold = 0.15\n\n\n\n\n\n\n\n(b) Lowest 15%\n\n\n\n\n\n\n\n\n\n(c) Disparity filter\n\n\n\n\n\n\n\n(d) Minimum threshold\n\n\n\n\nFigure 5. Pruning with different methods\n\n\n\n\n\n\n5.3 Patterns\nPatterns can be helpful to understand behavior, identify significant structures and help describe the process as detailed above. TNA supports identifying several types of patterns that can be expressed as n-clique. The cliques() function is designed to identify and visualize n-cliques from a TNA model. The function cliques() returns the number of cliques found, their respective matrices, while also allowing the user to visualize the cliques. Users can also pass custom visualization options e.g., edge, node sizes, color or layout. The arguments of cliques include the TNA model, size for specifying the size of cliques, so setting size = 2 would find all dyads and setting size = 3 would find all triads and so forth. The threshold argument sets the lowest weight for which an edge is considered part of the clique.\nThe sum_weights argument can be set to determine whether the sum of edge weights should be considered when forming cliques for instance, threshold = 0.1 and sum_weights = FALSE means that the edge has to have 0.1 weight in either direction. In contrast, threshold = 0.1 and sum_weights = TRUE means, that the sum of the weights in either direction has to be 0.1 regardless of individual edge weights.\nThe dyads below are a list of the strong interdependent dyads. The triads show the strong well-connected structures or patterns. We do not recommend looking at 4 or 5 cliques routinely but, of course, researchers could customize based on context.\nIn this code, four different calls to the cliques function are made to identify and visualize cliques of varying sizes (with size values of 2, 3, 4, and 5) from a network model named model. Each function call focuses on a different size of cliques, using a specific weight threshold to determine which edges are considered strong enough to form part of a clique. The show_loops argument to plot controls whether self-loops (edges connecting a node to itself) should be displayed in the visual output, but loops are never included in the actual computation of cliques.\nThe first function call identifies cliques of size 2, known as dyads (Figure 16.5), using a weight threshold of 0.1. This means only pairs of nodes that are connected by edges with with transitions weights higher than 0.1 are considered. The second function identifies 3-cliques (Figure 15.7), or triads, using a weight threshold of 0.05 which captures the three fully connected nodes in the network. The third call focuses on 4-node cliques, or quadruples (Figure 15.8), with a lower weight threshold of 0.03. Finally, the fourth call identifies 5-cliques, or quintuples (Figure 15.9), with a weight threshold of 0.1. In this case, the sum_weights = TRUE option means that the sum of the edge weights in both directions between nodes is considered.\n\n# Identify 2-cliques (dyads) from the TNA model, excluding loops in the visualization\n# A clique of size 2 is essentially a pair of connected nodes\ncliques_of_two <- cliques(\n  model, \n  size = 2, \n  threshold = 0.1  # Only consider edges with weight > 0.1\n)\nprint(cliques_of_two)\n\nNumber of 2-cliques: 5 (weight threshold = 0.1)\nShowing 5 cliques starting from clique number 1\n\nClique 1:\n           consensus coregulate\nconsensus      0.082      0.188\ncoregulate     0.135      0.023\n\nClique 2:\n          consensus plan\nconsensus     0.082 0.40\nplan          0.290 0.37\n\nClique 3:\n        discuss emotion\ndiscuss    0.19   0.106\nemotion    0.10   0.077\n\nClique 4:\n          consensus discuss\nconsensus     0.082    0.19\ndiscuss       0.321    0.19\n\nClique 5:\n         cohesion emotion\ncohesion    0.027   0.116\nemotion     0.325   0.077\n\n\n\nplot(cliques_of_two)\n\n\n\n\n\n\n\n\n(a) Dyad 1\n\n\n\n\n\n\n\n(b) Dyad 2\n\n\n\n\n\n\n\n(c) Dyad 3\n\n\n\n\n\n\n\n\n\n(d) Dyad 4\n\n\n\n\n\n\n\n(e) Dyad 5\n\n\n\n\nFigure 6. Identified dyads\n\n\n\n# Identify 3-cliques (triads) from the TNA_Model\n# A clique of size 3 means a fully connected triplet of nodes\ncliques_of_three <- cliques(\n  model, \n  size = 3, \n  threshold = 0.05 # Only consider edges with weight > 0.05\n)\nprint(cliques_of_three)\n\nNumber of 3-cliques: 3 (weight threshold = 0.05)\nShowing 3 cliques starting from clique number 1\n\nClique 1:\n          consensus discuss emotion\nconsensus     0.082    0.19   0.073\ndiscuss       0.321    0.19   0.106\nemotion       0.320    0.10   0.077\n\nClique 2:\n          consensus emotion plan\nconsensus     0.082   0.073 0.40\nemotion       0.320   0.077 0.10\nplan          0.290   0.147 0.37\n\nClique 3:\n           consensus coregulate discuss\nconsensus      0.082      0.188    0.19\ncoregulate     0.135      0.023    0.27\ndiscuss        0.321      0.084    0.19\n\n\n\nplot(cliques_of_three)\n\n\n\n\n\n\n\n\n(a) Triad 1\n\n\n\n\n\n\n\n(b) Triad 2\n\n\n\n\n\n\n\n(c) Triad 3\n\n\n\n\nFigure 7. Identified triads\n\n\n\n# Identify 4-cliques (quadruples) from the TNA_Model\n# A clique of size 4 means four nodes that are all mutually connected\ncliques_of_four <- cliques(\n  model, \n  size = 4, \n  threshold = 0.03 # Only consider edges with weight > 0.03\n)\nprint(cliques_of_four)\n\nNumber of 4-cliques: 5 (weight threshold = 0.03)\nShowing 5 cliques starting from clique number 1\n\nClique 1:\n          consensus emotion monitor plan\nconsensus     0.082   0.073   0.047 0.40\nemotion       0.320   0.077   0.036 0.10\nmonitor       0.159   0.091   0.018 0.22\nplan          0.290   0.147   0.076 0.37\n\nClique 2:\n           cohesion coregulate discuss emotion\ncohesion      0.027      0.119    0.06   0.116\ncoregulate    0.036      0.023    0.27   0.172\ndiscuss       0.048      0.084    0.19   0.106\nemotion       0.325      0.034    0.10   0.077\n\nClique 3:\n           consensus coregulate discuss emotion\nconsensus      0.082      0.188    0.19   0.073\ncoregulate     0.135      0.023    0.27   0.172\ndiscuss        0.321      0.084    0.19   0.106\nemotion        0.320      0.034    0.10   0.077\n\nClique 4:\n           cohesion coregulate emotion monitor\ncohesion      0.027      0.119   0.116   0.033\ncoregulate    0.036      0.023   0.172   0.086\nemotion       0.325      0.034   0.077   0.036\nmonitor       0.056      0.058   0.091   0.018\n\nClique 5:\n           consensus coregulate emotion monitor\nconsensus      0.082      0.188   0.073   0.047\ncoregulate     0.135      0.023   0.172   0.086\nemotion        0.320      0.034   0.077   0.036\nmonitor        0.159      0.058   0.091   0.018\n\n\n\nplot(cliques_of_four)\n\n\n\n\n\n\n\n\n\n\n\n(a) Quadruple 1\n\n\n\n\n\n\n\n(b) Quadruple 2\n\n\n\n\n\n\n\n(c) Quadruple 3\n\n\n\n\n\n\n\n\n\n(d) Quadruple 4\n\n\n\n\n\n\n\n(e) Quadruple 5\n\n\n\n\nFigure 8. Identified quadruples\n\n\n\n# Identify 5-cliques (quintuples) from the TNA_Model, summing edge weights\n# Here, the sum of both directions of an edge must meet the threshold\ncliques_of_five <- cliques(\n  model, \n  size = 5, \n  threshold = 0.015, # Consider edge weights greater than 0.1\n  sum_weights = TRUE # Sum edge weights in both directions when evaluating thresholds\n)\nprint(cliques_of_five)\n\nNumber of 5-cliques: 126 (weight threshold = 0.015)\nShowing 6 cliques starting from clique number 1\n\nClique 1:\n          discuss emotion monitor  plan synthesis\ndiscuss     0.195   0.106   0.022 0.012    0.1410\nemotion     0.102   0.077   0.036 0.100    0.0028\nmonitor     0.375   0.091   0.018 0.216    0.0161\nplan        0.068   0.147   0.076 0.374    0.0018\nsynthesis   0.063   0.071   0.012 0.075    0.0000\n\nClique 2:\n           coregulate discuss emotion monitor  plan\ncoregulate      0.023   0.274   0.172   0.086 0.239\ndiscuss         0.084   0.195   0.106   0.022 0.012\nemotion         0.034   0.102   0.077   0.036 0.100\nmonitor         0.058   0.375   0.091   0.018 0.216\nplan            0.017   0.068   0.147   0.076 0.374\n\nClique 3:\n           coregulate discuss emotion monitor synthesis\ncoregulate      0.023   0.274   0.172   0.086    0.0188\ndiscuss         0.084   0.195   0.106   0.022    0.1410\nemotion         0.034   0.102   0.077   0.036    0.0028\nmonitor         0.058   0.375   0.091   0.018    0.0161\nsynthesis       0.044   0.063   0.071   0.012    0.0000\n\nClique 4:\n           coregulate discuss emotion  plan synthesis\ncoregulate      0.023   0.274   0.172 0.239    0.0188\ndiscuss         0.084   0.195   0.106 0.012    0.1410\nemotion         0.034   0.102   0.077 0.100    0.0028\nplan            0.017   0.068   0.147 0.374    0.0018\nsynthesis       0.044   0.063   0.071 0.075    0.0000\n\nClique 5:\n           coregulate discuss monitor  plan synthesis\ncoregulate      0.023   0.274   0.086 0.239    0.0188\ndiscuss         0.084   0.195   0.022 0.012    0.1410\nmonitor         0.058   0.375   0.018 0.216    0.0161\nplan            0.017   0.068   0.076 0.374    0.0018\nsynthesis       0.044   0.063   0.012 0.075    0.0000\n\nClique 6:\n           coregulate emotion monitor  plan synthesis\ncoregulate      0.023   0.172   0.086 0.239    0.0188\nemotion         0.034   0.077   0.036 0.100    0.0028\nmonitor         0.058   0.091   0.018 0.216    0.0161\nplan            0.017   0.147   0.076 0.374    0.0018\nsynthesis       0.044   0.071   0.012 0.075    0.0000\n\n\n\nplot(cliques_of_five)\n\n\n\n\n\n\n\n\n\n\n\n(a) Quintuple 1\n\n\n\n\n\n\n\n(b) Quintuple 2\n\n\n\n\n\n\n\n(c) Quintuple 3\n\n\n\n\n\n\n\n\n\n(d) Quintuple 4\n\n\n\n\n\n\n\n(e) Quintuple 5\n\n\n\n\n\n\n\n(f) Quintuple 6\n\n\n\n\nFigure 9. Identified quintuples\n\n\n\n5.3.1 Centralities\n\n5.3.1.1 Node-level measures\nCentrality measures are important in identifying the important events, rank their value in certain processes. The tna package has a built-in function centralities() to compute centralities using the appropriate algorithm for a directed probabilistic process. The computation of centralities is simple with just passing the model object (model) to the function. By default, centralities() removes loops from the calculations which can be changed by the user by setting loops=TRUE. Removing the loops would entail that all computation of centralities will be performed without considering self-transitioning or staying in the same state. In that context, out-strength will mean stability of a state, the higher out-strength centrality, the more stable the state is as it less likely to transition to other states and vice versa. In our context, for example, students use adapt (out-strength = 1), means they always follow it by other processes. Whereas, plan has the lowest out-strength = 0.63, meaning that students may repeat planning steps several times before moving on. Please note, that out-strength centrality with loops = TRUE will always be 1 and therefore, will be meaningless to compute in non-pruned networks. In-strength centrality reflects the sum of received transitions and indicates the node is a common pathway that states end in.\nIn our example, we see that consensus task received most transitions from other events (in-strength = 2.7). Betweenness centrality (based on randomized shortest paths, RSP) reflects the events that mediate or bridge other transitions. Please note, that in tna it is advisable to consider the Betweenness RSP as it is more appropriate for probabilistic networks. In our example, adapt lied between most other transitions The function also computes several other centrality measures but we won’t discuss them one by one here. Researchers can use ?centralities to read the list of possible options. The default ones are shown in Figure 15.10.\n\n# Compute centrality measures for the TNA model\nCentralities <- centralities(model)\n\n# Visualize the centrality measures\nplot(Centralities)\n\n\n\n\nFigure 10. Centrality measures\n\n\n\n\nFurthermore, given that each TNA model can be converted to an igraph object, researchers can compute other centralities if they so wish. In the next code, we compute the hub and authority centralities.\n\n# Calculate hub scores and the authority scores for the network\nhits_scores <- igraph::hits_scores(as.igraph(model))\nhub_scores <- hits_scores$hub\nauthority_scores <- hits_scores$authority\n\n# Print the calculated hub and authority scores for further analysis\nprint(hub_scores)\nprint(authority_scores)\n\n\n\n     adapt   cohesion  consensus coregulate    discuss    emotion    monitor       plan  synthesis \n      0.96       1.00       0.65       0.69       0.74       0.82       0.74       0.87       0.90 \n\n\n     adapt   cohesion  consensus coregulate    discuss    emotion    monitor       plan  synthesis \n     0.122      0.301      1.000      0.195      0.439      0.333      0.122      0.511      0.059 \n\n\n\n\n5.3.1.2 Edge-level measures\nTNA can also compute edge level measures which would would rank edges according to their importance in the transition model or which transitions bridges other processes. We can do this by using the function betweenness_network which creates a network with betweenness centrality as edges (Figure 15.11).\n\nEdge_betweeness <- betweenness_network(model)\nplot(Edge_betweeness)\n\n\n\n\nFigure 11. Edge betweenness\n\n\n\n\n\n\n\n5.3.2 Community finding\nCommunity detection identifies groups of nodes (states, events or actions) that exhibit strong interconnections within the transition process [27]. Unlike the cliques, communities are identified algorithmically, and can be of any size. Therefore, it could offer a more realistic way of grouping similarly inter-connected transitions. TNA offers several community detection algorithms that are suited for transition networks (usually small, weighted and directed networks). Each of these algorithms offers a unique perspective to the grouping [19].\n\nThe walktrap algorithm detects tightly-knit groups of transitions based on random walks on the graph. It assumes that random walks will frequently stay within the same community before transitioning to another. finding smaller, cohesive communities within the network.\nThe Fast Greedy algorithm detects communities by optimizing modularity where the algorithm begins with each node in its community and merges nodes based on increases in modularity until no further improvements are possible.\nInfomap detects communities by optimizing a flow-based model that captures how information moves through the network.\nThe Edge Betweenness algorithm focuses on the edges (transitions) in the network rather than the nodes by measuring how many shortest paths uses this transition. Edges with high betweenness are removed iteratively to reveal communities. This method is useful in TNA for uncovering transitions that act as bridges between different learning behaviors.\nThe Spin Glass algorithm tries to find communities with strong internal connections and weak external connections.\n\nThe code below identifies and then visualizes community structures within a tna network model. It begins by using the communities() function with the model argument to detect communities. The result is stored in the communities object. Next, the code visualizes these communities using the plot() function, specifying the leading_eigen method for community detection (Figure 17.3). Other algorithms can be specified in the same way, for example to use the spinglass algorithm we could pass the argument method = \"spinglass\". Furthermore, the communities object contains the number of communities by each algorithm and the communities assignment.\n\ncommunities <- communities(model)\nprint(communities)\n\nNumber of communities found by each algorithm:\n        walktrap      fast_greedy       label_prop          infomap edge_betweenness \n               1                3                1                1                1 \n   leading_eigen        spinglass \n               3                2 \n\nCommunity assignments:\n       node walktrap fast_greedy label_prop infomap edge_betweenness leading_eigen spinglass\n1     adapt        1           1          1       1                1             1         1\n2  cohesion        1           1          1       1                1             1         1\n3 consensus        1           1          1       1                1             2         1\n [ reached 'max' / getOption(\"max.print\") -- omitted 6 rows ]\n\n\n\ncommunities <- communities(model)\nplot(communities, method = \"leading_eigen\")\n\n\n\n\nFigure 12. Communities detected using the leading_eigen algorithm\n\n\n\n\n\n\n\n5.4 Network inference\n\n5.4.1 Bootstrapping\nBootstrapping is a robust technique for validating edge weight accuracy and stability and consequently the model as a whole. Through bootstrapping, researchers can verify each edge, determine their statistical significance, and obtain a confidence interval for the transition probabilities. Bootstrapping is performed by repeatedly re-sampling the original data and creating new datasets. The new datasets (bootstrap samples) are created by randomly selecting data points (individual sequences in the case of sequence data) with replacement, meaning that some data points may appear multiple times in a sample while others may not appear at all [37]. For each bootstrap sample, the TNA network is recalculated, which involves recalculating the edge weight. This process is repeated many times (typically 1000), generating an empirical distribution of edge weight for each edge. As such, we compare the original edge weight to the distribution of edge weights across bootstrap samples and compute confidence intervals (CIs) for each edge. In this context, a 95% confidence interval means that the true edge weight falls within 95% of such intervals in repeated studies.\nBootstrapping also provides a p-value for edge significance by assessing how often a given edge appears with a weight significantly different from a chosen threshold value. Bootstrapping can therefore allow researchers to verify each edge (how likely a given edge would appear in future replications of the network structure) and also, the whole TNA network robustness i.e., if the network’s key features (e.g., important nodes or edges) remain consistent across re-sampled datasets, it suggests the network is robust and reliable. Therefore, we can test or verify hypothesis or advance an existing one. To carry out bootstrapping, we use the bootstrap() function with the model as an argument. The function has default value of 1000 bootstrap iterations (argument iter), and a higher number of iterations is of course better, but 1000 is often sufficient. We choose a threshold value of 0.05 to compare our bootstrapped transition probabilities via the argument threshold. This means that if we consistently observe that an edge is above this threshold in the bootstrapped samples, we deem it statistically significant.\nThis code below performs bootstrapping using using the bootstrap() function, with a seed = 265 for reproducibility. After the bootstrapping process, we can print a summary of the results showing relevant significance values for each edge in the network (CI and p-values). The result object also contains several other elements:\n\nweights_orig: The original transition matrix of the TNA model\nweights_sig: A matrix showing only statistically significant transitions (other weights are set to zero).\nweights_mean: The mean transition matrix across all bootstrap samples.\nweights_sd: The standard deviation matrix across all bootstrap samples.\nci_lower: The lower bound matrix of the bootstrap confidence intervals for transitions.\nci_upper: The upper bound matrix of the bootstrap confidence intervals for transitions.\np_values: A matrix of bootstrap p-values for each transition\nsummary: A data.frame containing the p-values, edge weights and the CIs for each edge in the network.\n\nThen we print specifically those edges that we found to be non-significant by the bootstrap (p-value greater than 0.05), offering insights into the stability of the network structure.\n\n# Perform bootstrapping on the TNA model with a fixed seed for reproducibility\nset.seed(265)\nboot <- bootstrap(model, threshold = 0.05)\n\n# Print a summary of the bootstrap results\nprint(summary(boot))\n\n   from         to weight p_value   sig ci_lower ci_upper\n2 adapt   cohesion 0.0029       1 FALSE  0.00059   0.0054\n3 adapt  consensus 0.0047       1 FALSE  0.00313   0.0065\n4 adapt coregulate 0.0162       1 FALSE  0.01078   0.0222\n5 adapt    discuss 0.0714       0  TRUE  0.06370   0.0795\n [ reached 'max' / getOption(\"max.print\") -- omitted 74 rows ]\n\n# Show the non-significant edges (p-value >= 0.05 in this case)\n# These are edges that are less likely to be stable across bootstrap samples\nprint(boot, type = \"nonsig\")\n\nNon-significant Edges\n\n   from         to weight p_value ci_lower ci_upper\n2 adapt   cohesion 0.0029       1  0.00059   0.0054\n3 adapt  consensus 0.0047       1  0.00313   0.0065\n4 adapt coregulate 0.0162       1  0.01078   0.0222\n6 adapt    emotion 0.0025       1  0.00071   0.0044\n7 adapt    monitor 0.0112       1  0.00621   0.0172\n [ reached 'max' / getOption(\"max.print\") -- omitted 31 rows ]\n\n\n\n\n5.4.2 Permutation\nTNA uses advanced statistical methods to compare models and determine if the differences between them are statistically meaningful. A straightforward but less precise way is to look at the raw differences in edge weights between models. This method can give a quick visual comparison, but it doesn’t provide the statistical rigor needed to confirm that these differences aren’t just due to random chance.\nTo tackle this issue, TNA uses a more rigorous approach through permutation. Permutation involves the generation of a distribution of differences by repeatedly shuffling and comparing the data. For each edge, the test provides a p-value, which helps researchers identify statistically significant differences. This rigorous approach ensures that the insights gained from TNA are not merely artifacts of chance but reflect true underlying differences. No other statistical method that offers such rigor in comparison of groups.\nLet’s look at an example using TNA to compare the transition networks of high achievers and low achievers with data from the group_regulation dataset. First, we create two separate TNA models—one for high achievers and one for low achievers. Then, we use the plot_compare() function to plot a simple comparison between these models, which just subtracts the edge weights without assessing statistical significance (Figure 15.13 (a)).\nTo determine the statistical significance of the differences between the two models, a permutation test is performed using the permutation_test() function. The measures=\"Betweenness\" argument specifies that the test also should compute the differences in betweenness centrality. The results of the permutation test are then plotted using the plot() function, which displays the significant differences between the transition networks of high-achievers and low-achievers (Figure 15.13 (b)). Finally, we can use the print() function to print the results of the permutation test or just print specific parts of the results, like the significantly different edges, or centralities.\n\n# Create TNA for the high-achievers subset (rows 1 to 1000)\nHi <- tna(group_regulation[1:1000, ])\n\n# Create TNA for the low-achievers subset (rows 1001 to 2000)\nLo <- tna(group_regulation[1001:2000, ])\n\n# Plot a comparison of the \"Hi\" and \"Lo\" models\n# The 'minimum' parameter is set to 0.001, so edges with weights >= 0.001 are shown\nplot_compare(Hi, Lo, minimum = 0.01)\n\n# Run a permutation test to determine statistical significance of differences\n# between \"Hi\" and \"Lo\"\n# The 'iter' argument is set to 1000, meaning 1000 permutations are performed\nPermutation <- permutation_test(Hi, Lo, iter = 1000, measures = \"Betweenness\")\n\n# Plot the significant differences identified in the permutation test\nplot(Permutation, minimum = 0.01)\n\n\n\n\n\n\n\n(a) Subtraction of high achievers minus low achievers\n\n\n\n\n\n\n\n(b) Significant differences identified in the permutation test\n\n\n\n\nFigure 13. Network comparison\n\n\n\n\nprint(Permutation$edges$stats)\n\n             edge_name diff_true p_value\n1       adapt -> adapt   0.00000   1.000\n2    cohesion -> adapt   0.00533   0.060\n3   consensus -> adapt  -0.00132   0.411\n4  coregulate -> adapt   0.01122   0.047\n5     discuss -> adapt  -0.09616   0.000\n6     emotion -> adapt   0.00167   0.459\n7     monitor -> adapt  -0.00019   0.943\n8        plan -> adapt   0.00077   0.220\n9   synthesis -> adapt  -0.15825   0.000\n10   adapt -> cohesion  -0.01476   0.749\n [ reached 'max' / getOption(\"max.print\") -- omitted 71 rows ]\n\nprint(Permutation$centralities$stats)\n\n       State  Centrality diff_true p_value\n1      adapt Betweenness        -9   0.220\n2   cohesion Betweenness         1   0.314\n3  consensus Betweenness         0   1.000\n4 coregulate Betweenness         3   0.322\n5    discuss Betweenness         2   0.005\n6    emotion Betweenness         0   1.000\n7    monitor Betweenness        -6   0.083\n [ reached 'max' / getOption(\"max.print\") -- omitted 2 rows ]\n\n\n\n\n5.4.3 Estimating the stability of centralities\nWhile most centrality calculation methods in traditional network analysis has no statistical method to verify their strength or reliability, TNA introduces a statistical method, the case-dropping bootstrap which is a technique to estimate the stability of centrality measures in networks. The “case-dropping” — as the name implies — refers to systematically removing a subset of observations or nodes from the network, recalculating the centrality indices each time, and then evaluating how much these recalculated values correlate with the original ones. This helps determine the correlation-stability of centrality measures. This technique allows us to assess whether the centrality indices are robust and not overly sensitive to slight variations in the network data. A high correlation-stability coefficient indicates that the centrality indices are robust, meaning they remain relatively consistent and the identified key nodes are reliably central in the network. Conversely, if the correlation coefficients vary widely or are consistently low, it indicates that the results may not be dependable.\nThe correlation-stability coefficients are useful for several reasons. Firstly, they emphasize the reliability of findings by ensuring that the identified influential nodes are not artifacts of a particular dataset or influenced by random variations. Secondly, they improve the generalizability of the results, as stable centrality measures are more likely to reflect underlying patterns that persist across different samples or conditions.\n\n\n5.4.4 Interpreting the Results of the Case-Dropping Bootstrap for Centrality Indices\nThe case-dropping bootstrap results in a series of correlation coefficients that represent the relationship between the centrality measures in the original network and those in the networks with data removed. These coefficients typically range from -1 to 1, where values closer to 1 indicate a strong positive correlation, meaning the centrality rankings remain consistent even when data is omitted. A high average coefficient (e.g., above 0.7) suggests that the centrality measure is stable and reliable; the nodes identified as most central in the original network remain central even when some data are missing. This stability implies that findings about the importance of certain nodes are robust and not heavily influenced by specific data points. If the average correlation coefficient is moderate (e.g., between 0.5 and 0.7), it indicates that there is some variability in the centrality measures when data is removed, however it is still acceptable. A low average correlation coefficient (e.g., below 0.5) points to instability in the centrality measure and the results may not be reliable. This instability could stem from a small sample size or measurement errors in the data. It’s also important to compare the stability of different centrality measures. For example, degree centrality often shows higher stability because it relies on direct connections, whereas betweenness centrality may be less stable due to its dependence on the shortest paths, which can change significantly with the removal of edges or nodes.\nThe code below assesses the centrality stability of centrality measures using the function named estimate_centrality_stability() (a shorthand alias estimate_cs() is also provided) which takes the model as the main argument. The results can be plotted in the usual way (Figure 16.10).\n\nCentrality_stability <- estimate_centrality_stability(model, detailed = FALSE)\nplot(Centrality_stability)\n\n\n\n\nFigure 14. Results of the Case-Dropping Bootstrap for Centrality Indices"
  },
  {
    "objectID": "chapters/ch15-tna/ch15-tna.html#conclusions",
    "href": "chapters/ch15-tna/ch15-tna.html#conclusions",
    "title": "15  Mapping Relational Dynamics with Transition Network Analysis: A Primer and Tutorial",
    "section": "6 Conclusions",
    "text": "6 Conclusions\nThis chapter introduced Transition Network Analysis (TNA) as a novel methodological framework that models the temporal dynamics of the learning process. TNA provides a theoretically grounded approach to modeling the stochastic nature of learning processes. This aligns with established theoretical frameworks, such as Winne and Perry’s conceptualization of self-regulated learning through occurrences, contingencies, and patterns. TNA operationalizes these theoretical constructs through its network representation, allowing researchers to empirically investigate theoretical propositions about learning dynamics. The method’s ability to identify communities, clusters, and recurring patterns offers new opportunities for theory building in educational research. Thus, TNA can help researchers validate existing theoretical constructs through data-driven pattern identification, discover new behavioral mechanisms and learning strategies, develop more nuanced understanding of how different learning activities interconnect, and identify critical transition points that may serve as targets for intervention.\nIn a way, TNA can be thought of as combining the temporal aspects of process mining with the analytical power of network theory to offer researchers a robust tool for understanding the complex dynamics of learning processes. The use of graph representation enhances PM by enabling researchers to examine the role of specific events in shaping learning processes through centrality measures. For example, measures such as betweenness centrality highlight nodes that bridge transitions, while in-degree centrality identifies events that receive the most transitions. Edge-level centralities provide further insights into the underlying dynamics of transitions, offering a deeper understanding of how learning unfolds. These measures and capabilities are hardly present in process mining with some exceptions like fuzzy miner, yet they are rather few, incomplete and not easy to interpret [45]. Also, TNA employs community detection to explore events that co-occur, revealing patterns that reflect behaviors and strategies. On the other hand, PM enhances network analysis by enabling the identification of clusters representing typical transition patterns, which are widely used to capture students’ tactics and strategies. However, TNA offers more than combination of either methods. The incorporation of bootstrapping provides a rigorous way to assess model stability and filter out unreliable connections. Similarly, permutation and case-dropping for assessing statistical inferences of models.\nA significant contribution of TNA is its incorporation of robust statistical validation techniques. The implementation of bootstrapping methods addresses a critical gap in current process mining and network analysis approaches in education. This advancement enables identification of significant transitions through confidence intervals and p-values, helps filter out spurious connections leading to more reliable models, provides a framework for assessing the generalizability of findings, and supports more rigorous hypothesis testing and theory validation. Furthermore, TNA’s ability to systematically compare different learning processes through permutation testing represents a methodological advance over simple visual comparison. Permutation allows researchers to identify statistically significant differences between learning processes across different contexts.\nHowever, several limitations should be considered when applying TNA. The method assumes Markovian properties, which may not fully capture some aspects of learning processes (refer to [25] for a frequency-based approach). Interpretation of network measures requires careful consideration of the educational context, and the approach may be computationally intensive for very large datasets. Additionally, some network measures may not be meaningful in fully connected networks. Future research should focus on developing additional validation techniques specific to educational contexts, extending the method to handle multi-level and longitudinal data, investigating the integration of TNA with other analytical approaches, creating user-friendly tools and guidelines for educational practitioners, and exploring applications in different educational domains and contexts."
  },
  {
    "objectID": "chapters/ch16-ftna/ch16-ftna.html",
    "href": "chapters/ch16-ftna/ch16-ftna.html",
    "title": "16  Capturing The Breadth and Dynamics of the Temporal Processes with Frequency Transition Network Analysis: A Primer and Tutorial",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch16-ftna/ch16-ftna.html#introduction",
    "href": "chapters/ch16-ftna/ch16-ftna.html#introduction",
    "title": "16  Capturing The Breadth and Dynamics of the Temporal Processes with Frequency Transition Network Analysis: A Primer and Tutorial",
    "section": "1 Introduction",
    "text": "1 Introduction\nCapturing the dynamics of learning has become an increasingly important goal for educators and researchers due the value it brings to understanding the learning process [1]. This chapter introduces a novel method for capturing such dynamics based on how events are dynamically interrelated [2]. Frequency-Based Transition Network Analysis (FTNA) is a type of transition network analysis (TNA) where the network structure is based on modelling the frequency of transitions. FTNA models the count or number of occurrence of each transition between states and use them as weights reflecting the magnitude of dynamic relationships [4]. This approach is a straightforward representation of the relational dynamics within the learning process, capturing how often one state follows another without conditioning on dependencies or making strong assumptions about the process.\nIn the previous chapter, we have studied TNA, which assumes that the learning process follows a Markov process where the transitions are probabilities and the current states depends on the previous state [2, 5]. In comparison, FTNA is well-suited when the research focus is on describing, summarizing, or visually analyzing the observed data without the probabilistic assumptions —and constraints— of Markov modeling. Additionally, there are contexts where FTNA is more suitable (see below). As a method, FTNA has a rich tool set of techniques and large array of analytical functions to harness the dynamics of the learning process and its temporality. These tools enable researchers to identify the dominant events, the notable patterns, as well as to compare different processes [5, 6]. More importantly, given that FTNA inherits most of the capabilities of TNA, it can be used to draw inferences with its statistical methods, e.g., permutation and bootstrapping [2]."
  },
  {
    "objectID": "chapters/ch16-ftna/ch16-ftna.html#the-basic-principles-of-ftna",
    "href": "chapters/ch16-ftna/ch16-ftna.html#the-basic-principles-of-ftna",
    "title": "16  Capturing The Breadth and Dynamics of the Temporal Processes with Frequency Transition Network Analysis: A Primer and Tutorial",
    "section": "2 The basic principles of FTNA",
    "text": "2 The basic principles of FTNA\nFTNA captures the dynamic relationship between events, i.e., how an event leads to another, follows another, or is dependent on a previous event. These events and the transitions between them are used to construct a transition (edge) between events. The aggregation of edges gives rise to the network of transitions [3, 4, 7]. This is similar to the well-known post-reply networks - yet in reverse direction- when a reply follows a post in forum discussions or chat messages [8]. However, no reply is required here, and the post-reply networks are not strictly sequential and only address conversations. FTNA extends such formulation to modeling transitions between any temporally related events, e.g., learning actions, strategies, roles, states, emotions to mention a few [2]. Also, FTNA models the transitions, or the succession of events rather than replies.\nWhile FTNA allows the full potential of traditional network analysis, it extends these functions with several modeling and statistical techniques (e.g., permutation, scaling, bootstrapping etc.) that enhance the analysis and provides tools for understanding how the events unfold and draw inferences using statistical rigor [5, 6]. Similarly, FTNA enables the use of most TNA functions but with a different interpretation and modeling purpose. That said, it is important here to draw a comparison with TNA which is based on Markov modeling and explain when and why FTNA may be needed [2].\nMarkov-based models assume that the transition probability between events depends only on the current state (the “Markov property”) and not on prior states, meaning that the process has no memory nor is it influenced by any events beyond the immediate events [9]. This assumption may be unrealistic in some processes, where transitions might depend on a broader sequence of previous states (e.g., reasoning over multiple steps). FTNA avoids this limitation, capturing sequences of behaviors without requiring this independence assumption which makes it more flexible for processes where this independence assumption may not hold. In some analyses, the goal may not be to predict future behavior based on current states (as in a Markov models) but rather to identify patterns and structural relationships among observed behaviors without imposing a model-based structure. FTNA models observed patterns of any length or breadth. This is because FTNA is flexible in representing longer-range dependencies or multi-step behaviors by aggregating transitions without conditioning on prior states. Lastly, FTNA is more appropriate for handling small datasets or processes where some transitions are rare. This is because Markov models often require substantial data to produce reliable probability estimates. In contrast, frequency-based TNA is often robust with smaller datasets, as it only tallies transitions rather than estimating conditional probabilities and therefore, does not inflate or conflate the modeled process [5, 6].\nIt should also be mentioned that FTNA holds resemblance with frequency based process mining [10], since both techniques can be used to represent the frequency of transitions between events or states. However, while process mining is a valuable exploration and visualization tool, it has been heavily criticized for its lack of statistical rigor when it comes to identifying (statistically) meaningful transitions and comparing networks, which are two limitations that FTNA overcomes. Moreover, current implementations in R do not allow to filter out infrequent transitions (only infrequent nodes), which limit the opportunities for meaningful analysis and visualization [9, 10]."
  },
  {
    "objectID": "chapters/ch16-ftna/ch16-ftna.html#the-building-blocks-of-ftna",
    "href": "chapters/ch16-ftna/ch16-ftna.html#the-building-blocks-of-ftna",
    "title": "16  Capturing The Breadth and Dynamics of the Temporal Processes with Frequency Transition Network Analysis: A Primer and Tutorial",
    "section": "3 The building blocks of FTNA",
    "text": "3 The building blocks of FTNA\nThe building blocks of FTNA are transitions between events where each event is a node, and the weight of the edge is the transition frequency between the edges. Rather than calculating probabilities, FTNA tracks the count of these transitions to reveal a straightforward picture of patterns in sequential data. Let us consider a hypothetical example where we model the learning transitions. Imagine we have five types of activities: Watch Video, Quiz, Read Materials, Submit Assignment, and Feedback. The FTNA model captures the frequency of students moving from one activity to another: 60 transitions from “Watch Video” to “Quiz” mean a frequency of 60, while “Quiz” leading to “Submit Assignment” shows a frequency of 100. This raw count provides a direct representation of how students move through the learning steps with no transformation or scaling (Table 16.1).\n\n\nTable 1. Frequency of transitions in the network\n\n\nFrom\nTo\nFrequency (weight)\n\n\n\n\nWatch Video\nQuiz\n60\n\n\nWatch Video\nRead Materials\n10\n\n\nQuiz\nSubmit Assignment\n100\n\n\nRead Materials\nSubmit Assignment\n20\n\n\nRead Materials\nFeedback\n30\n\n\nRead Materials\nQuiz\n50\n\n\nSubmit Assignment\nFeedback\n100\n\n\nWatch Video\nFeedback\n30"
  },
  {
    "objectID": "chapters/ch16-ftna/ch16-ftna.html#the-mathematical-basis-of-ftna",
    "href": "chapters/ch16-ftna/ch16-ftna.html#the-mathematical-basis-of-ftna",
    "title": "16  Capturing The Breadth and Dynamics of the Temporal Processes with Frequency Transition Network Analysis: A Primer and Tutorial",
    "section": "4 The mathematical basis of FTNA",
    "text": "4 The mathematical basis of FTNA\nThe FTNA network can be represented as \\(G = (V, E, W)\\) with three basic elements:nodes (\\(V\\)), directed edges (\\(E\\)), and weights (\\(W\\)). Nodes serve as the the elements, states or events while edges represent transitions between them. For example, if \\(v_i\\) is Watch Video and \\(v_j\\) is Quiz, the edge \\((v_i, v_j)\\) captures the transition from watching a video to attempting a quiz. The weight function \\(W(v_i, v_j)\\) represents the transition frequency—how often students move from activity \\(v_i\\) to \\(v_j\\). A weight of \\(W(v_i, v_j) = 60\\) signifies that students frequently transition from Watch Video to Quiz (60 times). From Quiz to Submit Assignment (\\(v_2 \\to v_4\\), \\(W(v_2, v_4) = 100\\)): showing that student transitioned from a quiz directly to submitting an assignment 100 times which is more than the previous edge.\nMathematically, this can be represented as follows: let \\(T_{ij}\\) denote the absolute frequency of transitions from state \\(v_i\\) to state \\(v_j\\). If we consider a state space \\(V = \\{v_1, v_2, \\ldots, v_n\\}\\), the transition matrix \\(T\\) has entries defined as \\[\nT_{ij} = \\text{number of transitions from } v_i \\text{ to } v_j.\n\\] This approach does not normalize the values, allowing the raw frequencies to capture the total count of each transition observed in the data. However, researchers can build networks in different ways and scaling that makes it easier to read or interpret namely normalized or rank-based FTNA which are discussed below.\n\n4.1 FTNA Networks based on normalized values\nCounts and raw frequencies may be conflated by several factors and are not directly comparable. That is why many network measures are best understood as ranks or scales. For instance, comparing across two classrooms where classroom A has 32 students and classroom B has 48. It becomes obvious that this is an unfair comparison, given that the larger classroom is expected to generate more data. The same can be applied about tasks which differ in duration, workload or design. Therefore, scaling allows a better way of comparison not only in the same dataset but also for comparison when data comes from different studies. In TNA, the scaling transform raw counts into values between zero and one, which as we mentioned help to compare transitions more uniformly by accounting for different sample sizes making it directly comparable regardless of absolute occurrence. One option in FTNA is min-max normalization. In our example, given that the minimum frequency is 10 and the maximum frequency 100, the formula for normalization is:\n\\[\n\\text{Normalized Weight} = \\frac{\\text{Frequency} - \\text{Minimum Frequency}}{\\text{Maximum Frequency} - \\text{Minimum Frequency}} = \\frac{\\text{Frequency} - 10}{100 - 10}\n\\]\nFor example, given that the transition from Quiz to Submit Assignment has the highest frequency, it has a normalized weight of 1.00, while Watch Video to Read Materials has a normalized weight of 0.00, because it is the least frequent transition in the network (Table 16.2) (Figure 16.1).\n\n\nTable 2. Raw counts and normalized values\n\n\n\n\n\n\n\n\nFrom\nTo\nFrequency\nNormalized Weight\n\n\n\n\nWatch Video\nQuiz\n60\n0.56\n\n\nWatch Video\nRead Materials\n10\n0.00\n\n\nWatch Video\nFeedback\n30\n0.22\n\n\nQuiz\nSubmit Assignment\n100\n1.00\n\n\nRead Materials\nSubmit Assignment\n20\n0.11\n\n\nRead Materials\nFeedback\n30\n0.22\n\n\nRead Materials\nQuiz\n50\n0.44\n\n\nSubmit Assignment\nFeedback\n100\n1.00\n\n\n\n\nAnother option is to simply use max normalization, i.e., dividing the weights by the largest weight. This option can be useful when the smallest weight is positive and we would like keep it so after normalization, whearas the min-max normalization would make such weights zero.\n\n\n\n\n\n\n\n(a) Raw frequencies\n\n\n\n\n\n\n\n(b) Normalized frequencies\n\n\n\n\nFigure 1. Network visualizations for raw frequencies and normalized values.\n\n\n\n\n4.2 FTNA Networks based on ranked Frequencies\nThe “ranked” option in TNA orders transitions in terms of their frequency and scales these ranks to fit within 0 to 1 for easier readability. Instead of counting or scaling the frequencies, this option assigns a rank based on how frequently each transition occurs, with the most common transition assigned the highest rank, and then scales these ranks. In case of ties, the average rank is used. Finally, the ranks are min-max normalized. Ranking can be particularly insightful in contexts where the order of transitions —rather than their absolute counts— offers meaningful insights. Equidistant ranking is also useful when comparing two processes generated with different mechanisms, i.e., to transform the data to comparable scales.\nThe equidistant normalized weight can be calculated for the previous table based on the rank as follows (Table 16.3):\n\nTop 1 and 2 (100): Normalized Weight \\(\\,= 1.0\\)\nTop 3 (60): Normalized Weight \\(\\, \\approx 0.769\\)\nTop 4 (50): Normalized Weight \\(\\,\\approx 0.615\\)\nTop 5 and 6 (30): Normalized Weight \\(\\,\\approx 0.385\\)\nTop 7 (20): Normalized Weight \\(\\,\\approx 0.154\\)\nTop 8 (10): Normalized Weight \\(\\, = 0.0\\)\n\n\n\nTable 3. Final Table of Raw Frequencies, Ranks, and Normalized Weights\n\n\n\n\n\n\n\n\n\nFrom\nTo\nFrequency\nRank\nScaled rank\n\n\n\n\nQuiz\nSubmit Assignment\n100\n7.5\n1.0\n\n\nSubmit Assignment\nFeedback\n100\n7.5\n1.0\n\n\nWatch Video\nQuiz\n60\n6\n0.769\n\n\nRead Materials\nQuiz\n50\n5\n0.615\n\n\nWatch Video\nFeedback\n30\n3.5\n0.385\n\n\nRead Materials\nFeedback\n30\n3.5\n0.385\n\n\nRead Materials\nSubmit Assignment\n20\n2\n0.154\n\n\nWatch Video\nRead Materials\n10\n1\n0.0\n\n\n\n\nWe can visualize the differences between the two networks in Figure 16.2.\n\n\n\n\n\n\n\n(a) Raw frequencies\n\n\n\n\n\n\n\n(b) Rank\n\n\n\n\n\n\n\n(c) Scaled rank\n\n\n\n\nFigure 2. Network visualizations for raw Frequencies, Ranks, and Normalized Weights."
  },
  {
    "objectID": "chapters/ch16-ftna/ch16-ftna.html#ftna-features",
    "href": "chapters/ch16-ftna/ch16-ftna.html#ftna-features",
    "title": "16  Capturing The Breadth and Dynamics of the Temporal Processes with Frequency Transition Network Analysis: A Primer and Tutorial",
    "section": "5 FTNA features",
    "text": "5 FTNA features\nFTNA inherits most of the capabilities of TNA that were discussed in the TNA chapter which will be mentioned here briefly. However, some features may not be transferable or applicable to FTNA and some features are only available in FTNA which will be discussed here [2].\n\n5.1 Visualization\nFTNA visualization helps researchers get a bird’s eye view of the modeled process and capture patterns, central learning events, and how they are temporally related to each other as well as the strength of these connections. FTNA plotting relies on the qgraph framework which offers a rich and modern framework for network visualization with wide array of features [11]. These visualization features include custom and fixed layouts to enable comparison across processes as well as several customization options for node, edge shapes, forms and sizes. Comparative plots can be enhanced by having fixed layouts to allow easy comparisons. Furthermore, most FTNA output can be plotted using the plot function which includes networks, centrality objects and communities.\n\n\n5.2 Network measures\nSimilar to TNA, FTNA offers network measures at the edge, node and network levels. Network level measures provide insights about the whole network structure, e.g., network connectivity, density, and dominance of events. Node level measures —e.g., centrality measures— allow researchers to quantify or rank the importance of a state or event in the network. Researchers can find which behaviors or cognitive states are central to the learning process, which events are more important (receive more transitions), or bridges other connections (betweenness centrality) [12–14].\nGiven that FTNA allows multi-step modeling, centralities that capture the downstream diffusion of events or propagation of a certain state are more plausible here. For instance, diffusion centrality can help understand which events are more likely to diffuse or result in more interactions or long threads of related events, e.g., longer “chains of thoughts” or multiple steps in a learning process [15]. Given that FTNA is built with emphasis on edges, edge centralities are rather important here. Edge-level measures —e.g., edge centralities— can tell which transitions are central in the learning process and are therefore critical to the models [16].\n\n\n5.3 Network structure and patterns\nPatterns are special configurations or constellations that manifest in the data as strong connections between network nodes. They are the hallmark of the structure and the building blocks of the dynamics of the learning process. What makes patterns unique is that they tell how the data was generated, how the underlying process shaped learning or led to the emergence of certain patterns [1, 2, 17]. For instance, we expect a strong connection in a network of self-regulation between planning and task execution in well-performing students. We also expect to see strong connections between dialogue moves of argument and agreement in cohesive groups. Finding such patterns allows us to, e.g., track students, offer them support, verify existing theories about the learning processes and also test existing ones. Like TNA, patterns may include cliques: dyads (two strongly connected nodes), triads (three strongly connected nodes), or communities (of any size) which reflect underlying patterns of connections inferred from the network [2].\n\n\n5.4 Network validation\nLike TNA, FTNA offer several statistical methods for verifying and establishing the significance of identified edges through bootstrapping. Bootstrapping is a re-sampling technique that in our context helps filter out small, negligible or unstable edges resulting in a rigorous model. In doing so, researchers can understand which parts of the model are weak and therefore, cannot be reliable in future analyses. Significant edges here represent transitions that are likely to appear in future iterations, and more importantly cannot be used for inference about future processes [6]. Bootstrapping is a unique feature in TNA and is not offered by other methods, e.g., process mining [10] or social network analysis [14]. The inclusion of bootstrapping in FTNA along with other statistical inference methods (permutation and case-dropping for centrality) are in fact what differentiates FTNA from traditional network analysis. This is of course in addition to the distinct data sources, the modeling choices as well as the contextual differences [2, 18].\n\n\n5.5 Comparison between groups\nResearchers and educators encounter conditions where groups may differ based on contexts, such as students who engage in collaborative versus non-collaborative learning, high versus low levels of achievement, or learning formats like problem versus project based learning. When researchers like to compare the learning process resulting from either conditions, they can do this using counts, e.g., comparison of frequencies or other techniques like process mining. Typically, such process comparisons relies on visual inspection, such as contrasting process maps or network models, which can reveal the descriptive differences but fall short of demonstrating the statistical significance or pinpointing where these differences matter the most [2, 19].\nFTNA enhances this comparison by using a robust permutation-based approach to process comparison. Permutation allows not only a clear visual representation of differences but also enables researchers to estimate the statistical significance of each transition in the network and automatically quantifying the magnitude of differences in transition frequencies. This level of statistical rigor supports more meaningful inferences and contributes to theoretical development."
  },
  {
    "objectID": "chapters/ch16-ftna/ch16-ftna.html#ftna-tutorial",
    "href": "chapters/ch16-ftna/ch16-ftna.html#ftna-tutorial",
    "title": "16  Capturing The Breadth and Dynamics of the Temporal Processes with Frequency Transition Network Analysis: A Primer and Tutorial",
    "section": "6 FTNA tutorial",
    "text": "6 FTNA tutorial\nFTNA is capable of analyzing any data that can be represented as as a temporal sequence with transitions between events or states occurring over time. FTNA is particularly well-suited for categorically ordered event data, such as sequences of learning events, phases, roles, dialogue moves, or interactions, to name a few. The source of this data can vary, including time-stamped learning management system data, coded interactions, event-log data, or ordered event data.\nIn this tutorial, we will use the tna R package [6], alongside other essential packages that facilitate data manipulation and visualization. It is assumed that the reader is familiar with the R programming language. If that is not the case, it is recommended to refer to previous tutorials on the basics of R [20], data cleaning [21], basic statistics [22], and visualization [23]. It is also recommended to have prior knowledge about Markov models [9] and to have read the previous chapter on TNA [2].\nInitially, the process involves loading the necessary packages to set the groundwork for analysis. The tna R package is the main package that we will rely on for analysis which will build the FTNA models. The tna package provides a wide range of functions for estimating, visualizing, and manipulating FTNA models. In addition, it offers various functions for calculating network metrics, such as centrality measures, and discerning patterns like dyads and triads. The package also includes verification functions, such as bootstrapping and permutation tests. We will also use the tidyverse package for data manipulation, wrangling and visualization [24].\nThe build_model function from the tna package accept several types of data, namely, sequence data, data frames in wide format and square matrices. Sequence data must be in stslist format (sequence object), which is typically created using the seqdef() function from the TraMineR package [25, 26]. This sequence defines the data and its order (for guidance on creating a sequence object, please refer to [26]). Also, tna can accept wide data format where each row represents sequential data and each column is a time point with no extra columns. Finally, the tna package can also process event data with the highly flexible prepare_data() function. Like in Table 16.4, each row represents a sequence by a person, group or a in a task, and column represent the order of such events.\n\n\nTable 4. Wide format data which can be used in FTNA\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n\n\n\n\nSubmit Assignment\nWatch Video\nRead Materials\nWatch Video\nRead Materials\n\n\nWatch Video\nRead Materials\nSubmit Assignment\nRead Materials\nWatch Video\n\n\nRead Materials\nWatch Video\nWatch Video\nSubmit Assignment\nRead Materials\n\n\nRead Materials\nSubmit Assignment\nWatch Video\nRead Materials\nWatch Video\n\n\nWatch Video\nRead Materials\nWatch Video\nRead Materials\nSubmit Assignment\n\n\n\n\nThe FTNA analysis starts by constructing the model which is simply performed by passing the data to the ftna() function or to the build_model() function specifying the type like build_model(data, type = \"frequency\"), or simply ftna(data). The FTNA model contains all the information necessary for subsequent analyses, such as plotting, centrality estimation, or model comparison. We can also scale the frequencies using the scaling argument, which can be:\n\n“minmax”: This option scales the frequencies between 0 and 1 via min-max normalization such that the lowest weight becomes 0 and the largest becomes 1. Note that if all frequencies are positive, then the lowest will be converted to a weight of zero.\n“max”: The frequencies are divided by the largest frequency. This option also scales the frequencies such that the largest weight is 1, but the lowest will only be 0 if some transition never occurred in the original data.\n“rank”: This involves ranking all the frequencies from highest to lowest with the average used for ties. Ranking highlights the most significant transitions and provides a standardized way to compare processes across different contexts.\n\nWe can also perform multiple types of scaling sequentially, for example scaling = c(\"rank\", \"minmax\") would first rank the frequencies and them scale them between 0 and 1 with the min-max normalization.\nThe (F)TNA model is organized as a list that has four elements to facilitate the analysis and can be accessed by the users if needed. These are:\n\nWeights (weights): These are the transition weights (frequencies or scaled frequencies) of moving from one state or event to another, serving as the primary element for various network processes, analyses, and visualizations.\nInitial States (inits): These are the probabilities of starting in each state at the first time point. Understanding these initial states helps show how events evolve in relation to their starting conditions.\nLabels (labels): The descriptive names for each node in the network and is included to enhance the interpretability of the analysis. The labels are automatically retrieved from the alphabet of the sequence object or the categories in the data frame.\nData (data): This is a version of the sequence data (or data frame) that contains all the necessary sequence data, converted into an internal format used by the tna package for further analysis (permutation, bootstrapping etc.).\n\nThe code below loads the tna package. Next, it retrieves the built-in group_regulation dataset included in the package, which has data on group behaviors related to regulation during a collaborative project based on the paper by [5]. Then we call the ftna() function with the group_regulation dataset as the data argument. The resulting model is an FTNA model with frequencies as edge weights and stored in an R object called model, which can be further utilized for analysis. We also use call print(model) to view the model. The model output shows the labels, the transition frequency matrix and the initial probabilities.\n\n# Install 'tna' package from CRAN if needed (uncomment if required).\n# install.packages(\"tna\")\n\n# Load packages\nlibrary(\"tna\")\nlibrary(\"tidyverse\")\n\n# Load example data provided within the 'tna' package, \n# representing group regulatory interactions\ndata(group_regulation)\n\n# Run FTNA on 'group_regulation' data using raw counts of \n# transitions (\"absolute\" type) and print the result\nmodel <- ftna(group_regulation)\n\n# Print the output to inspect the model\nprint(model)\n\n\n\nState Labels\n\nadapt, cohesion, consensus, coregulate, discuss, emotion, monitor, plan, synthesis \n\nTransition Frequency Matrix\n\n           adapt cohesion consensus coregulate discuss emotion monitor plan synthesis\nadapt          0      139       243         11      30      61      17    8         0\ncohesion       5       46       844        202     101     196      56  239         6\nconsensus     30       94       519       1188    1190     460     295 2505        48\n [ reached getOption(\"max.print\") -- omitted 6 rows ]\n\nInitial Probabilities\n\n     adapt   cohesion  consensus coregulate    discuss    emotion    monitor       plan  synthesis \n     0.011      0.060      0.214      0.019      0.175      0.151      0.144      0.204      0.019 \n\n\nAs we mentioned above, FTNA can be estimated with a scaled transition matrix where the highest transition frequency has the weight of 1 and the lowest has the weight of 0. The resulting transition matrix is easier to read and interpret and more importantly can be useful when comparing across datasets with different samples sizes. The following code estimates a FTNA model using the argument scaling = \"minmax\". All of the subsequent analysis will be performed using the scaled version given its ease of use, interpretability and ease of comparison across datasets. \n\n# Calculate the Transition Network Analysis (TNA) on the group_regulation \n# data with scaled weights between 0 and 1\nmodel_scaled <- ftna(group_regulation, scaling = \"minmax\")\nprint(model_scaled) # Print the FTNA model with scaled weights\n\n\n\nState Labels\n\nadapt, cohesion, consensus, coregulate, discuss, emotion, monitor, plan, synthesis \n\nTransition Frequency Matrix\n\n             adapt cohesion consensus coregulate discuss emotion monitor    plan synthesis\nadapt      0.00000  0.05549     0.097    0.00439  0.0120  0.0244 0.00679 0.00319   0.00000\ncohesion   0.00200  0.01836     0.337    0.08064  0.0403  0.0782 0.02236 0.09541   0.00240\nconsensus  0.01198  0.03752     0.207    0.47425  0.4750  0.1836 0.11776 1.00000   0.01916\n [ reached getOption(\"max.print\") -- omitted 6 rows ]\n\nInitial Probabilities\n\n     adapt   cohesion  consensus coregulate    discuss    emotion    monitor       plan  synthesis \n    0.0115     0.0605     0.2140     0.0190     0.1755     0.1515     0.1440     0.2045     0.0195 \n\n\n\n\n\nThe last type of model is the ranked model which can be estimated using the argument scaling = \"rank\". \n\n# Calculate the Transition Network Analysis (TNA) on the `group_regulation` \n# data with ranked weights\nmodel_ranked <- ftna(group_regulation, scaling = \"rank\")\nprint(model_ranked) # Print the FTNA model with ranked weights\n\n\n\nState Labels\n\nadapt, cohesion, consensus, coregulate, discuss, emotion, monitor, plan, synthesis \n\nTransition Frequency Matrix\n\n           adapt cohesion consensus coregulate discuss emotion monitor plan synthesis\nadapt        2.0     42.0        52       11.5    19.5    31.0      14  9.0       2.0\ncohesion     4.0     25.5        72       48.0    38.0    47.0      30 51.0       5.5\nconsensus   19.5     36.0        67       76.0    77.0    64.0      57 81.0      28.0\n [ reached getOption(\"max.print\") -- omitted 6 rows ]\n\nInitial Probabilities\n\n     adapt   cohesion  consensus coregulate    discuss    emotion    monitor       plan  synthesis \n    0.0115     0.0605     0.2140     0.0190     0.1755     0.1515     0.1440     0.2045     0.0195 \n\n\n\n6.1 Plotting\nPlotting the FTNA model is a simple process. We simply use the plot() function with the model as the argument i.e., the model_scaled object we estimated earlier. In fact, most FTNA objects can be visualized using the plot() function directly without the need to any arguments. This include centralities, communities, permutation.\nAs the code below shows, the plot is overly dense and hard to read. This is because even rare and small edges are plotted. In our model, we have 9 nodes, the model includes up to 81 edges and this is far from informative due to the multidimensionality and the difficulty in picking the patterns that matter. The easiest way it to set a minimum value which will hide (but not delete) the edges below that threshold. In the example below, we set the minimum to 0.01 just to show all the model but you can set it to 0.05 or more if you want.\n\n\n\n\n\n6.2 Interpretation of the model\nThe model shows strong transitions from “consensus” to “plan,” “discuss” to “consensus,” and “emotion” to “cohesion” highlighting a network structure where agreement, dialogue, and emotional regulation play central roles in influencing and organizing the self-regulated collaboration. The transition between “consensus” and “plan” shows a high transition frequency of 1000 (simply means that it is the highest transition frequency in the network). This indicates a strong direct link where achieving consensus is usually followed by planning further steps. Another prominent transition exists from “plan” to “consensus” with a transition strength of 0.71, showing a feedback loop reinforcing a cyclic pattern of planning and consensus building within SRL.\n“Discuss” also appears as a central node with strong outward transitions to “consensus” (0.51) and “emotion” (0.31), highlighting that discussions often yield mutual understanding or emotional bonding. Similarly, “emotion” shows a strong connection to “cohesion” with a transition frequency of 0.368, and to “consensus” (0.362), reflecting how emotional states may influence group cohesion and shared agreement. Other important transitions include “coregulate” to “consensus” (0.474) and “monitor” to “consensus” (0.091), revealing that collaborative regulation and monitoring are necessary for building agreement and cohesion. We also see, “adapt” and “synthesis” show lower transition frequencies overall, suggesting that these may either serve more limited roles in the learning process where synthesis or adaptation occurs with fewer preceding dependencies. In general, we see strong influence on planning and task enactment where emotions and discussion play an important role with less emphasis on adaptation or monitoring.\n\n\n6.3 Pruning\nIn many instances, you may want to prune the model, i.e., remove small edges and continue to work with a trimmed model that contain only strong-enough edges. The prune() function is designed to do that by removing —pruning— edges based on specified criteria. The options of pruning are designed to remove small edges (either based on threshold or percentile) or using an algorithm that can retrieve the backbone of the network. The three options are:\n\nThreshold-based pruning allows researchers to specify a fixed numeric value, which deletes all edge weights that are less than or equal to the defined value while keeping the network fully connected (or at least weakly so).\nPercentile-based pruning allows to define a numeric percentile where edges with weights that fall below this specified percentile will be deleted. For instance, if a user sets a percentile of 0.20, this will eliminate the lowest 20% of edges based on their weights and retain the rest of strong edges within the network. Both threshold-based and percentile-based pruning make sure that pruning does not disconnect the networks.\nDisparity filter uses a disparity filter algorithm to prune negligible edges and retain the backbone of the network. The disparity filter algorithm works by randomizing the network connections, allowing researchers to determine which edges are significantly stronger than expected by chance. Edges are retained if they are significant relative to a null model. However, disparity filter is strict and we recommend using an alpha level of around 0.5 to retain edges that are less likely to have resulted from chance. Please note that the three methods delete edges from the model.\n\nThe code below demonstrates the three approaches to pruning we just described, followed by plotting each pruned network for visual comparison (Figure 16.3). First, we use the prune() function with the arguments method = \"threshold\" and threshold = 0.1 to keep only the edges with weights above 0.1. The second approach, using method = \"lowest\" and lowest = 0.15, filters the network by retaining the top 85% of edges to remove the weakest 15%. The third method, method = \"disparity\" with level = 0.5, applies a disparity filter to keep only edges deemed statistically significant within the network structure, thereby helping to reveal meaningful links based on a significance level threshold of 0.5 or edges that are more likely than by chance.\n\n# Pruning with different methods\npruned_threshold <- prune(model_scaled, method = \"threshold\", threshold = 0.1)\npruned_lowest <- prune(model_scaled, method = \"lowest\", lowest = 0.15)\npruned_disparity <- prune(model_scaled, method = \"disparity\", alpha = 0.5)\n\n# Plotting for comparison\nplot(pruned_threshold)\nplot(pruned_lowest)\nplot(pruned_disparity)\nplot(model_scaled, minimum = 0.05, cut = 0.1)\n\n\n\n\n\n\n\n(a) Threshold = 0.15\n\n\n\n\n\n\n\n(b) Lowest 15%\n\n\n\n\n\n\n\n\n\n(c) Disparity filter\n\n\n\n\n\n\n\n(d) Not pruned (minimum threshold)\n\n\n\n\nFigure 3. FTNA scaled pruned with different methods\n\n\n\nAlternatively, to plot the model without deleting any edges, you can hide small edges while retaining them for further computations by setting the minimum argument to a value e.g., 0.05. This will hide the edges below this value (0.05) but will retain the full model for further analysis (Figure 16.4).\n\nplot(model_scaled, minimum = 0.1, cut = 0.01)\n\n\n\n\nFigure 4. FTNA scaled\n\n\n\n\n\n\n6.4 Patterns\nPatterns provide insights into the the underlying mechanisms (e.g., behavior) that generated the data. These mechanisms give rise to recurring structures and connections between the learning processes. The study of such structures can help researchers interpret the underlying process, build new hypotheses or verify existing ones and identify areas that may need support (e.g., patterns indicating lack of regulation). FTNA allows users to detect and visualize n-cliques or groups of nodes that are all interconnected with transitions meeting specific criteria.\nCliques are strong, recurring patterns where nodes (representing behaviors or states) are strongly interconnected. While one can get several types of cliques, in FTNA we focus on dyads and triads. Dyads are two strongly linked nodes and triads are three well-connected nodes that reflect more complex dependencies. These strong structures are in a way what form or shape the entire process.\nThe cliques() function in FTNA allows users identify different cliques of different sizes by specifying the number of nodes in each clique. For instance, setting size = 2 will identify dyads with strong pairwise mutual relationships. The function also has a threshold argument to get only dyads which are strongly connected above that threshold to ensure that only meaningful connections are visualized. Triads can be also identified in the same way by setting the size argument as size = 3 which will capture patterns of three interconnected nodes above the stated threshold, in our case it is 0.05.\nThe clique function has a sum_weights argument that lets users decide if the sum of edge weights should be considered in forming the cliques. For example, setting the threshold to 0.10 and sum_weights = FALSE means each of the two edges should have a weight of at least 0.10 to be considered as part of the same clique. While sum_weights = TRUE means that only the sum of the weights of the two edges needs to be 0.1, i.e., an edge can be 0.04 and the other can be 0.06.\nThe code below is straightforward as it identifies and plots the cliques and shows examples of dyads, triads and 4-cliques and 5-cliques. In the first example, we set the size = 2 and threshold = 0.1 to get the dyads with strong transitions, while size = 3 with a lower threshold = 0.05 captures triads, identifying three-node interdependencies. Larger cliques, such as structures with 4 or 5 nodes (or more) can be obtained in the same way. We can also print and plot the cliques. For brevity, we only show the plot for the dyads (Figure 16.5).\n\n# Identify 2-cliques (dyads) from the FTNA model with a weight threshold, \n# excluding loops in visualization.\n# A 2-clique represents a pair of nodes that are strongly connected based on \n# the specified weight threshold.\ncliques_of_two <- cliques(\n  model_scaled,      # The FTNA model with scaled edge weights\n  size = 2,          # Looking for pairs of connected nodes (dyads)\n  threshold = 0.1    # Only include edges with weights greater than 0.1\n)\n\n# Print and visualize the identified 2-cliques (dyads)\nprint(cliques_of_two)  # Display details of 2-cliques\n\nNumber of 2-cliques: 8 (weight threshold = 0.1)\nShowing 6 cliques starting from clique number 1\n\nClique 1:\n          consensus plan\nconsensus     0.207 1.00\nplan          0.714 0.92\n\nClique 2:\n          consensus discuss\nconsensus     0.207   0.475\ndiscuss       0.507   0.307\n\nClique 3:\n        discuss emotion\ndiscuss   0.307   0.167\nemotion   0.115   0.087\n\nClique 4:\n        emotion  plan\nemotion   0.087 0.113\nplan      0.361 0.920\n\nClique 5:\n          consensus emotion\nconsensus     0.207   0.184\nemotion       0.363   0.087\n\nClique 6:\n        monitor  plan\nmonitor  0.0104 0.123\nplan     0.1856 0.920\n\n\n\nplot(cliques_of_two)   # Visualize 2-cliques in the network\n\n\n\n\n\n\n\n\n(a) Dyad 1\n\n\n\n\n\n\n\n(b) Dyad 2\n\n\n\n\n\n\n\n(c) Dyad 3\n\n\n\n\n\n\n\n\n\n(d) Dyad 4\n\n\n\n\n\n\n\n(e) Dyad 5\n\n\n\n\n\n\n\n(f) Dyad 6\n\n\n\n\nFigure 5. Identified dyads\n\n\n\n# Identify 3-cliques (triads) from the FTNA model.\n# A 3-clique is a fully connected set of three nodes, indicating a strong \n# triplet structure.\ncliques_of_three <- cliques(\n  model_scaled,      # The FTNA model with scaled edge weights\n  size = 3,          # Looking for triplets of fully connected nodes (triads)\n  threshold = 0.05   # Only include edges with weights greater than 0.05\n)\n\n# Print and visualize the identified 3-cliques (triads)\n# Uncomment the code below to view the results\n# print(cliques_of_three) # Display details of 3-cliques\n# plot(cliques_of_three)  # Visualize 3-cliques in the network\n\n# Identify 4-cliques (quadruples) from the FTNA model.\n# A 4-clique includes four nodes where each node is connected to every other \n# node in the group.\n# Uncomment the code below to view the results\ncliques_of_four <- cliques(\n  model_scaled,      # The FTNA model with scaled edge weights\n  size = 4,          # Looking for quadruples of fully connected nodes (4-cliques)\n  threshold = 0.03   # Only include edges with weights greater than 0.03\n)\n\n# Print and visualize the identified 4-cliques (quadruples) \n# Uncomment the code below to view the results\n# print(cliques_of_four)  # Display details of 4-cliques\n# plot(cliques_of_four)   # Visualize 4-cliques in the network\n\n# Identify 5-cliques (quintuples) from the FTNA model, summing edge weights.\n# Here, the sum of edge weights in both directions must meet the specified \n# threshold for inclusion.\n# Uncomment the code below to view the results\ncliques_of_five <- cliques(\n  model_scaled,      # The FTNA model with scaled edge weights\n  size = 5,          # Looking for quintuples of fully connected nodes (5-cliques)\n  threshold = 0.1,   # Only edges with total bidirectional weights greater than 0.1\n  sum_weights = TRUE # Sum edge weight in both directions when computing  threshold\n)\n\n# Print and visualize the identified 5-cliques (quintuples)\n# print(cliques_of_five)  # Display details of 5-cliques\n# plot(cliques_of_five)   # Visualize 5-cliques in the network\n\n\n\n6.5 Graph level measures\nFTNA provides provide an easy access to all graph level measures through the summary() function. The summary function gives insights about nodes, the frequency and strength of transitions, and the overall connectedness of the network. The network metrics include the number of unique states (nodes) and the number of transitions (edges) which form the network’s basic structure. Measures of density, mean distance, and reciprocity and other metrics which that help interpret the dynamic structure of the transition network:\n\nNode Count: The number of unique states or events being analyzed in the network.\nEdge Count: Total number of transitions recorded between states.\nNetwork Density*: The proportion of possible transitions that are actually observed, where a density of 1 implies every possible transition between states has occurred at least once. Please note that in most cases of networks that are not pruned, density metric does not offer much value.\nMean Distance: The average number of transitions (or steps) needed to move between any two states in the network reflecting the network’s overall “connectivity” or flow.\nMean Out-Strength*: The total strength (sum of transition weights) of transitions that each state initiates.\nMean In-Strength*: The mean of total strength (sum of transition weights) directed towards each state.\nMean Out-Degree*: The mean of number of distinct states to which a particular state transitions, representing its transition “breadth.”\nMean In-Degree*: The number of distinct states transitioning into a specific state, indicating its tendency to be a transition destination.\nCentralization*: Measures how much the network is dominated by a central “hub” state, often a highly frequent transition point or common intermediary.\nReciprocity*: Reflects the tendency of transitions to be bidirectional, meaning if a transition exists from State A to B, it is likely B also transitions back to A, highlighting cyclic or reversible behavior within the network.\n\nPlease note that the measures marked with * do not offer much information in fully connected networks which is the case in most cases of FTNA and therefore should be interpreted with caution in these situations. However, the metrics are particularly useful in pruned models. See and contrast below two examples: the original model on the left side and the pruned model on the right side.\n\n\n\n\n\nsummary(model_scaled)\n\n\n\n  \n\n\n\n\n\n\n\n\nsummary(pruned_disparity)\n\n\n\n  \n\n\n\n\n\n\n\nThe results of the summary() function of our examined networks show the network consists of 9 nodes and 78 edges, resulting in a high density of 1.0, indicating that all possible connections between nodes are present. The mean distance between nodes is minimal at 0.024, reflecting a tightly connected network where nodes are generally very close to each other in terms of path length. The average out-strength and in-strength for nodes are both approximately 1.1325, with standard deviations of 0.886 and 0.878, respectively. These values indicate some variability in the strength of connections. Similarly, the mean out-degree (the average number of connections each node has outward) is 8.67, with a low standard deviation of 0.707, showing that most nodes have nearly equal connectivity. Network centralization for both out-degree and in-degree is low 0.016. Reciprocity is 0.986 indicating that nearly all directed connections in this network are mutual, or reciprocated, which is typical of networks where bidirectional interaction is common. As mentioned above these numbers are expected in a fully connected network before pruning. Compare these values to the pruned network.\n\n\n6.6 Node level measures\nCentrality measures can help identify the influential events or states and rank their importance within the learning process [14]. The tna package provides the centralities() function designed for calculating various centralities in directed weighted networks. This function requires only the FTNA model object as input and it automatically computes a range of centralities, namely, out-strength, in-strength, closeness, betweenness, fiffusion and clustering measures. By default, the function does not consider self-transitions (loops) as they do not represent changes of a state. Removing loops helps focus the analysis on transitions between distinct states rather than within-state repetitions. Of course, users can set loops = TRUE to count loops which may be useful in cases where researchers are estimating stability of states, e.g., remaining in the same state or other contextual reasons.\nOut-strength centrality indicates how frequently a state transitions to other states which captures the change and stability within that state or that event. A higher out-strength suggests that the state often leads to different actions, while a lower out-strength implies stability, repetition or being stuck in the same state. In a learning context, a state like “plan” might have a high out-strength, indicating that students quickly transition from planning to other steps. In-strength centrality, by contrast, represents how frequently a state receives transitions and being a common destination. For instance, if “consensus” has high in-strength centrality it signifies that other learning states frequently result in consensus, reflecting group dynamics of agreement and cohesion.\nBetweenness centrality reflects which states act as a bridge or mediates between other transitions. For instance, a state with high betweenness, like “synthesis”, might connect various learning actions signifying its role as a juncture in the process. Given the flexible multi-step conceptualization of the network, several other centralities can be useful here —depending on the context of course— like diffusion centrality which is a good indication of diffusion of ideas or actions across the network.\nThe code below begins by calculating centrality measures using the centralities() function on model_scaled. After these measures are computed, we print them out to inspect the results (Table 16.5). Next, we visualize these centrality measures with plot(centrality_measures) (Figure 16.6).\n\n# Compute centrality measures for the FTNA model\ncentrality_measures <- centralities(model_scaled)\n\n# Print the calculated centrality measures in the FTNA model\nprint(centrality_measures)\n\n\n\n\n\nTable 5. Centrality measures plot for the FTNA model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nState\nOutStrength\nInStrength\nClosenessIn\nClosenessOut\nCloseness\nBetweenness\nBetweennessRSP\nDiffusion\nClustering\n\n\n\n\nadapt\n0.20\n0.21\n14.56\n5.83\n21.59\n20\n91\n9.14\n0.39\n\n\ncohesion\n0.66\n0.67\n6.61\n6.54\n18.69\n0\n86\n32.55\n0.50\n\n\nconsensus\n2.32\n2.34\n1.25\n5.89\n7.68\n0\n2\n87.21\n0.22\n\n\ncoregulate\n0.77\n0.82\n11.28\n4.02\n12.98\n0\n83\n30.27\n0.63\n\n\ndiscuss\n1.27\n1.26\n7.22\n3.30\n7.68\n0\n82\n46.98\n0.37\n\n\nemotion\n1.05\n1.02\n4.46\n6.68\n16.93\n0\n75\n43.56\n0.53\n\n\nmonitor\n0.56\n0.48\n10.84\n5.24\n13.05\n7\n86\n22.90\n0.58\n\n\nplan\n1.54\n1.56\n11.60\n5.88\n16.81\n9\n1\n68.86\n0.43\n\n\nsynthesis\n0.26\n0.28\n11.99\n5.72\n16.93\n21\n91\n10.90\n0.52\n\n\n\n\n\n\n\n# Visualize the calculated centrality measures in the FTNA model\nplot(centrality_measures)\n\n\n\n\nFigure 6. Centrality measures plot for the FTNA model\n\n\n\n\nWe can also compute other centralities using the function as.igraph(model_scaled) which converts the FTNA model into an igraph object and thus enables all possible centralities and measures that can be computed using the igraph R package. For instance, the below code calculates the hub and authority scores for the FTNA network model using the hits_scores() function from the igraph package, it first converts the FTNA model (model_scaled) into an igraph object. Hub scores measure how often a node (event or state) points to other important nodes, reflecting its influence as a source. Authority scores measure how frequently a node is targeted by others, indicating its importance as a destination or key reference point. The resulting scores provide insight into which states act as influential hubs or authoritative endpoints within the learning process.\nThe code below exemplifies how to compute extra measures. We first convert our scaled FTNA model (model_scaled) into an igraph object. The igraph package supports a large array of graph analyses, including the HITS (Hyperlink-Induced Topic Search) algorithm, which calculates “hub” and “authority” scores for each node.\n\n# Convert the FTNA model to an igraph object and \n# calculate HITS (Hub and Authority) scores\nhits_results <- igraph::hits_scores(as.igraph(model_scaled))\n\n# Extract the hub and authority scores from the HITS results for further analysis\nhub_scores <- hits_results$hub\nauthority_scores <- hits_results$authority\n\n\n# Print the hub and authority scores to view influential nodes\nprint(hub_scores)\nprint(authority_scores)\n\n\n\n     adapt   cohesion  consensus coregulate    discuss    emotion    monitor       plan  synthesis \n    0.0556     0.2404     0.9551     0.2557     0.3768     0.3053     0.1895     1.0000     0.0738 \n\n\n     adapt   cohesion  consensus coregulate    discuss    emotion    monitor       plan  synthesis \n    0.0331     0.1292     0.6724     0.2925     0.4368     0.3437     0.1735     1.0000     0.0564 \n\n\n\n\n6.7 Edge level measures\nFTNA can also compute edge level measures which would show which edges are important in the transition model and act as a bridge. Namely, FTNA can compute the edge betweenness.\n\nedge_between <- betweenness_network(model_scaled)\nplot(edge_between)\n\n\n\n\nFigure 7. Edge betweenness network\n\n\n\n\n\n\n6.8 Community detection\nCommunity detection can help identify groups of nodes —such as states, events, or actions— that exhibit strong interconnections between the nodes. Unlike cliques, which are defined by criteria of mutual connections among a fixed set of nodes and thresholds, communities are identified algorithmically and can vary in size and connection strength. This flexibility allows for a more nuanced and realistic approach to grouping transitions that share similar connectivity patterns regardless of their size, be it two nodes or five, as long as they exhibit shared interconnections.\nFinding communities has been a central theme in social science for finding groups of constructs that are related (e.g., meta-cognition related nodes) to understand the structure of these constructs and their tight-knit relationships. Other examples include finding latent variables, propose new constructs and understand the semantics of discourse or behavior.\nTNA includes several community detection algorithms that are designed for transition networks (which are typically small, weighted, and directed). Each algorithm provides a distinct perspective on the grouping process. Some algorithms may focus on maximizing modularity, while others might emphasize flow or path length. As such the researcher can implement whatever suites their context. The code below performs community detection with the function communities() which takes the model argument and computes the communities, e.g., communities(model_scaled). The results are stored in the detected_communities object which contains the results of the algorithms: Walktrap, Label Propagation, Infomap, Edge Betweenness, Leading Eigen, and Spinglass. To visualize the communities assignment, you can simply use the function plot with the name of the community that you want to use (Figure 17.3). The detected_communities object can also be printed.\n\ndetected_communities <- communities(model_scaled)\nplot(detected_communities, minimum = 0.05)\n\n\n\n\nFigure 8. Community Detection using Fast Greedy Method\n\n\n\n\n\nprint(detected_communities)\n\nNumber of communities found by each algorithm:\n        walktrap      fast_greedy       label_prop          infomap edge_betweenness \n               1                3                1                1                1 \n   leading_eigen        spinglass \n               3                3 \n\nCommunity assignments:\n       node walktrap fast_greedy label_prop infomap edge_betweenness leading_eigen spinglass\n1     adapt        1           1          1       1                1             1         3\n2  cohesion        1           2          1       1                1             3         2\n3 consensus        1           3          1       1                1             2         1\n [ reached 'max' / getOption(\"max.print\") -- omitted 6 rows ]\n\n\n\n\n6.9 Bootstrapping\nBootstrapping is a technique for assessing the accuracy and stability of edge weights in the network. With bootstrapping, we can estimate the likelihood that each transition would replicate or appear with similar strength in repeated sampling. In fact, bootstrapping is one of the strengths of FTNA as it allows us to verify the strength of each edge in the network and make sure that it is strong enough to be interpretable. Bootstrapping entails re-sampling the data repeatedly. Each of these sub-samples is created by randomly selecting data points from the original dataset with replacement. Then we reconstruct an FTNA network, recalculate the edge weights each time (i.e., build a new FTNA model). The process is commonly performed a large number of times (usually 1,000 iterations) to generate a distribution of edge weights for each transition in the network. Then we compare the original edge weight against the range of edge weights obtained from the 1,000 bootstrap samples and calculate confidence intervals for each edge. We can also compute p-values for each edge to estimate how different the edge is given a certain threshold value (e.g., 0.05) across the bootstrap samples. When an edge consistently exceeds this threshold, we consider it statistically significant. Bootstrapping also offers an idea about the stability and the robustness of the entire FTNA network as a whole.\nTo perform bootstrapping we use the bootstrap() function, which requires the model argument as an argument. The function typically uses a default of 1,000 bootstrap iterations (iter = 1000), though increasing the number of iterations may improve reliability. Additionally, by setting a threshold (e.g., threshold = 0.05), we can directly test whether an edge’s weight is consistently above this value, indicating statistical significance across samples. To view the edge data, confidence intervals and their p-values, we can print a summary output which contains the full bootstrap results. \n\n# Perform bootstrapping on the FTNA model with a fixed seed for reproducibility\nset.seed(265)\nboot <- bootstrap(model_scaled, threshold = 0.05)\n\n# Print the combined results data frame containing\nprint(summary(boot))\n\n   from         to weight p_value   sig ci_lower ci_upper\n2 adapt   cohesion 0.0020       1 FALSE 0.000397  0.00364\n3 adapt  consensus 0.0120       1 FALSE 0.007822  0.01631\n4 adapt coregulate 0.0128       1 FALSE 0.008502  0.01767\n5 adapt    discuss 0.1126       0  TRUE 0.099843  0.12688\n [ reached 'max' / getOption(\"max.print\") -- omitted 74 rows ]\n\n# View non-significant edges  which are less likely to be stable across bootstrap samples\nprint(boot, type = \"nonsig\")\n\nNon-significant Edges\n\n   from         to  weight p_value ci_lower ci_upper\n2 adapt   cohesion 0.00200       1 0.000397  0.00364\n3 adapt  consensus 0.01198       1 0.007822  0.01631\n4 adapt coregulate 0.01277       1 0.008502  0.01767\n6 adapt    emotion 0.00279       1 0.000795  0.00489\n7 adapt    monitor 0.00639       1 0.003546  0.00992\n [ reached 'max' / getOption(\"max.print\") -- omitted 34 rows ]\n\n\n\n\n6.10 Comparing Models\nTwo methods can be used to compare models. The first is to compare the raw difference between the weights of the models. The second is to use a statistical technique that shows to what extent each edge differ statistically. FTNA has both methods, the first method offers a descriptive view of the differences with an intuitive visualization that shows the differences. To perform such comparison, we use the plot_compare() function to visualize the differences. In the plot, green edges are higher in the first model, red means lower edges and so is the case in the pie.\nHowever, to rigorously compare models, FTNA uses a permutation test which estimates which edges are statistically significantly different and produces a p-value for each edge. The code below uses the group_regulation dataset which has two subsets. The first subset, group_regulation[1:1000,], represents the high achievers model, while the second subset group_regulation[1001:2000,] represents the low achievers. For each of the subsets we create a model by applying the tna() function to its respective subset. Next, we perform a permutation test with permutation_test() to assess whether the observed differences between the “Hi” and “Lo” models are statistically significant. Setting iter = 1000 instructs the function to run 1000 iterations, creating a distribution of differences by repeatedly shuffling and comparing the groups. Finally, we plot the significant differences identified through the permutation test using plot(Permutation, minimum = 0.001) (Figure 16.9). This final plot visually highlights edges that have a statistically significant difference in transition weight between the “Hi” and “Lo” models.\n\n# Create FTNA for the high-achievers subset (rows 1 to 1000)\nHi <- ftna(group_regulation[1:1000, ], scaling = \"minmax\")\n\n# Create FTNA for the low-achievers subset (rows 1001 to 2000)\nLo <- ftna(group_regulation[1001:2000, ], scaling = \"minmax\")\n\n# Plot a comparison of the \"Hi\" and \"Lo\" models\n# The 'minimum' parameter is set to 0.001, so edges with weights >= 0.001 are shown\nplot_compare(Hi, Lo, minimum = 0.0001)\n\n# Run a permutation test to determine statistical significance of \n# differences between \"Hi\" and \"Lo\"\n# The 'it' parameter is set to 1000, meaning 1000 permutations are performed\nPermutation <- permutation_test(Hi, Lo, it = 1000)\n\n# Plot the significant differences identified in the permutation test\nplot(Permutation, minimum = 0.01)\n\n\n\n\n\n\n\n(a) Subtraction of high achievers minus low achievers\n\n\n\n\n\n\n\n(b) Significant differences identified in the permutation test\n\n\n\n\nFigure 9. Network comparison\n\n\n\n\n\n6.11 Centrality stability\nIn contrast to most network models. FTNA has a robust mechanism to test the stability and the significance of centrality measures through case-dropping. Case-dropping involves sequentially removing data points and re-calculating correlation coefficients that compare the centrality measures in the original network with those derived from networks missing the data points. A correlation coefficient closer to 1 suggests that the centrality rankings of nodes remain stable even when data is removed which indicates that the centrality value is resilient to sample variability and more likely to generalize. A correlation coefficient above 0.7, for instance, would indicate that the centrality measure is reliable; the most central nodes in the original FTNA network remain central even as data points are omitted. An average coefficient between 0.5 and 0.7 reflects moderate stability. However, an average coefficient below 0.5 would indicate low stability.\nThe function estimate_centrality_stability() (abbreviated as estimate_cs()) can be used to assess the stability of centrality measures in FTNA models. The function generates bootstrap results that can then be visualized (Figure 16.10), offering a clear representation of centrality stability within the FTNA framework.\n\nCentrality_stability <- estimate_centrality_stability(model_scaled, detailed = FALSE)\nplot(Centrality_stability)\n\n\n\n\nFigure 10. Centrality stability"
  },
  {
    "objectID": "chapters/ch16-ftna/ch16-ftna.html#conclusion",
    "href": "chapters/ch16-ftna/ch16-ftna.html#conclusion",
    "title": "16  Capturing The Breadth and Dynamics of the Temporal Processes with Frequency Transition Network Analysis: A Primer and Tutorial",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nThis chapter presented FTNA as a flexible and robust approach to analyzing temporal data of learning processes. FTNA is a novel tool for analyzing complex learning processes. Its flexibility, scalability, and straightforward modeling of transition frequencies make it well-suited for exploring and comparing learning dynamics across varied educational contexts. Unlike traditional Transition Network Analysis (TNA) [2], which models transitions probabilistically based on Markov assumptions, FTNA focuses on the absolute frequency of transitions between states, offering a straightforward representation of learning dynamics without conditioning on prior states. This method is particularly advantageous where dependencies span multiple steps, such as collaborative learning or complex problem-solving tasks or for contexts with rare transitions, small sample sizes, or processes.\nWhat makes FTNA stand out in comparison to existing techniques is its statistical rigor. In that, FTNA allows meaningful comparisons across different datasets or conditions with permutation. Statistical validation methods, such as bootstrapping increase the rigor of FTNA and provide a validation method for the resulting models. Additionally, FTNA inherits the network capabilities of TNA, such as centrality measures and pattern detection [18] which offer a deeper understanding of structural relationships within the learning process."
  },
  {
    "objectID": "chapters/ch17-tna-clusters/ch17-tna-clusters.html",
    "href": "chapters/ch17-tna-clusters/ch17-tna-clusters.html",
    "title": "17  Mining Patterns and Clusters with Transition Network Analysis: A Heterogeneity Approach",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch17-tna-clusters/ch17-tna-clusters.html#introduction",
    "href": "chapters/ch17-tna-clusters/ch17-tna-clusters.html#introduction",
    "title": "17  Mining Patterns and Clusters with Transition Network Analysis: A Heterogeneity Approach",
    "section": "1 Introduction",
    "text": "1 Introduction\nClusters, within the context of Transition Network Analysis (TNA), represent distinct subgroups within the sequence data that exhibit unique transition patterns between states or events [1]. In other words, these clusters share similar transition dynamics that are closer to each other within the same cluster and rather different from other clusters. Identifying these clusters rests on the assumption that students’ behavior varies across contexts and conditions giving rise to patterns of similar behavior that can be captured by appropriate clustering techniques. In doing so, clustering mines these unobserved patterns that may be overlooked when studying the data as a whole without considering the heterogeneity. Not accounting for the heterogeneity in the data may obscure individual differences [2], let alone the true underlying structure.\nClusters often arise from an underlying data generation mechanism that shapes how learners interact with tasks, contexts, and each other. The data generating mechanism reflects the interplay of cognitive, emotional, and social processes, as well as the structure and demands of the learning environment. For instance, self-regulation as a data generating mechanism shapes the transitions between states —such as planning, monitoring, and adaptation— which are driven by learners’ efforts to achieve goals, respond to feedback, or enact their tasks [3]. External factors, such as the presence of collaborative tasks or deadlines, may also influence these transitions by prompting learners to emphasize specific strategies like socio-emotional regulation or task coordination. Additionally, the heterogeneity in learners’ dispositions contributes to diverse patterns in how they regulate their learning as well as the facilitation and contextual variables. Together, these factors generate the distinct SRL transition patterns that are captured and represented with TNA clusters offering a window into their learning processes.\nCapturing such patterns allows to accurately understand students’ learning processes and their variations. Furthermore, it provides a foundation for theory building by linking these clusters to specific underlying mechanisms, e.g., self-regulation tactics and strategies. Clusters also represent episodes —sessions, epochs or times— where learners manifest certain behaviors. For instance, in a learning management system (LMS), a cluster could represent students’ pattern of focus on assessment, another may represent students’ engagement with their lectures, and so on. In the same vein, in self-regulated learning, clusters reveal variations in learning strategies, such as learners who adapt effectively by transitioning from monitoring to task adaptation, demonstrating advanced metacognitive skills. In contrast, some clusters may show learners stuck in cycles of monitoring and exploration without progressing, highlighting inefficiencies in self-regulation that could benefit from targeted interventions.\nThe learning analytics literature has a rich tradition of grouping students’ similar behavior using similar methods, e.g., in sequence mining of LMS data these clusters are often referred to as tactics and aims to find clusters of online behavior [4], self-regulation [5], or engagement [6]. One can say that clusters in TNA are similar, however, the emphasis here is on clusters that share similar transition dynamics.\nClustering in TNA not only identifies subgroups, but also broadens our understanding of the strategies learners employ [7]. When we analyze the group as a whole, only the average or most common patterns are visible. Instead, clustering allows to place the focus in patterns that are unique to specific subgroups, which may have been eclipsed by the general trends [2]. Moreover, clustering allows researchers to ask questions about the conditions under which specific patterns emerge if we use covariates and explanatory variables as part of the clustering model. For instance, do learners who frequently transition between planning and monitoring perform better in problem-solving tasks, and how does this vary in collaborative versus individual learning settings? Do emotional regulation clusters correlate with sustained engagement in challenging tasks, and how can these insights guide interventions?\nIn TNA, clustering the sequence data is performed through identifying distinct transition patterns using mixture Markov models (MMM) [8]. Unlike simple Markov models that assume homogeneity in transition dynamics across all data, MMM accounts for the coexistence of multiple latent subgroups, each governed by its own unique transition matrix. MMM works by assigning each sequence to one of latent subgroups which share similar transition probabilities. In general it is a robust and flexible technique for clustering sequential data and more importantly it allows the inclusion of covariates that help identify or explain why certain clusters emerged.\nUsing covariates allows researchers to explore what factors are associated with the emergence of specific clusters [9]. For example, higher-performing groups might consistently be involved in clusters of transitions involving monitoring and adaptation, reflecting emphasis on a metacognitive approach to their tasks. Such level of analysis contextualizes the clusters and also provides actionable insights into the conditions under which specific behaviors emerge.\nIn this chapter, we provide a tutorial on how to identify and study clusters within TNA. We first review related works addressing the heterogeneity of transition dynamics between states or events in the education literature. We then provide a step-by-step tutorial using R on how to implement clustering in TNA using the seqHMM [10] and the tna [11] packages."
  },
  {
    "objectID": "chapters/ch17-tna-clusters/ch17-tna-clusters.html#related-work",
    "href": "chapters/ch17-tna-clusters/ch17-tna-clusters.html#related-work",
    "title": "17  Mining Patterns and Clusters with Transition Network Analysis: A Heterogeneity Approach",
    "section": "2 Related work",
    "text": "2 Related work\nEducational research has evolved from adopting a one-size-fits-all approach to acknowledging the existing heterogeneity among students and their behavior [7]. The increasing availability of data about learners has made it possible to study heterogeneity when interacting in or with learning environments, like the LMS or discussion forums. This has enabled researchers to identify distinct self-regulated learning phases, learning tactics and strategies, or collaboration patterns. For example, looking at the transitions between actions that students carried out in the LMS, Saqr et al. [4] identified four learning tactics using a mixture hidden mixture Markov model: lecture read, forum read, forum interact, and diverse. Villalobos et al. [12] clustered sequences of students’ weekly interactions with the LMS using a mixture hidden Markov model and identified three distinct weekly strategies: quiz-focused, material-focused, and diverse.\nLooking into the overall LMS engagement levels instead, López-Pernas and Saqr [13] identified three distinct trajectories of students’ engagement profiles throughout each course of a study program using a mixture Markov model: engaged, fluctuating and persistently disengaged. They found that engagement dynamics fit the salient features of a complex dynamic system, where students that are disengaged rarely transition to higher engagement states, and engaged students —in the rare occasion they transition to lower engagement— return back to their engaged state right away. In a similar context, Saqr et al. [6] clustered students using a mixture hidden Markov model according to their patterns of LMS engagement and achievement and found three distinct trajectories of students: engaged high-achieving starters, average starters, and disengaged starters.\nAnother recent trend in the literature is the study of emotions and how they influence learning outcomes. For instance, Saqr and López-Pernas [14] used a mixture hidden Markov model to cluster students’ according to the transitions between their emotions throughout two lectures. They found three trajectories: one with consistent anxiety, another one with constant happiness and alertness, and a third one with fluctuations between motivation-satisfaction and happiness-alertness. In another study, Törmänen et al. [5] used mixture hidden Markov models to cluster multimodal data: affective states and regulation were identified from video data, and activation was captured from electrodermal activity data. The authors identified four clusters of students with distinct socio-emotional interaction episodes —frequent regulation, occasional regulation, positive, and negative— which showed differences in the variability of affective states and the activation of learning regulation."
  },
  {
    "objectID": "chapters/ch17-tna-clusters/ch17-tna-clusters.html#tutorial-clusters-in-tna",
    "href": "chapters/ch17-tna-clusters/ch17-tna-clusters.html#tutorial-clusters-in-tna",
    "title": "17  Mining Patterns and Clusters with Transition Network Analysis: A Heterogeneity Approach",
    "section": "3 Tutorial: Clusters in TNA",
    "text": "3 Tutorial: Clusters in TNA\nIn this section we present a step-by-step tutorial on how to identify and study clusters within TNA. Specifically, we will illustrate how to identify clusters using MMMs, visualize them, compare them, detect communities and compute centralities. We also illustrate how to implement other clustering and grouping techniques. To be able to follow the tutorial, it is assumed that the reader is familiar with the R programming language. If that is not the case, it is recommended to refer to previous tutorials on the basics of R [15], data cleaning [16], basic statistics [17], and visualization [18]. It is also recommended to have previous knowledge about Markov models [8] and to have read Chapter 15 about TNA [19].\n\n\n\n\n3.1 Importing libraries\nAs a first step in our tutorial, we will import the necessary libraries:\n\ntidyverse: A collection of R packages for data science, including tools for data manipulation (dplyr), visualization (ggplot2), tidying data (tidyr), and more, designed to work seamlessly together [20].\nTraMineR: A package for sequence analysis that provides methods to analyze and visualize sequential data, like event transitions or state changes [21].\nseqHMM: A package for sequence analysis supporting modeling and clustering of sequential data using various Markov models [10].\ntna: A package for implementing TNA, offering tools to estimate, visualize and investigate transitions over time using a network lens [11].\nrio: A package for reading data in several formats [22].\n\n\nlibrary(tidyverse) # install.packages(\"tidyverse\")\nlibrary(TraMineR) # install.packages(\"TraMineR\")\nlibrary(seqHMM) # install.packages(\"seqHMM\")\nlibrary(tna) # install.packages(\"tna\")\nlibrary(rio) # install.packages(\"rio\")\n\n\n\n3.2 Preparing the data\nWe will use a simulated dataset based on a recent study by Saqr and López-Pernas [23], containing students’ coded utterances in project-based learning implemented through online forum discussions. In this dataset, each row represents a team’s collaboration on a problem, containing the ordered coded utterances in each column. In addition, the dataset contains the grade for each student. There are 3000 sequences in total and 10 different types of coded utterances: agree, argue, disagree, discuss, evaluate, facilitate, organize, question, share, and socialize.\n\nsimulated_data <- import(\"https://github.com/lamethods/data2/raw/main/pbl/pbl.RDS\")\n\nWe can use the dataset to create a sequence object using the seqdef function from TraMineR and pass our coded data as an input (excluding the grades).\n\nsequ <- seqdef(simulated_data[, -1])\n\n\n\n3.3 Clustering\nIn our next step, we will cluster our sequence data with the aim of finding distinct groups that are governed by different transition dynamics. Specifically, we will fit an MMM to our data. A Markov model is a statistical model used to describe systems that transition between states with certain probabilities [8]. Markov models work under the assumption that the probability of moving to the next state depends only on the current state, not on the sequence of states that preceded it. A mixture Markov model extends the Markov model by allowing for multiple subpopulations or groups, each with its own set of transition probabilities (see Figure 17.1). Instead of assuming a single transition structure for all data, it models the data as a combination (or mixture) of multiple Markov models, where each component represents a different latent subgroup. For more details on these concepts, please refer to [8].\n\n\n\nFigure 1. Markov model vs. mixture Markov model\n\n\nWe will use the seqHMM library [10] to cluster our sequences of students’ collaboration in groups using an MMM. When dealing with long sequences, the algorithm in charge of finding the best fitting model may take a long time. Therefore, it is recommended to provide a starting value of initial and transition probabilities that we hypothesize will be close to the real one that we expect the algorithm to find. As is the case in most clustering techniques, we do not know the optimum number of clusters. We illustrate the process for four clusters in this tutorial. In practice, we should estimate different models with different number of clusters to see which is the best fitting (lowest Bayesian Information Criterion level). Please, refer to a previous tutorial on Markov models for more details on this step [8].\nTo simulate the transition probabilities, we use the function simulate_transition_probs from seqHMM. As the first input, we provide the number of events or states in our model (ten in our case). As the second input we provide the number of clusters that we expect to find in this model (as mentioned before, we should try several combinations).\nTo simulate the initial probabilities, we extract the first coded utterance for the complete sequence object, and we use table on the first column to calculate the proportion of times each code appears as the first step in the collaboration. We replicate these probabilities 4 times (one for each hypothesized cluster).\n\nset.seed(1234)\n\n# Creating constants for the number of codes (10) and the number of clusters (4)\nN_CODES <- length(alphabet(sequ))\nN_CLUSTERS <- 4\n\n# Transition probabilities (10 codes, 4 clusters)\ntrans_probs <- simulate_transition_probs(N_CODES, N_CLUSTERS, diag_c = 1)\n\n# Initial probabilities (proportion of each code in t = 1)\ninit_probs <- as.numeric(prop.table(table(sequ[, 1])[1:N_CODES]))\n\n# Replicate initial probabilities for each cluster\ninit_probs <- replicate(N_CLUSTERS, init_probs, simplify = FALSE)\n\nWe can now use build_mmm to define our MMM. We provide the sequence object as an input along with the number of clusters we expect to find (n_clusters), the transition probabilities, and the initial probabilities. In addition, an advantage of mixture Markov models —similarly, to other mixture models— is that it can take covariates. To do so, we have to provide the complete dataframe and specify the column(s) that we want to use as covariates, in our case, Grade. It is important to note that this dataframe should be in the same order as our sequence object. In addition, we need to provide a formula. In this case we use the formula ~ 0 + Grade, which indicates that the cluster membership is explained by the students’ grades.\n\n# Build mixture Markov model for 4 clusters\nmmm <- build_mmm(sequ, \n                  n_clusters = N_CLUSTERS,\n                  transition_probs = trans_probs,\n                  data = simulated_data,\n                  algorithm = \n                  formula = ~ 0 + Grade,\n                  initial_probs = init_probs)\n\nLastly, we can fit our model using fit_model. We provide the model that we have just defined (mmm) as an input. We can also define a series of parameters to specify the runtime of the algorithm that will find the optimum cluster solution. For example, we can specify how many CPU threads we would like to use (threads argument, 1 by default), that it should have 500 iterations (restart within control_em argument) as well as the optimization algorithm (algorithm within control_em argument, \"NLOPT_GD_MLSL_LDS\" by default), among others. See [8] and the seqHMM documentation [10] for more details. When dealing with long sequences and complex data like ours, this process can take a long time so you might need to leave your computer working for a few hours to arrive to the best solution. Once it has completed, we will have out fitted mixture Markov model in fit_mmm. Remember that in a real situation you would have to repeat this process for a range of clustering solutions (e.g., 2–10) to identify the optimum number of clusters.\n\nfit_mmm <- fit_model(\n  mmm, \n  global_step = TRUE,\n  local_step = TRUE,\n  threads = 60,\n  control_em = list(\n    restart = list(times = 500,  n_optimum = 501),\n    algorithm = \"NLOPT_GD_MLSL_LDS\"\n))\n\n\n\n\n\n\n3.4 Creating and visualizing TNA results\nNow that we have our model fitted, we can use TNA —through the tna package [11]— to inspect the clusters that we have identified. We can use the function group_tna to create a grouped TNA model based on the clustering results. Specifically, we need to pass the MMM object (fit_mmm$model) as an argument.\n\ntna_group <- group_tna(fit_mmm$model)\n\nThe functions that we have learned in Chapter 15 [19] apply also to our grouped model created from the clustering results. For instance, we can plot the model using the plot function, and a separate plot will be created for each cluster, each with their own initial and transition probabilities (Figure 17.2). As a reminder, the pie around each node represents the initial probability of each code, whereas the arrows represent the transition probabilities between them.\nWe can see that Cluster 1 is characterized by discussions starting with socialize, with strong transitions between share and discuss, and disagree and facilitate, argue and discuss, and agree and discuss. Cluster 2 is characterized by starting with argue, and also a strong transition between share and discuss, but also disagree and socialize. Cluster 3 encompasses more diverse of initial probabilities —especially discuss and evaluate. This cluster has a very strong repetition of organize, as well as facilitate. Similar to the previous clusters, share and discuss is the predominant transition between different codes, followed by agree and discuss. Cluster 4 has fewer but stronger transitions, especially between socialize and disagree, share and discuss, and argue and discuss.\n\nplot(tna_group)\n\n\n\n\n\n\n\n(a) Socially triggered\n\n\n\n\n\n\n\n(b) Argumentation triggered\n\n\n\n\n\n\n\n\n\n(c) Facilitated discussions\n\n\n\n\n\n\n\n(d) Information triggered\n\n\n\n\nFigure 2. Plot of each TNA model\n\n\n\nWe can also use the summary function to obtain a combined summary of the models corresponding to each cluster (Table 17.1). We can see differences in the overall network properties of each cluster. Cluster 3 is the most connected cluster, with the largest number of edges, and highest density value, while Cluster 4 is the least connected one. Clusters 1 and 2 lie in between, Cluster 2 being more dense and interconnected (although with higher variability), while Cluster 1 has higher reciprocity and is more stable.\n\nsummary(tna_group)\n\n\n\n\n\nTable 1. Summary of the grouped tna model\n\n\n\n\n\n\n\n\n\nmetric\nCluster 1\nCluster 2\nCluster 3\nCluster 4\n\n\n\n\nNode Count\n10.00\n10.00\n10.00\n10.00\n\n\nEdge Count\n61.00\n65.00\n76.00\n45.00\n\n\nNetwork Density\n0.68\n0.72\n0.84\n0.50\n\n\nMean Distance\n0.18\n0.16\n0.03\n0.23\n\n\nMean Out-Strength\n1.00\n1.00\n1.00\n1.00\n\n\nSD Out-Strength\n0.97\n1.12\n0.86\n1.31\n\n\nMean In-Strength\n1.00\n1.00\n1.00\n1.00\n\n\nSD In-Strength\n0.00\n0.00\n0.00\n0.00\n\n\nMean Out-Degree\n6.10\n6.50\n7.60\n4.50\n\n\nSD Out-Degree\n2.69\n3.17\n1.90\n1.84\n\n\nCentralization (Out-Degree)\n0.32\n0.42\n0.25\n0.37\n\n\nCentralization (In-Degree)\n0.32\n0.30\n0.25\n0.37\n\n\nReciprocity\n0.85\n0.75\n0.83\n0.60\n\n\n\n\n\n\nSince we entered the grade as a covariate in the clustering process, we can study how grades are related to cluster membership. To obtain a summary of the effect of the covariates, we can use the mmm_stats function from tna. The output of this function is a data frame containing the estimate associated with each covariate (only one in our case: grade) and its confidence interval, the standard error, the t-score, and the p-value. The results are calculated taking Cluster 1 as a reference. From Table 17.2, we can see that grades had a small but statistically significant effect on the membership of clusters 2 and 3 (p-value < 0.05), positive in the case of Cluster 2 (meaning that high achievers were more likely to belong to this cluster), and negative in the case of Cluster 3. For cluster 4, the effect was not statistically significant.\n\nmmm_stats(fit_mmm$model)\n\n\n\n\n\nTable 2. Covariate effect\n\n\n\n\n\n\n\n\n\n\n\n\nCluster\nVariable\nEstimate\np_value\nCI_Lower\nCI_Upper\nStd_Error\nt_value\n\n\n\n\nCluster 2\nGrade\n0.004\n0.00\n0.003\n0.006\n0.001\n6.31\n\n\nCluster 3\nGrade\n-0.007\n0.00\n-0.009\n-0.006\n0.001\n-8.45\n\n\nCluster 4\nGrade\n0.000\n0.58\n-0.001\n0.002\n0.001\n0.55\n\n\n\n\n\n\nTo ease the interpretation of our results henceforward, we can provide the clusters with more meaningful names. We can use the rename_groups function to provide the new names that are representative of each cluster’s dynamics.\n\ncnames <- c(\"Socially triggered\", \"Argumentation triggered\", \n            \"Facilitated discussions\", \"Information triggered\")\ntna_group <- rename_groups(tna_group, cnames)\n\n\n\n3.5 Communities\nSimilarly to simple TNA, we can study the specific patterns that characterize each cluster. For example, we can study whether we can detect distinct communities of states or events that often transition to one another to a greater extent than to the rest of the nodes, as well as compare if these communities are similar between clusters. We can obtain the community assignments using the communities function.\n\ncommunities_group <- communities(tna_group)\n\nWe can plot the results using the plot function and specifying the method of community detection we want to use. Refer to [24] for more details on the different community finding algorithms. For the example below we chose the Leading Eigen algorithm since it clearly illustrates the existence of different communities (depicted by different colors) in each cluster (Figure 17.3), each encompassing different nodes. Some similarities that can be perceived are that argue and discuss belong to the same community in each of the four clusters, and so do disagree and facilitate and share and argue. Organize is assigned to a community of its own in two clusters, suggesting that this action occurs rarely as a part of the collaboration process —probably more often at the beginning.\nOther algorithms would have yielded different results. For example the Infomap [25] algorithm does not detect any communities (i.e., the whole network is a single community) in any of the clusters.\n\nplot(communities_group, method = \"leading_eigen\")\n\n\n\n\n\n\n\n\n(a) Socially triggered\n\n\n\n\n\n\n\n(b) Argumentation triggered\n\n\n\n\n\n\n\n\n\n(c) Facilitated discussions\n\n\n\n\n\n\n\n(d) Information triggered\n\n\n\n\nFigure 3. Plot of each TNA model communities\n\n\n\n\n3.6 Centralities\nCentrality measures in transition networks allow to identify the role that each state or event takes in the process. For example, betweenness centrality captures how often a state acts as a bridge within the network, connecting different parts of the process. Out-degree centrality reflects the number of direct transitions originating from a state, indicating to which extent it drives the process towards other states. See [19] for a complete rationale of the role of centrality measures in TNA, and [26] for a detailed description of the most common centrality measures.\nWhen working with clusters in TNA, since each cluster has a different transition probability matrix, the nodes also have different centrality measures. The way to compute them is the same as in regular TNA, i.e., using the centralities function. We can specify which specific centralities we want to compute and plot them using the plot function (Figure 17.4). Betweenness centrality shows the greatest differences. In the Facilitated discussions cluster, Facilitate has the highest betweenness followed by Disagree. In the Information triggered cluster, Argue is the node with the highest betweenness, followed by Socialize. In Argumentation triggered, the highest betweenness is that of the Evaluate node, followed by Share. In the Socially triggered cluster, Facilitate is the node with the highest betweenness. In terms of In-strength, the four clusters follow similar patterns, with Discuss being the node with the highest value, followed by Socialize. Lastly, regarding Out-strength, Share and Discuss are the nodes with the highest overall centrality values, sharing similar patterns among the clusters. The Facilitate node has the lowest out-strength for all clusters, indicating that is many times a recurring action that is repeated several times in a row. Similarly, the Facilitated discussions cluster has a remarkably low value of out-strength centrality for Organize, meaning that students might be commonly stuck at that point. To better understand the meaning of each of the centrality measures, please refer to [19, 27]. We can also print the centralities for each cluster to find out their exact value.\n\ncents <- centralities(\n  tna_group, \n  measures = c(\"Betweenness\", \"InStrength\", \"OutStrength\")\n)\n\n\nplot(cents, ncol = 3) \n\n\n\n\nFigure 4. Plot of each TNA model centralities\n\n\n\n\n\nprint(cents)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.7 Cliques\nTNA provides functions to identify groups of nodes that have a reciprocal relationship, i.e., strong transition probabilities to and from one another. These can be groups of two nodes (dyads), three (triads), and so on. When we wish to detect these patterns when working with clusters, we can use the same function as in simple TNA (cliques), but we need to specify for which of the clusters we want to identify these cliques. For example, if we would like to identify and plot all dyads on the Socially triggered cluster, we must subset the tna_group object by cluster name and then specify the size of the cliques (2 for dyads), as well as any additional parameters (e.g., threshold for considering strong reciprocal connections). We can plot the resulting dyads using the plot function. Do not forget that this function is interactive, so you will need to hit enter in the console to plot all dyads (alternatively, you may add the following argument to the plot function: ask = FALSE). Figure 17.5 shows that there are three dyads in the Socially triggered cluster: one between Argue and Discuss, Discuss and Socialize, and Evaluate and Question.\n\ndyads1 <- cliques(tna_group[[\"Socially triggered\"]], size = 2, threshold = 0.15)\nplot(dyads1)\n\n\n\n\n\n\n\n\n(a) Dyad 1\n\n\n\n\n\n\n\n(b) Dyad 2\n\n\n\n\n\n\n\n(c) Dyad 3\n\n\n\n\nFigure 5. Dyads for cluster 1\n\n\n\n\n3.8 Other forms of clustering and grouping\nThroughout the tutorial we have seen how to identify and study clusters of students that are ruled by different transition patterns. However, we might want to identify groups of students through other mechanisms and only afterwards investigate their properties and transition patterns. In this section, we review different ways in which we can do this.\n\n3.8.1 Clustering sequences\nA common way to study log data is through sequence analysis. Within this framework, several dissimilarity measures exist that allow to calculate how similar sequences of events or states are to one another, and cluster them in distinct groups. Distance measures can be broadly categorized in edit distances — such as optimal matching—, shared attributes — such as longest common subsequence (LCS), or longest common prefix (LCP), and distance based on state distributions —e.g., euclidean distance or chi-squared.\nIn the next code chunk, we calculate the dissimilarity between our sequences using the seqdist function from the TraMineR package, using LCP as the distance measure, which means that sequences that start in the same way will have a shorter distance between them. We then cluster the sequences using hierarchical clustering (as implemented in the hclust function of the stats package) using Ward’s method minimum variance method (\"ward.D2\"). We establish the number of clusters by using the function cutree and retrieve the cluster assignment. We assign the result to a variable named lcp_clusters, to be able to operate with the cluster assignment later on. We can then visualize them using seqdplot (Figure 17.6) in which we see the distribution of states at each time point. For more details about this process, please refer to [28]. Upon inspection, we can assign names to each of the clusters.\n\n# Compute dissimilarities between sequences using the LCP method.\ndissimilarities <- seqdist(sequ, method = \"LCP\", indel = 1)\n\n# Perform hierarchical clustering on the computed dissimilarities \nclusters_sessionsh <- hclust(as.dist(dissimilarities), method = \"ward.D2\")\n\n# Cut the hierarchical clustering tree into 4 clusters\nlcp_clusters <- cutree(clusters_sessionsh, k = 4)\n\n# Plot a distribution plot (seqdplot) for the sequences\n# grouped by their cluster assignments.\nseqdplot(sequ, group = lcp_clusters, ncol = 5)\n\n# Define meaningful cluster names based on interpretation of the data.\ncnamesd <- c(\"Facilitated discussions\", \"Information triggered\", \n             \"Argumentation triggered\", \"Socially triggered\")\n\n# Assign the corresponding cluster names to the cluster assignment vector.\nlcp_clusters <- cnamesd[lcp_clusters]\n\n\n\n\nFigure 6. Distance-based sequence clusters\n\n\n\n\nOnce we have our cluster assignments, we can create a new clustered TNA object with the group_tna function, by passing the sequence object and the cluster assignments (lcp_clusters) as arguments. We can visualize the resulting clusters using the plot function as usual (Figure 17.7), as well as perform all of the other operations that we have showcased throughout the chapter (centralities, communities, cliques, etc.).\n\nseq_tna <- group_tna(sequ, group = lcp_clusters)\nplot(seq_tna)\n\n\n\n\n\n\n\n(a) Argumentation triggered\n\n\n\n\n\n\n\n(b) Facilitated discussions\n\n\n\n\n\n\n\n\n\n(c) Information triggered\n\n\n\n\n\n\n\n(d) Socially triggered\n\n\n\n\nFigure 7. TNA using distance-based clusters\n\n\n\n\n\n3.8.2 Fixed groups\nIn addition to clustering, we can also create groups based on deterministic or known groups (e.g., gender, grades, course, etc.). In the following chunk, we create a variable named Achievers which is a vector containing the value \"High\" if the corresponding student has a grade above 50, and \"Low\" otherwise. We pass this vector as the group argument of group_tna, just like we did in the previous example and we can then us all the range of functions of tna on the two groups (Figure 17.8). This helps us visualize the difference in transition probabilities between low and high achievers.\n\nAchievers <- ifelse(simulated_data$Grade > 50, \"High\", \"Low\")\ncourse_tna <- group_tna(sequ, group = Achievers)\nplot(course_tna)\n\n\n\n\n\n\n\n(a) High Achievers\n\n\n\n\n\n\n\n(b) Low Achievers\n\n\n\n\nFigure 8. TNA using groups instead of clusters\n\n\n\n\n\n\n3.9 Other forms of TNA\nAs demonstrated in [19], the TNA framework encompasses other forms of studying transitions, such as frequency-based transition networks. Instead of representing transition probabilities, frequency-based transition networks operate on the absolute values, i.e., the number or share of times each transition has appeared in the whole sequence object. This operationalization might be more appropriate in cases that there are not enough data to make inferences. This mode also supports cluster, allowing to inspect and compare transition frequencies among clusters.\nIn the chunk below, we create a frequency-based transition network (Figure 17.9), providing the results of the distance-based clustering as the group argument.\n\ntna_group_scaled <- group_ftna(sequ, group = lcp_clusters)\nplot(tna_group_scaled)\n\n\n\n\n\n\n\n(a) Argumentation triggered\n\n\n\n\n\n\n\n(b) Facilitated discussions\n\n\n\n\n\n\n\n\n\n(c) Information triggered\n\n\n\n\n\n\n\n(d) Socially triggered\n\n\n\n\nFigure 9. FTNA of each group"
  },
  {
    "objectID": "chapters/ch17-tna-clusters/ch17-tna-clusters.html#discussion",
    "href": "chapters/ch17-tna-clusters/ch17-tna-clusters.html#discussion",
    "title": "17  Mining Patterns and Clusters with Transition Network Analysis: A Heterogeneity Approach",
    "section": "4 Discussion",
    "text": "4 Discussion\nStudying the learning process as a homogeneous phenomenon can overlook important variations that exist across different subgroups or contexts, leading to conclusions that may not fully capture the complexity of the phenomenon. Such an approach risks not only diluting the explanatory power of scientific findings but also perpetuating inequalities by failing to account for how distinct groups may experience or respond to the same processes differently. In turn, we should “capitalize on the heterogeneity that characterizes most effects in science” [2] (p. 86), so we can better understand variations, improving the relevance and applicability of our findings across diverse populations.\nIn this chapter, we have demonstrated how to identify clusters of transition networks, within the scope of TNA. We have illustrated this process for MMM clustering as well as for other forms of clustering. We have also showcased the features of TNA related to centralities, communities, and cliques, which provides as with a rich toolkit to study the heterogeneity of relational dynamics. Centrality measures, for instance, allow us to identify central states or transitions that shape the flow of behaviors or interactions. Influential nodes can vary significantly across subgroups, representing the diversity in how processes unfold.\nSimilarly, communities within the network can uncover tightly connected groups of transitions that represent cohesive patterns, dynamics, or strategies. These sub-networks might reveal distinct behavioral tendencies or shared practices that are specific to certain groups or contexts. Cliques, as smaller subsets of closely interconnected transitions, provide yet another layer of detail, capturing unique micro-patterns that might otherwise go overlooked.\nUnderstanding the different patterns that can be identified from transition networks and how they prevail or differ among clusters, provides a comprehensive view on learning processes from a heterogeneity lens. In other words, identifying and studying different subgroups help us move beyond simplistic, one-size-fits-all interpretations to recognize the complexity and diversity inherent in behavior and interaction. Such interpretations have the potential to advance learning theories and support the design of interventions or policies that are sensitive to the diverse ways individuals and groups experience and respond to learning processes, which is one of the driving goals of learning analytics research. Embracing heterogeneity constitutes a step forward in ensuring that findings are equitable and capture diversity."
  },
  {
    "objectID": "chapters/ch18-idio/ch18-idio.html",
    "href": "chapters/ch18-idio/ch18-idio.html",
    "title": "18  Individualized Analytics: Within-Person and Idiographic Analysis",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch18-idio/ch18-idio.html#introduction",
    "href": "chapters/ch18-idio/ch18-idio.html#introduction",
    "title": "18  Individualized Analytics: Within-Person and Idiographic Analysis",
    "section": "1 Introduction",
    "text": "1 Introduction\nThe last decades of education research witnessed a rapid increase of data availability and advancements in quantitative methods powered by the rise in computing power and methods [1]. Such an increase has motivated the emergence of data-intensive fields, namely learning analytics and educational data mining. Learning analytics is a relatively nascent field that aims to understand and optimize learning and learning environments by utilizing the burgeoning amounts of available data through digital —and non-digital— tools [2, 3]. However, the abundance of data and the availability of methods have mostly resulted in scaling across samples, populations, and institutions rather than building highly precise person-specific solutions [4]. In other words, learning analytics tried to build algorithms that could work across a population (e.g., a course or an institution), for instance, to predict the students who may drop out using data from previous years or other classrooms [5]. The premise was that by gathering more data we will be able to use more powerful algorithms, to obtain more accurate predictions and to perform more effective interventions. However, that has not been the case. So far, few are the systems that have been implemented across institutions, and far fewer are the systems that have delivered those predictions or proved to scale up if any [6].\nSeveral studies have proven that scaling, generalizing, and applying a one-size-fits-all strategy is ineffective, no matter how hard researchers have tried to gather more data, use fancy algorithms, or bigger samples, go with multimodal or triangulate across multiple sources of data [7, 8]. The failure of big data to deliver big returns stems from the innate fragility of the idea that the sheer size of the data will automatically lead to more accurate and effective results. The reality is that more data from a larger sample cannot solve problems of individualized learning, nor can it account for the complexity of learning or individual differences [9].\nIn garish contrast to the one-size-fits-all approach, person-specific methods aim to precisely model each individual, from their own data, find the distinct unique peculiarities of each, and tailor models to their own needs to deliver accurate and adaptive learning experiences [10]. By definition —or by design— the focus is narrower and the resolution is equal to a single student or what is often referred to as N=1, single subject, or idiographic. Person-specific (i.e., idiographic) methods have been advocated not only to address the shortcomings of other methods but also for their potential strengths and advantages [11, 12]. These methodologies originate and have been widely discussed in psychological and social sciences [13–15], however, so far under-used in education.\nIn this chapter, we will introduce these approaches and offer a background about idiographic and within-person analysis in general. We will first present an overview of relevant concepts such as nomothetic, idiographic, and within-person analysis. Next, we will review issues with the dominant nomothetic approach, namely, poor generalisability. Finally, we discuss the potential as well as challenges, and limitations of idiographic methods for learning analytics."
  },
  {
    "objectID": "chapters/ch18-idio/ch18-idio.html#background",
    "href": "chapters/ch18-idio/ch18-idio.html#background",
    "title": "18  Individualized Analytics: Within-Person and Idiographic Analysis",
    "section": "2 Background",
    "text": "2 Background\nTwo types of approaches are commonly used: between-person and within-person. The first approach is widely prevalent in science at large —and in education in particular— and entails collecting data from a sample, analyzing the data, and drawing conclusions. The other approach entails collecting data several times from an individual —or multiple individuals— and analyzing these data variations across time compared to the previous self. In the next section, we will explain in detail these approaches, their rationale, and how they relate to idiographic analytics.\n\n2.1 Between-person (nomothetic approach)\nNomothetic, between-person, inter-individual, group-level, or population-based research are mostly synonymous terms that converge to the same idea of collecting data from a group of different people to study their inter-individual variance [10, 16]. As shown in Figure 18.1 A, where data are collected from different people at a point in time i.e., cross-sectional. The premise is: studying a group of individuals would allow researchers to model the overall trends or obtain summary measures e.g., mean or median. We may collect data from a group of students and we would say that the average of the group represents the summary or the average level of engagement that most students are assumed to revolve around [10, 16, 17].\n\n\n\nFigure 1. The nomothetic approach (A) focuses on between-person variability where data is collected from a group of different people to create a cross-section average. The within-person approach focuses on within-person variability where data is collected from the same people several times and variance is computed compared to the same person’s previous state (note the arrows directions), still we get the average of many people (B). The idiographic approach focuses on within-person variability of a single person and each is studied separately (C).\n\n\nSimilarly, the variable-centered approach is another type of between-person research paradigm that focuses on the relationships among variables rather than the individuals themselves. All of these approaches are similar and represent the same idea of studying multiple people to capture enough variations. In all of such cases, it is assumed that the results generalize to most students. In fact, it is assumed that the findings of this approach are generalizable norms that everyone more or less conforms to. Deviations from these norms are considered outliers, noise, or exceptions. To date, most of the research is done with this approach, and nomothetic or between-person research is dominant and can be seen everywhere [18].\n\n\n2.2 Within-person\nWithin-person research captures the intra-individual mechanisms, variations, and changes within the same person (or sometimes multiple persons) over time or occasions. It compares individuals to their own past self to capture how individuals’ experiences, behaviors, or psychological states fluctuate over time compared to their own average. As shown in Figure 18.1 B, data is collected from the same persons repeatedly to measure the intra-individual variations or changes (often called the within-person variance) [10, 18].\nSuppose a student scores a 9 in mathematics. In the next exam, she scores a 10. That indicates that her performance improved from her past self. If we have her average score for the last 10 exams, and it was 8.5, we can say that she is improving compared to her past self, and given that the last two exams were 9 and 10, she is on an ascending trend. If she obtained a grade of 7, it would be lower than her average. In turn, if we took another student with an average of 5, a grade of 7 would be an improvement from her own average. These are within-person variations given that we are comparing variations within the individual. This approach can be generalized to other processes, e.g., engagement, self-regulation, and well-being —to mention a few. In such a case, we will say that the engagement of a student has improved (compared to her own previous engagement). Put another way, the comparison is strictly within the same person. This is why it is called within-person.\nUsing a within-person approach, we can study how learning processes change by tracking the intra-individual fluctuations and the factors that affect them [19]. We can also identify the patterns and trends within the person, highlighting how different contexts or situations can impact the person’s behavior. To summarize, the within-person approach collects data from a person or multiple persons over several time points, and analyses how variables change within individuals over time.\nWhen within-person research addresses a single person, it is referred to as idiographic (N=1 or single subject) [20–23]. One can say that it is a special case of within-person analysis and in fact, the dominant one. As shown in Figure 18.1 C, idiographic research collects data from a single person and seeks to give person-specific insights about that exact person [10, 16]. Idiographic research can address many people but still would study each and every person separately as a whole unit of analysis. Therefore, findings through idiographic methods cannot be expected to generalize to other persons.\nIt is worth mentioning that not all within-person-analysis is idiographic, some of it averages across students to create average within-person variance and therefore, it is not idiographic (because we have data from more than one student)."
  },
  {
    "objectID": "chapters/ch18-idio/ch18-idio.html#does-group-based-research-generalize-to-individuals",
    "href": "chapters/ch18-idio/ch18-idio.html#does-group-based-research-generalize-to-individuals",
    "title": "18  Individualized Analytics: Within-Person and Idiographic Analysis",
    "section": "3 Does group-based research generalize to individuals?",
    "text": "3 Does group-based research generalize to individuals?\nThe basic rationale for group-based or nomothetic research rests on the assumption that variance observed between individuals (between-person or inter-individual) mirrors the variance observed within individuals (intra-individual or within-person) [10, 19, 24]. When we gather data from many individuals to capture several situations and variability, we will be able to capture enough variations similar to what happens within the person or persons. In other words, since it may be difficult to gather data from the same person one thousand times to capture all possible variations, we can collect data from one thousand people to gather enough variations. We assume that these two variations are essentially the same. This means researchers assume that patterns of behavior that can be observed at the group level can also be generalized to the individual level. This assumption is problematic for several reasons. We will explain it in detail in the following subsection [5, 11, 13].\n\n3.1 Ergodicity\nA key theoretical framework to understand the between/within-person mechanisms is the theory of ergodicity [10, 25]. In short, a group-level variance of a variable (e.g., motivation) generalizes to an individual-level trend if and only the variable meets the ergodicity conditions [10, 26].\n\nHomogeneity means people are similar or homogeneous with no significant differences. The values of a variable are consistent so that if we select any individual, it would yield comparable results without impacting the study’s outcomes.\nStationarity means the variable remains stable over time. So, it won’t matter when we are collecting data, the variable doesn’t change over time in any way.\n\nHowever, we know that there is significant heterogeneity and differences among the population and many psychological constructs evolve over time. Therefore, ergodicity is neither realistic nor has been ever empirically found. In fact, the mounting evidence across all psychological processes is that they change over time, and vary across populations. Therefore, group-level analysis poorly reflects each individual or any individual in particular [10, 26, 27].\n\n\n3.2 Ecological fallacy\nEcological fallacy refers to the logical error of biased inferences about individuals based on aggregate data from groups or populations. In other words, thinking that between-person transfers as is at an individual level. This fallacy occurs when researchers assume that the observed relationships at the group level hold true for all individuals within that group, leading to potentially misleading conclusions [24, 28].\n\n\n\nFigure 2. An imaginary trend of 5 students. On average the blue line shows that there was an increase in the class mean. But we see that three students deteriorated and only two improved. In other words, where the mean is positive, the individual cases are not essentially so and can be even worse.\n\n\nImagine a study that measures student engagement in two groups of students who are exposed to two different teaching methods (Figure 18.2). Both groups have an initial engagement of 6 out of 10. After the intervention, Classroom B has an average engagement score of 9 out of 10, while Classroom B has an average score of 7 out of 10. Based on these averages, one might conclude that all students in Classroom A have improved their engagement to a greater extent than those in Classroom B. However, this is an example of ecological fallacy. In Classroom A, some students might have their engagement scores decreased (e.g., 4 or 5) and in Classroom B, some students may have their engagement improved. In other words, it is the aggregate average that improved, driven by some students in both classrooms but not each and every student. This fallacy can lead to erroneous interpretations and ineffective policy recommendations. A stark case of ecological fallacy is Simpson’s paradox which we will detail next.\n\n\n3.3 Simpson’s paradox\nTo grasp the idea of ecological fallacy even more, let us consider the following scenario, known as Simpson’s paradox [29]. Simpson’s paradox occurs when the direction of a relationship at the group-level is the opposite than at the person-level, e.g., increases on average and decreases for the individual.\nFigure 18.3 shows the simulated relationship between homework scores and hours of study of all students (N=20) over five weeks. You observe that there is a clear negative correlation between these variables on average (i.e., the more study hours, the lower the scores). Therefore, the teacher might consider reducing the amount of homework in the following week, or offer help to those who study a lot, but this results in relatively lower homework scores. However, if you focus on Student A, you notice that there is a totally opposite trend in her learning outcome.\n\n\n\nFigure 3. Simulated data of homework scores and questionnaire records over the past five weeks for 20 students. The simple linear regression is fitted to the whole data including the data points of Student A (black line) and only to the data points of Student A (red line).\n\n\nIn other cases, Simpson’s paradox is more systematic. Let us take an example, students who are competent at mathematics are more likely to be fast in solving problems and make fewer mistakes than students who are less competent. The black line in Figure 18.4 shows this average trend. One can conclude that there is a direct positive relationship between speed and maths proficiency (at between-person level). However, even within the competent group, the faster they attempt the math problems, the more likely they are to make errors. In that case, there is a direct negative relationship between the speed and accuracy of problem-solving (at the within-person level).\n\n\n\nFigure 4. Simulated data on the relationship between the number of errors and solving time of mathematics problems. On average, students with better marks solve problems faster with fewer errors. However, within each grade group of students, the faster students handle problems, the more they make mistakes."
  },
  {
    "objectID": "chapters/ch18-idio/ch18-idio.html#the-idiographic-approach",
    "href": "chapters/ch18-idio/ch18-idio.html#the-idiographic-approach",
    "title": "18  Individualized Analytics: Within-Person and Idiographic Analysis",
    "section": "4 The idiographic approach",
    "text": "4 The idiographic approach\nThe previous section reviewed the theoretical rationale behind between vs. within-person approaches. In this section, we consider the potential and challenges of an idiographic, person-specific approach.\n\n4.1 Why idiographic methods\nGroup-level insights obtained by nomothetic analyses do not necessarily apply at the individual level [4, 24, 30]. In other words, there is no average person that would fit our expectations of the average. There are numerous examples in the recent literature that group-level insights do not generalize to each individual [31, 5, in psychology: 24, e.g. in education: 4, 32]. In addition, in many cases, group-level insights can be also biased toward over-represented groups of subjects [33, 34]. In learning analytics, for example, students with disabilities have been an underrepresented group in the literature [35]. Hence, the idiographic approach allows to reflect individual subjects through fine-grained person-specific analyses as well as overcomes the fairness issues encountered by nomothetic approaches.\nFurthermore, idiographic methods may provide an answer to the poor generalizability of group-based analysis by offering an alternative that could provide precise personalized insights to each individual [10, 36]. In other words, individuals could rely on their own data instead of relying on analysis generalized from others. It stands to reason that if precise individualized insights or interventions are needed, we use methods that reflect the person herself.\nData collection through or with personal devices may bring privacy advantages [37–39]. Since the idiographic approach does not aggregate data across people, ideally it would be possible to complete the process of analytics, from data collection to giving feedback, within the digital devices of each individual so that the collected data are not shared by anyone else [38]. However, if idiographic analysis is used for a clinical purpose, the data is necessarily provided to the practitioner, essentially involving ethical consideration.\n\n\n4.2 Challenges to adoption and implementation\nDespite its significant potential, the adoption of idiographic methods in behavioral sciences has been remarkably slow [40], especially in education research [11]. This is primarily due to data collection as this approach necessarily requires a large amount of micro-level data from a single individual [11, 41]. However, recent technologies such as mobile phones and wearable devices have rapidly advanced data availability for idiographic analysis [11, 42]. In addition, the development of computational power and methods has also increasingly allowed for idiographic research [11, 13, 26]. Thus, more research and practice would be anticipated to employ idiographic methods in the future.\nAdditionally, choosing measurement intervals is another challenge in practice [13, 42]. For example, autoregressive models are a commonly used statistical technique of idiographic analysis, and the models allow to investigate the correlation between different variables over time lags [22, 26]. However, this method only reveals the correlations at a given lag. This could cause problematic evaluation or measurement unless the stationary assumption holds [42]. The stationary assumption implies that relationships between variables do not change within the measurement time, which of course, could happen. For instance, if two variables do not satisfy the stationarity condition, it is possible that their relation could be positive in the first half of the semester and negative in the second half, concluding on no relationship on average.\nTo address this issue, researchers can employ various methods to test for stationarity. If data is found to be non-stationary, techniques like differencing or transformation (e.g., logarithmic or seasonal adjustments) can help stabilize the mean and variance, allowing for a more accurate analysis of relationships over time. For example, Fisher et al. [43] collected idiographic (person-specific) multivariate time series data, where data were found to be non-stationary and the intervals were not uniform. So, the authors first fit an ordinary least square linear regression model so that residuals are the fluctuations from the regression line (i.e. trait), and subsequently, a cubic spline interpolation was applied to address the uneven intervals [43]. In cases where idiographic analysis may be difficult, some forms of within-person analysis could offer an approximation of the idiographic variance [44]."
  },
  {
    "objectID": "chapters/ch18-idio/ch18-idio.html#data-and-methods-for-within-person-and-idiographic-analysis",
    "href": "chapters/ch18-idio/ch18-idio.html#data-and-methods-for-within-person-and-idiographic-analysis",
    "title": "18  Individualized Analytics: Within-Person and Idiographic Analysis",
    "section": "5 Data and methods for within-person and Idiographic analysis",
    "text": "5 Data and methods for within-person and Idiographic analysis\nIn this section, we provide a literature review of the existing within-person, idiographic research potentially relevant to learning analytics, focusing on common data sources and methods.\n\n5.1 Data\nWithin-person analysis —and also idiographic methods— requires repeated data collection by design to be able to capture enough variations of the participants. Therefore, the hallmark of data collection for such an approach is repetitive data collection, be it through surveys, sensors, or digital data. Given the widespread adoption of quantitative methods, intensive longitudinal data collection methods have become highly common. We first delineate several data sources. Then, the key data collection methods, Ecological Momentary Assessment (EMA), and Experience Sampling Method (ESM), will be explained.\n\n5.1.1 .Data sources\n\n5.1.1.1 Self-reported data\nIn behavioral sciences, collecting self-report data —often through some form of a questionnaire— is widely used to measure psychological constructs that are otherwise difficult to directly evaluate. The self-report approach is also used for the within-person analysis. Self-reported data collected within this context differs from the data collected when adopting a between-person approach. In between-person research, the focus is often on overall perceptions and experiences, with questionnaire items being phrased as “I often think…”, “In general, I like…”, “Overall, I do…” [27]. In within-person research, instead, the focus is on capturing in the moment feelings or attitudes, and therefore, questionnaire items are phrased accordingly, e.g., “Right now I feel…”, “At the moment, I am…”, etc. Also, these surveys are shorter and repeat multiple times per day to gather several time-points for each individual across time. These surveys are called experience sampling or ecological momentary surveys given that they collect experiences as they occur and therefore less prone to recall bias.\nAlso, well-known questionnaires that are often applied in between-person research, e.g., Motivated Strategies for Learning Questionnaire (MSLQ) [45], are not directly applicable to within-person research, as they would need to be rephrased to capture the momentary experiences of the research subjects. There are questionnaires that are already suitable for this purpose, such as the Positive and Negative Affective Scale (PANAS), which aims to capture momentary emotions [46].\nA key limitation of self-report data is the potential bias in the subjective self-assessment of a participant. If everyone has different magnitudes of self-report bias, conducting between-person analysis on self-report data might introduce a significant overall bias. However, the self-report bias would potentially be reduced in within-person analysis because it focuses on the fluctuation relative to the past. Nonetheless, it may be important to complement self-report data with other data sources described below [47, 48].\n\n\n5.1.1.2 Digital log data\nDigital mobile devices, learning management systems (LMS), and applications constantly collect data for every action or click. Similarly, self-tracking apps which can log students’ study habits, location, or social interactions, creating a broad picture of how these factors influence their academic performance.\nSaqr [5] used the log data in an LMS from 286 students across 48 module instances. Then mixed linear models were fit at traditional group-level and as the hybrid of between- and within-person to predict the final marks. As a result, the within-between hybrid model demonstrated better performance than the group-level nomothetic model, indicating that within-person factors or trajectories reflect academic performance [5]. Another interesting example is the analysis of self-reported data combined with digital footprints collected from smartphones by Aalbers et al. [49]. In this extensive study, 247 university students self-reported their subjective stress five times a day at random points. In addition, the log data on their smartphones such as use of specific applications were extracted. Machine learning models were trained with the survey data merged with the log data for both the group-level (nomothetic) and each individual (within-person). The findings showed that sleep hours and prolonged social network apps consistently predicted momentary subjective stress at group-level [49]. However, these variables were not sufficiently predictive for most students at within-person level, and what digital markers explain the subjective momentary stress significantly varied between persons [49].\n\n\n5.1.1.3 Physiological data\nAdditionally, physiological data from wearable sensors could be also used for within-person analysis. This type of data could, for instance, help track physiological responses, such as heart rate or electrodermal activity, and may offer insights into, e.g., how stress affects learning.\nDhindsa et al. [50] investigated the phenomenon of mind wandering of students during live lectures by recording electroencephalography (EEG). 15 medical students attended an orthopedic surgery lecture wearing caps to measure EEG, and students were also asked to self-report the degree of mind wandering around every four minutes during the lecture. Discriminative spatial filters and support vector machines were applied to the data to classify whether the participant was in the mind-wandering state or not. In particular, machine learning models were fit at both group (nomothetic) and individual (within-person) levels. The results verified that key EEG factors for mind wandering which had been previously reported in laboratory settings were somewhat reproduced in the live lecture setting on average, while individual patterns were highly heterogeneous [50].\n\n\n5.1.1.4 Other data sources\nOther data sources also include daily diary methods where students —or their parents and teachers— might keep a daily diary documenting their academic experiences, challenges faced, and strategies used [51]. Also, interviews with students about their learning experiences can complement quantitative data, providing a richer narrative. For example, Ordem [52] tracked a single student studying German as a second and foreign language and collected diary as well as monthly interview data for qualitative analysis of the dynamics of motivation in language learning. The result showed that the participant’s motivation was negatively affected by a lack of linguistic self-confidence and the problem of social milieu [52].\n\n\n\n5.1.2 Data collection methods\n\n5.1.2.1 Ecological Momentary Assessment and Experience Sampling Methods\nEcological Momentary Assessment (EMA) involves collecting real-time data about a participant’s experiences in their natural environment (hence the name ecological). The data collection can be done through observation, physiological sensors, or through self-reports. For example, in an educational setting, students might be asked to report their attitude about a specific subject or task at various points throughout the day. A student could receive a phone notification asking about their belief in their ability to do their learning tasks (self-efficacy). Typically, this is repeated several times a day and over some time (weeks or months) to record the fluctuations in self-efficacy and how they correlate with other variables e.g., performance, engagement or lesson subjects.\nFor instance, Respondek et al. [53] employed EMA to collect survey data from 98 undergraduate students during an exam week. In this study, participants received notifications on their mobile devices at three randomly selected times a day, and asked to report their achievement emotions. The EMA data combined with the exam grades were then analyzed using structural equation modeling. The results indicate the potential impact of previous experience, in addition to trait (within-person expected value) and situation (cross-sectional expected value at that time point or state), on the achievement emotion [53].\nExperience Sampling Method (ESM) is similar to EMA—and oftentimes they are used interchangeably—but may involve scheduled prompts, encouraging students to reflect on their thoughts and experiences at preset times. As the name implies, it aims at capturing students’ experiences when they happen and as close to the natural environment as it can happen, to avoid recall bias [54]. Similar to EMA, educators could use ESM to gauge students’ engagement and motivation during different classes throughout the week.\n[55] used data from students over several weeks to study their self-regulation and reveal the idiographic variations among each student. In another study by the authors, they used EMA data to predict student studying and regulation and reported that each and every student had unique predictors [4]. Xie et al. [56] carried out ESM on 52 undergraduate students during two pre-exam weeks. In the study, participants were asked to respond through a mobile application to surveys after planned study-related events (thus experience-based sampling). Hierarchical regression analysis on the collected data revealed that situation-specific self-efficacy is highly related with engagement (i.e. how well students engaged with the planned events) [57].\nAnother example of the use of ESM is a more extensive study by Muenks et al. [58] who collected survey data from 902 undergraduate students after every STEM class over two weeks. The surveys asked about students’ psychological experiences in the STEM (science, technology, engineering, and mathematics) classes in order to investigate the effect of their perception on professors’ mindset beliefs about students’ intelligence ability. The findings demonstrated that, when students perceive that their STEM professors have a fixed mindset, this perception tends to negatively affect their psychological vulnerability, in-class engagement, and even the dropout intention [58].\nGiven that these methods gather data at the moment, they are commonly referred to as momentary data. The analysis of momentary data either collected by ESM or EMA could reveal patterns, changes and fluctuations and most importantly, enable within-person analysis as we have enough data points with enough power to enable within-person or idiographic analysis.\n\n\n\n\n5.2 Analytical methods\nQuantitative data collected through EMA/ESM are essentially multivariate time series, containing information about temporal processes of individuals. Here we provide several examples of approaches to handle such types of data for within-person, idiographic analysis.\n\n5.2.1 P-technique\nThe most classical —yet still impactful— method for within-person analysis is the P-technique. The original P-technique developed in 1940’s is essentially a factor analysis applied to data obtained from a single individual, while to date several extensions have been proposed to deal with more nuanced information of temporal changes. The factor analysis approach has an advantage over other network analysis approaches (to be subsequently presented) in that it handles latent variables, or factors. Thus, this method would be suitable for testing theory or identifying latent constructs.\nFor example, Musher-Eizenman et al. [59] employed the P-technique dynamic factor analysis [60] to study the relationship between children’s perceived control over their work and their academic achievement taking into account concurrent and lagged effects within individuals. A total of 152 primary school pupils in Germany were included in the study, and participants were asked to self-report psychological indicators (e.g. motivation, anxiety) after each homework assignment and after the return of graded assignments for German and maths classes. First, the within-person dynamic factor analysis failed to replicate the previous findings by cross-sectional analysis that children with better control over their work achieve higher academic performance and showed that rather there are greater individual differences in the relationship between perceived control and achievement [59]. Second, the within-person analysis also demonstrated that compared to low-achieving pupils, high-achieving pupils exhibit a clearer pattern that the perceived control and performance predict further control in the subsequent time points, perhaps indicating more organized control for high-achieving children [59].\n\n\n5.2.2 Graphical VAR\nThe recent advancement in network science has extended several psychological networks methods to within-person, idiographic analysis. This is particularly useful when an analyst is interested in the temporal changes (i.e., how the previous state affects the following state) and the contemporaneous processes (i.e., how different constructs co-occur simultaneously) of observed constructs in individual subjects [22, 44].\nGraphical vector autoregression (VAR) models are especially useful for fully idiographic, person-specific analysis. They are able to capture the temporal and contemporaneous processes of psychological constructs within a single subject. For example, Saqr and López-Pernas [61] collected survey data by the EMA method from an undergraduate student (N=1) once a day for the duration of a month. The survey questions represented common dispositions of self-regulated learning such as motivation, reflection and planning. After the linear trends in variables were removed to approximately ensure the stationarity, graphical VAR was applied. The findings indicated that for the participant, the feeling of hope as well as motivation positively affect the task engagement on the following day, while working on assignment negatively correlates with the engagement on the following day [61]. Other examples can be also seen within the same authors’ work on self-regulated learning where they studied the network models of 18 students and showed that each student has a different profile of self-regulation. The authors concluded that if personalization is the goal, then idiographic methods should be prioritized [55].\nWhile the previous example utilized temporal networks, Malmberg et al. [62] employed contemporaneous networks in addition to temporal ones to investigate metacognitive monitoring in collaborative learning in the context of self-regulated learning. Video data of 12 upper secondary school students during collaborative work of an advanced physics course were collected, and monitoring events were manually coded. The within-person analysis of contemporaneous networks by graphical VAR suggested that different monitoring phases (e.g., task definition, goal setting) tend not to co-occur in a short time window, but rather monitoring events within a group lead to enactment before the group proceeds to another phase of regulation [62]. In addition, graphical VAR analysis on the within-person temporal networks showed that motivational interactions lead to monitoring of task definition and task enactment in the following step, driving the early phases of task execution [62].\n\n\n5.2.3 Sequence mining\nAnother approach to working with time series, especially longitudinal, data is sequence mining. While collecting self-report data through EMA/ESM from the same individuals over a long period of time (e.g. several months) could overload participants, log data or physiological data automatically collected through LMS or portable digital devices could be relatively longitudinal. Sequence mining allows for within-person analysis of such data to investigate sequential patterns of learning strategies of students or educational interventions.\nSaqr et al. [5] collected LMS activity log data of 135 students for 10 successive modules in a higher education program of healthcare (corresponding to the period of three years). At the lowest level, for each student and for each module the sequences of activity log were segmented into sessions such that each session represents a non-stop learning activity. Then the series of sessions were clustered into tactics within each module for each student. Subsequently, these tactics were coded as individual module-level learning strategies for each module and each student. Finally, the series of module-level strategies were clustered over the participants to detect program-level trajectories. The results showed that at the program-level students’ learning trajectories fall into three types: stable, fluctuating, and light interactive [63]. Additionally, it was demonstrated that only a small subset of students with stable, intense trajectories showed consistent strategies over different courses (i.e., transferred the strategies) [63].\n\n\n5.2.4 Idiographic machine learning\nThe increasing availability of big data at the individual level allows for applying machine learning. The idiographic machine learning approach potentially exhibits enhanced predictive power while transparency and interpretability would be reduced for complex models compared to other parametric within-person analysis.\nSaqr et al. [4] collected survey data from 17 secondary school students twice a day for 45 days using the EMA method. Three machine learning models are applied to predict key SRL variables (effort, metacognition, motivation, and enjoyment): RF, elastic-net regression (ENR), and the best-items scale that is cross-validated, correlation-weighted, informative, and transparent (BISCWIT). Similarly to the previous example, the results indicated that the predictability of the machine learning models differed across individual participants [4]. In addition, feature importance on prediction also varies across individuals. Moreover, the authors also applied the same machine learning techniques to the whole dataset (which is the nomothetic approach) and showed that the feature importance of the nomothetic models did not match any single individual’s feature importance [4].\nAalbers et al. [49] collected survey data five times a day as well as smartphone log data for app use over a maximum of 60 days. Then three different machine learning models are applied to predict the level of subjective stress at individual and group levels: least absolute shrinkage and selection operator (LASSO) regression, support vector machine (SVM,) and random forest (RF). Features included the use of particular categories of smartphone app, sleep, and timestamp information. The results show that the predictions of nomothetic models are positively correlated with the actual stress of most participants while the models’ predictions significantly differ from the actual values for a small subgroup of individuals [49]. The prediction performance was similar for the idiographic models; the idiographic modeling approach successfully predicted momentary stress, but for a minority of participants [49]. It should be noted that in this study, and often in general, the sample size is necessarily much smaller for idiographic machine learning than for the nomothetic models.\n\n\n5.2.5 Group Iterative Multiple Model Estimation (GIMME)\nGroup Iterative Multiple Model Estimation (GIMME) integrates structural equation modeling (SEM) and dynamic modeling to capture both group-level and individual-level temporal processes [64]. It can be applied to longitudinal time series data for a group of people.\nChaku et al. [65] used a survey dataset that assessed working memory, somatic symptoms, anxiety, and intellectual interests of 26 college students over 75 days during the 2016 US presidential election and in 2017 (as a control group). Data collection employed the ESM approach, asking participants to complete the survey after 8 pm or after daily activity on electronic devices each day. The data was analyzed using one-lag GIMME, which produces group-level and person-specific networks that illustrate contemporaneous and lagged effects of the four variables on each other. The results demonstrated that on average the election group had higher network complexity, potentially reflecting the negative effect of the stressful time on learning-related processes [65]. However, the person-specific models exhibit noticeable differences from the group-level models for both the 2016 election group and the control group, which indicates remarkable heterogeneity on the relationship between negative psychological indicators and learning [65].\nThompson et al. [66] analyzed the effect of teacher feedback on students’ motivation in the context of an intervention program for children who tend to exhibit challenging behavior. After screening, 58 fifth-graders with challenging behavior were included in the study. Collected data included self-monitoring surveys of daily motivation and negative teacher feedback measured as the discrepancy between self-evaluation and teacher’s evaluation on the achievement of learning goal over 8 weeks. The analysis by GIMME showed that negative teacher feedback likely improves student motivation on the same day, while it negatively affects the subsequent day’s motivation [66]. Nevertheless, high heterogeneity is also observed, and at the individual level, some students exhibited that higher motivation leads to negative teacher feedback, potentially reflecting overconfidence in these students [66]."
  },
  {
    "objectID": "chapters/ch18-idio/ch18-idio.html#discussion",
    "href": "chapters/ch18-idio/ch18-idio.html#discussion",
    "title": "18  Individualized Analytics: Within-Person and Idiographic Analysis",
    "section": "6 Discussion",
    "text": "6 Discussion\nIn this chapter, we have introduced the foundations of within-person analysis, contrasting it with the dominant nomothetic approach in education research. Specifically, we have placed our focus on a special case of within-person analysis —idiographic methods— which aim to capture unique, person-specific variations. We outline the main data sources and methods used for within-person research, providing relevant examples from the literature.\nDespite the evident strengths and wealth of insights that can be gathered from adopting a within-person approach —and from idiographic analysis in particular—, the adoption of this lens in education research has been relatively limited compared to other fields like psychology [67]. In psychology, such a lens has been extensively used to examine individual differences and temporal patterns in constructs such as mood and cognition [26]. In turn, education has been slower to integrate within-person analysis, largely due to systemic challenges such as a historical reliance on nomothetic paradigms [68], a lack of expertise in temporal analyses, and the complexity of collecting repeated, high-resolution data from individuals [4].\nHowever, although the idiographic approach has not been that common in educational research, its practical applicability has been exploited to a somewhat larger extent. For example, personalized learning analytics dashboards are heavily utilized for students to be able to visualize their own data, monitor their progress, and —ideally— obtain actionable feedback [38, 69, 70]. Students in special education have been given individualized support based on their own specific needs. Lastly, adaptive learning technologies use idiographic principles to recommend customized content and pacing [56].\nLastly, it is worth noting that idiographic methods may require a privacy and ethics framework of their own [39]. On the one hand, idiographic analysis alleviates some of the concerns related to the nomothetic approach [38], since the results yielded from this analysis are completely individualized and therefore do not undermine minorities or suffer from the generalizability issues underlined in this chapter. However, this comes at the expense of intensive data collection for each single individual, often involving sensitive data [39].. As such, additional safeguards must be in place to protect participants’ confidentiality.\nThroughout the subsequent chapters of the book, we showcase various methods suitable for within-person and idiographic analysis, accompanied by hands-on tutorials in R. For instance, we describe how to apply different forms of regression to capture between- and within-person relationships [21]. In another chapter we introduce psychological networks and demonstrate their application in modeling both contemporaneous and temporal between and within-person relationships, including the use of graphical VAR techniques [22]. Lastly, we also explore the use of automatic ML to scale the application of ML algorithms for idiographic analysis [23]."
  },
  {
    "objectID": "chapters/ch19-three-levels/ch19-three-levels.html",
    "href": "chapters/ch19-three-levels/ch19-three-levels.html",
    "title": "19  The Three Levels of Analysis: Variable-centered, Person-centered and Person-specific Analysis in Education",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch19-three-levels/ch19-three-levels.html#introduction",
    "href": "chapters/ch19-three-levels/ch19-three-levels.html#introduction",
    "title": "19  The Three Levels of Analysis: Variable-centered, Person-centered and Person-specific Analysis in Education",
    "section": "1 Introduction",
    "text": "1 Introduction\nThere is an ongoing scientific debate about the best approach to analyze data in order to create effective, personalized recommendations and interventions. This debate centers on two approaches: a group-based approach that seeks generalizable insights and another approach with focus on heterogeneity and individual differences. The group-based approach assumes subjects are fundamentally similar, suggesting that group averages are representative of most individuals. In contrast, the heterogeneity approach views each subject as unique, necessitating individualized data and methods [1–6].\nOne significant critique of the group-based approach is that it can obscure individual differences by averaging scores, leading to standardized recommendations that may work for some but overlook those with distinct needs [2, 3, 5, 7]. On the other hand, the idiographic approach requires intensive data collection for each individual which is “far from practical and largely exhausting” in many contexts [8] (p. 11). In this chapter, we provide an overview of these approaches and a tutorial on how to implement them using R. The tutorial implements different variations of statistical models to account both for both the between- and within-person variability in repeated measures data. We base the tutorial on a recent editorial published in the journal Learning and Individual Differences [5]."
  },
  {
    "objectID": "chapters/ch19-three-levels/ch19-three-levels.html#three-different-approaches",
    "href": "chapters/ch19-three-levels/ch19-three-levels.html#three-different-approaches",
    "title": "19  The Three Levels of Analysis: Variable-centered, Person-centered and Person-specific Analysis in Education",
    "section": "2 Three different approaches",
    "text": "2 Three different approaches\n\n2.1 Variable-centered approaches\nIn educational research, and subsequently in learning analytics research, the variable-centered approach is one of the most widely used approaches. This approach is sometimes referred to by different names, such as nomothetic, group-based, between-person, or inter-individual. In this approach, researchers typically aim to explain the relationships between variables across a population [3, 5]. To understand the population, researchers collect data from multiple individuals —commonly at the same point in time— on one or more occasions depending on their aims and research questions [3, 5, 9]. This approach relies on the homogeneity assumption, where individuals are considered more or less the same [1]. In that, the average represents the majority of individuals [2–5], and strong deviations from the average are considered outliers [5, 10]. Researchers typically conduct statistical tests to correlate or compare averages, e.g., correlations, t-test, ANOVA or regression to address research questions such as, ‘What is the association between cognitive ability dimensions and job performance?’ [3]. Since this approach is based on the homogeneity assumption and aims to understand the population, it typically produces results that are assumed to be generalizable to the population at large [3, 5]. However, the fact that these insights are based on a sample of the population that is indeed heterogeneous makes their generalization doubtful to each and every person due to overlooking individual differences [2, 5, 9–11]. Considering individuals’ unique contexts and the shift in learning analytics toward more personalized recommendations and interventions [5, 7], complementing variable-centered approaches with other methods may be critical to ensure that each individual receives what is relevant to them especially when it comes to personalization and individualizing education.\n\n\n2.2 Person-centered approaches\nThe person-centered approach is another analytical method that targets individualized insights and looks at finding similarities and patterns within the data. It might also be referred to as the person-based or class-centered approach. In the person-centered approach, the researchers try to group similar individuals into separate sub-populations by considering their distinct characteristics [3–5, 12]. Similar to variable-centered approaches, the researchers collect data from multiple individuals, but the person-centered approach tries to find subgroups that are similar to each other. In other words, it tries to find hidden subgroups within the population rather than considering each individual as part of a single population [3–5]. Examples of methods include mixture regression analysis [5, 13, 14], latent class analysis [15], latent profile analysis [16], factor mixture analysis [17, 18], mixture growth curve modeling [19], latent transition analysis [20] or latent Markov factor analysis [21]. All of such methods produce different clusters —subgroups, latent groups, or classes— each with different parameters or coefficients. For instance, rather than having one regression coefficient for the whole analysis, mixture regression analysis allows all or selected coefficients to differ across (unobserved) subgroups. In that way, person-centered methods find the different heterogeneous subgroups and directly model this heterogeneity.\nThe potential research question might include: “What emergent subpopulations can be identified through the dimensions of cognitive ability?” [3]. With the help of this approach, the literature has revealed diverse and critical insights. For example, Kleimola et al. [15] explored nursing students’ motivational profiles with 132 participants. With the help of latent class analysis, they identified three motivational profiles of vocational practical nurse students. Trautner and Schwinger [18] investigated self-concepts related to the academic ability of primary school students with mild learning challenges. They analyzed data from more than 400 students using factor mixture models and found two subgroups that differed in their levels of self-concept. In another study, Gillet et al. [20] conducted latent profile and latent transition analysis with 504 university students’ data in order to examine their academic motivation profiles and the stability of these profiles. They discovered six distinct motivation profiles, which remained stable across two measurement points.\n\n\n2.3 Person-specific approaches\nThe person-specific approaches might be considered a novel method in learning analytics research, though they have been used previously in psychological research. This approach is sometimes referred to as idiographic, idiosyncratic, within-person, intra-individual, or single subject. In this approach, the researchers view each student as the whole unit of analysis (N = 1) [3, 6, 11, 22 ]. Since psychological processes such as learning or emotion can vary for each individual, attempting to understand a person’s processes using group data may overlook individual differences and variations [22]. Moreover, this can even cause ineffective recommendations and interventions for some individuals [23]. In this regard, the person-specific approaches can be valuable by conducting focused studies on individuals [3].\nUnlike variable-centered and person-centered approaches, person-specific approaches do not require data to be collected from multiple individuals; even a single subject may be sufficient; however, to be able to make inferences at the person level, data must be collected at multiple time points and with enough data points for the analysis [6, 9, 11]. Figure 19.1 summarizes the comparison of the three approaches. In Figure 19.1—A, data is collected from multiple subjects at the same time point, with all subjects considered to belong to a single group (homogeneity). In Figure 19.1—B, data are again collected from multiple subjects at the same time point; however, this time, the subjects belong to different subgroups (heterogeneity and homogeneity). Conversely, in Figure 19.1—C, data are collected from a single subject across multiple time points to investigate individual variations and changes in a detailed way (heterogeneity).\n\n\n\nFigure 1. Illustration of the difference between variable-centered, person-centered, and person-specific analysis\n\n\nResearchers conduct diverse analyses using a person-specific lens such as vector autoregression [6, 10], time-series analysis [24], dynamic factor analysis [24], group iterative multiple model estimation [11], idiographic machine learning [8] or mixed linear models [9].\nCompared to variable-centered and person-centered methods, person-specific approaches do not produce generalizable results since they require intensive investigation of individuals rather than attempting to understand the population, however; they yield the more individualized results [3, 5, 25]. Therefore, utilizing idiographic approaches in learning analytics research studies could be a solution to offering more personalized recommendations.\nIn the literature, there are a few research studies that utilize idiographic approaches. For example, Saqr and López-Pernas [25] conducted single-subject research to investigate the self-regulation strategies of a student. They gathered data daily over 30 days, and the data analysis revealed that the student needed to improve their reflection and planning strategies. Furthermore, they emphasized the potential of idiographic methods for examining students’ behavior at the individual level.\nIn summary, each of the three approaches has its advantages and limitations. Relying on a single approach may lead to inadequate solutions in education and learning analytics. Instead, integrating and leveraging the strengths of all three approaches could provide a complete picture of the phenomenon under study and contribute to the design of more effective learning analytics interventions. Table 19.1 summarizes all three approaches in a detailed way.\n\n\nTable 1. Summary of method comparison\n\n\n\n\n\n\n\n\n\nVariable-centered\nPerson-centered\nPerson-Specific\n\n\n\n\nAlternative Names\n\nNomothetic\nGroup-based\nBetween-person\nInter-individual\n\n\nPerson-based\nClass-centered\n\n\nIdiographic\nIdiosyncratic\nSingle Subject\nN=1\n\n\n\n\n\n\n\n\n\nDescription\nExplain how various variables are correlated or different across groups [3, 5]\nCategorize similar individuals into separate sub-populations [3, 5, 12]\nView each student as a whole unit of analysis [5, 6, 10, 22]\n\n\n\n\n\n\n\n\nAssumption\nAssumes Homogeneity [2, 3, 5]\nAssumes heterogeneity[3–5]\nAssumes every one is unique [5, 6, 9, 11]\n\n\n\n\n\n\n\n\nSample Size\nThe suggested minimum sample is 30 however larger are better [3]\nThe suggested sample size typically is larger e.g., 200 subjects or more [3, 5], but it heavily depends on the analysis\nA single subject may be sufficient but requires 30-120 or even more data-points from the same individual depending on the analytical method [3, 5, 6, 22]\n\n\n\n\n\n\n\n\nData Collection\nFrom multiple individuals [3, 5, 9]\nFrom multiple individuals [3]\nFrom a single individual at multiple time points (or multiple individuals however the analysis is performed individually)[6, 11]\n\n\n\n\n\n\n\n\nSample Research Questions\nWhat is the association between engagement and academic performance [26]?\nWhat different profiles of engagement can be identified and how do they change? [27]\nWhat are the unique profiles of each student in self-regulating their own studies [10].\n\n\n\n\n\n\n\n\nSample Statistical Analysis\n\nt-test [28]\nANOVA [29]\nCorrelation [30]\nRegression [31]\nFactor Analysis [32]\nStructural Equation Modeling [33]\n\n\nMixture regression analysis [5]\nLatent class analysis [15]\nLatent profile analysis [16]\nFactor mixture analysis [17]\nMixture growth curve modeling[34]\nLatent transition analysis [20]\nLatent Markov factor analysis [27]\nMixed Hidden Markov Models [35]\n\n\nVector autoregression [6]\nTime-series analysis methods [24]\nDynamic factor analysis [24]\nGroup iterative multiple model estimation [11]\nIdiographic machine learning [8]\nPsychological networks [10]\n\n\n\n\n\n\n\n\n\nExample applicatino\nStudying the general relationship between two variables across a group of students to design general interventions that work for the majority.\nIdentifying subgroups of students with distinct profiles to tailor interventions for each subgroup’s specific needs.\nAnalyzing how each single student fluctuates daily to create personalized interventions."
  },
  {
    "objectID": "chapters/ch19-three-levels/ch19-three-levels.html#a-tutorial-along-the-heterogeneity-spectrum",
    "href": "chapters/ch19-three-levels/ch19-three-levels.html#a-tutorial-along-the-heterogeneity-spectrum",
    "title": "19  The Three Levels of Analysis: Variable-centered, Person-centered and Person-specific Analysis in Education",
    "section": "3 A tutorial along the heterogeneity spectrum",
    "text": "3 A tutorial along the heterogeneity spectrum\nIn this section we present a tutorial on variable-centered, person-centered and person-specific analysis. The tutorial assumes that the reader is familiar with the R programming language. If that is not the case, it is recommended to refer to previous tutorials on the basics of R [36], data cleaning [37], basic statistics [38], and visualization [39]. To illustrate the main principles and techniques of the three presented approaches, we will use a dataset from a previous study that used the Experience Sampling Method (ESM) to investigate the relationship between self-regulation and students’ everyday experiences [40]. ESM is a method where participants report on their behaviors and feelings multiple times throughout the day, allowing for the collection of real-time data on individual experiences. Most importantly, ESM allows the collection of enough data for individual analysis.\nIn this study, the focus was primarily on how certain aspects such as mindfulness, self-control, and affective states influence students’ daily life experiences. The data collection period involved up to 48 measurement points per participant. This intensive longitudinal design allowed the researchers to capture the changes in participants’ experiences over time, with each participant providing multiple repeated measures (a requirement for person-specific analysis). Though there are many interesting variables for analysis in the dataset, in this tutorial, we will focus on the relationship between task value and happiness, in alignment with Expectancy-Value Theory (EVT). To access the data from the study to follow this tutorial, you must first fill the consent form in the dataset webpage [40] and download the data files.\n\n3.1 Preparing the data and libraries\nTo analyze the dataset with R, we first need to install and load the necessary libraries:\n\ntidyverse is a collection of packages for data manipulation and visualization\nflexmix is used for finite mixture modeling\nlme4 is for fitting linear mixed-effects models\nmodeltools provides tools for model fitting and validation\njtools helps with summarizing and visualizing regression models\nanytime is for converting date-time strings into proper datetime objects\nrio imports and exports data in several formats\n\n\nlibrary(tidyverse)\nlibrary(flexmix)\nlibrary(lme4)\nlibrary(modeltools)\nlibrary(jtools)\nlibrary(anytime)\nlibrary(rio)\n\noptions(scipen = 99)\n\n\n\n\nNow we can import the data using the function import from rio. We store it in a variable named data.\n\n# Import data downloaded from https://rdc-psychology.org/grund_fries_senker_2022\ndata <- import(\"gdal18pr18_pd.txt\")\n\n\n\n\nWe will perform some cleaning and pre-processing of the data to be able to work with it. First, we convert the date and time columns of the data into a single timestamp value with a datetime format.\n\ndata$Timestamp = anytime(paste(data$DATE, data$TIME))\n\nWe then remove (using filter) empty responses or responses that present invalid values (which were coded with negative numbers, so we keep those above 0 only), and then we order (arrange) the responses within each participant (group_by). We tag each response with the order they occupy in the sequence of responses (mutate).\n\n# Order responses per participant\ndata_seq <- data |>\n  filter(KF_P06_VA >= 0) |> # Remove rows where happiness (KF_P06_VA) is invalid\n  filter(KF_SDT_2 >= 0) |> # Remove entries where task value (KF_SDT_2) is invalid\n  group_by(PARTICIPANT) |> # Group by participant\n  arrange(Timestamp) |> # Sort by timestamp\n  mutate(Sequence = seq_along(Timestamp)) |> # Create a sequential ID for responses\n  ungroup()\n\nWe now select only the columns that we need, corresponding to the participant identifier, the order of the response, and the responses provided for happiness (KF_P06_VA) and task value (KF_SDT_2).\n\ndata_filtered <- data_seq |>\n  select(PARTICIPANT, Sequence, Happiness = KF_P06_VA, TaskValue = KF_SDT_2)\n\nWe can preview the data to make sure we have everything we need.\n\nhead(data_filtered)\n\n\n\n  \n\n\n\n\n\n3.2 Variable-centered\nThe first analytical lens we are exploring is variable-centered analysis. This approach focuses on understanding the relationships between specific variables across individuals. In variable-centered analysis, the main objective is to identify general trends and patterns by analyzing how one variable (such as task value) relates to another (such as happiness) across a population. This type of analysis is typically used to study overall associations or effects, assuming that the relationships between variables are consistent across individuals, i.e., we treat all individuals as being drawn from the same population. It is a suitable approach when the goal is to make generalized conclusions about how variables relate to each other at a population level, rather than focusing on individual differences.\nTo illustrate the variable-centered approach, we will model the relationship between task value and happiness using a simple linear regression. Specifically, we will calculate how students’ overall task value perception is related to their overall happiness. Since our dataset contains data collected over several time points, we can aggregate the individual observations for each participant by calculating their mean task value and mean happiness scores. This aggregation offers a more stable mean. In other words, it offers a more accurate idea about each individual given that we do not just randomly collect their data at a time-point but we rather aggregate and average their responses over many time-points. In so doing, we focus on the overall enduring attitude of task value and happiness for each participant. This allows us to look at broad, general trends across participants. The following excerpt of code calculates the mean happiness and task value for each participant and stores it in a new variable named data_meaned.\n\ndata_meaned <- data_filtered |> \n  group_by(PARTICIPANT) |> # Group by participant\n  summarize_at(vars(Happiness, TaskValue), mean)  # Summarize (average) columns\n\nWhen modeling the relationship between task value and happiness, happiness will serve as the dependent variable —the outcome we aim to explain— while task value will be the independent variable —the predictor of interest. The linear regression model will provide an estimate of the relationship between task value and happiness by means of a regression coefficient. This coefficient will indicate how much we expect happiness to change for a one-unit increase in task value. A positive coefficient would suggest that higher task value is associated with higher happiness, while a negative coefficient would suggest the opposite. Furthermore, the model will provide a p-value to assess whether the relationship is statistically significant, meaning that it is unlikely to have occurred by chance. The model can be expressed mathematically as follows:\n\\[\n\\text{Happiness}_{i} = \\beta_0 + \\beta_1 \\cdot \\text{Task Value}_{i} + \\epsilon_{i}\n\\]\nWhere:\n\n\\(i\\) indexes each observation.\n\\(\\beta_0\\) is the fixed intercept (overall mean happiness across all observations).\n\\(\\beta_1\\) is the fixed effect of task value (how task value affects happiness on average across observations).\n\\(\\epsilon_{i}\\) represents the residual error for each observation, capturing variability not explained by task value.\n\nWe can implement the linear regression using the following code. The lm function calculates the relationship between task value and happiness, and returns the aforementioned regression coefficient. The summ function is used to provide a detailed summary of the regression results, including model fit statistics, the regression coefficient, and whether the relationship is statistically significant. For a tutorial on basic statistical methods with R, please refer to [41].\n\nmodel_lm <- lm(Happiness ~ TaskValue, data = data_meaned)\nsumm(model_lm) # Summarize the model\n\n\n\n\n  \n    Observations \n    107 \n  \n  \n    Dependent variable \n    Happiness \n  \n  \n    Type \n    OLS linear regression \n  \n\n \n\n  \n    F(1,105) \n    5.13 \n  \n  \n    R² \n    0.05 \n  \n  \n    Adj. R² \n    0.04 \n  \n\n \n \n  \n      \n    Est. \n    S.E. \n    t val. \n    p \n  \n \n\n  \n    (Intercept) \n    1.97 \n    0.92 \n    2.15 \n    0.03 \n  \n  \n    TaskValue \n    0.46 \n    0.20 \n    2.26 \n    0.03 \n  \n\n\n Standard errors: OLS\n\n\n\nThe results show that the relationship between task value and happiness is strong (estimate = 0.46), which means that average happiness and task value are closely related. Moreover, the p-value is 0.03 —which is lower than the usual significance level of 0.05— and, therefore, this relationship is statistically significant.\nFollowing the model fitting, we can visually inspect the relationship between task value and happiness by plotting the regression results using ggplot2. Figure 19.2 shows a scatter plot (geom_point) of the averaged data points, along with a fitted linear regression line (geom_smooth) to represent the overall trend. For a tutorial on data visualization with ggplot2, please refer to [39].\n\nlabel = paste0(\"Estimate: \", round(model_lm$coefficients[\"TaskValue\"], 3))\n# Plot the regression results\nggplot(data_meaned, aes(x = TaskValue, y = Happiness)) +\n  geom_point(alpha = 0.5) + # Scatter plot with transparent points\n  geom_smooth(method = \"lm\", fill = NA) + # Add a linear regression line\n  labs(x = \"Task Value\", y = \"Happiness\") # Label the axes\n\n\n\n\nFigure 2. Relationship between participants’ mean task value and their corresponding mean happiness scores. Each point represents the average task value and happiness for a participant. The linear regression line (in blue) illustrates the overall trend, indicating that higher task value is associated with increased happiness.\n\n\n\n\nThe plot shows a small but positive relationship between task value and happiness. As task value increases, happiness tends to increase slightly. However, the scatter points show a moderate spread around the regression line. This means that, while the general trend is upward (positive correlation), there is a lot of variability in happiness for a given level of task value, suggesting that other factors may also play a role in determining happiness.\nIf instead of modeling the relationship between overall task value and overall happiness we would like to model the momentary relationship —i.e., how task value and happiness are related in the same time point, we can use a multilevel linear regression. This approach uses the complete data (not averaged) including all individual responses per student and time point. To account for the nestedness of the data —i.e., each student has several data points—, we should add a random effect accounting for each individual person. Moreover, since we are modeling how task value and happinness are related, we need to account for the fact that different people have different baseline levels of task value and happiness. It should be noted that, while the random intercept captures individual baseline differences, the model assumes that coefficient for task value is the same for everyone. In other words, the model assumes that the way task value influences happiness is consistent across participants, even though each participant starts from a unique baseline. The model can be represented as follows:\n\\[\n{Happiness}_{ij} = \\beta_0 + \\beta_1 \\cdot \\text{Task Value}_{ij} + u_j + \\epsilon_{ij}\n\\]\nWhere:\n\n\\(i\\) indexes the repeated measures within participants.\n\\(j\\) indexes the participants.\n\\(\\beta_0\\) is the fixed intercept (the overall mean happiness across all participants).\n\\(\\beta_1\\) is the fixed effect of task value (how task value affects happiness on average across participants).\n\\(u_j\\) is the random intercept for participant \\(j\\), capturing between-participant variability in baseline happiness.\n\\(\\epsilon_{ij}\\) represents the residual error for each observation, capturing within-participant variability that is not explained by task value or the random effect.\n\nTo implement the mixed-effects model described above, we will use the lmer function from the lme4 package [42] to account for both the fixed effect of task value and the random intercepts for participants (1 | PARTICIPANT). As explained earlier, this model allows us to estimate the overall effect of task value on happiness, while accounting for the fact that multiple observations are nested within participants, who may differ in their baseline happiness levels. As we can see in the chunk of code below, the formula is almost the same as before but we add the component representing the random intercept. Also note that we are providing the entire data, not the averaged version.\n\nmodel_lmer <- lmer(Happiness ~ TaskValue + (1 | PARTICIPANT), data = data_filtered)\nsumm(model_lmer)  # Summarize the mixed-effects model\n\n\n\n\n  \n    Observations \n    3866 \n  \n  \n    Dependent variable \n    Happiness \n  \n  \n    Type \n    Mixed effects linear regression \n  \n\n \n\n  \n    AIC \n    12583.50 \n  \n  \n    BIC \n    12608.54 \n  \n  \n    Pseudo-R² (fixed effects) \n    0.01 \n  \n  \n    Pseudo-R² (total) \n    0.43 \n  \n\n \n \nFixed Effects\n  \n      \n    Est. \n    S.E. \n    t val. \n    d.f. \n    p \n  \n \n\n  \n    (Intercept) \n    3.61 \n    0.12 \n    30.14 \n    223.97 \n    0.00 \n  \n  \n    TaskValue \n    0.09 \n    0.01 \n    6.34 \n    3794.34 \n    0.00 \n  \n\n\n  p values calculated using Satterthwaite d.f. \n \n \nRandom Effects\n  \n    Group \n    Parameter \n    Std. Dev. \n  \n \n\n  \n    PARTICIPANT \n    (Intercept) \n    1.00 \n  \n  \n    Residual \n     \n    1.17 \n  \n\n \n \nGrouping Variables\n  \n    Group \n    # groups \n    ICC \n  \n \n\n  \n    PARTICIPANT \n    107 \n    0.42 \n  \n\n\n\n\nThe model shows a small but significant positive relationship between task value and happiness. However, the fixed effect of task value explains only a small portion of the variance (1%), indicating that other factors are likely influencing happiness. In turn, the Intraclass Correlation Coefficient (ICC) of 0.42 for the mixed-effects model indicates that 42% of the total variance in happiness can be attributed to differences between participants. This relatively high ICC suggests substantial variation in baseline happiness across participants, highlighting the importance of accounting for these individual differences. In the estimated model, the estimates are “informed by and shrunk toward group-level averages” [43] (p. 88), meaning that individual differences in the relationship between task value and happiness are not explicitly modeled. Instead, everyone is assumed to have the same relationship between task value and happiness. Therefore, it is still considered a variable-centered approach, as it prioritizes the average effect across all individuals rather than capturing heterogeneity in individual patterns. In contrast, a person-specific approach would allow modeling each individual’s unique relationship between task value and happiness.\n\n\n3.3 Person-centered\nUp to this point, we have focused on the overall trend and relationship between task value and happiness across all participants. However,it is possible that different participants experience the relationship between task value and happiness in different ways. For example, some individuals may be happier with tasks they value highly (in line with the results we obtained from the variable-centered analysis), while others might experience little change in happiness regardless of task value. This heterogeneity in participant experiences suggests that subgroups or clusters of individuals may exist, each with distinct task value-happiness relationships that the overall average does not capture. When these varied patterns are averaged together in a variable-centered analysis, the heterogeneity can result in an apparent absence of a relationship if the positive and negative patterns offset each other.\nIn this next section of the tutorial, we will learn how to implement person-centered analysis, which shifts the focus from population-wide trends to understanding patterns across specific subgroups of individuals. Rather than modeling how one variable influences another for all participants collectively, person-centered approaches aim to identify distinct profiles, clusters, groups, or trajectories that represent unique configurations of task value and happiness within the data.\nThe person-centered approach relies on techniques such as cluster analysis to help us capture the heterogeneity in experiences that a variable-centered approach may overlook. The focus is on detecting subgroups of individuals that share similar patterns that differ from everyone else. As such, person-centered methods acknowledge and embrace the intrinsic heterogeneity among participants, recognizing that each subgroup has unique characteristics that may remain hidden when analyzing the population as a whole.\nTo illustrate the person-centered approach, we will use a mixture regression model to classify participants based on the relationship between task value and happiness. The goal of a mixture model is to identify subgroups (or classes) within the data where each class has a different relationship between the predictor (task value) and the outcome (happiness).\n\\[\nHappiness_{i} = \\beta_{0k} + \\beta_{1k} \\cdot TaskValue_{i} + \\epsilon_{i}\n\\]\nwhere:\n\n\\(i\\) indexes individual data points.\n\\(k\\) indexes the clusters (e.g., Cluster 1 and Cluster 2), where each cluster has its own parameters (\\(\\beta_{0k}\\) and \\(\\beta_{1k}\\)).\n\\(\\beta_{0k}\\) is the intercept for cluster \\(k\\), representing the baseline level of happiness specific to each cluster.\n\\(\\beta_{1k}\\) is the slope for cluster \\(k\\), capturing how task value influences happiness within each cluster.\n\\(\\epsilon_{i}\\) represents the residual error for each observation, capturing variability not explained by task value or cluster assignment.\n\nWe are using flexmix to fit a mixture of linear regressions, meaning it estimates separate regression lines for two distinct subgroups within the data (data_meaned). Each cluster has its own intercept (\\(\\beta_{0c}\\)) and slope (\\(\\beta_{1c}\\)), allowing the model to identify subgroups where the relationship between TaskValue and Happiness differ. For instance, one cluster may show a stronger positive relationship, while another may have a weaker or even negative relationship. The final model assigns each data point to one of the two clusters based on how well it fits each cluster’s parameters.\nTo illustrate this method, we are fitting a model with two classes (k = 2), meaning that we expect to find two distinct clusters or subgroups with different relationship between task value and happiness. In practice, we would estimate the model for a range of different numbers of clusters (for example, k = 1 to k = 10) and select the optimal model. The model with the lowest BIC (Bayesian Information Criterion) indicates the best balance between model fit and complexity. For the sake of simplicity, we are using two classes here, and we rely on the flexmix library in R for estimating the mixture regression model. The following code —after setting a seed for reproducibility— fits a finite mixture model using flexmix with two clusters (k = 2) —termed “components” in the output. The parameters function then displays each cluster’s intercepts, coefficients, and standard deviations, showing the unique relationships within each group. Finally, the code calculates the BIC for the model, which can help compare this two-cluster model with other models (e.g., with 3, 4, or more clusters) to identify the best-fitting model.\n\nset.seed(022024) # Set seed for reproducibility\n\n# Fit the mixture model with two components (k = 2)\nmodel_k2 <- flexmix(Happiness ~ TaskValue, k = 2,\n                    data = data_meaned)\n\n# Display the parameters (intercepts, coefficients, and SDs) for each class\nparameters(model_k2)\n\n                 Comp.1 Comp.2\ncoef.(Intercept) -0.797 11.597\ncoef.TaskValue    1.069 -1.708\nsigma             0.907  0.604\n\n# Calculates the BIC to be able to compare with other models (e.g., k = 3,4,5...)\nBIC(model_k2)\n\n[1] 332\n\n\nThe results show that Class 1 ($Comp.1) has a positive relationship between task value and happiness, as indicated by the coefficient of 1.07. In contrast, Class 2 shows an even stronger negative relationship between task value and happiness, with a coefficient of -1.71. This indicates that for participants in this class, higher task value is associated with a decrease in happiness. The standard deviations (sigma values) show that Class 1 has more variability in the happiness responses to task value variations compared to Class 2. Class 2 has a smaller SD (0.60), indicating that participants in this class have less variability in their happiness responses to task value.\nAfter identifying the two distinct classes and understanding the initial coefficients and relationships between task value and happiness, we now need to explore the model further to gain more information about each class. The next step involves refitting the mixture model using the refit function. This will provide a more exhaustive summary of the model for each of the identified classes, and their intercepts, coefficients, and standard deviations for each component. When refitting the model, we fine-tune the estimates for each class so that we can accurately inspect the relationships between task value and happiness within each group of participants. Thus, we ensure that we are basing our interpretations on the most reliable and detailed estimates.\n\n# Refit the model to obtain more detailed summaries for each class\nrm2 <- refit(model_k2)\nsummary(rm2)\n\n$Comp.1\n            Estimate Std. Error z value Pr(>|z|)   \n(Intercept)   -0.819      1.497   -0.55    0.584   \nTaskValue      1.074      0.327    3.28    0.001 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$Comp.2\n            Estimate Std. Error z value           Pr(>|z|)    \n(Intercept)   11.670      1.491    7.83 0.0000000000000051 ***\nTaskValue     -1.724      0.344   -5.01 0.0000005481101304 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFrom the detailed summaries we get not only the estimate value as before, but also the standard error, z-value and p-value of both the intercept and the slope for each cluster. We see that all statistics are statistically significant except the intercept of Cluster 1.\nWe can extract cluster memberships (i.e., which participants belong to which class) and display the cluster sizes (i.e., the number of participants assigned to each class) to get an idea of how balanced the clusters are.\n\ncluster_k2 <- clusters(model_k2)\ntable(cluster_k2)  # Display the number of participants in each cluster\n\ncluster_k2\n 1  2 \n91 16 \n\n\nWe see that the first class has 91 participants assigned, and the second class has 16. It is important to take into account the distribution of participants across the classes to guide further analyses or interventions targeted at these distinct groups. To visualize the results of the mixture regression model and the differences between the two classes, we can plot the regression lines for each class alongside the participant data.\nThe following code creates a plot using ggplot. The scatter plot (geom_point) shows the individual data points, color-coded by class (scale_color_manual), while the regression lines (geom_smooth) represent the distinct task value-happiness relationships for each class (1 or 2). To extract the class variable —i.e., assignment of each data point to a specific cluster (component)— we use cluster_k2 and we convert it to a factor:\n\nggplot(data_meaned, aes(x = TaskValue, y = Happiness, color = factor(cluster_k2))) +\n  geom_point(alpha = 0.5) +  # Plot the points with transparency\n  geom_smooth(method = \"lm\", se = F) +  # Plot the regression lines\n  scale_color_manual(name =\"Class\", values = c(\"darkgreen\",\"firebrick2\")) # Color scale\n\n\n\n\nFigure 3. Relationship between task value and happiness for two classes using a two-class mixture model- Class 1 shows a positive relationship, Class 2 shows a negative relationship\n\n\n\n\nFigure 19.3 demonstrates the clear heterogeneity in how participants experience the relationship between task value and happiness. Some participants (Class 1) show a positive association, while others (Class 2) show a negative association. Therefore, the use of a mixture model clearly reveals that there are two distinct subgroups of participants with opposing trends, which would not have been apparent using a variable-centered approach. This is a clear example of how a person-centered approach helps uncover the diversity in how individuals experience the task value-happiness relationship.\n\n\n3.4 Person-specific\nAfter completing both the variable-centered and person-centered analyses, we move on to the final lens: person-specific analysis. The previous approaches have helped us understand general trends across the population and identify subgroups with distinct relationships between task value and happiness. In person-specific analysis, the goal is to model each individual’s unique experience rather than aggregate or cluster individuals. This approach allows us to focus on within-person variability over time. Each individual is treated as their own unit of analysis, allowing us to investigate the momentary relationship between task value and happiness within each person. This method is particularly important when there is a high degree of variability within individuals across different contexts and when the focus is on understanding how each person’s task value relates to their happiness on a moment-to-moment basis. We will implement person-specific analysis by fitting a linear regression for each person including all of their individual responses.\nThe first step is to filter the dataset to ensure that each participant has sufficient data points for meaningful individual analysis. Specifically, we will filter out participants with fewer than 30 data points. This ensures that there are enough observations for each participant to estimate their own unique task value-happiness model. Participants with fewer data points might not provide enough information to capture meaningful within-person variability, and could lead to unreliable estimates.\n\ndata_filtered30 <- data_filtered |> group_by(PARTICIPANT) |> filter(n() >= 30)\n\nThe next step is detrending the data. Detrending is necessary when there is a temporal trend that could confound the relationship between task value and happiness. Detrending helps to remove this systematic temporal effect, allowing us to focus on the within-person relationship between task value and happiness that is not influenced by time-based trends. For example, Figure 19.4 illustrates how if happiness (red line) generally increases over time for participants (e.g., due to external factors like an academic semester, weather, or life events), it could falsely inflate or mask the effect of task value on happiness. When we remove this temporal trend (grey line), we are able to isolate the effect of task value on happiness (magenta line) and ensure that the observed relationship is not simply due to changes over time.\n\n\n\n\n\nFigure 4. Illustration of the detrending process\n\n\n\n\nThe code below creates the detrender function, which takes the dataset (data) as an argument, the variables to detrend (vars), and the variables that represent time (timevar). The function iterates through the variables to detrend and, for each of them, it fits a linear model (fit) to check for a significant trend over the timevar variable. If the linear trend is significant (p < 0.05 from anova(fit)), the original values of the variable in the data are replaced with the residuals from fit, thus removing the trend component. After processing all specified variables, to_detrend contains the modified data where significant trends over time have been detrended for each relevant variable, and the function returns this updated dataset.\n\ndetrender <- function(data, vars, timevar) {\n  to_detrend <- data\n  for (v in seq_along(vars)) {\n    # Fit linear model for each variable over time and detrend if trend is significant\n    ff <- as.formula(paste0(vars[[v]], \" ~ \", timevar))\n    fit <- lm(ff, data = to_detrend)\n    if (anova(fit)$P[1] < 0.05) {  # Check if the trend is statistically significant\n      # Replace values with residuals\n      to_detrend[[vars[v]]][!is.na(to_detrend[[vars[[v]]]])] <- residuals(fit)  \n    }\n  }\n  to_detrend\n}\n\nWe now apply the detrender function to our dataset for the variables corresponding to happiness and task value, along the sequence variable that collects the order of the responses in time.\n\n# Apply the detrending function to the happiness and task value variables\ndata_detrended <- detrender(data_filtered30, \n                            c(\"Happiness\",\"TaskValue\"), \"Sequence\")\n\nWe can separate each individual student’s data and create a list of all students’ detrended responses so we can easily operate in each student individually.\n\n# Create a list of data frames, one for each participant\ndata_detrended_list <- split(data_detrended, data_detrended$PARTICIPANT)\n\nNow we are finally ready to conduct a person-specific analysis. We will fit a separate linear regression model for each participant to model the unique relationship between task value and happiness for each individual student. This approach helps us identify within-person differences in the relationship between these two variables for each participant. It is worth mentioning that we are estimating the contemporaneous effect of task value on happiness, meaning that we are examining how task value at a given moment is associated with happiness at the same timepoint for each individual participant. For ESM data, such as the one in this dataset, analyses typically estimate auto-regressive effects (the extent to which a variable at a given timepoint predicts itself at a subsequent timepoint) and cross-regressive effects (the extent to which one variable at a given timepoint predicts another variable at a subsequent timepoint) using lagged predictors.\nThe following code fits a separate linear regression model for each participant. We first create an empty list (model_lm_person) to store each individual model. Then we iterate through the list of participants’ data and we fit the model to each individual dataset using lm. We store each linear model in model_lm_person.\n\n# Create an empty list where we will store each participant's model\nmodel_lm_person <- list()\n\n# Fit the models\nfor (k in seq_along(data_detrended_list)) {\n  model_lm_person[[k]] <- lm(Happiness ~ TaskValue, data = data_detrended_list[[k]])\n}\n\nNow we iterate through the models and we extract the results (coefficient, p-value, etc.) for each of them using tidy. Then we combine (cbind) this information with each student’s name that we extract from the data (PARTICIPANT). We store the model results for each student in a list (est_person).\n\n# Create an empty list where we will store each participant's estimate\nest_person <- list()\n\n# Extract slope estimates\nfor (k in seq_along(model_lm_person)) {\n  estimate <- tidy(model_lm_person[[k]])[2,]\n  PARTICIPANT <- data_detrended_list[[k]][1, \"PARTICIPANT\"]\n  est_person[[k]] <- cbind(PARTICIPANT, estimate)\n}\n\nTo get an idea of how unique the relationship between task value and happiness is, we can visualize the distribution of these person-specific estimates. We first combine the individual slope estimates (data_est_person$estimate) into a single data frame (data_est_person). We do that using the bind_rows function, which combines a list of dataframes into a single combined dataframe and adds a new column (specified by .id = \"Person\") to identify which list each subset of the data came from. Then, we plot a histogram (geom_histogram) of the slope estimates to see how the relationship between task value and happiness varies across participants.\n\n# Combining the individual results into one data frame\ndata_est_person <- bind_rows(est_person, .id = \"Person\")\n\n# Histogram of the estimates to visualize the distribution across participants\nggplot(data_est_person, aes(x = estimate)) + \n  # Create histogram with 20 bins\n  geom_histogram(bins = 20, fill = \"lightgray\", color = \"black\", alpha = 0.7) +  \n  labs(x = \"Estimates\", y = \"Frequency\")   # Label the axes\n\n\n\n\nFigure 5. Histogram of the slope estimates of each person-specific model. The mean slope is 0.09, and the standard deviation is 0.22, suggesting that there is great variability, centered around close to zero (no relationship)\n\n\n\n\nTo provide a diverse sample to illustrate how the person-specific method works, we will select participants for detailed analysis who display both significant and non-significant relationships between task value and happiness. This diverse sample will allow us to explore the range of individual relationships between task value and happiness, from strong predictors to negligible effects, and better understand the heterogeneity in how students experience the relationship between these variables.\nThe following chunk filters and selects participants based on the significance of their results. First, it identifies participants with significant results by filtering data_est_person (the dataframe that contains the model results for each participant) for participants whose models have a p-value < 0.05, storing these rows in the significant variable. Next, it selects the first 10 participants with non-significant results (p.value >= 0.05) and stores them in the variable nonsignificant. Finally, we obtain the list of all the selected participants for further analysis, i.e., we combine both sets of participants’ IDs into a single object (ids), by using rbind to merge significant and nonsignificant, arranging the result in descending order by the estimate column, and extracting the PARTICIPANT column only which contains the identifiers.\n\n# Select all participants with significant results\nsignificant <- data_est_person |> filter(p.value < 0.05) \n# Select a few (10) participants with non-significant results\nnonsignificant <- data_est_person |> filter(p.value >= 0.05) |> head(10)\n# Combine the IDs of the selected participants for visualization\nids <- rbind(significant, nonsignificant) |> arrange(desc(estimate)) |> pull(PARTICIPANT)\n\nTo visualize the regression lines for the sampled students, we will focus on plotting the relationships between task value and happiness for each of the selected participants. With this, we showcase the person-specific variability in how task value impacts happiness, showing both significant and non-significant effects from our diverse sample.\nWe will depart from our detrended data (data_detrended) and we will keep only those students that we selected in the previous step (ids). We will create a new column named Person that contains the participant identifier converted into a factor and ordered by the variable ids, which contains the participant identifiers ordered by the regression estimates in reverse order (from larger to smaller). This will allow us to plot the participants in this order.\n\n# Get only students in the selected sample\ndata_in <- data_detrended |>\n  filter(PARTICIPANT %in% ids) |>  # Filter the selected participants\n  # Convert participant ID to factor for plotting\n  mutate(Person = factor(PARTICIPANT, levels = ids)) \n\nNow, we can proceed to plot the participants. We use ggplot to create the plot, geom_jitter to plot the data points as dots, stat_smooth to fit the regression line, and facet_wrap to create an individual subplot for each participant (Figure 19.6).\n\n# Plot individual regression lines for the selected participants\nggplot(data_in, aes(x = TaskValue, y = Happiness, group = Person)) +\n  geom_jitter(alpha = 0.35) + # Add dots\n  geom_smooth(method = \"lm\", fill = NA) +  # Add regression lines\n  facet_wrap(\"Person\",  ncol = 8, scales = \"fixed\")  # Create individual plots\n\n\n\n\nFigure 6. Each student’s individual regression\n\n\n\n\nThis plot provides a clear visual representation of the individual differences in the relationship between task value and happiness. For some participants (towards the beginning of the plot), the slope is positive and significant, indicating a strong relationship between task value and happiness, while for others (towards the bottom of the plot), the slope may be close to zero or even negative, suggesting a weaker or non-existent relationship. This visualization helps capture the heterogeneity in how task value affects happiness across individuals. Chapter 22 in the book [44] presents more advanced methods for automating the generation of person-specific predictive models."
  },
  {
    "objectID": "chapters/ch19-three-levels/ch19-three-levels.html#discussion",
    "href": "chapters/ch19-three-levels/ch19-three-levels.html#discussion",
    "title": "19  The Three Levels of Analysis: Variable-centered, Person-centered and Person-specific Analysis in Education",
    "section": "4 Discussion",
    "text": "4 Discussion\nIn this chapter, we have described three different analytical approaches: variable-centered, person-centered, and person-specific. We have illustrated different methods to apply each of these lenses. Starting with the variable-centered approach, we used a linear model to estimate the overall relationship between task value and happiness. The results showed a positive relationship, i.e., average task value is associated with higher average happiness.\nWe then moved on to the person-centered approach to investigate whether there was some heterogeneity in our data that could be hidden by using a variable-centered approach. Namely, we fitted a mixture of regressions model, which aims to detect whether not one single population but rather different subpopulations exist within the data, where each subgroup can have their own (linear) relationships between task value and happiness. When setting the number of models to 2, we saw that there are two opposing groups: a larger group with a positive relationship between task value and happiness, and a smaller group with a negative relationship. Lastly, in the person-specific approach, we fitted a simple linear model to each participant’s dataset. When inspecting each individual student’s within-person model we saw the complete range of estimates and therefore the full range of heterogeneity of our dataset, which went from participants with a strong positive relationship between task value and happiness, to the stark opposite.\nWhile multi-level model models take into account individual differences (see the last part of Section 19.3.2), they essentially focus on group-level inferences and impose shared assumptions across individuals. The resulting coefficients are still pooled towards the group mean, which does not capture idiographic processes. The pooling towards the group mean may provide stability, but at the cost of individual differences. Pooling individual estimates towards the group mean obscures the unique patterns or processes. Thus, individuals whose data deviate significantly from the group average will see their estimates pulled towards the group mean masking their unique variability.\nThe differences among each of these approaches point to the need for careful consideration when analyzing students’ data. It becomes apparent that making inferences at the group-level does not generalize at the individual level. Moreover, the heterogeneity of the data should be considered to make sure that all individual needs and preferences are taken into account and not hidden by the general trend that represents most students. It is precisely the ones who are not following the “default” path that need most help and support [45], so the analytical lenses that we use in research and practice need to support this principle.\nIn conclusion, variable-centered approaches are appropriate when we have a single measure for each student, when we do not expect this measure to fluctuate in the short term, and when we do not expect —or cannot afford to account for— great heterogeneity. We rely on variable-centered analysis when we wish to extract general trends or what works for the majority. We can use on this approach when creating course-level interventions, for example, whether to extend the deadline of an assignment or introduce a certain type of learning activity. When we expect to find different trends and subgroups, we rely on person-centered methods. This allows us to create interventions for each subgroup. It is of special relevance when for one of the subgroups the intervention that works for most is not only not helpful but harmful. For example, adding gamification elements to a course might be motivating for many, irrelevant for some, and distracting or detrimental for others [46]. Lastly, when we are studying variables that change momentarily, and we have the possibility to conduct intensive data collection to capture this within-person variability, we can resort to person-specific methods to issue precise interventions tailored at each specific individual [47]."
  },
  {
    "objectID": "chapters/ch20-var/ch20-var.html",
    "href": "chapters/ch20-var/ch20-var.html",
    "title": "20  Idiographic Networks: A tutorial on Graphical Vector Autoregression and Unified Structural Equation Modeling",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch20-var/ch20-var.html#introduction",
    "href": "chapters/ch20-var/ch20-var.html#introduction",
    "title": "20  Idiographic Networks: A tutorial on Graphical Vector Autoregression and Unified Structural Equation Modeling",
    "section": "1 Introduction",
    "text": "1 Introduction\nNetwork analysis provides a suite of methods for examining both static and dynamic relationships within systems. These methods reveal insights into collaborations between students or student-teacher interactions, knowledge construction on a topic, social dynamics between groups, and system-wide properties, including how these connections evolve over time [1–3]. Applications of network analysis span diverse contexts, from understanding how groups of students learn on massive online platforms [4] to identifying how a single learner evolves over time [5]. Representing elements of learning processes—such as cognitive and social aspects—within a network has become a well-established research method, encompassing various analytical approaches, including social network analysis (SNA), epistemic network analysis (ENA), ordered network analysis (ONA), and temporal network analysis (TNA) [3, 6, 7]. More recently, transition network analysis (TNA) has also emerged[7].\nWhile much of the prior work in network analysis focuses on group-level phenomena, this chapter takes an idiographic perspective, where the primary unit of analysis is the individual learner [8]. The idiographic approach diverges from traditional nomothetic methods by emphasizing the unique patterns, processes, and dynamics that characterize a single person, thereby offering a more granular understanding of learning as a personal and evolving process [9, 10]. This focus aligns with a growing recognition of the importance of personalized education, where interventions and analyses are tailored to the distinctive needs and experiences of individuals [9, 10].\nThis chapter focuses on a specific type of network: probabilistic networks. These networks represent variables as nodes and the statistical relationships between them (e.g., correlations or regression) as edges. Probabilistic networks, discussed in detail in the previous book [11], have been increasingly employed to model processes as they emerge, progress, or unfold, with their elements represented as nodes [12, 13]. These networks allow researchers to understand interactions and dynamics within a process.\nHere, we adopt an idiographic perspective, treating the student as the central unit of analysis. This approach examines data from a single student, continuing our exploration of individualized processes, as detailed in the chapter on idiographic machine learning [14]. In this chapter, we demonstrate how to use Graphical Vector Autoregression (GVAR) and Unified Structural Equation Modeling (uSEM) to analyze an individual’s process. We begin with a background section that defines key terms and concepts and provides a concise literature review. Following this, we provide a step-by-step tutorial using R to implement the discussed techniques."
  },
  {
    "objectID": "chapters/ch20-var/ch20-var.html#background",
    "href": "chapters/ch20-var/ch20-var.html#background",
    "title": "20  Idiographic Networks: A tutorial on Graphical Vector Autoregression and Unified Structural Equation Modeling",
    "section": "2 Background",
    "text": "2 Background\n\n2.1 The cognitive process as a networked system\nRepresenting elements of the cognitive and social processes as a network is an established research method. Such representation has afforded researchers a way to visualize the structure of these processes and to measure the magnitude of association between their elements, and to devise statistical indices that allow a precise interpretation of the resulting graphs [15]. Recent advances in network science have seen a surge in probabilistic network models, also referred to as GGMs, which are designed to examine the relationships of multivariate data or elements of a cogintive or psycholgocial process [[15]. GGMs are applied within the literature to map and visualize the dynamic relationships between processes and their elements, considering them as a complex system [15–19]. GGMs commonly estimate a partial correlation between variables while controlling for the influence of all other variables and their relationships within the system. Within these networks, two nodes are connected if the covariance of the edge cannot be explained by any other variable in the network, eliminating the possibility of confounding variables [15, 16]. Resulting outputs of these models show only significant relationships between nodes, being the weight of the significant edges proportional to the strength of the relationships, as well as whether the relationship is positive or negative, and mediation pathways (i.e., indirect relationships between two variables transmitted through one or more intermediary variables within the same network) [15].\nVector autoregression (VAR)is an extension of GGM methods has enabled the capability for modeling temporal processes, i.e., how a variable predicts another in the next time window. VAR estimates a directed network (in contrast to undirected in GGM); the nodes are variables (e.g., motivation, behavior or attitude) and the link between them are temporal relationships (a variable predicts another in the next time window) [15].\nThe abundance of intensive time-stamped data (e.g., time-series data) has led to the existence of enough observations from individual subjects across time using e.g., data collected with experience sampling methods, observational data and physiological data, which enabled the study of an individual as a unique case (N=1) [15, 20]. The representation and estimation of the temporal network allows researchers to study the individual phenomena, the progression of behavior, predict future behavior, as well as create relevant intervention. In other words, the network allows the idiographic assessment of the person dispositions and their temporal dynamics. The network is commonly represented by drawing an arrow from the node that represents the variable (e.g., motivation) to the variable that it predicts in the next time window of measurement (e.g., engagement) [21].\nTo explain it, an example is presented in Figure 20.1. We created a simulated dataset about working and achievement within an individual on a daily basis. The graph shows that motivation predicts work, as well as feeling of achievement of goals within the next day. Similarly, working predicts feeling of goal-achievement. However, engaging with work predicts slight stress the next day, and having stress negatively predicts feeling of achievement. Helping that individual could be done though offering stress management advice. Another type of network is the contemporaneous network (associative), a partial correlation network that maps the correlation between the elements of the studied phenomena within the same time window. For instance, when the subject is having comfort, he/she also eats snacks at the same time. In our study this is used to study the co-temporal association between daily events, for instance, how motivation predicts working on the task within the same day.\n\n\n\nFigure 1. A fictional temporal network of four constructs. The circles are variables. Blue lines are positive partial correlations. The thickness of the line is proportional to the magnitude of the correlation. The direction of the arrow points to the direction of the temporal correlation. Extracted from [22]"
  },
  {
    "objectID": "chapters/ch20-var/ch20-var.html#review-of-network-analysis-applications-in-education-literature",
    "href": "chapters/ch20-var/ch20-var.html#review-of-network-analysis-applications-in-education-literature",
    "title": "20  Idiographic Networks: A tutorial on Graphical Vector Autoregression and Unified Structural Equation Modeling",
    "section": "3 Review of Network Analysis Applications in Education Literature",
    "text": "3 Review of Network Analysis Applications in Education Literature\nProbabilistic network models, such as GGMs, have become an essential tool for examining how complex learning processes unfold and how the interconnectedness of psychological constructs contributes to enhanced learning outcomes. For instance, Malmberg et al. [22] explored how students monitor events and demonstrated that this monitoring facilitates functional regulatory behaviors during collaborative physics tasks. Their findings indicated that cycles of regulation during collaborative learning do not always coexist, revealing inherent limitations in group-level regulation. Moreover, they highlighted the temporal interplay between motivation and task monitoring, showing that motivation persists across different regulatory phases and significantly predicts how tasks are enacted.\nSimilarly, Zhou and Kang [23] employed network analysis to investigate collaborative problem-solving behaviors in teams during an astronomy simulation. Using multivariate autoregression, they identified temporal patterns in problem-solving behaviors, revealing how individual exploration within the simulation catalyzed group discussions and joint engagement. Their findings underscored the importance of individual reflections in driving group coordination and the construction of shared knowledge.\nCloser to the methods discussed in this tutorial, vector autoregression (VAR) networks have been applied to examine students’ self-regulation. Saqr [21] demonstrated that self-regulation profiles differ significantly across individuals, with each student exhibiting a unique regulatory pattern. Importantly, none of the students conformed to the aggregate patterns observed when data were analyzed at the group level. This underscores the critical need for idiographic approaches to capture the distinctive dynamics of each learner, challenging traditional nomothetic methodologies [24].\nAdditional research has used network analysis to uncover how students regulate cognitive and emotional processes, both individually and in groups, across diverse learning contexts, such as language acquisition and physics education [5, 25–31]. These studies establish network analysis as a powerful and reliable framework for identifying how learners’ regulatory strategies influence motivation, engagement, and ultimately, learning outcomes.\n\n3.1 Loading data and pre-processing\nBefore proceeding to the actual analysis, we will need to load the packages, import the data, and process it to prepare for the analysis. Below are the necessary packages:\n\ntidyverse: A renowned R package for data manipulation, wrangling and visualization [32].\nskimr: A package to perform summary statistics and help us get an overview of the dataset and in particular the variable distributions, any missing data, and basic descriptive statistics [33].\ngraphicalVAR: An R package which will be used for modeling and visualizing the graphicalVAR models as described before [34].\nqgraph: An R package for network visualization [35].\npompom: An R package that offers an implementation of unified structural equation modeling [36].\ncaret: Short for Classification And REgression Training, caret is a set of functions that attempt to simplify the creation and evaluation of predictive models [37].\nrio: A package for importing data in several formats [38].\n\nIn addition, we will import an additional R file with helper functions that we will need throughout the chapter (aux.R).\nOnce we have loaded the necessary R packages and dependencies, we import the dataset synthetic_Data_share.RDS (step 1). This dataset contains ESM measurements of 36 students that reported how they regulate their own learning activities including aspects like planning, monitoring their progress, seeking help, managing their environment, and reflecting on feedback. We then create a vector for the variables of interest (Vars), which includes nine SRL processes that will be the target of our the analysis (step 2).\nIn the next step (step 3), we create a Day variable for each student (by grouping by the name variable) using seq_along() to create sequential numbers for each time-point in the data for each individual. Next (step 4), given that we would like to analyze a single student, we will filter the data of a student, in our case, we chose Grace for analysis. In this step we also select only the specific SRL-related variables that we would like to analyze, defined by Vars, along with the new Day variable, preparing the filtered data for further analysis specific to Grace’s behaviors.\n\n# Step 1: Load the dataset\ndf <- import(\"https://github.com/lamethods/data2/raw/main/srl/srl.RDS\")\n\n# Step 2: Define variables for the graphical VAR model\nVars <- c(\"planning\", \"monitoring\", \"effort\", \"control\", \n          \"help\", \"social\", \"organizing\", \"feedback\", \"evaluating\")\n\n# Step 3: Create a \"Day\" variable for each entry within a person\ndf <- df |>\n  group_by(name) |>\n  mutate(Day = seq_along(name)) |>\n  ungroup()\n\n# Step 4: Filter the data for a specific individual named 'Grace'\nGrace_Data <- df |>\n  filter(name == \"Grace\") |>\n  dplyr::select(all_of(Vars), Day)\n\n\n3.1.1 Exploring the data\nIt is always a good practice to explore the data to judge the distribution of variables, missing data or any unexpected surprises in the data. We do so using the skimr package, which provides a detailed overview of the SRL dataset, including measures like mean, median, spread, and the presence of missing values. To do so, we simply use skim(Grace_Data).\n\nGrace_descriptives <- skim(Grace_Data)\nprint(Grace_descriptives)\n\n── Data Summary ────────────────────────\n                           Values    \nName                       Grace_Data\nNumber of rows             156       \nNumber of columns          10        \n_______________________              \nColumn type frequency:               \n  numeric                  10        \n________________________             \nGroup variables            None      \n\n── Variable type: numeric ──────────────────────────────────────────────────────\n   skim_variable n_missing complete_rate mean   sd p0  p25  p50   p75 p100 hist \n 1 planning              0             1 61.7 20.4  0 54.8 64.5  74.2  100 ▂▁▃▇▂\n 2 monitoring            0             1 63.7 17.7  0 50.7 68.7  76.1  100 ▁▁▆▇▃\n 3 effort                0             1 50.6 21.9  0 36   52    68    100 ▃▆▇▇▂\n 4 control               0             1 43.6 26.0  0 22.6 38.7  61.3  100 ▆▇▆▃▅\n 5 help                  0             1 75.8 16.4 25 70   80    87.5  100 ▁▁▃▇▆\n 6 social                0             1 45.4 18.3  0 31   48    56    100 ▂▅▇▃▁\n 7 organizing            0             1 62.7 16.5  0 48.1 66.7  74.1  100 ▁▁▅▇▁\n 8 feedback              0             1 60.8 24.4  0 42.1 62.9  80.7  100 ▁▆▆▇▇\n 9 evaluating            0             1 46.5 18.6  0 39.1 45.7  52.7  100 ▁▂▇▁▁\n10 Day                   0             1 78.5 45.2  1 39.8 78.5 117.   156 ▇▇▇▇▇\n\n\nThe next chunk of code visualizes the data and, since it is a longitudinal repeated measure dataset, we plot the data across time. For that purpose, we reshape it into a long format using pivot_longer to organize it so that the SRL variables (e.g., planning, monitoring) are represented in a single column (variable), and their corresponding values in another column (value). We then use ggplot2, where each SRL variable is plotted on the y-axis against time (Day) is plotted on the x-axis. We apply faceting so that each SRL construct is visualized in its own subplot (Figure 20.2).\n\n# Reshape the data to long format\nGrace_data_long <- Grace_Data |>\n  pivot_longer(cols = Vars, # Assuming 'id' and 'interaction' are the relevant columns\n               names_to = \"variable\", \n               values_to = \"value\")\n\n# Plot with faceting by variable\nggplot(Grace_data_long, aes(x = Day, y = value)) +\n  geom_line(color = \"blue\") +\n  geom_point() +\n  geom_smooth(method = \"loess\", color = \"red\", se = FALSE) + # Smoothing (LOESS)\n  facet_wrap(~variable, scales = \"free_y\", ncol = 3) +\n  theme_minimal() \n\n\n\n\nFigure 2. Time Series of Variables with Smoothing\n\n\n\n\n\n\n3.1.2 Stationarity\nStationarity is an important pre-requisite for graphicalVAR models which means that the data statistical properties, —e.g., mean and variance— do not change over time. Non-stationary data, which exhibits trends or other systematic temporal patterns may lead to spurious results and misleading interpretations. Therefore, we need to examine the presence of trends, and if present, these trends need to be removed (de-trended) with appropriate techniques. De-trending improves the model’s performance and the validity of its inferences and ensures that the relationships among variables are not artificially inflated or deflated by overarching trends. This is particularly important in graphicalVAR models where we focus is on understanding the network of dynamic relationships among variables. The next function detrender (defined in the file aux.R imported earlier) examines the presence of trends, and prints the output. If there are trends in the data, the functions performs detrending. As we can see the data has no trends and so we will proceed with the data without further modifications.\n\ndetrender(Grace_Data, vars = Vars, timevar = \"Day\")\n\nNo significant trend for planning - p-value: 0.30521859863304\n\n\nNo significant trend for monitoring - p-value: 0.0805897468195874\n\n\nNo significant trend for effort - p-value: 0.429465303760845\n\n\nNo significant trend for control - p-value: 0.972736537512728\n\n\nNo significant trend for help - p-value: 0.452982605134614\n\n\nNo significant trend for social - p-value: 0.925685121385736\n\n\nNo significant trend for organizing - p-value: 0.460175358168101\n\n\nNo significant trend for feedback - p-value: 0.456419063913471\n\n\nNo significant trend for evaluating - p-value: 0.993005264585157\n\n\n\n\n\n3.2 Estimation of the graphicalVAR Model\nFitting the graphicalVAR model is straightforward, for that we use the graphicalVAR() function with the data argument set to the dataset of Grace data = Grace_Data. The beepvar argument specifies the time variable in the data (the temporal sequence of our observation). In our case, we use beepvar = \"Day\", which refers to the day-to-day sequential measurements of Grace’s behaviors. We set the lambda_beta argument to 0.1 to control the regularization. A value of 0.1 is rather moderate given that we have a relatively large dataset with 156 measurements. Regularization as we mentioned before would shrink small edges, help avoid over-fitting and makes the model more interpretable. Other optional arguments, like verbose, can be set to TRUE if you want to display more detailed output during the fitting process, but it defaults to FALSE to suppress extra information.\nThe output of the summary() function of the model of Grace provides some useful information about the data and generated networks. As the results show, the model include 9 nodes corresponding to the SRL variables (e.g., planning, monitoring, effort, etc.). The Extended Bayesian Information Criterion (EBIC) hyperparameter was at the default value of 0.5 to control the model complexity and help avoid over-fitting. The optimal EBIC score achieved was 1329.994, indicating the best trade-off between fit and complexity. The Partial Contemporaneous Correlations (PCC) network had 11 non-zero correlations which means the network was relatively sparse where about 69% of the possible edges being zero as indicated by the PCC sparsity of 0.694. The algorithm tested 50 different values for the regularization parameter to find the optimal level. The Partial Directed Correlations (PDC) or the temporal network had 9 non-zero directed edges meaning that the network was rather sparse where PDC sparsity was 0.75, meaning 75% of potential temporal connections between variables were zero. The summary tells us that both the contemporaneous and temporal networks are stored within the object inside $PCC and $PDC, objects respectively.\n\nset.seed(265) # for replicability\n\n# Fit the `graphicalVAR` model to the data from 'Grace' with 'lambda_beta' \n# to 0.1 to apply moderate regularization \nGraphVAR_Grace <- graphicalVAR(Grace_Data, beepvar = \"Day\", lambda_beta = 0.1)\n\n# Output a summary of the fitted model to review the model's structure, \n# including estimated parameters and fit indices.\nsummary(GraphVAR_Grace)\n\n\n\n=== graphicalVAR results ===\nNumber of nodes: 9 \nNumber of tuning parameters tested: 50 \nEBIC hyperparameter: 0.5 \nOptimal EBIC: 1329.994 \n\nNumber of non-zero Partial Contemporaneous Correlations (PCC): 11 \nPCC Sparsity: 0.6944444 \nNumber of PCC tuning parameters tested: 50 \nPCC network stored in object$PCC \n\nNumber of non-zero Partial Directed Correlations (PDC): 9 \nPDC Sparsity: 0.75 \nNumber of PDC tuning parameters tested: 1 \nPDC network stored in object$PDC \n\nUse plot(object) to plot the estimated networks.\n\n\n\n\n3.3 Visualization and interpretation of the graphicalVAR Model\nTo visualize the results of Grace, we can use the built-in plot function with the required network argument. In that, we use the argument include = \"PCC\" , or “PCC\", to get the Partial Contemporaneous Correlations network or simply the contemporaneous network. We also use the argument include=\"PDC\" argument to retrieve the Partial Directed Correlations network, or simply, the temporal network.\nIn graphicalVAR contemporaneous plots, and, in fact, in most of such networks, the color of the edges reflects the direction of the correlation where blue (sometimes green) indicates a positive correlation between the two nodes and red indicates a negative correlation. The thickness of the edge indicates the strength of the relationship, with thicker edges indicating stronger associations. In this PCC network, in Figure 20.3 (a), we see a strong positive correlation between monitoring and help suggesting that when Grace is actively monitoring her learning, she tends to seek more help. We also see a negative correlation between social and help indicating that when Grace engages in a social activity, she tends do less monitoring of her learning. There is also a moderate association between monitoring and effort and feedback and help. These are all indicative of how Grace regulates her learning. Furthermore, in partial correlation networks, the absence of associations is also interpretable, in a way that these processes are independent of each other, e.g., we see no association between effort and control indicating that when Grace is investing good effort in her learning, she may not control her environment and distractions.\nThe visualization of the temporal network or the Partial Directed Correlations (PDC) network in technical terms show the temporal (lagged) relationships between self-regulated learning (SRL) behaviors. The arrows between the nodes represent how one behavior at time t predicts another at time t+1. The color and thickness of the arrows provide information on the direction and strength of these lagged relationships in the same way as the contemporaneous network. As we can see in the figure, there is a positive relationship between planning and effort, meaning that when Grace engages in planning at one day, she is more likely to put in more effort in the next day. In the same way, we also see a strong link between organizing and evaluating indicating that time management (organizing) leads to evaluation of progress in the next day. Interestingly, we see that social is followed usually by time management (organizing) which makes sense, that when Grace socializes, she tends to catch up and organizes her duties next day. Negative links in temporal networks mean that behaviors are less likely to happen after each other. We see for instance, that evaluating is less likely to be followed by seeking help and monitoring is less likely to be followed by evaluating.\nOverall, the network is informative and shows show that Grace regulates her learning. She puts her plans into effect by investing effort as well as her time management skills are a driver for evaluation and implementing feedback.\n\n# Visualize the Partial Contemporaneous Correlations (PCC) within the model \n# using a circular layout.\nplot(GraphVAR_Grace, include = \"PCC\", layout = \"circle\", \n     theme = \"colorblind\", labels = Vars, titles = F)\n\n# Similarly, visualize the Partial Directed Correlations (PDC), highlighting \n# temporal directed influences between variables.\nplot(GraphVAR_Grace, include = \"PDC\", layout = \"circle\", \n     theme = \"colorblind\", labels = Vars, titles = F)\n\n\n\n\n\n\n\n(a) Partial Contemporaneous Correlations (PCC)\n\n\n\n\n\n\n\n(b) Partial Directed Correlations (PDC)\n\n\n\n\nFigure 3. Graphical VAR plots\n\n\n\nGiven that the built-in plotting function is very basic, we can augment the plot with the plotting function as below. The plotting function creates a nicer visualization and shows the edge weights to allow judging the actual edge weights.\n\n plotting <- function(Network, title = \"\", ...){\n      qgraph(Network,\n       layout = \"circle\",          # Circular node arrangement\n       title = title,              # Setting a title\n       loop = 0.7,                 # Curvature for self-loops\n       node.width = 2,             # Node width\n       repulsion = 0.8,            # Spacing between nodes\n       label.font = 1,             # Label font styling\n       label.fill.vertical = 1,    \n       label.fill.horizontal = 1,  \n       esize = 7,                  # Edge thickness\n       vsize = 6,                  # Node size\n       color = \"#ffeece\",          # Light pink node color\n       edge.label.cex = 1.5,       # Edge label size\n       edge.width = 1,             # Edge width\n       border.width = 2,           # Border width for nodes\n       edge.labels = TRUE,         # Show edge labels\n       asize = 3,                  # Arrow size\n       labels = Vars,              # Variable labels,\n       negDashed = TRUE,           # Negative edges dashed\n       theme = \"colorblind\", ...   )}\n\nplotting(GraphVAR_Grace$PCC)\nplotting(GraphVAR_Grace$PDC)\n\n\n\n\n\n\n\n(a) Partial Contemporaneous Correlations (PCC)\n\n\n\n\n\n\n\n(b) Partial Directed Correlations (PDC)\n\n\n\n\nFigure 4. Plotting the networks nicely\n\n\n\n\n\n3.4 Estimation of multiple idiographic models n>1\nBesides estimating idiographic models, we can use mlGraphicalVAR to estimate multiple idiographic models for multiple people. Besides idiographic models, mlGraphicalVAR offers an aggregate or an average within-person and between person networks which are kind of approximation of the general group-level picture. As such, the results of mlGraphicalVAR contain several components: a between-person network (betweenNet), which captures the average stationary relationships between variables across different subjects. The betweenNet network is created from the mean values of each person in the data and then creates a partial correlation between the means of each person. A network of correlations between the means represents the stable average dynamics. Please note that we did not have this betweenNet before, given that we had only one student. Also, mlGraphicalVAR produces a fixed-effect partial directed network, fixedPDC, which captures the temporal relationships on average across the whole sample. Similarly, it produces a fixed effects partial contemporaneous network fixedPCC which captures the average contemporaneous correlations across all individuals. Think of these networks as a summary of all the networks in the data across the whole sample.\nMost importantly, the function creates an analysis for each individual (like the one we did for Grace) in the data by setting the subjectNetworks argument to TRUE. In other words, it produces a specific analysis for Bob, Chen, Eve, etc and every other person in the data. To get a specific subject results, we can query the object for that person. For instance, to get get the summary of the model of the first person we can use GraphVAR_all$subjecResults[[1]], to get the contemporaneous network GraphVAR_all$subjectPCC[[i]] and to get the GraphVAR_all$subjectPDC[[i]]. In that way, we can automate the analysis of a large sample and get an a view of the average picture as well as detailed individual analysis of each person.\nTo estimate the mlGraphicalVAR model for multiple people, we use the mlGraphicalVAR function and specify the data df containing the multiple individuals’ responses. Here, we need to specify the idvar = \"name\" and subjectNetworks = TRUE to create the individual analysis.\nAs we said before, the output of the mlGraphicalVAR model includes several components: The fixedPCC, fixedPDC, the betweenNet as well as the subjectPCC (the subject contemporaneous network) and subjectPDC (the subject temporal network) for each of the 36 subjects in the dataset.\nThe code below applies the de-trending function to assess any trends in the data for each person and in case they are present, the functions de-trends these variables. Then, mlGraphicalVAR is used to estimate the models as explained above, one for each person.\n\n3.4.1 Plotting the mlGraphicalVAR results\nmlGraphicalVAR produces two types of networks: the general group-level networks, and subject specific networks. Below, we plot the group level networks with the function we created (plotting). The interpretation of these networks shows the SRL dynamics on average across our sample or the expected common behavior.\nFor instance, the between-person network (Figure 20.5 (a)) shows that students who use feedback always seek help (0.49). Similarly, we see a strong association between effort and organizing (0.33) as well as control and organizing (0.26) and feedback and evaluating (0.26). There is moderate association between organizing and planning (0.22), and effort and social (0.22), as well as help and effort (0.21), and social and planning (0.21). The rest of associations can be interpreted in the same ways.\nIn the contemporaneous network (Figure 20.5 (b)), we see a snapshot of how self-regulated learning constructs are associated with each other within the same day. We see a strong relationship between planning and effort (0.24), indicating that students who actively plan their work tend to regulate their effort in the same time. Another moderate association exists between effort and control (0.16) suggesting that students who manage their effort tend to exert control over their environment in the same time. We can also see other associations, like feedback and evaluating (0.17), feedback and organizing (0.13), and organizing and control (0.15). These dynamics are relatively different from the between-person given the different time-scale.\nIn the temporal network (Figure 20.5 (c)), the arrows represent the direction of influence between self-regulated learning (SRL) constructs over time i.e., from a day to the next. The strongest positive influence is between evaluating and control (0.02), suggesting that social interactions is followed by effort next day which could be a boost or simply that student catch up next day of socializing. There is a feedback look between planning and social (each 0.01), indicating socially-shared regulation. There is a negative influence of organizing on monitoring (-0.02), where these two activities occur in that order.\n\nplotting(GraphVAR_all$betweenNet, minimum = 0.05)\nplotting(GraphVAR_all$fixedPCC, minimum = 0.05)\nplotting(GraphVAR_all$fixedPDC, minimum = 0.01)\n\n\n\n\n\n\n\n(a) Between person\n\n\n\n\n\n\n\n(b) Contemporaneous network\n\n\n\n\n\n\n\n(c) Temporal network\n\n\n\n\nFigure 5. Plotting the mlGraphicalVAR results\n\n\n\n\n\n3.4.2 Plotting and interpreting subject level networks\nTo plot each individual student network, we can use GraphVAR_all$subjectPCC[[i]] and GraphVAR_all$subjectPCC[[i]] to get the contemporaneous and temporal networks respectively, we can also get their names by using GraphVAR_all$ids[[i]] and use the plotting function to plot them. In the code below, we choose randomly five students numbered from 20 to 24 and plot their networks (Figure 20.6). Of course, you can plot the whole dataset by setting the loop to iterate from 1:36 (the number of unique students in the data). The interpretation is similar, but in this case, Judy’s network represents only Judy and cannot be generalized to anyone else. So is the case for Karin, Lars, Layla, and Li. Please note also the vast differences between these students. Each and every network is rather different.\n\nfor (i in 20:24) {\n  # Apply plotting to subjectPCC and subjectPDC\n  plotting(GraphVAR_all$subjectPCC[[i]], minimum = 0.05, \n           title = paste(\"Contemporaneous network for \", GraphVAR_all$ids[[i]]))\n  plotting(GraphVAR_all$subjectPDC[[i]], minimum = 0.05, \n           title = paste(\"Temporal network for \", GraphVAR_all$ids[[i]]))\n}\n\n\n\n\n\n\nFigure 6. Plotting the PCC and PDC individual student networks\n\n\n\n\n\n\n\n3.5 Unified Structural Equation Modeling (uSEM)\nUnified Structural Equation Modeling (uSEM) is another method that has the capability to offer an idiographic analysis of the individual’s behavior as a complex dynamic system [39–42]. As the name implies, uSEM is built around structure equation modelling (described in [43]). The term “unified” means that the estimation method of uSEM combines both temporal (lagged relationships or t-1) and contemporaneous relationships (same time or t) simultaneously (compared to sequential estimation of the temporal and contemporaneous networks in GraphivalVar).\nWhile uSEM share some similarities with graphivalVar, they have differences. In uSEM, the temporal relationships are estimated through a multivariate autoregressive model (MAR) and the contemporaneous relations are estimated through conventional SEM. Another difference is that uSEM estimation processes result in a single matrix that contains both the temporal and the contemporaneous relations (not two separate matrices or networks like Graphical VAR). Last, whereas graphical VAR uses regularization to prune the model, uSEM uses an iterative process where it begins with an empty model and applies Lagrange multiplier tests to assess which parameter, if added, would best improve the model’s fit. In doing so, the model is refined in a step wise process, with pathways added one at a time if they enhance overall fit. While some evidence suggests that Graphical VAR may offer better test-retest consistency in short time series, recent improvements in uSEM, such as integrating regularization may help solve these issues [44, 45].\n\n3.5.1 uSEM analysis\nFor uSEM analysis we will use the pompom R package which is designed for person-specific (idiographic) modeling analysis of multivariate time series data. In particular, pompom takes a hybrid approach that combines intraindividual variability with network analysis to model the individuals as complex dynamic systems. Also, pompom offers impulse response analysis metrics (iRAM) which helps quantify how variables interact dynamically, such as how one node’s perturbation affects others over time (this will not be discussed here given it is beyond the scope of our chapter). In particular, pompom R package uSEM estimates a person-specific model, meaning each individual’s data is analyzed separately [36]. If we want to analyze multiple individuals, we need to do so individually, possibly using a loop (see the section below).\nuSEM requires the data to be standardized so that it captures within-person fluctuations around each person’s own mean. When the data is standardized, the data can be treated as stationary, which allows us to focus on the dynamic, short-term changes in SRL without the influence of between-person differences or long-term trends. The code below standardize the data for Grace by subtracting the mean and dividing by standard deviation.\n\nGrace_data_centered <- Grace_Data |>\n  mutate(across(everything(), ~ scale(.x, center = TRUE, scale = TRUE))) |> \n  select(-Day)\n\nWe then proceed to estimating the uSEM model for Grace using the same data we used before, the nine SRL variables after standardization. The estimation functionuSEM() takes four parameters: data = Grace_data_centered: The dataset which we have standardized to remove scale differences and convert the data to stationary and the var.number = 9 which tells uSEM the number of variables. The lag.order = 1 specifies a time lag of 1, meaning we are analyzing how variables affect each other from one time point to the next.The argument verbose = FALSE tells the model to run quietly. Finally, trim = TRUE tells the model to trim insignificant relationships between variables to simplify the output show only the important interactions.\n\nUsim_Grace <- uSEM(var.number = 9, \n               data = Grace_data_centered, \n               lag.order = 1, \n               verbose = FALSE,\n               trim = TRUE)\n\nFor the uSEM model to be reliable, we need to evaluate fitness criteria. uSEM computes four criteria: Comparative Fit Index (CFI), Tucker-Lewis Index (TLI), Root Mean Square Error of Approximation (RMSEA), and Standardized Root Mean Square Residual (SRMR). Ideally, the model should pass at least three of the evaluation criteria. The CFI and TLI assess how well the model fits the data compared to a baseline model where values closer to 1 indicates a better fit. The RMSEA estimates the error of approximation in the model where lower values (ideally below 0.08) indicates a good fit. The SRMR assesses the difference between observed and predicted correlations, with values below 0.08 being ideal.\nIn the code below, we use the model_summary function to evaluate the fitness of the model. We need to provide the model, tell the function that our data has nine variables, and set the time-lag order 1. The function store the evaluation criteria in Usim_Grace_fit. We can now retrieve the values of TLI, RMSEA, and SRMR from the model fit object Usim_Grace_fit and calculate the CFI by summing its values.\nThe code below retrieves the four criteria, and programmatically evaluates them. We see that the CFI value of 1 indicates a perfect fit, so it is evaluated as “Passed”. Similarly, a TLI value higher than 0.95, an RMSEA of 0 (indicating no error in approximation), and an SRMR below 0.08 also indicate a good fit. Based on these criteria, our model passes four evaluation criteria and is therefore reliable.\n\nUsim_Grace_fit <- model_summary(model.fit = Usim_Grace,\n                          var.number = 9, \n                          lag.order = 1)\n\ncfi_val <- sum(Usim_Grace_fit$cfi)\ntli_val <- Usim_Grace_fit$tli\nrmsea_val <- Usim_Grace_fit$rmsea\nsrmr_val <- Usim_Grace_fit$srmr\n\n# Print the values of each fit index\ncat(\"CFI:\", cfi_val, \"->\", ifelse(cfi_val > 0.95, \"Passed\", \"Failed\"), \"\\n\")\ncat(\"TLI:\", tli_val, \"->\", ifelse(tli_val > 0.95, \"Passed\", \"Failed\"), \"\\n\")\ncat(\"RMSEA:\", rmsea_val, \"->\", ifelse(rmsea_val < 0.08, \"Passed\", \"Failed\"), \"\\n\")\ncat(\"SRMR:\", srmr_val, \"->\", ifelse(srmr_val < 0.08, \"Passed\", \"Failed\"), \"\\n\")\n\n\n\nCFI: 1 -> Passed \n\n\nTLI: 1.001069 -> Passed \n\n\nRMSEA: 0 -> Passed \n\n\nSRMR: 0.04020178 -> Passed \n\n\nThe uSEM networks are a bit different because we get both temporal and contemporaneous relationships in the same plot. Here, the red edges represent negative relationships, the green edges represent positive relationships. Dashed edges indicate lag-1 temporal relationships. Solid edges indicate contemporaneous relationships. Edge width is proportional to the the strength of the relationship where thicker edges indicate stronger positive or negative relationships. As we can see in the plot, Grace has a strong temporal relationship (dashed green line) between planning and effort, suggesting that when planning increases her effort follows. We also see a negative contemporaneous relationship between monitoring and social. Compare this to the previous Graphical Var models. The default plotting function is restrictive and does not show the labels (Figure 20.7 (a)), so, we have created a slightly modified function usemplot to improve the plot and show the labels (Figure 20.7 (b)).\n\nplot_network_graph (Usim_Grace_fit$beta, var.number = 9)\n\nusemplot(Usim_Grace_fit$beta, var.number = 9, \n          labels = colnames(Grace_data_centered))\n\n\n\n\n\n\n\n(a) Defalt plotting function\n\n\n\n\n\n\n\n(b) Custom plotting function\n\n\n\n\nFigure 7. Plotting the uSEM networks\n\n\n\n\n\n3.5.2 Analysis of multiple individuals with uSEM\nGiven that pompom does not have a function that automates the analysis across multiple individuals, we need to iterate through each individual in our dataset and repeat the analysis. However, before doing so, we need to check the data if it has any variables that are completely missing or have no variance e.g., has the same value. We do so by the function nearZeroVar from the caret package.\n\ndf |> \n  group_by(name) |> \n  reframe(across(all_of(Vars), ~ nearZeroVar(as.data.frame(.))))\n\n# A tibble: 0 × 10\n# ℹ 10 variables: name <chr>, planning <int>, monitoring <int>, effort <int>,\n#   control <int>, help <int>, social <int>, organizing <int>, feedback <int>,\n#   evaluating <int>\n\n\nSince none of the data entries present any problems, we can proceed with the analysis. The next code simply iterates through each individual in our dataset (df), applies a uSEM, and fit the data. First, we filter the dataset to isolate data for each individual, then define variables of interest i.e., Vars, and centers/scales the data. A uSEM model is run with the nine variables, and the model’s fit is the estimated.\n\n# Initialize a list to store the results\nresults_list <- list()\n\n# Define a vector of individual names (replace with actual names in your data)\nindividual_names <- unique(df$name)\n\n# Loop through each individual dataset\nfor (individual in individual_names) {\n  \n  # Filter the dataset for the current individual\n  individual_data <- df |> \n    filter(name == individual) |>\n    select(all_of(Vars))\n  \n  # Center and scale the data\n  centered_data <- individual_data |>\n    mutate(across(everything(), ~ scale(.x, center = TRUE, scale = TRUE)))\n  \n  # Perform uSEM operations\n  uSEM_result <- uSEM(var.number = 9, \n                      data = centered_data, \n                      lag.order = 1, \n                      verbose = FALSE,\n                      trim = TRUE)\n  \n  # Get the model summary\n  model_fit <- model_summary(model.fit = uSEM_result, \n                             var.number = 9, \n                             lag.order = 1)\n  \n  \n  # Store the results in the list\n  results_list[[individual]] <- list(\n    uSEM_result = uSEM_result,\n    model_fit = model_fit\n  )\n}\n\nWe then plot each model by iterating through the result list. We show five as an example in Figure 20.8.\n\nfor (i in 20:24) {\n  usemplot(results_list[[i]]$model_fit$beta, var.number = 9, labels = Vars,\n           title = individual_names[i])\n}\n\n\n\n\n\n\nFigure 8. Plotting multiple uSEM networks"
  },
  {
    "objectID": "chapters/ch20-var/ch20-var.html#conclusion",
    "href": "chapters/ch20-var/ch20-var.html#conclusion",
    "title": "20  Idiographic Networks: A tutorial on Graphical Vector Autoregression and Unified Structural Equation Modeling",
    "section": "4 Conclusion",
    "text": "4 Conclusion\nStudying individual processes and the complexity of behavior is essential for understanding human dynamics, particularly in educational and psychological contexts. Behavior is inherently multifaceted, driven by the interplay of cognitive, emotional, and social processes that unfold uniquely for each person. Idiographic approaches, which focus on individual patterns rather than group averages, are crucial for capturing these complexities. They allow researchers to uncover how specific factors influence behavior over time and reveal the dynamic interactions between processes like planning, monitoring, and evaluating in self-regulated learning. This perspective respects the uniqueness of individual trajectories, providing actionable insights for personalized interventions that align with each person’s needs and context. By embracing the complexity of behavior, idiographic methods offer a deeper understanding of human experiences and support more effective, tailored strategies for fostering growth and learning [10].\nThis chapter introduced the use of probabilistic networks, specifically graphical Vector Autoregression (graphicalVAR) and Unified Structural Equation Modeling (uSEM), to study individual learning processes. The idiographic approach provides a personalized, dynamic view of how cognitive, social, and motivational factors interrelate over time and provides insights at N=1. This approach highlights the unique profiles of students.This approach also supports more accurate personalized learning interventions. Given that the modeling captures the unique dynamics of each student, we can tailor strategies that address their specific needs. Identifying key factors that influence performance allows for targeted interventions to improve learning outcomes.\nFurthermore, the integration of both temporal and contemporaneous networks enhances our ability to model the real-time interactions between variables. Temporal networks, which map how one variable predicts another in subsequent time windows, offer insight into the cause-and-effect relationships that govern student behavior over time. Contemporaneous networks, on the other hand, allow us to examine the relationships between variables within the same time window, offering a snapshot of the dynamic interdependencies at a given moment. These two complementary approaches give us a fuller picture of the cognitive, emotional, and behavioral processes that underpin learning. For instance, in our example dataset, motivation was shown to predict work behavior and achievement, while stress, in turn, negatively impacted achievement, underscoring the importance of understanding these processes as interconnected rather than isolated."
  },
  {
    "objectID": "chapters/ch21-mle/ch21-mle.html",
    "href": "chapters/ch21-mle/ch21-mle.html",
    "title": "21  Detecting Long-Memory Psychological Processes in Academic Settings Using Whittle’s Maximum Likelihood Estimator: An Application with R",
    "section": "",
    "text": "Forthcoming\n\n\n\n\n\nReferences\n\n\n1. Gernigon C, Den Hartigh RJR, Vallacher RR, Van Geert PLC (2024) How the Complexity of Psychological Processes Reframes the Issue of Reproducibility in Psychological Science. Perspectives on Psychological Science 19:952–977. https://doi.org/10.1177/17456916231187324\n\n\n2. Van Geert P, De Ruiter N (2022) Toward a Process Approach in Psychology: Stepping into Heraclitus’ River. Cambridge University Press, Cambridge\n\n\n3. Saqr M, Vogelsmeier LVDE, López-Pernas S (2024) Capturing where the learning process takes place: A person-specific and person-centered primer. Learning and Individual Differences 113:102492. https://doi.org/10.1016/j.lindif.2024.102492\n\n\n4. Peng C-K, Mietus J, Hausdorff JM, Havlin S, Stanley HE, Goldberger AL (1993) Long-range anticorrelations and non-Gaussian behavior of the heartbeat. Physical Review Letters 70:1343–1346. https://doi.org/10.1103/PhysRevLett.70.1343\n\n\n5. Mandelbrot BB, Van Ness JW (1968) Fractional Brownian Motions, Fractional Noises and Applications. SIAM Review 10:422–437\n\n\n6. Delignières D, Marmelat V (2013) Theoretical and Methodological Issues in Serial Correlation Analysis. In: Richardson MJ, Riley MA, Shockley K (eds). Springer, New York, NY, pp 127–148\n\n\n7. Phinyomark A, Larracy R, Scheme E (2020) Fractal Analysis of Human Gait Variability via Stride Interval Time Series. Frontiers in Physiology 11: https://doi.org/10.3389/fphys.2020.00333\n\n\n8. Likens AD, Mangalam M, Wong AY, Charles AC, Mills C (2023) Better than DFA? A Bayesian Method for Estimating the Hurst Exponent in Behavioral Sciences. ArXiv arXiv:2301.11262v1\n\n\n9. Mangalam M, Wilson T, Sommerfeld J, Likens AD (2023) Optimizing a bayesian method for estimating the hurst exponent in behavioral sciences\n\n\n10. Roume C (2023) A guide to Whittle maximum likelihood estimator in MATLAB. Frontiers in Network Physiology 3: https://doi.org/10.3389/fnetp.2023.1204757\n\n\n11. Beran J, Feng Y, Ghosh S, Kulik R (2013) Long-Memory Processes: Probabilistic Properties and Statistical Methods. Springer, Berlin, Heidelberg\n\n\n12. Almurad ZMH, Delignières D (2016) Evenly spacing in Detrended Fluctuation Analysis. Physica A: Statistical Mechanics and its Applications 451:63–69. https://doi.org/10.1016/j.physa.2015.12.155\n\n\n13. Diebolt C, Guiraud V (2005) A Note On Long Memory Time Series. Quality and Quantity 39:827–836. https://doi.org/10.1007/s11135-004-0436-z\n\n\n14. Gernigon C, Altamore R, Vallacher RR, Van Geert PLC, Den Hartigh RJR (2024) Almost, but not quite there: Research into the emergence of higher-order motivated behavior should fully embrace the dynamic systems approach. https://doi.org/10.31234/osf.io/rt6wj\n\n\n15. Gernigon C, Vallacher RR, Nowak A, Conroy DE (2015) Rethinking Approach and Avoidance in Achievement Contexts: The Perspective of Dynamical Systems. Review of General Psychology 19:443–457. https://doi.org/10.1037/gpr0000055\n\n\n16. López-Pernas S, Saqr M (2024) How the dynamics of engagement explain the momentum of achievement and the inertia of disengagement: A complex systems theory approach. Computers in Human Behavior 153:108126. https://doi.org/10.1016/j.chb.2023.108126\n\n\n17. Tuller B, Case P, Ding M, Kelso JAS (1994) The nonlinear dynamics of speech categorization. Journal of Experimental Psychology: Human Perception and Performance 20:3–16. https://doi.org/10.1037/0096-1523.20.1.3\n\n\n18. Bandura A (1997) Self-efficacy: The exercise of control. W H Freeman/Times Books/ Henry Holt & Co, New York, NY, US\n\n\n19. Crocker J, Wolfe CT (2001) Contingencies of self-worth. Psychological Review 108:593–623. https://doi.org/10.1037/0033-295X.108.3.593\n\n\n20. Teboul A, Klosek C, Montiny C, Gernigon C (2019) Development and Validation of the Approach-Avoidance System Questionnaire (AASQ). Frontiers in Psychology 10:2531. https://doi.org/10.3389/fpsyg.2019.02531\n\n\n21. Kopra J, Tikka S, Heinäniemi M, López-Pernas S, Saqr M (2024) An R Approach to Data Cleaning and Wrangling for Education Research. In: Saqr M, López-Pernas S (eds). Springer Nature Switzerland, Cham, pp 95–119\n\n\n22. López-Pernas S, Misiejuk K, Tikka S, Kopra J, Heinäniemi M, Saqr M (2024) Visualizing and Reporting Educational Data with R. In: Saqr M, López-Pernas S (eds). Springer Nature Switzerland, Cham, pp 151–194\n\n\n23. Delignières D, Marmelat V (2012) Fractal Fluctuations and Complexity: Current Debates and Future Challenges. Critical Reviews™ in Biomedical Engineering 40: https://doi.org/10.1615/CritRevBiomedEng.2013006727\n\n\n24. Gilden DL (2001) Cognitive emissions of 1/f noise. Psychological Review 108:33–56. https://doi.org/10.1037/0033-295x.108.1.33\n\n\n25. West BJ, Shlesinger M (1990) The Noise in Natural Phenomena. American Scientist 78:40–45\n\n\n26. Delignières D, Fortes M, Ninot G (2004) The fractal dynamics of self-esteem and physical self. Nonlinear Dynamics, Psychology, and Life Sciences 8:479–510\n\n\n27. De Ruiter NMP, Den Hartigh RJR, Cox RFA, Van Geert PLC, Kunnen ES (2015) The Temporal Structure of State Self-Esteem Variability During Parent–Adolescent Interactions: More Than Random Fluctuations. Self and Identity 14:314–333. https://doi.org/10.1080/15298868.2014.994026\n\n\n28. Wong AE, Vallacher RR, Nowak A (2014) Fractal dynamics in self-evaluation reveal self-concept clarity. Nonlinear Dynamics, Psychology, and Life Sciences 18:349–369\n\n\n29. Holden JG, Van Orden GC, Turvey MT (2009) Dispersion of response times reveals cognitive dynamics. Psychological Review 116:318–342. https://doi.org/10.1037/a0014849\n\n\n30. Kello CT, Beltz BC, Holden JG, Van Orden GC (2007) The emergent coordination of cognitive function. Journal of Experimental Psychology: General 136:551–568. https://doi.org/10.1037/0096-3445.136.4.551\n\n\n31. Van Orden GC, Holden JG, Turvey MT (2003) Self-organization of cognitive performance. Journal of Experimental Psychology: General 132:331–350. https://doi.org/10.1037/0096-3445.132.3.331\n\n\n32. Lowie W, Plat R, De Bot K (2014) Pink Noise in Language Production: A Nonlinear Approach to the Multilingual Lexicon. Ecological Psychology 26:216–228. https://doi.org/10.1080/10407413.2014.929479\n\n\n33. Nourrit-Lucas D, Tossa AO, Zélic G, Delignières D (2015) Learning, Motor Skill, and Long-Range Correlations. Journal of Motor Behavior 47:182–189. https://doi.org/10.1080/00222895.2014.967655\n\n\n34. Den Hartigh RJR, Cox RFA, Gernigon C, Yperen NWV, Van Geert PLC (2015) Pink Noise in Rowing Ergometer Performance and the Role of Skill Level. Motor Control 19:355–369. https://doi.org/10.1123/mc.2014-0071\n\n\n35. Den Hartigh RJR, Otten S, Gruszczynska ZM, Hill Y (2021) The Relation Between Complexity and Resilient Motor Performance and the Effects of Differential Learning. Frontiers in Human Neuroscience 15: https://doi.org/10.3389/fnhum.2021.715375\n\n\n36. Delignières D, Torre K (2009) Fractal dynamics of human gait: A reassessment of the 1996 data of Hausdorff et al. Journal of Applied Physiology 106:1272–1279. https://doi.org/10.1152/japplphysiol.90757.2008\n\n\n37. Goldberger AL, Amaral LAN, Hausdorff JM, Ivanov PCh, Peng C-K, Stanley HE (2002) Fractal dynamics in physiology: Alterations with disease and aging. Proceedings of the National Academy of Sciences 99:2466–2472. https://doi.org/10.1073/pnas.012579499\n\n\n38. Hausdorff JM, Mitchell SL, Firtion R, Peng C-K, Cudkowicz ME, Wei JY, Goldberger AL (1997) Altered fractal dynamics of gait: Reduced stride-interval correlations with aging and Huntington’s disease. Journal of Applied Physiology (Bethesda, Md: 1985) 82:262–269. https://doi.org/10.1152/jappl.1997.82.1.262\n\n\n39. Gottschalk A, Bauer MS, Whybrow PC (1995) Evidence of chaotic mood variation in bipolar disorder. Archives of General Psychiatry 52:947–959. https://doi.org/10.1001/archpsyc.1995.03950230061009\n\n\n40. Wijnants ML, Hasselman F, Cox RFA, Bosman AMT, Van Orden G (2012) An interaction-dominant perspective on reading fluency and dyslexia. Annals of Dyslexia 62:100–119. https://doi.org/10.1007/s11881-012-0067-3\n\n\n41. Wijnants ML, Bosman AMT, Hasselman F, Cox RFA, Van Orden GC (2009) 1/f scaling in movement time changes with practice in precision aiming. Nonlinear Dynamics, Psychology, and Life Sciences 13:79–98\n\n\n42. Arrieta C, Navarro J, Vicente S (2008) Factores asociados a la emergencia de patrones diferenciales de la motivación en el trabajo. [Variables related to the emergence of differential patterns in work motivation.]. Psicothema 20:745–752\n\n\n43. Navarro J, Rueff-Lopes R (2015) Healthy Variability in Organizational Behavior: Empirical Evidence and New Steps for Future Research. Nonlinear Dynamics, Psychology, and Life Sciences 19:529–552\n\n\n44. Den Hartigh RJR, Cox RFA, Van Geert PLC (2017) Complex versus Complicated Models of Cognition. In: Magnani L, Bertolotti T (eds). Springer International Publishing, Cham, pp 657–669\n\n\n45. Wallot S, Kelty-Stephen DG (2018) Interaction-Dominant Causation in Mind and Brain, and Its Implication for Questions of Generalization and Replication. Minds and Machines 28:353–374. https://doi.org/10.1007/s11023-017-9455-0\n\n\n46. Fisher AJ, Medaglia JD, Jeronimus BF (2018) Lack of group-to-individual generalizability is a threat to human subjects research. Proceedings of the National Academy of Sciences 115:E6106–E6115. https://doi.org/10.1073/pnas.1711978115\n\n\n47. Hamaker EL (2024) The Curious Case of the Cross-Sectional Correlation. Multivariate Behavioral Research 59:1111–1122. https://doi.org/10.1080/00273171.2022.2155930\n\n\n48. Molenaar PCM, Campbell CG (2009) The New Person-Specific Paradigm in Psychology. Current Directions in Psychological Science 18:112–117. https://doi.org/10.1111/j.1467-8721.2009.01619.x\n\n\n49. Harrar K, Hamami L, Lespessailles E, Jennane R (2013) Piecewise Whittle estimator for trabecular bone radiograph characterization. Biomedical Signal Processing and Control 8:657–666. https://doi.org/10.1016/j.bspc.2013.06.009\n\n\n50. Koopmans M (2015) A Dynamical View of High School Attendance: An Assessment of Short-term and Long-term Dependencies in Five Urban Schools. Nonlinear Dynamics, Psychology, and Life Sciences 19:65–80\n\n\n51. Koopmans M (2018) On the Pervasiveness of Long Range Memory Processes in Daily High School Attendance Rates. Nonlinear Dynamics, Psychology, and Life Sciences 22:243–262\n\n\n52. Hilpert JC, Marchand GC (2018) Complex Systems Research in Educational Psychology: Aligning Theory and Method. Educational Psychologist 53:185–202. https://doi.org/10.1080/00461520.2018.1469411\n\n\n53. Koopmans M (2020) Education is a Complex Dynamical System: Challenges for Research. The Journal of Experimental Education 88:358–374. https://doi.org/10.1080/00220973.2019.1566199\n\n\n\n\n\n\nWhite noise is a stochastic process, that is, which is reflected by a signal that unfolds randomly over time and thus has a null autocorrelation.↩︎\nOriginally, Brownian motion is the random movement of a particle in a liquid or gas due to successive shocks from surrounding molecules.↩︎\n1/\\(f\\) noise or pink noise features distributions of temporal fluctuations of a phenomenon displaying the following properties: The frequencies of fluctuations remain invariant across time scales; the frequency of a particular magnitude of fluctuation is inversely proportional to that magnitude; lagged autocorrelations decay slowly, as an inverse power of lag (i.e., long-range temporal correlations).↩︎\nARFIMA (Autoregressive Fractionally Integrated Moving Average) models are statistical tools that model time series with long memory. By allowing the differencing parameter \\(d\\) to take fractional values, they generalize ARIMA (Autoregressive Integrated Moving Average) models, which themselves generalize ARMA (Autoregressive Moving Average) models to non-stationary series.↩︎"
  },
  {
    "objectID": "chapters/ch22-automl/ch22-automl.html",
    "href": "chapters/ch22-automl/ch22-automl.html",
    "title": "22  Automating Individualized Machine Learning and AI Prediction using AutoML: The Case of Idiographic Predictions",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch22-automl/ch22-automl.html#introduction",
    "href": "chapters/ch22-automl/ch22-automl.html#introduction",
    "title": "22  Automating Individualized Machine Learning and AI Prediction using AutoML: The Case of Idiographic Predictions",
    "section": "1 Introduction",
    "text": "1 Introduction\nPredictive analytics is a branch of analytics that uses students’ data, statistical algorithms, or machine learning (ML) techniques to forecast future outcomes [1] [2]; [3]; [4]. Prediction is important for developing theories, hypothesis testing, and evaluating the relevance of the theoretical knowledge to research and practice. Therefore, prediction has been a central pillar of of scientific research [5]. Moreover, predictive analytics enables organizations to make informed decisions, optimize resources, and anticipate future risks or opportunities [6]. As such, predictive analytics has been widely adopted across industries, from healthcare to marketing, due to its ability to transform data into actionable insights. As data becomes increasingly diverse and voluminous, predictive analytics plays a critical role in handling big data and providing actionable insights [7]; [4]; [8], which has been a central interest of researchers in learning analytics and artificial intelligence (AI) in education [9].\nIn the previous chapters we studied modeling full datasets and explained them [10–13]. In this chapter we will learn how to model each student individually as a whole unit of analysis. In other words, the full dataset belongs to a single student. This is done by collecting several data points from the same student over time that is enough for building a predictive model for the student. These models are known as idiographic e.g., [8, 14, 15]. Idiographic models are novel and promise accurate insights at the resolution of single students. For a detailed reading about the these models, interested readers can resort to chapter [15] or these papers [8], [16–19].\nGiven the need to model students individually, the process needs to be automated to process multiple datasets. Therefore, in this chapter, we will learn how to implement AutoML for a large number of idiographic datasets using h2o, which is a cross-platform open-source, scalable machine learning framework that supports a wide range of algorithms [20]. Among the algorithms that h2o supports are generalized linear models (GLM), gradient boosting machines (GBM), random forests (RF), deep learning (DL), k-means clustering, and extreme gradient boosting (XGBoost). H2O is designed to handle large datasets can efficiently use distributed computing capabilities. The h2o package works seamlessly with R and is also available for Python among others. It performs automation of the ML process from model selection and hyperparameter tuning and reporting of results. Also, h2o includes tools for model explanation and supports easy visualization and plotting of results [20]."
  },
  {
    "objectID": "chapters/ch22-automl/ch22-automl.html#automated-machine-learning-automl",
    "href": "chapters/ch22-automl/ch22-automl.html#automated-machine-learning-automl",
    "title": "22  Automating Individualized Machine Learning and AI Prediction using AutoML: The Case of Idiographic Predictions",
    "section": "2 Automated Machine Learning (AutoML)",
    "text": "2 Automated Machine Learning (AutoML)\nThe core of predictive analytics lies in building predictive models, which are often built using techniques such as regression analysis, decision trees, and neural networks [2, 4]. These models are trained to recognize complex relationships within the data, making it possible to forecast future trends with a degree of confidence [2, 7]. Constructing such predictive models entails several steps which are often repetitive which roughly includes data pre-processing and preparation for analysis, feature engineering and selection to identify the relevant variables. Next, model selection, training and evaluation of model’s performance . Based on these results, the model is tuned and optimized for better accuracy [10, 11].\nSince the steps to conduct the predictive analytics workflow are mostly the same each time, it is possible to automate them using AutoML [21–23]. AutoML is a relatively novel and growing area in machine learning (ML) that aims to —as the name implies— automate the process of applying ML to real-world problems [24, 25]. In fast-paced environments, models need to be updated and redeployed regularly to adapt to new data or changing conditions. AutoML automates many steps of the ML life cycle, making it feasible to frequently refresh models as part of a continuous integration pipeline. Furthermore, when working with a large number of similar datasets, it becomes impractical to repeat the ML process for each manually. As such, AutoML allows to automate this process, optimizing ML model to each specific dataset. Besides the obvious scalability benefits, research has found that AutoML can achieve good results as traditional ML with expert tuning [8, 24, 25].\nThe core function of AutoML lies in optimizing ML workflows, from data pre-processing to model deployment. Recent advancements in AutoML techniques have led to substantial improvements in predictive performance [26]. For example, the framework now encompasses methodologies such as neural architecture search, which automatically designs neural networks tailored to specific tasks. This has led to models that achieve state-of-the-art performance across numerous benchmarks [27]. Additionally, the introduction of automated feature engineering techniques enables models to learn relevant features from raw data, enhancing model accuracy without extensive manual intervention [28].\nFurthermore, AutoML streamlines the process of feature selection, making it easier to handle high-dimensional data [20]. The automation of hyperparameter tuning and model selection significantly reduces the time required for model development. AutoML systems may outperform traditional methods in terms of efficiency, leading to quicker time-to-insight and facilitating more rapid iterations in model improvement [29]. AutoML can also achieve competitive or superior performance compared to human experts. Through exhaustive search strategies and advanced algorithms, AutoML often yield models that may surpass human-crafted solutions [30]. In doing so, AutoML can make high-quality models accessible to non-experts while also reducing the time and computational costs traditionally associated with ML development [23]. Further, automating the time-consuming aspects of model development allows AutoML tools to lower the barriers for organizations and individuals to utilize ML, enabling a broader range of industries to integrate AI-driven solutions [31]. This is increasingly relevant, as organizations across various sectors progressively rely on data-driven decision-making to maintain competitive advantage.\nThere are also several challenges and opportunities within AutoML. One prominent challenge is the computational cost, particularly as the demand for larger and more complex models (such as large language models, LLMs) grows. Applying AutoML to such models, which require enormous computational resources, poses difficulties for scaling and sustainability [21]. Another issue is the environmental impact of AutoML processes, as they consume significant energy and computational resources. The integration of “Green AI” principles, which emphasize energy-efficient algorithms and workflows, has been proposed to address these concerns [32]."
  },
  {
    "objectID": "chapters/ch22-automl/ch22-automl.html#automl-in-education-research",
    "href": "chapters/ch22-automl/ch22-automl.html#automl-in-education-research",
    "title": "22  Automating Individualized Machine Learning and AI Prediction using AutoML: The Case of Idiographic Predictions",
    "section": "3 AutoML in education research",
    "text": "3 AutoML in education research\nThe use of AutoML has not been very prominent in the education scientific literature. However, an increasing number of studies recognize its potential. The study by Garmpis et al. [22] addressed the challenges posed by recent policy changes in higher education system in Greece, which led to departmental unification and increased dropout rates. The researchers utilized AutoML to identify the best predictive models and parameters and analyzed a dataset of 23,687 students enrolled in the “Computer Networking” module, employing the open-source data science platform for model development and evaluation. The model’s accuracy was assessed using established evaluation metrics, including precision, recall, and F-measure. Their findings indicated that the model accurately predicted approximately 92% of student performance, specifically identifying students at risk of dropping out or facing delays in graduation. This approach demonstrates the potential of AutoML in enhancing educational outcomes and supporting strategic interventions for student success [22]. Similarly, Zhao and Wang [33] used AutoML to predict online learning adaptability and achieved a high accuracy of 90.9%. Their empirical findings showed that AutoML yielded higher performance than traditional methods for all the evaluated metrics.\nIn another study, Bosch [34] conducted experiments using AutoML to streamline feature engineering for student models in the National Assessment of Educational Progress data mining competition. Bosch compared two AutoML feature engineering methods, Featuretools and TSFRESH (Time Series FeatuRe Extraction on basis of Scalable Hypothesis tests), which had rarely been applied to student interaction log data. The TSFRESH method provides a novel feature selection technique to manage over 4,000 heterogeneous features with a relatively small student sample, The study findings revealed that TSFRESH features were significantly more effective than both Featuretools and expert-engineered features [34]. However, the interpretability of these models remains limited. To overcome the limitation of interoperability in Bosch’s study [34], Buñay-Guisñan and colleagues [35] developed a dashboard that combines AutoML and Explainable AI to predict student dropout, allowing non-experts to upload datasets and automatically generate predictive models, along with counterfactual explanations to help users understand the factors affecting student dropout.\nIn another study, Tsiakmaki et al. [36] focused on predicting students’ academic performance, researchers explored the use of AutoML strategies, a critical area within Educational Data Mining and Learning Analytics. Recognizing the need for accurate learning models at a low cost, they introduced a fuzzy-based active learning method that integrates AutoML practices in a modular fashion. This approach leverages fuzzy logic to create high-performance and transparent models while reducing the time and cost of labeling data by effectively utilizing a small set of labeled data alongside a larger set of unlabeled data. Given the complexity of ML algorithms and the high-dimensional input space, selecting the right method and optimizing parameters can be challenging. To address this, they conducted numerous experiments that demonstrated the efficiency of their proposed method in accurately identifying students at risk of failure. These insights not only have the potential to enhance the learning experience but also contribute significantly to the broader science of learning.\nThe few existing research studies in AutoML appear to take advantage of this technique to optimize their models and to make ML accessible to non-experts. However, the potential of AutoML to automate the ML pipeline for many individual models to enable idiographic ML, has barely been fulfilled. To the knowledge of the authors, only one article has come close to implementing this technique to automatically create, fit and optimize idiographic models [8], which is the aim of this tutorial. In said study, the authors automated the feature selection, as well as the model training and evaluation of the prediction of students’ variables related to self-regulated learning based on previous measures."
  },
  {
    "objectID": "chapters/ch22-automl/ch22-automl.html#tutorial-using-h2o-for-ml-for-idiographic-ml",
    "href": "chapters/ch22-automl/ch22-automl.html#tutorial-using-h2o-for-ml-for-idiographic-ml",
    "title": "22  Automating Individualized Machine Learning and AI Prediction using AutoML: The Case of Idiographic Predictions",
    "section": "4 Tutorial: Using h2o for ML for idiographic ML",
    "text": "4 Tutorial: Using h2o for ML for idiographic ML\nIn this tutorial, we will explore how to use the h2o ML platform to build, evaluate, and compare idiographic models. It is assumed that the reader is familiar with the R programming language. If that is not the case, it is recommended to refer to previous tutorials on the basics of R [37], data cleaning [38], basic statistics [39], and visualization [40], as well as prediction [2, 10, 10, 11] and explainable AI [12, 13]. When working with idiographic analysis, we treat each individual person as an independent dataset and, therefore, we need a separate ML model for each person [15]. Implementing all steps of the ML pipeline for each individual dataset is extremely time consuming for researchers, and impossible to do in real time if the ML models are to be deployed and regularly updated when new data is collected. Therefore, AutoML is more suitable for idiographic analysis.\nAs a first step, we need to load the essential R packages. We load the tidyverse package for data manipulation tasks, and the h2o package for AutoML. The next code loads the dataset with the readRDS() function. We also set a random seed that ensures the split is reproducible, helping to maintain consistency in results.\n\nset.seed(265)\n# Load necessary libraries\nlibrary(tidyverse)  # For data manipulation\nlibrary(h2o)    # For H2O ML platform\n\n# Read the dataset\nsynthetic_data <- import(\"https://github.com/lamethods/data2/raw/main/srl/srl.RDS\")\n\n\n\n\nThe dataset used in this tutorial is a synthetic dataset based on the work by [8] which collected data using the Concise Self-Regulation Survey (CSRL) [41], as well as phone sensor and contextual data. However, we will use only the a combination of CSRL and sensor data here. In the next step, we select and define the variables that we will use as inputs for our ML models. We start by categorizing the variables into two groups: phone activity variables and self-regulated learning (SRL) variables.\nThe phone activity variables represent various physical and behavioral patterns related to daily activities, giving us insights into a person’s physical and digital behavior, which may affect learning performance.\nThese include the following features:\n\n\"meanpace\": average pace of movement\n\"steps\": the number of steps taken\n\"freqscreen\": frequency of screen use\n\"duration\": duration of screen usage\n\"scrrenoff\": time the screen is off\n\"nightduroff\": duration of screen-off time during the night\n\"maxoffscreennight\": the longest period during which the screen is off at night.\n\nThe SRL variables capture key aspects of cognitive and emotional self-regulation, which are crucial for understanding how learners manage and control their learning processes. These variables are derived from self-reported data reflecting various self-regulated learning strategies. They include:\n\n\"efficacy\": The belief in one’s ability to complete learning tasks effectively.\n\"value\": The perceived importance or value of learning tasks.\n\"planning\": The ability to plan and organize learning activities ahead of time.\n\"monitoring\": Keeping track of learning progress and task completion.\n\"effort\": The amount of effort put into learning tasks.\n\"control\": The ability to resist distractions and stay focused on learning.\n\"help\": Seeking help from teachers, friends, or other resources when needed.\n\"social\": Interactions and feelings of belonging within the college or learning community.\n\"organizing\": Organizing and structuring study time and materials.\n\"motivated\": The level of motivation and enthusiasm towards learning and achieving better grades.\n\"feedback\": Learning from feedback to improve performance.\n\"evaluating\": Self-evaluating one’s work to improve skills and performance.\n\"anxiety\": The level of anxiety or stress experienced during learning or in tasks.\n\"enjoyment\": The amount of enjoyment and satisfaction gained from completing tasks and achieving goals.\n\nNext, we combine both the self-regulated learning (SRL) variables and the phone activity variables into a single vector of predictors. We then define the response variable as \"learning\" (how much the student learnt during this day or period or gained knowledge), which serves as the target variable that the model will attempt to predict based on the input predictors.\n\n# Define the phone activity variables\nphone_vars <- c(\"meanpace\", \"steps\", \"freqscreen\", \"duration\", \n                \"scrrenoff\", \"nightduroff\", \"maxoffscreennight\")\n\n# Define the SRL variables\nsrl_vars <- colnames(select(synthetic_data, efficacy:enjoyment))\n\n# Combine all variables into the Predictors list\npredictors <- c(srl_vars, phone_vars)\n\n# Set the response variable\nresponse_variable <- \"learning\"\n\n\n4.1 Building an idiographic model\nBefore working with the H2O AutoML framework, we need to initialize the h2o cluster. This involves allocating resources such as the number of threads and the maximum memory size for the computation. Here, initialize h2o with the nthreads parameter set to 6, which allows h2o to utilize 6 CPU cores. Additionally, the max_mem_size parameter is set to \"12G\", specifying that up to 12GB of memory can be used for model training and processing. You can adjust these settings based on your system’s resources to ensure optimal performance during model training and evaluation. The initialization results in a “Connection successful!” message and h2o prints the h2o cluster information which include the up time, the timezone and detailed version information.\n\n# Step 1: Initialize h2o with specified resources\nh2o.init(nthreads = 12, max_mem_size = \"12G\")  # Adjust memory and threads if necessary\n\nGiven that idiographic models are single-subject by definition, we need to select a single student from the dataset who will be the target of our initial analysis. Using the unique() function, we extract a list of unique student names and choose a particular student by specifying their index (e.g., the 5th student which carries the fake name Alice). Once Alice is selected, we filter the dataset using the filter() function from the dplyr package to include only the rows that correspond to Alice’s data (the 5th student). After filtering, we remove the \"name\" column, as it is no longer needed with the select() function. This results in a filtered dataset that contains only the relevant variables for Alice for further analysis. If we know the student already, we could of course use a simpler way by filtering only Alice data like the code below using synthetic_data %>% filter(name == \"Alice\")\n\n# Step 2: Retrieve the specific name (e.g., the first unique name)\nspecific_name <- unique(synthetic_data$name)[5]  \n\n# Step 3: Filter the dataset for the selected person and remove the 'name' column\nfiltered_data <- synthetic_data %>%\n  filter(name == specific_name) %>%  # Filter rows where 'name' matches the selected person\n  select(-name)  # Exclude the 'name' column from the data\n\n# An alternative approach to selecting\nfiltered_data <- synthetic_data %>%\n  filter(name == \"Alice\") %>%  # Filter rows where 'name' matches the selected person\n  select(-name)  # Exclude the 'Alice' column from the data\n\nThe following steps are similar to traditional ML with some minor differences. We prepare the data by first converting it into an h2o data frame by splitting it into training and testing sets, and defining the predictors and response variable. The data transformation into an h2o format is done using as.h2o() function. This transformation is necessary because h2o can only work with h2o data format. h2o’s requires this format for better optimization and efficient distributed processing and parallel computations which standard R data frames cannot provide.\n\n# Disable progress bar for clarity\nh2o.no_progress()\n# Step 4: Convert the filtered data into an h2o frame\nh2o_data <- as.h2o(filtered_data)  # Convert into an h2o format for processing\n\nNext, we split the data into training and testing sets using h2o.splitFrame() and ratios = 0.8, which means 80% of the data and will be used to train the model, while the remaining 20% is reserved for testing. Splitting the data is standard procedure for evaluating the model’s performance on unseen data and ensuring it generalizes well, also, we will use the testing set to evaluate the model. We also ensure that the seed is set using seed = 256 for reproduciblity.\n\n# Step 5: Split the h2o frame into training (80%) and testing (20%) sets\n# Use seed for reproducibility\nsplits <- h2o.splitFrame(data = h2o_data, ratios = 0.8, seed = 256)  \ntrain_data_h2o <- splits[[1]]  # Training data (80%)\ntest_data_h2o <- splits[[2]]   # Testing data (20%)\n\nFinally, we define the predictors and the response variable. The predictors include all the relevant features in the dataset except the target outcome, which is the \"learning\" variable. The model will use these predictors to learn and make predictions, with the response variable representing the learning outcome we aim to predict.\n\n# Step 6: Define predictors and response variable for model training\npredictors <- setdiff(names(filtered_data), \"learning\")  \nresponse_variable <- \"learning\"  # Specify the response variable name\n\nNext, we will use h2o’s AutoML which will automate the complete ML pipeline for us. The function h2o.automl() simplifies the process of model training and evaluation by automatically testing multiple algorithms and performing hyperparameter tuning within a user-defined time limit. It stores all the models in the results, allowing us to easily compare their performance. Let’s go through the parameters one by one.\n\nx = predictors: This specifies the predictor variables or features that the model will use to make predictions. The predictors list includes all the variables we want the model to learn from, excluding the target variable which are the SRL and phone activity data.\ny = response_variable: This is the response variable, also known as the target or dependent variable. In this case, it’s set to \"learning\", which represents the outcome we are trying to predict.\ntraining_frame = train_data_h2o: The training_frame is the dataset used to train the model, which must be an h2o Frame. In this case, it’s the 80% split of the filtered data that we converted into an h2o Frame.\nnfolds = 5: This specifies the number of cross-validation folds. Cross-validation is a technique where the data is split into multiple parts, or “folds,” and the model is trained on different subsets of the data while being validated on the remaining parts. By setting nfolds = 5, the training data is split into 5 folds, helping ensure that the model generalizes well to new data and doesn’t over-fit and gives an average of the five training folds.\nmax_runtime_secs = 900: This sets the maximum time (in seconds) that the AutoML process can run. It limits the overall time spent on training and evaluating models. In this case, we set a short time limit of 900 seconds, so the process will stop once this limit is reached, even if all possible models haven’t been trained. This parameter can be adjusted based on the available computational resources and the task.\nseed = 256: When you specify a seed, the model training process can be repeated with the same results. This is important when you want to compare results or ensure consistency across different runs.\n\n\n# Step 7: Train the model using h2o AutoML\nautoml_model <- h2o.automl(\n  x = predictors,  # Predictor variables\n  y = response_variable,  # Response variable\n  training_frame = train_data_h2o,  # Training data\n  nfolds = 5,  # Number of cross-validation folds\n  max_runtime_secs = 900,  # Maximum runtime in seconds\n  seed = 256  # Random seed for reproducibility\n)\n\n\n\n\nAfter h2o AutoML completes the job of training the models, it creates a leaderboard (a list models ranked from best to worst performing model). The leaderboard allows us to rank the resulting models that were estimated with different algorithms and hyperparameters. Given that there are multiple metrics to evaluate performance, there are several ways also to rank the algorithms. The h2o default methods for regression is Root Mean Squared Error (RMSE). To get the leaderboard, we use h2o.get_leaderboard we supply the automl_model given that it the object that contains the models, and set extra_columns = \"ALL\" to get all the possible evaluation parameters. To view the top 10 models in the leaderboard, we use the head(leaderboard, 10) function which will display a ranked list of models trained by AutoML.\n\n# Step 8: View the leaderboard of models\nleaderboard <- h2o.get_leaderboard(automl_model, extra_columns = \"ALL\")\nhead(leaderboard, 10)\n\n\n\n\n\nTable 1. Leaderboard showing a list models ranked from best to worst performing model.\n\n\n\n\n\n\n\n\n\n\n\n\nmodel_id\nrmse\nmse\nmae\nrmsle\nmean_res_dev\ntraining_ms\nalgo\n\n\n\n\nGBM_grid_1_AutoML_93_20240930_00046_model_57\n9.03\n81.53\n3.07\n0.39\n81.53\n384\nGBM\n\n\nStackedEnsemble_BestOfFamily_3_AutoML_93_20240930_00046\n9.16\n83.91\n3.50\n0.38\n83.91\n110\nStackedEnsemble\n\n\nStackedEnsemble_BestOfFamily_5_AutoML_93_20240930_00046\n9.41\n88.57\n3.95\n0.45\n88.57\n215\nStackedEnsemble\n\n\nStackedEnsemble_BestOfFamily_4_AutoML_93_20240930_00046\n9.64\n92.88\n5.15\n0.53\n92.88\n514\nStackedEnsemble\n\n\nGBM_grid_1_AutoML_93_20240930_00046_model_2\n9.68\n93.80\n3.39\n0.39\n93.80\n472\nGBM\n\n\nGBM_grid_1_AutoML_93_20240930_00046_model_54\n9.76\n95.30\n3.24\n0.40\n95.30\n354\nGBM\n\n\nGBM_grid_1_AutoML_93_20240930_00046_model_3\n10.02\n100.38\n3.45\n0.42\n100.38\n284\nGBM\n\n\nGBM_grid_1_AutoML_93_20240930_00046_model_63\n10.34\n106.96\n3.23\n0.40\n106.96\n462\nGBM\n\n\nGBM_grid_1_AutoML_93_20240930_00046_model_35\n10.36\n107.31\n3.33\n0.41\n107.31\n265\nGBM\n\n\nGBM_5_AutoML_93_20240930_00046\n10.41\n108.45\n3.64\n0.40\n108.45\n204\nGBM\n\n\n\n\n\n\nThe leaderboard has several fields which we will discuss (Table 22.1). The model ID identifies each model and specifies the algorithm used for training. For instance, the model named GBM_grid_1_AutoML_93_20240930_00046_model_57 refers to a Gradient Boosting Machine (GBM) model while StackedEnsemble_BestOfFamily_3_AutoML_93_20240930 uses the StackedEnsemble algorithm. The h2o package uses a naming convention that can provide insights about the model. In that, StackedEnsemble_BestOfFamily_3_AutoML_93_20240930 indicates a stacked ensemble model that selects the best-performing models from the AutoML run, as indicated by “BestOfFamily.” The number “3” shows that it is the third iteration of the model. “AutoML_93” means it is part of the 93rd AutoML run, and “20240930” is the date the AutoML process was executed, formatted as YYYYMMDD (September 30, 2024). The next field, RMSE (Root Mean Squared Error), is a well-known metric for evaluating regression models that measures the average prediction error. For example, the model GBM_grid_1_AutoML_93_20240930_00046_model_57 has an RMSE of 9.03, which indicates that its predictions deviate by an average of 9.03 units from the actual values of the target variable (learning). Since it has the lowest RMSE on the leaderboard, this model is the best-performing model in our analysis. Please note that our variables scale ranges from 0-100 so, 9 points is still small. MSE (Mean Squared Error) is another evaluation metric that refers to the squared average prediction error. The GBM_grid_1_AutoML_93_20240930_00046_model_57 model has a value of 81.53 and confirming its strong performance. MAE (Mean Absolute Error) measures the average magnitude of the errors in the predictions, without considering their direction. In this case, GBM_grid_1_AutoML_93_20240930_00046_model_57 has the lowest MAE at 3.07, indicating that its absolute prediction errors are smaller, on average, compared to other models, which have MAEs around 3.50–5.15. RMSLE (Root Mean Squared Logarithmic Error) is particularly useful when the target variable exhibits large variations. The StackedEnsemble_BestOfFamily_3_AutoML_93_20240930 has an RMSLE of 0.38, showing good performance in scenarios with varying target values. Mean Residual Deviance is similar to MSE and represents the average error per prediction. The GBM_grid_1_AutoML_93_20240930_00046_model_57 has the lowest value at 81.53, indicating strong predictive accuracy. Finally, the training time (ms) measures how long it took to train the model. The StackedEnsemble_BestOfFamily_3_AutoML_93_20240930 model trained in 110 milliseconds, while the GBM_grid_1_AutoML_93_20240930_00046_model_57 took 384 milliseconds.\n\n\n\n\n4.1.1 Best models\nTo retrieve the best model based on pre-set criteria we can use the function h2o.get_best_model, which allow us to specify the metric that we want to use to select the best model or the algorithm (e.g., RF, GBM or GLM). To do so, we use the function h2o.get_best_model() with the parameter automl_model to obtain the best model and assign it to an R object best_model. In that case, h2o will retrieve the best model based on RMSE since it is the default ranking criteria. If you want to change that, you can use criterion = \"MAE\" for instance or other metrics e.g., RMSE.\n\n# Step 9: Extract the best-performing model\nbest_model <- h2o.get_best_model(automl_model)\n\n\n\n\n\n\n4.1.2 Evaluation of the model\nTo evaluate how the model could perform in the wild we need to use completely new data that the model has not seen before. To test the model performance with data that the model has not seen before we can use h2o.performance(best_model) with the parameter newdata = test_data_h2o. This enables us to understand what the model can do in real-life applications. The h2o.performance function calculates several metrics such as MSE, RMSE, MAE, and RMSLE.\n\n# Step 9.1: Evaluate the best model's performance on the test data\nbest_model_performance <- h2o.performance(best_model, newdata = test_data_h2o)\n\n# Step 9.2: Print the performance metrics on test data\nprint(best_model_performance)\n\n# Step 9.3: Generate predictions on the test data\npredictions <- h2o.predict(best_model, test_data_h2o)\n\nIn our case, the metrics for the best GBM model were:\n\nMSE: 159.6543\nRMSE: 12.63544\nMAE: 4.978724\nRMSLE: 0.2364497\nMean Residual Deviance: 159.6543\n\nThe RMSE indicates that the GBM model’s predictions were 12.64 points on average different from the values of the target variable (learning). This RMSE suggests that the model has a reasonable level of error in predicting the target variable. Similarly, the MSE (Mean Squared Error) is 159.65, which reflects the squared average of the errors, while the MAE is 4.98, showing the average absolute difference between predicted and actual values. The RMSLE is 0.24, which is helpful for understanding the model’s performance when the target variable has large variations. The Mean Residual Deviance is also 159.65, representing the average error per prediction, which aligns with the MSE. Please note that these parameters are different than the above ones in the leaderboard table, since we evaluated the model with unseen data, and it is expected to see some drop in model performance.\nTo visualize the evaluation, we can create predictions on the test data using h2o.predict(best_model, test_data_h2o) to visualize the model predictions with the new data. We do so by plotting the predicted versus actual values using a scatter plot, as demonstrated below. Note that we must convert the h2o data frames into standard data frames to use them with traditional R packages. It may be also helpful to analyze the residuals (the differences between the actual and predicted values). Ideally, residuals should be randomly distributed around zero without any clear patterns. The code below computes the residuals, and plots them. As the figure shows, we can see that most observations are around 0 however, some extreme values are also there indicating that the model fared bad in some days. This is of course expected given the variability of daily performance.\n\n#Step 2: Convert the test data and predictions to R data frames}\ntest_data_df <- as.data.frame(test_data_h2o) # Convert h2o test data to dataframe\npredictions_df <- as.data.frame(predictions)# Convert h2o predictions to dataframe\n\n# Step 3: Combine the predictions with the original test data\nresult_df <- cbind(test_data_df, Predicted_Learning = predictions_df$predict)\n\n# Scatterplot of actual vs predicted values\nggplot(result_df, aes(x = learning, y = Predicted_Learning)) +\n  geom_point(color = \"blue\", alpha = 0.5) +  # Scatter points\n  # Ideal line (y = x)\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +  \n  labs(x = \"Actual Learning\",\n       y = \"Predicted Learning\") +\n  theme_minimal()\n\n# Create a residual column\nresult_df <- result_df %>%\n  mutate(Residuals = learning - Predicted_Learning)\n\n# Scatterplot of residuals vs predicted values\nggplot(result_df, aes(x = Predicted_Learning, y = Residuals)) +\n  geom_point(color = \"darkred\", alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(x = \"Predicted Learning\",\n       y = \"Residuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n(a) Actual vs. Predicted Learning\n\n\n\n\n\n\n\n(b) Residuals vs. Predicted Learning\n\n\n\n\nFigure 1. Visualization of the evaluation\n\n\nHowever, the best model chosen automatically by h2o may not be the optimal solution for every case. One has to balance accuracy, training time, and model interpretability. For instance, GBM and XGBoost are capable of handling complex relationships and often provide good predictive performance maybe computationally expensive. Random Forest (DRF) is a faster algorithm and is particularity distinguished for its inherent interpretability making it suitable when simplicity and explainaility are needed. Deep Learning is adept at capturing non-linear patterns is also computationally expensive and harder to explain. Stacked ensembles combine several models to enhance performance but increases complexity and training time. In our case, we need a model that offers performance and be explainable e.g., GBM or DRF.\nTo select a certain model, h2o also allows us to extract top algorithms by type by specifying them in the function call e.g., GBM, XGBoost, DRF, DeepLearning, and StackedEnsemble. To do so, we call the function h2o.get_best_model with the parameter algorithm = \"algorithm_name\". For example, to get the best GBM model, we call h2o.get_best_model(automl_model, algorithm = \"gbm\") and then we store it in the object best_gbm. In the same way, we can get other models like the code below.\nThe next code retrieves the best performing models for various ML algorithms. Specifically, it gets the best GBM, XGBoost, Distributed Random Forest, Deep Learning, and Stacked Ensemble models, and stores them in the best_gbm, best_xgboost, best_drf, best_deeplearning, and best_stackedensemble variables respectively.\n\nbest_gbm <- h2o.get_best_model(automl_model, algorithm = \"gbm\")\nbest_xgboost <- h2o.get_best_model(automl_model, algorithm = \"xgboost\")\nbest_drf <- h2o.get_best_model(automl_model, algorithm = \"drf\")\nbest_deeplearning <- h2o.get_best_model(automl_model, algorithm = \"deeplearning\")\nbest_stackedensemble <- h2o.get_best_model(automl_model, algorithm = \"stackedensemble\")\n\n\n\n\nExplainability offers better transparency and helps build trust in the model’s decisions. This is especially useful when making critical decisions based on ML predictions. The h2o package offers several well-crafted functions for evaluating the models with rich visualizations through h2o.explain(). Besides basic model performance metrics, it provides a wide range of diagnostic information and visualizations, such as variable importance, residual analysis, and prediction explanations plots. See [12, 13] for a full explanation on explainable ML models and techniques.\nThe code below runs h2o.explain() function to analyze and explain the behavior of the model. We here choose another model, given that we already seen the GBM model. We choose the best_drf (a previously trained model) to be evaluated on the test_data_h2o. The function generates a detailed explanation of each plot and what it means, so we will not repeat them here. Of interest to our case is the Variable Importance Plot, which ranks the variables based on their contribution to the model’s predictions. The variable importance plot shows that feedback, effort, and duration are the most influential variables in predicting learning outcomes. Other variables like help, mean pace, and steps also contribute, while factors such as planning, evaluating, and social engagement have comparatively less impact.\nThe SHAP summary plot provides detailed visualization of individual predictions by showing how much each feature contributed to pushing a prediction higher or lower for a specific observation. In SHAP plots, the x-axis represents the SHAP value (i.e., the contribution to the prediction) where positive values increasing predictions and negative values lowering them. The color scale is the normalized value of each feature (where blue is low and red is high). For instance, when effort has higher values (in red), it tends to push predictions towards the positive side of the SHAP axis, suggesting that greater effort is associated with improved learning. In contrast, lower values of effort (in blue) push the prediction towards the negative side, indicating that less effort is linked to lower predicted outcomes. Another example is the mean pace feature. Higher values of mean pace (red points) are associated with positive contributions to the prediction, while lower values (blue points) show a negative contribution. This indicates that faster pacing positively influences learning.\nPartial Dependence Plots (PDPs) show how specific variables affect predictions (given that all other variables are held constant). In other words, PDPs allow us to see to which extent a change in a certain feature influences the predicted outcome if all other variables did not change. The green line indicates that feedback has a relatively consistent impact on predictions, with a slight increase at higher values. The histogram below highlights that most feedback values fall between 50 and 80, where the predictions remain stable, suggesting minimal change in the outcome based on feedback levels within this range.\nThe Individual Conditional Expectation (ICE) plot shows how a variable affects the model’s predictions across different percentiles. For instance, in the duration plot, the x-axis represents duration, and the y-axis shows the predicted response. Each colored line represents a different percentile, with the partial dependence (dashed black line) indicating the average relationship between duration and the response. At lower duration values, predictions remain flat across percentiles, meaning the feature has little impact. However, around the 20,000 mark, predictions increase, especially for lower percentiles (e.g., the 0th percentile). This suggests that longer duration significantly improve predictions for these groups.\n\n# Run h2o.explain to explain the model's behavior\nexplanained_drf <- h2o.explain(best_drf, test_data_h2o)\n\n# View the explanation output\nprint(explanained_drf)\n\n\n\n\n\n\n\n\n(a) Residual Analysis\n\n\n\n\n\n\n\n(b) Learning Curve Plot\n\n\n\n\n\n\n\n(c) Variable Importance\n\n\n\n\n\n\n\n\n\n(d) SHAP Summary\n\n\n\n\n\n\n\n(e) Partial Dependence Plot (feedback)\n\n\n\n\n\n\n\n(f) Partial Dependence Plot (effort)\n\n\n\n\n\n\n\n\n\n(g) Partial Dependence Plot (duration)\n\n\n\n\n\n\n\n(h) Partial Dependence Plot (help)\n\n\n\n\n\n\n\n(i) Partial Dependence Plot (meanpace)\n\n\n\n\n\n\n\n\n\n(j) Individual Conditional Expectations (feedback)\n\n\n\n\n\n\n\n(k) Individual Conditional Expectations (effort)\n\n\n\n\n\n\n\n(l) Individual Conditional Expectations (duration)\n\n\n\n\n\n\n\n\n\n(m) Individual Conditional Expectations (help)\n\n\n\n\n\n\n\n(n) Individual Conditional Expectations (meanpace)\n\n\n\n\nFigure 2. Explainer\n\n\n\n\n\n4.2 Multiple idiographic models\nIn the previous steps, we have automated the process of finding the best performing model for one of our students (Alice). As mentioned before, manually creating —and fine tuning— a unique model for each individual is an exhausting time-consuming process and can be impractical when done for several dozens of students as in our case. The AutoML implemented by h2o automates this process by handling model selection, hyperparameter tuning, evaluating and reporting the best models (Figure 22.3). This is where precise h2o AutoML offers an efficient solution that require little interference and offers individualized ML. In fact, this is the case in most of the cases where idiographic models are estimated when we have to deal with multiple people and here where h2o and AutoML becomes more compelling to use.\n\n\n\nFigure 3. AutoML as a tool for idiographic ML\n\n\nSince each student’s data patterns may be rather different, the best algorithm may vary from one to another. One student might have better results with GBM, while another might see more accurate predictions from an RF model. AutoML ensures that the best-performing model is chosen based on the characteristics of each person’s data, optimizing the predictions on a per-person basis (idiographically). Also, each individual’s dataset may require different hyperparameter tuning—a functionality that AutoML handles efficiently. Thus, h2o ensures that the model for each individual is not only correctly selected but also fine-tuned for best performance for each individual. The next code, goes through finding the best model for each student in our dataset. The process is technically similar to before, however, with a main difference is that we will loop through each person and use h2o to find the best model as we did before for the single individual.\n\n4.2.1 Estimating multiple algorithms\nSimilar to what we did in training of the single idiographic model, we begin by loading the necessary libraries: dplyr for data manipulation and h2o for model training. We read the synthetic_Data_share.RDS using readRDS() which contains the data for the individual students. We the define the predictors, the phone activity variables that capture aspects like mean pace, steps, and screen frequency, and the SRL variables which captures students’ SRL as well as self-efficacy and emotions e.g., anxiety and enjoyment. We then combine the variables into a single vector and define the response variable, learning as the target for our prediction.\n\n# Load necessary libraries\nlibrary(dplyr)  # For data manipulation\nlibrary(h2o)    # For H2O ML platform\nset.seed(265)\n# Read in the dataset\nsynthetic_data <- readRDS(\"synthetic_Data_share.RDS\")\n\n# Define the phone activity variables\nphone_vars <- c(\"meanpace\", \"steps\", \"freqscreen\", \"duration\", \n                     \"scrrenoff\", \"nightduroff\", \"maxoffscreennight\")\n\n# Define the SRL variables\nsrl_vars <- colnames(select(synthetic_data, efficacy:enjoyment))\n\n# Combine all variables into the Predictors list\npredictors <- c(srl_vars, phone_vars)\n\n# Set the response variable\nresponse_variable <- \"learning\"\n\nThen, to do the estimation, we begin by initializing the h2o cluster with h2o.init(nthreads = -1, max_mem_size = \"12G\"). This command ensures that h2o uses all available CPU threads (nthreads = -1) and caps memory usage at 12GB (max_mem_size = \"12G\"). Since we are creating idiographic models for each person individually we need to allocate sufficient computational resources for the intensive tasks of fitting several hundred models running on multiple datasets.\nOur dataset has a name field name that identifies each student dataset. Here, we use unique(synthetic_data$name) to extract all distinct names and later isolate their data. We will later use the list of dataset_names to loop through each person’s data. Inside the loop, we start by filtering the current person data person data. Given that the name is not a predictor, we need to remove the name column too, we do so by using filtered_data <- synthetic_data %>% filter(name == specific_name) to isolate the rows for the individual currently being processed, and select(-name) to remove the name column since it’s no longer needed in the modeling process. After we have filtered the data for the currently modeled student, it needs to be converted to h2o’s data frame format. For that we use the function as.h2o(filtered_data) as we did before in the individual model. The next steps are familiar by now, and entail splitting the data into training and testing datasets using h2o.splitFrame(data = h2o_data, ratios = 0.8, seed = 256). We split the data here into 80% for training and 20% for testing (so the the model can be evaluated on new unseen data). We also seed seed = 256 to ensure that the split remains consistent if we run the model again.\nNext, we run h2o.automl() for that student to automatically train multiple ML models on the individual’s training data. We use the predictors list with all the input features (the self-regulated learning variables and phone activity data), and response_variable is set to learning the outcome we want to predict. We also set max_runtime_secs = 300 to specify how long the AutoML process runs for each individual, please that this number is used here for demonstration purposes and you are advised to use higher numbers.\nHaving trained the model, we can get the best model using h2o.get_best_model(automl_model) to retrieve the best-performing model for the individual. This model is chosen based on its performance (and more specifically based on RMSE which is the default criterion). The next step, we evaluate the model performance on the test data (the unseen data) using h2o.performance(best_model, newdata = test_data_h2o).\nIn the last step, we store each student’s data, the best model, and the evaluation results into a list named results which contains for each specific_name:\n\nTrainingData: The data used to train the model.\nTestData: The data used to evaluate the model’s performance.\nAutoMLModel: The full AutoML object containing all the models tried during the AutoML process.\nBestModel: The best performing model chosen by AutoML.\nPerformance: The performance metrics of the best model on the test data.\n\nStoring these components helps to easily access and compare the models across individuals. For example, we can later retrieve the best model for any individual and review its performance on the test data and compare or aggregate or study the patterns.\nThe last two lines of code print the performance of each model for each student. As the reader can see, we created a loop that iterates through the data and performs the basic steps of analysis that we learned in the single person example for each person in our sample, and stores the results in the R object results.\n\n# Initialize h2o with specified resources\nh2o.init(nthreads = -1, max_mem_size = \"60G\")\n\n# Get unique dataset names (subsets)\ndataset_names <- unique(synthetic_data$name)\n\n# Set seed for reproducibility\nset.seed(2202)\n\n# Initialize a list to store results for each subset\nresults <- list()\n\n# Loop through each unique dataset (name)\nfor (specific_name in dataset_names) {\n  \n  # Filter the dataset for the current subset and remove the 'name' column\n  filtered_data <- synthetic_data %>%\n    filter(name == specific_name) %>%\n    select(-name)  # Exclude the 'name' column\n  \n  # Convert the filtered data into an h2o frame\n  h2o_data <- as.h2o(filtered_data)\n  \n  # Split the h2o frame into training (80%) and testing (20%) sets\n  splits <- h2o.splitFrame(data = h2o_data, ratios = 0.8, seed = 256)\n  train_data_h2o <- splits[[1]]  # Training data (80%)\n  test_data_h2o <- splits[[2]]   # Testing data (20%)\n  \n  # Train the model using h2o AutoML\n  automl_model <- h2o.automl(\n    x = predictors,\n    y = response_variable,\n    training_frame = train_data_h2o,\n    nfolds = 5,\n    max_runtime_secs = 300,  # Adjust this as needed\n    seed = 256\n  )\n  \n  # Get the best model\n  best_model <- h2o.get_best_model(automl_model,algorithm= c( \"drf\", \"gbm\",\n                                                              \"glm\", \"xgboost\"))\n  \n  # Evaluate the best model on the test data\n  performance <- h2o.performance(best_model, newdata = test_data_h2o)\n  \n  # Store the data, models, and performance for the current subset\n  results[[specific_name]] <- list(\n    TrainingData = train_data_h2o,\n    TestData = test_data_h2o,\n    AutoMLModel = automl_model,\n    BestModel = best_model,\n    Performance = performance\n  )\n}\n\n\n\n\n\n\n4.2.2 Extracting and Plotting Performance Metrics for Multiple Data sets\nAfter training all the models, now we extract the performance metrics for the multiple datasets and visualize them. We will do so by iterating through the results to extract the performance metrics, and create plots to visualize the distributions of metrics. To store the metrics, we need to create an empty data frame with columns corresponding to the metrics we want to collect: RMSE, MSE, MAE, and R². The Dataset column will store the names of each student to be able to link the results to students. We retrieve the performance object we computed before for the student: results[[dataset_name]]$Performance and then use the function h2o.rmse(), h2o.mse(), h2o.mae(), and h2o.r2() to we extract the values and store it performance_data. To plot the data, we will have to reshape it: convert it from wide to long format to make it easier plotting with ggplot2. In doing so, each row will represent one metric and its corresponding value for a dataset which allows us to have a single graph with multiple facets (sub-plots). Finally, we use ggplot2 to create histograms for each metric (see last step in Figure 22.3).\nThe plots show the distribution of the best idiographic individualized models e.g., MAE, MSE, RMSE, R². In the MAE plot, which measures the average absolute difference between predicted and actual values, we can see a relatively even spread across values ranging from around 2.5 to 7.5. This spread suggests that most models have a moderate error rate. Similarly, the RMSE plot, which places greater emphasis on larger errors, shows a wide range of values from around 4 to 16, indicating variability in prediction quality across different models. The MSE plot reflects squared errors, showing a heavier concentration between 0 and 150, with only a few outliers, meaning that while most models are performing relatively well, a few may struggle with larger errors. The R² (Coefficient of Determination) plot indicates the proportion of variance explained by the model, shows an interesting spread where most R² values are concentrated between 0.5 and 1.0, indicating that the models perform well in explaining the variance for most datasets, excluding “Diana.”\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Initialize an empty dataframe to store performance metrics\nperformance_data <- data.frame(Dataset = character(), RMSE = numeric(), \n                               MSE = numeric(), MAE = numeric(), \n                               R2 = numeric(), stringsAsFactors = FALSE)\n\n# Loop through the results list and extract performance metrics directly\n# from results$Performance\nfor (dataset_name in names(results)) {\n  \n  # Extract performance metrics directly from the results list\n  performance <- results[[dataset_name]]$Performance\n  \n  # Extract individual performance metrics (RMSE, MSE, MAE, R2) from \n  # already computed data\n  rmse_value <- h2o.rmse(performance)\n  mse_value <- h2o.mse(performance)\n  mae_value <- h2o.mae(performance)\n  r2_value <- h2o.r2(performance)\n  \n  # Append the extracted metrics to the performance dataframe\n  performance_data <- rbind(performance_data, data.frame(\n    Dataset = dataset_name,\n    RMSE = rmse_value,\n    MSE = mse_value,\n    MAE = mae_value,\n    R2 = r2_value\n  ))\n}\n\n# Reshape the data to long format for easier plotting with ggplot\nperformance_data_long <- performance_data %>%\n  gather(key = \"Metric\", value = \"Value\", -Dataset) %>% \n  filter(!(Metric == \"R2\" & Dataset == \"Diana\")) \n\nggplot(performance_data_long, aes(x = Value)) +\n  geom_histogram(bins = 10, color = \"black\", fill = \"skyblue\") +\n  facet_wrap(~ Metric, scales = \"free\") +\n  labs(title = \"Distribution of Performance Metrics\", \n       x = \"Value\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\nFigure 4. Distribution of Performance Metrics\n\n\n\n\n\n\n4.2.3 Explanatory variables\nHaving plotted the evaluation metrics, now we extract the top five most important variables for each student based on the best model stored in the results list that we created before. The code similarly loops through each student’s data, retrieves the variable importance from their best model, and stores the top variables in the top_variables list. The code begins by creating an empty list called top_variables. This list will serve as a container for storing the top five variables for each student as we loop through the results list. As the loop iterates over the results list, it first extracts the name of each student and the corresponding “best” model for that student. This model is extracted using results[[student_name]]$BestModel. Then we extract the important variables using h2o.varimp(best_model) function. This function returns the variables ranked by their contribution to the model’s predictive power.\nFrom this table of variables, the script selects the top five variables by using the head(varimp[, \"variable\"], 5) function. These variables are then stored in the top_variables list under the student’s name making it easy to analyze and compare across different models.\nThe process is repeated for each student in the dataset, so that every individual’s top variables are extracted and stored. Once the loop has finished, the top_variables list contains a detailed mapping of the top five variables for every student. At the end of the script, we can view or inspect the top_variables list to see the most important variables for each student. We offer a summary plot also for the most frequent variables and their ranking to examine which variables were most frequent, for instance, at frist position.\n\n# Initialize an empty dataframe to store variable positions for plotting\nvariable_positions <- data.frame(Variable = character(), Position = integer(), \n                                 stringsAsFactors = FALSE)\n\n# Loop through each student and collect variable positions\nfor (student_name in names(top_variables)) {\n  \n  # Get the top 5 variables for the student\n  top_vars <- top_variables[[student_name]]\n  \n  # Ensure we only process students with valid top 5 variables\n  if (is.null(top_vars) || length(top_vars) == 0) next\n  \n  # Assign position 1 to 5 to each variable and store in dataframe\n  for (i in seq_along(top_vars)) {\n    variable_positions <- rbind(variable_positions, \n                                data.frame(Variable = top_vars[i], Position = i))\n  }\n}\n\n# Count the total frequency of each variable and arrange them in descending order\nvariable_positions <- variable_positions %>%\n  group_by(Variable) %>%\n  mutate(Frequency = n()) %>%\n  ungroup() %>%\n  arrange(desc(Frequency))\n\n# Set up a color palette using RColorBrewer\nmy_colors <- brewer.pal(10, \"Set3\")  # Choose a color palette with 5 colors\n\n# Plot the frequency of variables in each position, arranged by overall frequency\nggplot(variable_positions, \n       aes(x = reorder(Variable, -Frequency), fill = as.factor(Position))) +\n  geom_bar(position = \"stack\", color = \"black\") +\n  scale_fill_manual(values = my_colors) +  # Apply the selected color palette\n  labs(x = \"Variable\", \n       y = \"Frequency\", \n       fill = \"Position\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate x-axis labels \n\n\n\n\n\n\nFigure 5. Frequency of Variables in Top 5 Positions (Sorted by Frequency)"
  },
  {
    "objectID": "chapters/ch22-automl/ch22-automl.html#conclusion",
    "href": "chapters/ch22-automl/ch22-automl.html#conclusion",
    "title": "22  Automating Individualized Machine Learning and AI Prediction using AutoML: The Case of Idiographic Predictions",
    "section": "5 Conclusion",
    "text": "5 Conclusion\nIn this chapter, we have learned how to implement an AutoML pipeline covering several common ML algorithms. The tutorial has illustrated how to replicate this pipeline across multiple datasets belonging to different individuals, each resulting into an optimized model that most accurately predicts said individual. As exemplified, this technique is very suitable to be able to scale up idiographic analysis [15], since it requires that each individual has its very own model: a task that becomes unfeasible as the number of individuals increases. We demonstrate how AutoML simplifies the ML pipeline, enabling the creation of individually optimized models for multiple datasets efficiently. Moreover, we illustrate how to apply explainable AI techniques to automate the description of the main features and characteristics of each model. The complete pipeline demonstrated in the chapter holds potential to provide automated real-time insights based on idiographic analysis in a transparent and trustable way [42].\nHowever, idiographic analysis is not the only use case for AutoML. AutoML can be used as an alternative to manual analysis [10, 11] in any other case since, as mentioned before, empirical results show AutoML models perform better than —or at least on par with– human-made models. AutoML is especially useful in scenarios that require continuous update of the models, for example real-time dashboards or adaptive learning systems, where it is impossible for a person to perform all of the steps of the ML pipeline in time to provide useful insights. Lastly, it should be noted that AutoML can be used for use cases beyond prediction, such as clustering [43], for example, enabling the automatic detection of students’ groups or profiles."
  }
]