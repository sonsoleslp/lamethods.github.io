[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning analytics methods and tutorials",
    "section": "",
    "text": "Learning Analytics Unit, University of Eastern Finland\n\n\n\n\nThis is the website for the book “Learning analytics methods and tutorials: A practical guide using R” published by Springer in 2024.\n\n\nThe lack of resources and methodological guidance on learning analytics has been a problem ever since the field started, and continues to be a problem today. We thought that the arduous journey in learning analytics should not be endured by everyone and we decided to make that resource with the help of the learning analytics community as well as our collaborators.\n\n\nIn this book, we tried to include all the basics of R as a programming language as well as the basics of data cleaning, statistics, and data manipulation. In doing so, we wanted the newcomers to find an easy entry to the field. We also tried to be as comprehensive as we could and included almost all major methodologies. For every method, we started with the basics, explaining the main concepts, the essential techniques, and basic functions. In subsequent chapters, we went deeper into advanced methods that are at the forefront of novel methodological innovations. We also used real-life learning analytics data and made it readily available to researchers. To do so, we collaborated with world-renowned researchers, package developers, and methodological experts from other fields to offer an unprecedented resource on novel topics that are hard to find any resource for.\n\n\nWe hope the readers find this book useful as a guide through learning analytics methods, highlighting the ways in which data-driven insights can benefit educators, learners, and researchers.\n\n\nThe editors\n\n\nMohammed Saqr and Sonsoles López-Pernas"
  },
  {
    "objectID": "contributors.html#editors",
    "href": "contributors.html#editors",
    "title": "Contributors",
    "section": "Editors",
    "text": "Editors\nMohammed Saqr, University of Eastern Finland\nSonsoles López-Pernas, University of Eastern Finland"
  },
  {
    "objectID": "contributors.html#associate-editors",
    "href": "contributors.html#associate-editors",
    "title": "Contributors",
    "section": "Associate Editors",
    "text": "Associate Editors\nMiguel Conde, Universidad de León\nRogers Kaliisa, University of Oslo\nKamila Misiejuk, University of Bergen"
  },
  {
    "objectID": "contributors.html#foreword",
    "href": "contributors.html#foreword",
    "title": "Contributors",
    "section": "Foreword",
    "text": "Foreword\nDragan Gašević, Monash University\nRyan Baker, University of Pennsylvania"
  },
  {
    "objectID": "contributors.html#authors",
    "href": "contributors.html#authors",
    "title": "Contributors",
    "section": "Authors",
    "text": "Authors\nEmorie Beck, UC Davis\nJavier Conde, Universidad Politécnica de Madrid\nMiguel Ángel Conde-González, University of León\nCarlos Cuenca-Enrique, Universidad Politécnica de Madrid\nLaura Del-Río-Carazo, Universidad Politécnica de Madrid\nMarion Durand, University of Eastern Finland\nMerja Heinäniemi, University of Eastern Finland\nJouni Helske, University of Jyväskylä\nSatu Helske, University of Turku\nÁngel Hernández-García, Universidad Politécnica de Madrid\nRogers Kaliisa, University of Oslo\nJelena Jovanovic, University of Belgrade\nJoran Jongerling, Tilburg University\nJuho Kopra, University of Eastern Finland\nSonsoles López-Pernas, University of Eastern Finland\nKamila Misiejuk, University of Bergen\nKeefe Murphy, Maynooth University\nGilbert Ritschard, University of Geneva\nAndrew Ruis, University of Wisconsin\nMohammed Saqr, University of Eastern Finland\nMarieke Schreuder, KU Leuven\nLuca Scrucca, Università degli Studi di Perugia\nMatthias Studer, University of Geneva\nDavid Williamson Shaffer, University of Wisconsin\nZachari Swiecki, Monash University\nYuanru Tan, University of Wisconsin\nSanttu Tikka, University of Jyväskylä\nAdrienne Traxler, University of Copenhagen\nLeonie V.D.E. Vogelsmeier, Tilburg University"
  },
  {
    "objectID": "contributors.html#reviewers",
    "href": "contributors.html#reviewers",
    "title": "Contributors",
    "section": "Reviewers",
    "text": "Reviewers\nGökhan Akçapınar, Hacettepe University\nDaniel Amo-Filva, Universitat Ramon Llull\nEnrique Barra, Universidad Politécnica de Madrid\nUmar Bin Qushem, University of Turku\nManuel Castejón-Limas, Universidad de León\nJavier Conde, Universidad Politécnica de Madrid\nTudor Cristea, TU Eindhoven\nLaura Del-Río-Carazo, Universidad Politécnica de Madrid\nRamy Elmoazen, University of Eastern Finland\nDilrukshi Gamage, Tokyo Tech\nBrian P. Godor, Erasmus University Rotterdam\nFrancisco José García Peñalvo, Universidad de Salamanca\nSami Heikkinen, LAB University of Applied Sciences\nRogers Kaliisa, University of Oslo\nQinyi Liu, University of International Business and Economics\nSonsoles López-Pernas, University of Eastern Finland\nKamila Misiejuk, University of Bergen\nAndrés Munoz-Arcentales, Universidad Politécnica de Madrid\nNicolae Nistor, Ludwig-Maximilians-Universität München\nMerlijn Olthof, Radboud University\nDijana Oreški, Sveučilište u Zagrebu\nSambit Praharaj, Ruhr University Bochum\nMiroslava Raspopović Milić, Belgrade Metropolitan University\nMohammed Saqr, University of Eastern Finland\nYuanru Tan, University of Wisconsin-Madison\nSanttu Tikka, University of Jyväskylä\nTiina Törmänen, University of Oulu\nAdrienne Traxler, University of Copenhagen\nDaphne van de Bongardt, Erasmus University Rotterdam\nThijs Vroegh, Netherlands eScience Center\nJin Zhou, Central China Normal University"
  },
  {
    "objectID": "foreword_dg.html",
    "href": "foreword_dg.html",
    "title": "Foreword by Dragan Gašević",
    "section": "",
    "text": "My introduction to learning analytics was unexpected. A brief conversation with George Siemens in early June 2010 sparked a transformative journey for me, with lasting impact on many. George proposed co-organizing a conference on “learning analytics.” It was the first time I’d heard of the phrase. The phrase resonated deeply, connecting with my previous collaborations with Jelena Jovanovic, Chris Brooks, Marek Hatala, Griff Richards, Gord McCalla, Colin Knight and others colleagues in Canada’s LORNET research network (mid-2000s).\nExcited to collaborate, I was initially a bit skeptical about George’s ambition to organize the conference by September 2010. However, I learned to appreciate his eagerness to implement ideas swiftly. We eventually reconnected and agreed on a late February 2011 date, giving us extra six months. Despite remaining nervous, I underestimated the potential impact. The experience proved me wrong in the most positive way, and I’m forever grateful to George, a dear friend, for his unwavering vision and belief.\nThe conference we organized, the 1st International Conference on Learning Analytics and Knowledge (LAK), was held in Banff, Canada, in late February and early March 2011. The frigid temperatures (-35°C) were matched by the conference’s energy. It attracted a large audience and fostered an open, vibrant, productive, and transformative community of researchers and practitioners. This conference is often considered the birth of learning analytics, and the definition we used in the chairs’ message in the conference proceedings remains widely used in the field.\nLearning analytics is now a well-established field. Since our early conversations, we’ve emphasized the importance of methods. My dear friend and esteemed colleague, Shane Dawson, a co-founder of the Society for Learning Analytics Research and field pioneer, first referred to learning analytics as a “bricolage field,” one that borrows methods and theories from data science, artificial intelligence, network science, psychology, psychometrics, sociology, and natural language processing.\nAs founding editors-in-chief of the Journal of Learning Analytics, Shane and I recognized the need to introduce and promote diverse learning analytics methods. This led us to collaborate with Mykola Pechenizkiy, then-president of the International Educational Data Mining Society, to edit a special section on learning analytics tutorials featuring five papers on different methods. These papers, based on tutorials and workshops from the first two Learning Analytics Summer Institutes, became some of the journal’s most impactful publications. It demonstrated the need for learning analytics methods tutorials. However, eight years have passed, a significant timeframe considering the rapid advancements in artificial intelligence and the methods they’ve driven.\nTherefore, I’m delighted to see “Learning Analytics Methods and Tutorials” edited by Mohammed Saqr and Sonsoles López-Pernas. This timely book addresses a critical need in learning analytics.\nThe book offers a comprehensive guide to data analysis methods for researchers and practitioners of all experience levels. It starts with the basics, equipping beginners with R programming and data analysis skills through chapters on data cleaning and exploration. These skills are fundamental for understanding student data and preparing it for further analysis. Even for experts, the book offers advanced methods while emphasizing the broader applicability of these techniques beyond education.\nThe core of the book explores various analytical approaches. Machine learning methods receive well-deserved attention, including introductions to commonly used methods – specifically, predictive modeling and clustering. Predictive modeling is frequently used in learning analytics to identify at-risk students or classify online discussions by analyzing past data patterns. This allows for early intervention to support students at different progress levels. Cluster analysis , another machine learning technique, groups students based on similar characteristics, behaviors, or learning outcomes. It’s commonly used to analyze learning strategies in different learning environments and can provide educators with valuable insights to tailor teaching support to diverse student needs.\nWe’ve long recognized the dynamic nature of learning, with many learning processes unfolding over time. This highlights the need for temporal analytic methods in learning analytics. I’m pleased to see the book provides a rich guide to various temporal methods that analyze the order and timing of events in data about learning and learners. Techniques like sequence analysis and process mining leverage student activity traces to understand how learning unfolds over time. This is crucial for studying longitudinal processes like student engagement throughout a program or explaining connections between cognitive and metacognitive processes in solo and group learning activities.\nNetwork analysis has been a prominent methodological approach since the early days of learning analytics. The book offers excellent coverage of network analytic approaches that explore the relational aspects of data about learners and learning environments. By examining interactions between learners, teachers, and topics, researchers and practitioners can understand collaboration patterns and identify student communities. These methods can be further enriched by combining them with temporal analysis to explore how these relationships evolve over time. The book also covers recent advancements in quantitative ethnography, introducing epistemic network and ordered network analysis, techniques I’ve extensively used with collaborators in recent years.\nI have recently been advocating for the need to establish stronger collaborative ties between learning analytics and psychometrics. Psychometrics is a well-established discipline that can offer many relevant methods to learning analytics. Specifically, psychometrics can help us address issues that have received insufficient attention in learning analytics – reliability and validity. Without addressing these issues, learning analytics can be jeopardized. Therefore, I am delighted that the final section of the book delves into psychometrics, a field that investigates relationships between psychological constructs (like metacognition) and observable data (like test scores). The book explores techniques like factor analysis and structural equation modeling, which help researchers test hypotheses and theories about these relationships. By mastering these methods, learning analytics researchers and practitioners can gain valuable insights from student data to improve learning experiences and outcomes.\nThe book editors and chapter authors must be commended for their valuable contributions to learning analytics. What is outstanding about this book is that it provides source code illustrating all the methods introduced. Readers can directly use the code to build hands-on skills on the many high-importance methods for learning analytics that are so thoughtfully introduced in this book. The book can be directly used in any graduate or postgraduate course on learning analytics. I wish this book had been available three years ago when we developed the graduate certificate in learning analytics at Monash University. Many of its chapters would have been directly used as part of the graduate certificate program. And, I can easily see that many similar programs around the world will greatly benefit from this outstanding book. The book is equally suitable for practitioners and researchers in education institutions, schools, or industry who either want to develop basic data analytic skills or advance their skills with methods they haven’t used before.\nThe overall learning analytics community is significantly richer because of this book. For that, we all owe immense thanks to Mohammed Saqr and Sonsoles López-Pernas, the book editors, for their incredible leadership and vision!\nDragan Gašević Distinguished Professor of Learning Analytics  Director of Research in the Department of Human Centred Computing  Director of the Centre for Learning Analytics  Monash University"
  },
  {
    "objectID": "foreword_rb.html",
    "href": "foreword_rb.html",
    "title": "Foreword by Ryan Baker",
    "section": "",
    "text": "Learning analytics and its sister community of educational data mining have, over the years, matured into a field where an increasing community of scholars and practitioners work together to understand the best methods for exploring the high-volume data available about learners and their learning. This has led to an increasing number of handbooks and online courses. However, none of these prior resources had focused on providing step-by-step tutorials for how to carry out the various types of analyses conducted in learning analytics. This book, by Saqr and Lopez-Pernas, and their colleagues, fills this important gap, offering clear and concise step-by-step tutorials combined with high level explanations of why each step is necessary. As such, it is an important contribution to the field and represents a manuscript that I am sure many learners will find extremely valuable – both newcomers and relatively experienced practitioners looking to learn a new method.\nRyan Baker Professor of Education and Computer Science  University of Pennsylvania"
  },
  {
    "objectID": "chapters/ch01-intro/intro.html",
    "href": "chapters/ch01-intro/intro.html",
    "title": "1  Capturing the Wealth and Diversity of Learning Processes with Learning Analytics Methods",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch01-intro/intro.html#introduction",
    "href": "chapters/ch01-intro/intro.html#introduction",
    "title": "1  Capturing the Wealth and Diversity of Learning Processes with Learning Analytics Methods",
    "section": "1 Introduction",
    "text": "1 Introduction\nThe official birth of the field of learning analytics is often ascribed to the first Learning Analytics & Knowledge Conference in 2011, where the widely used definition was coined as “the measurement, collection, analysis, and reporting of data about learners and their contexts, for purposes of understanding and optimizing learning and the environments in which it occurs” [1]. Over the years, learning analytics has grown in scope, scale, and diversity and has since attracted researchers from diverse backgrounds bringing several disciplines together under one umbrella. Such increasing interest has resulted in a large number of publications, research projects, specialized units, and funding of large projects. Although other data-intensive fields started before learning analytics (e.g., academic analytics in 2007 and educational data mining in 2008), interest in such methods —and in closely related fields at large— has surged only after the rapid adoption of learning analytics. The unique position of learning analytics at the intersection of education and computer science while reaching out to several other disciplines such as statistics, psychometrics, econometrics, mathematics, and linguistics has accelerated the growth and expansion of the field. Therefore, it is a crucial endeavor for learning analytics researchers to stay abreast of the latest methodological and computational advances to drive their research forward. The diversity and complexity of the existing methods can make this task overwhelming both for newcomers to the learning analytics field and for experienced researchers. When conducting learning analytics research, researchers need to decide which data can and must be collected from learners to give answers to their research questions. They need to understand and decide which analytical methods may apply to such data and their potential limitations. They also need to interpret the findings and contextualize them in light of the existing literature and learning theories. With the motivation to accompany researchers in this challenging journey, this book aims to provide a methodological guide for researchers to study, consult, and take the first steps toward innovation.\nEvery method described in this book was developed before learning analytics, for instance, predicting students’ performance can be traced back to almost a century ago [2], and using social network analysis to understand students’ networks dates back to over five decades ago [3]. Similarly, process mining, sequence analysis, and Markov models can all be traced to times before the birth of the field of learning analytics (e.g., [4–6]). Not only were these methods born outside learning analytics, but they also continue to be developed, refined and advanced in their relevant fields. Nevertheless, learning analytics has succeeded in taking advantage of these methodological developments as well as the increasingly available digital data, computational resources, and data science, and in popularizing data-intensive research in education to bring this field together [7].\nThe diversity of methods and fields that have given rise to the field of analytics is evident in the list of authors of the chapters of this book, which include authors of R packages, and experts in methods and applications to drive a state-of-the-art blend. In the first category, we have the world renowned sequence analysis experts Gilbert Ritschard and Matthias Studer (University of Geneva), who are the creators of TraMineR, which is the most central R package for sequence analysis. Closely related, we have Satu and Jouni Helske (University of Turku), developers of seqHMM, an innovative R package for Mixture Hidden Markov Models of sequential data with tons of visualizations and statistical analysis potentials. Also, our list of authors includes Luca Scrucca (University of Perugia), author of mclust, one of the most widely used clustering packages for classification, and density estimation using Gaussian finite mixture models, and Keefe Murphy (Maynooth University), creator of several R packages such as MoEClust for Gaussian parsimonious clustering models with covariates. Our list of authors also includes David Williamson Shaffer (University of Wisconsin-Madison), the creator of rENA (an R package for Epistemic Network Analysis) along with other members of the rENA and the Ordered Network Analysis groups: Yuanru Tan and Andrew Ruis (University of Wisconsin-Madison), and Zachari Swiecki (Monash University). We also have Leonie V.D.E. Vogelsmeier (Tilburg University), author of lmfa, an innovative R package for latent Markov factor analysis which offers transition, and mixture factor analysis. We also have Santtu Tikka (University of Jyväskylä), author of the dynamite R package for dynamic multivariate panel models. In addition, among our authors, we have prominent methodology experts in several domains: Joran Jongerling (Tilburg University); Emorie Beck (UC Davis); Merja Heinäniemi and Juho Kopra (University of Eastern Finland), and Marieke Schreuder (KU Leuven). Lastly, we count on several senior and emerging learning analytics researchers: Jelena Jovanović (University of Belgrade); Ángel Hernández-García, Javier Conde, Laura Del-Río-Carazo, and Carlos Cuenca-Enrique (Universidad Politécnica de Madrid); Adrienne Traxler (University of Copenhagen); Kamila Misiejuk (University of Bergen); Miguel Ángel Conde (University of Leon), and Marion Durand (University of Eastern Finland). Besides, the editors (Mohammed Saqr and Sonsoles López-Pernas) from the University of Eastern Finland have a long history of learning analytics research working with diverse methods and interdisciplinary research that spans most learning analytics methods.\nThanks to the breadth and diversity of authors’ backgrounds and expertise, the book offers a comprehensive array of methods that are described thoroughly. A step-by-step tutorial using the R programming language with real-life datasets and case studies is presented for each method. The book starts with an introductory section for readers to get up-to-speed with R programming, in which we cover the basics of the language, data preprocessing, basic statistics, and data visualization. Then, we move to classic machine learning methods, such as prediction and clustering applied to educational data. These methods enable readers to predict achievement, dropout, and students at risk, as well as to group students into different groups or profiles according to certain characteristics such as motivation or engagement. This is followed by an extensive section devoted to temporal methods such as sequence analysis, Markovian modeling, and process mining, which allow taking advantage of the endless possibilities of trace log data. The book then moves on to discuss network analysis in its many forms including social network analysis, epistemic network analysis, ordered network analysis, and temporal networks. Such methods are crucial for understanding collaboration and relationships between individuals and concepts, which are key aspects of learning. The book concludes with a section on psychometrics such as psychological networks, factor analysis, and structural equation modeling, which are fundamental tools for the analysis of self-reported data among others. We hope the readers find this book useful as a guide through learning analytics methods, higlighting the ways in which data-driven insights can benefit educators, learners, and researchers alike.\nThe book targets learning analytics researchers (and education researchers at large) at all stages. It is suitable to teach newcomers to the field, even with no experience in R, since the introductory chapters are aimed at getting readers acquainted with the basics of R programming and data analysis. The book also covers advanced methods that may be of interest to experts in the field of learning analytics or data science in general. Moreover, the skills taught are transferable to other fields, i.e., can be applied in other contexts outside education.\nWe hope the readers find this book useful as a guide through learning analytics methods, higlighting the ways in which data-driven insights can benefit educators, learners, and researchers."
  },
  {
    "objectID": "chapters/ch01-intro/intro.html#how-the-book-is-structured",
    "href": "chapters/ch01-intro/intro.html#how-the-book-is-structured",
    "title": "1  Capturing the Wealth and Diversity of Learning Processes with Learning Analytics Methods",
    "section": "2 How the book is structured",
    "text": "2 How the book is structured\n\n2.1 Introductory chapters\nThe first section of the book provides the basis for getting up to speed with the R programming language and the data that will be analyzed throughout the book. This section covers the fundamental steps of the data analysis process, such as data preprocessing and exploratory analysis. During data preprocessing, educational data is cleaned and prepared for further analysis. Many crucial decisions about building and conceptualizing learning indicators from raw data are made in this essential step of the learning analytics process. Exploratory analysis enables an early detection of interesting phenomena that can be discovered in the data using visualizations or simple statistics. Using these techniques helps to guide the direction of the in-depth analysis and the selection of more advanced analytical methods.\nChapter 2: A Broad Collection of Datasets for Educational Research Training and Application [8]\nSonsoles López-Pernas, Mohammed Saqr, Javier Conde, Laura Del-Río-Carazo\nSince the goal of this book is to provide a guide and tutorial on how to implement learning analytics methods, the use of relevant data is a key aspect of the contextualization of these methods within learning analytics research. Chapter 2 kicks off the book with an introduction to the most relevant types of data in learning analytics and provides a diverse collection of curated datasets that we will use throughout the book to illustrate the different methods. Understanding the data under examination is a crucial step for the interpretability of the analyses that we will learn to perform in this book, and therefore, readers should familiarize themselves with the datasets described in this chapter to facilitate following the tutorials presented in subsequent ones.\nChapter 3: Getting started with R for Education Research [9]\nSanttu Tikka, Juho Kopra, Merja Heinäniemi, Sonsoles López-Pernas, Mohammed Saqr\nThe first tutorial-like chapter of the book provides an introduction to the basics of R programming, with a focus on the Rstudio integrated development environment and the tidyverse programming paradigm. The R programming language has become a popular tool for conducting data analysis in the field of learning analytics. The chapter covers topics such as data types and structures, control structures, pipes, functions, loops, and input/output operations. By the end of the chapter, readers should have a solid understanding of the basics of R programming and have the necessary tools to learn more in-depth topics such as data wrangling and basic statistics using R.\nChapter 4: An R Approach to Data Cleaning and Wrangling for Education [10]\nJuho Kopra, Santtu Tikka, Merja Heinäniemi, Sonsoles López-Pernas, Mohammed Saqr\nAfter learning the basics of the R programming language, Chapter 4 goes one step further by introducing the reader to data wrangling, also known as data cleaning and preprocessing. Data wrangling is a critical step in the data analysis process, particularly in the context of learning analytics. This chapter provides an introduction to data wrangling using R and covers topics such as data importing, cleaning, manipulation, and reshaping with a focus on tidy data. Specifically, readers will learn how to read data from different file formats (e.g., CSV, Excel), how to manipulate data using the dplyr package, and how to reshape data using the tidyr package. Additionally, the chapter covers techniques for combining multiple data sources.\nChapter 5: Introductory Statistics with R for Educational Researchers [11]\nSanttu Tikka, Juho Kopra, Merja Heinäniemi, Sonsoles López-Pernas, Mohammed Saqr\nStatistics play a fundamental role in learning analytics, providing a means to analyze and make sense of the vast amounts of data generated by learning environments, visualize relationships, test hypotheses, and make comparisons. Chapter 5 in this book provides an introduction to basic statistical concepts using R and covers topics such as measures of central tendency, variability, correlation, and regression analysis. Specifically, readers will learn how to compute descriptive statistics, test hypotheses, and perform simple linear regression analysis. The chapter also includes practical examples using realistic data sets from the field of learning analytics. By the end of the chapter, readers should have a solid understanding of the basic statistical concepts and methods commonly used in learning analytics, as well as a practical understanding of how to use R to conduct statistical analysis of learning data.\nChapter 6: Visualizing and Reporting Educational Data with R [12]\nSonsoles López-Pernas, Kamila Misiejuk, Santtu Tikka, Mohammed Saqr, Juho Kopra, Merja Heinäniemi\nVisualizing data is central in learning analytics research, underpins learning dashboards, and is a prime method for reporting results and insights to stakeholders. Chapter 6 guides the reader through the process of generating meaningful and aesthetically pleasing visualizations of different types of datasets using well-known R packages. The main visualization types will be demonstrated with an explanation of their usage and use cases. Furthermore, learning-related examples will be discussed in detail. For instance, readers will learn how to visualize learners’ logs extracted from learning management systems to show how trace data can be used to track students’ learning activities. Readers will also be able to generate professional-looking tables with summary statistics.\n\n\n2.2 Machine learning methods\nThe next section follows with some of the classic machine learning methods of learning analytics, which date to the early beginnings of the field: predictive modelling and cluster analysis. Predictive modelling is a supervised learning method used widely in learning analytics research, where past data patterns are analysed to predict students’ future outcomes. Clustering is an unsupervised learning method that detects similar patterns in the data and is typically used to group students based on their personal characteristics, observed behavior, or learning outcomes. Both methods are applied to address various challenges in education, such as preventing student drop-out, comparing strategies to improve academic performance, or identifying disengaged students. In addition, the results are often used to trigger specific interventions to help students succeed or to raise awareness about students’ performance based on specific indicators.\nChapter 7: Predictive Modelling in Learning Analytics using R [13]\nJelena Jovanovic, Sonsoles López Pernas, Mohamed Saqr\nPrediction of learners’ course performance has been a central theme in learning analytics since the inception of the field. The main motivation behind it has been to identify learners who are at risk of low achievement so that they could be offered timely support based on intervention strategies derived from analysis of learners’ data. To predict student success, numerous indicators, from varying data sources, have been examined and reported in the literature, as well as various predictive algorithms. Chapter 7 introduces the reader to predictive modeling, through a review of the main objectives, indicators, and algorithms that have been operationalized in previous works as well as a step-by-step tutorial on how to perform predictive modeling in learning analytics using R. The tutorial demonstrates how to predict student success using learning traces originating from a learning management system, guiding the reader through all the required steps from the data preparation to the evaluation of the built models.\nChapter 8: Dissimilarity-based Clustering Educational Data using R [14]\nKeefe Murphy, Sonsoles López-Pernas, Mohammed Saqr\nChapter 8 presents another central method in learning analytics research: clustering. Clustering is a collective term that refers to techniques aimed at uncovering patterns and subgroups within the data. Finding patterns or differences among students enables teachers and researchers to improve their understanding of the diversity of students —and their learning processes— and tailor their support to different needs. This chapter introduces the theory underpinning the dissimilarity-based clustering methods. Then, we focus on some of the most widely-used heuristic dissimilarity-based clustering algorithms; namely, \\(K\\)-means, \\(K\\)-medoids, and agglomerative hierarchical clustering. Methods for choosing the optimal number of clusters are provided, particularly the criteria that can guide the choice of clustering solution among multiple competing methodologies, and not only the choice of the number of clusters \\(K\\) for a given method. All of these are demonstrated in detail with a tutorial in R using a real-life educational dataset.\nChapter 9: An Introduction and R Tutorial to Model-based Clustering in Education via Latent Profile Analysis [15]\nLuca Scrucca, Mohammed Saqr, Sonsoles López-Pernas, Keefe Murphy\nChapter 9 presents an alternative approach for capturing different patterns or subgroups within students’ behavior or functioning. Assuming that there is an average pattern that represents the entirety of student populations requires the measured construct to have the same causal mechanism, the same development pattern, and affect students in exactly the same way. Using a person-centered method (Finite Gaussian mixture model or latent profile analysis), this chapter offers an introduction to model-based clustering that includes the principles of the methods, a guide to the choice of number of clusters, an evaluation of clustering results and a detailed guide with code and a real-life dataset. The tutorial part shows how to uncover the heterogeneity within engagement data by identifying latent or unobserved clusters. The discussion elaborates on the interpretation of the results, the advantages of model-based clustering as well as how this method compares with others.\n\n\n2.3 Temporal methods\nWe continue our journey with an introduction to temporal methods in learning analytics. Unlike the methods based on mere counts of events or activities, temporal methods acknowledge the order and temporality of events, as well as the transitions thereof, which are key aspects of learning. Temporal methods have garnered increasing attention since they allow researchers to take advantage of the trace log data that students leave behind when using educational technology and also to study longitudinal processes (e.g., a whole study program). Such methods originate in social sciences and have been imported and adapted into the learning analytics field. We provide three chapters focused on sequence analysis, and two on transition analysis through Markovian modeling and process mining.\nChapter 10: Sequence Analysis in Education: Principles, Technique, and Tutorial with R [16]\nMohammed Saqr, Sonsoles López-Pernas, Satu Helske, Marion Durand, Keefe Murphy, Matthias Studer, Gilbert Ritschard\nSequence analysis is a data mining technique that is increasingly gaining ground in learning analytics. Sequence analysis enables researchers to extract meaningful insights from sequential data, i.e., to summarize the sequential patterns of learning data and classify those patterns into homogeneous groups. Chapter 10 introduces readers to sequence analysis techniques and tools through real-life step-by-step examples of sequential trace log data of students’ online activities. Readers are guided on how to visualize the common sequence plots and interpret such visualizations. An essential part of sequence analysis is the discovery of patterns within sequences through clustering techniques. Therefore, this chapter demonstrates the various sequence clustering methods, calculation of cluster indices, and evaluation of clustering results.\nChapter 11: Modeling the Dynamics of Longitudinal Processes in Education. A tutorial with R for The VaSSTra Method [17]\nSonsoles López-Pernas, Mohammed Saqr\nBuilding upon the knowledge acquired in the previous chapter, Chapter 11 covers VaSSTra, a method for analyzing multiple variables across multiple time points. The idea behind this method is to summarize multiple variables at each time point into a single state using person-based methods. Then, sequence analysis can be used to analyze the sequences of such states for each person, and clustering techniques can be implemented to detect similar trajectories of the evolution of such states. The method is illustrated in a case study about engagement. Several engagement-related variables are derived from students’ online activities (frequency of each activity, regularity, etc.). These variables are used for clustering students into three states (active, moderate, and disengaged) at each course. Then, sequence analysis is used to map the sequence of engagement states across a whole program. Lastly, clustering mechanisms are used to detect distinct trajectories of engagement.\nChapter 12: A Modern Approach to Transition Analysis and Process Mining with Markov Models in Education [18]\nJouni Helske, Satu Helske, Mohammed Saqr, Sonsoles López-Pernas, Keefe Murphy\nChapter 12 presents Markov models, a widely used technique to model temporal processes. Contrary to the deterministic approach seen in the previous sequence analysis chapters, Markovian models are probabilistic models, focusing on the transitions between states instead of studying sequences as a whole. The chapter provides an introduction to Markov models and differentiates between its most common variations: first-order Markov models, hidden Markov models, mixture Markov models, and hidden mixture Markov models. All implementations are illustrated with a step-by-step tutorial using the R package seqHMM using students’ longitudinal data. The chapter also provides a complete guide to performing stochastic process mining with Markovian models as well as plotting, comparing, and clustering different process models\nChapter 13: Multichannel Sequence Analysis in Educational Research Using R [19]\nSonsoles López-Pernas, Satu Helske, Mohammed Saqr, Keefe Murphy\nWhen dealing with learners’ data, sometimes one single source of information is not enough to capture all of the dimensions of the learning process. Fortunately, sequence analysis as a method supports the examination of multiple sequences (termed channels) at the same time as long as they follow the same time scheme. Chapter 13 covers multi-channel sequence analysis, allowing the reader to study and visualize synchronized sequences together, and cluster them into distinct trajectories based on the values of the various channels. We present two methods for clustering: one distance-based (see Chapter 10) and one based on Markovian models (see Chapter 12). We illustrate the method by studying the longitudinal association between student engagement and achievement across a study program.\nChapter 14: The Why, the How, and the When of Educational Process Mining in R [20]\nSonsoles López-Pernas, Mohammed Saqr\nProcess mining is a recent analytical method that enables the extraction of meaningful insights from time-ordered event logs. The goal of process mining is to discover processes from the data, evaluate process efficiency, and help or enhance processes. Since its introduction in education, process mining has been used to map students’ learning processes, visualize learners’ strategies, as well as demonstrate differences in approach to learning across different learning groups. Chapter 14 illustrates how to prepare learners’ data for process mining and how to visualize the process data using the bupaverse framework. Moreover, readers will learn how to examine the transitions between phases or activities within a learning process.\n\n\n2.4 Network Analysis\nThe next section of the book deals with the relational aspects of analyzing educational data such as relationships between students, teachers, and topics. Network analysis is the underlying method used to study such relational aspects. Social network analysis allows researchers to study collaboration and discussion between peers and understand the role each student occupies in the network. Moreover, community finding allows the detection of distinct groups of peers in the network that interact with each other more than with the rest. We can even combine network analysis with the temporal methods presented in the last section through temporal networks or use epistemic or ordered network analysis to explore topic or construct co-occurrence.\nChapter 15: Social Network Analysis: A Primer, a Guide and a Tutorial in R [21]\nMohammed Saqr, Sonsoles López-Pernas, Miguel Ángel Conde, Ángel Hernández-García\nFor five decades, learning networks have been used to map collaboration networks among students, study the influence of peers, and capture the relational dimension of collaborative learning. Additionally, networks have been used to study the semantics of discourse, relations between behaviors, and patterns of relations among teachers. Networks offer a powerful framework with vast potential for data analysis. Chapter 15 introduces the concept and methods of social network analysis and a detailed guide on how researchers can use network analysis using real-world data. The chapter demonstrates network analysis and visualisation with an emphasis on learners’ roles and relevance to the educational context. The chapter further provides a mathematical analysis and interpretation of the different social network metrics such as centrality and betweenness measures with several examples of how they can be used in practice.\nChapter 16: Community Detection in Learning Networks Using R [22]\nÁngel Hernández-García, Carlos Cuenca-Enrique, Adrienne Traxler, Sonsoles López-Pernas, Miguel Ángel Conde, Mohammed Saqr\nIn learning situations, communities can be groups of students within a whole cohort who collaborate with each to a larger extent than with other students in a learning situation. Finding these communities is integral to understanding the interaction process, the structure and behaviour of the formed groups and how they contribute to the overall learning process. Chapter 16 builds on the principles of social networks from Chapter 15 and introduces the topic of community detection. The main aim of community detection is to identify different groups or clusters of nodes within the network that share some similar characteristics. One way of understanding communities in social networks is as subnetworks where the number of internal connections is larger than the number of external connections, and therefore members of a community have a higher probability of being connected to each other than to members of other communities. The chapter focuses on detecting communities (groups of highly connected nodes) within a wider network and shows how to visualize them using R.\nChapter 17: Temporal Network Analysis: Introduction, Methods, and Analysis with R [23]\nMohammed Saqr\nLearning can be viewed as relational, interdependent, and temporal and therefore, methods that account for such multifaceted dynamic processes that unfold overtime are required. Chapter 17 combines the temporal and relational aspects in a single analytics framework: temporal networks. Temporal networks allow modeling of the temporal learning processes i.e., the emergence and flow of activities, communities, and social processes through fine-grained dynamic analysis. This can provide insights into phenomena like knowledge co-construction, information flow, and relationship building. This chapter introduces the basic concepts of temporal networks, their types (i.e, contact and interval), and techniques. The chapter further provides a detailed guide to temporal network analysis, which involves network building, visualization, and statistical analysis at the graph and node level.\nChapter 18: Epistemic Network Analysis and Ordered Network Analysis in Learning Analytics [24]\nYuanru Tan, Zachari Swiecki, Andrew Ruis, David Williamson Shaffer\nThe increasing use of technology in many areas of society and life has led to an increasing amount of Big Data about human behavior and interaction. However, this volume of data is usually too large and strains the capabilities of human interpretation and the traditional social science research approaches. Chapter 18 presents two quantitative ethnographic approaches that links the power of statistics and in-depth ethnographic approaches to understand learning behaviour through large-scale qualitative data. Epistemic Network Analysis (ENA) and Ordered Network Analysis (ONA), are two methods for quantifying, visualizing, and interpreting network data. Taking coded data as input, ENA and ONA represent associations between codes (e.g., topics or categories) in undirected or directed weighted network models, respectively. Both techniques measure the strength of association among codes and illustrate the structure of connections in network graphs, quantify changes in the composition and strength of those connections over time, and enable comparison of networks. The chapter presents a thorough description of the methods and a step-by-step guide on how to implement them with R.\n\n\n2.5 Psychometrics\nWe finalize the book with a section on psychometrics. In the field of educational psychology, psychometrics aims to study how psychological constructs (e.g., intelligence or aptitude) are related to observable variables (e.g., test scores). Traditionally, psychometric methods in educational psychology have relied on self-reported data from validated questionnaire-like instruments, although nowadays researchers have begun to make use of digital data. We present several techniques to investigate the relationship between measured variables and to test hypotheses and theories: psychological networks, factor analysis, and structural equation modeling (SEM).\nChapter 19: Psychological Networks: A Modern Approach to Analysis of Learning and Complex Learning Processes [25]\nMohammed Saqr, Emorie Beck, Sonsoles López-Pernas\nWhen analyzing psychological phenomena that take place in educational settings, a multitude of variables are at play that may interact with, trigger, and influence each other. To understand such dependency between variables, it is not enough to analyze the linear relationships between each pair of variables, but rather such complexity calls for using more sophisticated methods that capture the full breadth of the interplay between variables: psychological networks. As opposed to social networks where nodes often represent people and edges represent the interactions or relations between them, the nodes in psychological networks represent observed psychological variables and edges represent a statistical relationship between them. Chapter 19 opens the section on psychometric methods by presenting the concept of psychological networks as well as a tutorial for their estimation, visualization, and interpretation with R.\nChapter 20: Factor Analysis in Education Research using R [26]\nLeonie V. D. E. Vogelsmeier, Mohammed Saqr, Sonsoles López-Pernas, Joran Jongerling\nChapter 20 presents factor analysis, a method employed to reduce a large number of variables into fewer numbers of factors. The method is commonly used to identify which observable indicators are representative of latent, not directly-observed constructs. This is a key step in developing valid instruments to assess latent constructs such as student engagement in educational research. The chapter describes the two main approaches for conducting factor analysis in detail and provides a tutorial on how to implement both techniques. The first is confirmatory factor analysis (CFA), a more theory-driven approach, in which a researcher actively specifies the number of underlying constructs as well as the pattern of relations between these dimensions and observed variables. The second is exploratory factor analysis (EFA), a more data-driven approach, in which the number of underlying constructs is inferred from the data, and all underlying constructs are assumed to influence all observed variables (at least to some degree).\nChapter 21: Structural Equation Modeling with R for Education Scientists [27]\nJoran Jongerling, Sonsoles López-Pernas, Mohammed Saqr, Leonie V. D .E. Vogelsmeier\nChapter 21 presents the last method in our book: Structural Equation Modeling (SEM). SEM is a suitable and useful method for modeling the multitude of relationships between latent variables and the observable indicators, as well as the relationship between the latent variables themselves to test theories. In its most common form, SEM combines CFA (covered in Chapter 20) with another method named path analysis. Just like CFA, SEM relates observed variables to latent variables that are measured by those observed variables and, as path analysis does, SEM allows for a wide range of regression-type relations between sets of variables (both latent and observed). This chapter presents an introduction to SEM, an integrated strategy for conducting SEM analysis that is well-suited for educational sciences, and a tutorial on how to carry out an SEM analysis in R."
  },
  {
    "objectID": "chapters/ch01-intro/intro.html#the-companion-code-and-data",
    "href": "chapters/ch01-intro/intro.html#the-companion-code-and-data",
    "title": "1  Capturing the Wealth and Diversity of Learning Processes with Learning Analytics Methods",
    "section": "3 The companion code and data",
    "text": "3 The companion code and data\nTo enhance your learning experience and practical understanding of the concepts discussed in this book, we have developed a companion code repository that accompanies each chapter. The repository contains all the code included in the step-by-step tutorials that illustrate how to implement the different learning analytics methods covered in the book chapters. The code also contains custom functions that automate complex operations, making it easier for anyone to apply the techniques to their own datasets. Moreover, the code will guide the reader on how to generate visualizations, graphs, and plots aimed at helping to interpret and communicate findings effectively. The companion code repository can be accessed at:\n https://github.com/lamethods/code\nAlong with the code, we provide a collection of datasets carefully curated to represent educational scenarios, allowing readers to experiment with the techniques discussed in the book and beyond. Each dataset is described in detail in Chapter 2.\n https://github.com/lamethods/data"
  },
  {
    "objectID": "chapters/ch02-data/ch2-data.html",
    "href": "chapters/ch02-data/ch2-data.html",
    "title": "2  A Broad Collection of Datasets for Educational Research Training and Application",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch02-data/ch2-data.html#introduction",
    "href": "chapters/ch02-data/ch2-data.html#introduction",
    "title": "2  A Broad Collection of Datasets for Educational Research Training and Application",
    "section": "1 Introduction",
    "text": "1 Introduction\nLearning analytics involves the combination of different types of data such as behavioral data, contextual data, performance data, and self-reported data to gain a comprehensive understanding of the learning process [1, 2]. Each type of data provides a unique perspective of the learning process and, when analyzed together, can provide a more complete picture of the learner and the learning environment. Throughout the book, we will work with different types of learning analytics data to illustrate the analysis methods covered. This chapter explores the most common types of data that are commonly used in learning analytics and that we will work with in the subsequent book chapters. Such data include demographic and other contextual data about students, performance data, online activity, interactions with other students and teachers, and self-reported data.\nThis chapter also describes a set of datasets that will be used throughout the book, as well as additional datasets that may be useful for readers to put the newly learned methods into practice. We will discuss the characteristics, structure, and contents of each dataset, as well as the context in which they have been used within the book. The goal of this chapter is to give readers a solid foundation for working with the datasets used in the book, as well as to provide a starting point for those interested in exploring additional data sources."
  },
  {
    "objectID": "chapters/ch02-data/ch2-data.html#types-of-data",
    "href": "chapters/ch02-data/ch2-data.html#types-of-data",
    "title": "2  A Broad Collection of Datasets for Educational Research Training and Application",
    "section": "2 Types of data",
    "text": "2 Types of data\n\n2.1 Contextual data\nContextual data refer to data that provide information about the environment in which learning takes place, such as demographic information, socioeconomic status, and prior academic achievement. This type of data can be used to understand how external factors may impact learning, to identify students with similar profiles, and to develop targeted interventions. Demographic data can be used to understand the characteristics of the learners, such as age, gender, race, and ethnicity [3]. Socioeconomic data can be used to examine the impact of the socio-economic status of learners, such as income, employment status, and education level [4]. Prior academic achievement data can be used to understand how the academic background of the learners, such as their previous grades and test scores, may influence their learning at present [5]. The data about the learning context is also relevant to better understand and support students; for example, the level and difficulty of the educational program and courses, format (online vs. face-to-face), or pedagogical approach (e.g., flipped classroom, laboratory course, etc.) [6].\nDescriptive statistics can be used to summarize and describe the main characteristics of the contextual data, such as the mean, median, and standard deviation. In Chapters 3 [7] and 4 [8], we will learn how to clean and manipulate data and how to summarize it using descriptive statistics. In Chapter 6 [9], we will learn how to create different types of plots that will allow us to better understand the data. Cluster analysis can also be used to group similar students together. This can be used to identify patterns in the data and, for example, to understand which different groups of students exist in a course or degree and whether such groups differ in terms of, e.g., performance [10]. We cover clustering in Chapters 8 and 9 [11, 12].\nIt is important to bear in mind that contextual data are essential to understand learners’ and the learning process, but they should be used in combination with other types of data to obtain a comprehensive understanding [13]. It is also crucial to comply with data protection laws and regulations and to consider the ethical implications of collecting and operationalizing this type of data, especially when it comes to the existence of bias when making decisions based on contextual data [14].\n\n\n2.2 Self-reported data\nSelf-reported data refers to data provided by students themselves (or other relevant stakeholders), such as data collected through surveys or questionnaires. This type of data can provide valuable insight into learners’ and teachers’ attitudes, motivation, and perspectives on their learning experiences, and can be used to inform the design of educational programs [15]. It is important to keep in mind that the data should be cleaned and pre-processed before applying any analytical techniques, especially when dealing with qualitative data (e.g., free text, video, or recordings), and the results should be interpreted with caution, keeping in mind the limitations of self-reported data [16].\nRegarding the techniques employed to analyze self-reported data, descriptive statistics and data visualization are commonly used to understand the distribution of responses and to identify patterns in the data (see Chapters 4 [8] and 6 [9]). Moreover, inferential statistics can be used to make inferences about a population based on a sample of data. This can include techniques such as t-tests and analysis of variance to identify significant differences between groups or chi-squared tests to identify associations in the data. Chapter 5 will help us better understand the most common statistical tests and how to implement them with R [17]. Depending on the research question, the type of data, and the level of detail required, a more sophisticated choice of analytical techniques might be needed. For instance, Factor Analysis is a statistical technique that can be used to identify underlying factors or dimensions that explain the relationships between multiple variables [18]. We will learn about it in Chapter 20 [19]. Similarly, Structural Equation Modeling (SEM) can be used to test complex models that involve multiple observed and latent variables that depend on one another. We cover this method in Chapter 21 [20]. Moreover, self-reported data can be analyzed using psychological networks, a relatively new approach in the field of psychology that focuses on understanding psychological phenomena as interconnected networks of individual components. We cover this method in Chapter 19 [21]. Lastly, text mining can be used to analyze unstructured data, such as open-ended responses to surveys or interviews. It can be used to identify key concepts and themes, perform sentiment analysis, and summarize text [22]. This type of analysis is beyond the scope of this book.\n\n\n2.3 Activity data\nActivity data in learning analytics refers to the data that is collected about a student’s interactions with educational technology. Activity data can include information such as the learning resources a student visits, the time spent on a resource, the buttons clicked, and the messages posted [23]. Data can be collected automatically by the learning management system (LMS) or other educational technology (e.g., a game, an intelligent tutoring system, eBooks, or coding environments). Log activity data can be used to track student progress, identify areas where students may be struggling, and personalize instruction [24]. For example, if a student is spending an excessive amount of time on a particular concept, it may indicate that they are having difficulty understanding that concept. In this case, the teacher can provide additional support or re-teach the concept to help the student improve. Log activity data can also be used to measure student engagement with the course content and to identify students who are not engaging with the material [25]. Log activity data have been used to detect students’ online tactics and strategies [26] paying attention not only to the frequency but to the order and timing of students’ events.\nBesides basic analysis using descriptive and inferential statistics, activity logs have been operationalized in many ways in learning analytics, especially using temporal methods that allow to take advantage of the availability of large amounts of timestamped data. For example, process mining — which we cover in Chapter 14 [27] — has been used to investigate how students navigate between different online activities [28]. Sequence analysis has been used to detect and interpret students’ online tactics and strategies based on the order of learning activities within learning sessions [29]. We dedicate several chapters to this technique [30–33]. Such analyses have been complemented with cluster analysis, which allows to detect distinct patterns of students with different online behavior [34] (see Chapters 8 and 9 [11, 12]).\n\n\n2.4 Social interaction data\nSocial interaction data in learning analytics refers to the data collected about students’ interactions with each other (and sometimes teachers too) in a learning environment, social media, or messaging platforms. This can include data such as the frequency and nature of interactions, the content of discussions, and the participation of students in group work or collaborative activities. Social interaction data can be used to understand how students are engaging with each other and to identify patterns or roles that students assume [35]. For example, if a student is not participating in group discussions, it may indicate that they are feeling disengaged or are having difficulty understanding the material. Furthermore, social interaction data can be used to study how students’ depth of contributions to the discussion influences performance [36]. For example, an analysis of social interaction data may reveal that students who receive more replies from other students perform better in the course than students whose contributions do not spark a lot of interest.\nSocial Network Analysis (SNA) is the most common method to study social interaction data. SNA comprises a wealth of quantitative metrics that summarize relationships in a network. In most cases in learning analytics, this network is formed based on students’ interactions. These metrics, named centrality measures, pave the path to apply other analytical methods such as cluster analysis to detect collaboration roles [37], or predictive analytics to determine whether performance can be predicted from students’ centrality measures [38]. We cover the basics of SNA in Chapter 15 of the book [39], community finding in Chapter 16 [40], and temporal network analysis in Chapter 17 [41]. Moreover, the nature and content of students’ interactions can be analyzed with Epistemic Network Analysis (ENA), a method for detecting and quantifying connections between elements in coded data and representing them in dynamic network models [42]. We cover this method in Chapter 18 [43].\n\n\n2.5 Performance data\nPerformance data refers to data that measures how well learners are able to apply what they have learned. This type of data can be used to evaluate the effectiveness of a particular educational activity, to identify areas where additional support may be needed, or to detect students at risk. Performance includes assessment data from tests, quizzes, projects, essays, exams, and other forms of evaluation used to track students’ progress. Assessment can be performed by different entities, such as teachers, peers or automated assessment tools. Moreover, assessment data can have different levels of granularity: it can be the grade for a specific task, a midterm or final exam, or a project; it can be the final grade for a course, or even a whole program GPA [44]. Performance data used for learning analytics may not necessarily be assessment data. For instance, pre-test and post-test data are used to evaluate the effectiveness of a particular educational intervention [45]. Another example is the data captured by audience response systems (ARSs) [46], which are often used to evaluate learners’ knowledge retention during lectures.\nPerformance data are rarely analyzed on its own, but rather used in combination with other sources of data. For example, a common use case in learning analytics is to correlate or predict grades with indicators from several sources [13, 47], such as demographic data, activity data or interaction data. In the book, we cover predictive modelling in Chapter 7 [48]. Moreover, grades are often compared among groups or clusters of students, for example, to evaluate the performance of students that use different online learning strategies [29] or to establish whether students’ assuming different levels of collaboration also show differences in performance [49]. Clustering is covered in Chapters 8 and 9 [11, 12].\n\n\n2.6 Other types of data\nIn recent years, the landscape of data used for learning analytics has undergone a remarkable expansion beyond demographics, grades, surveys and digital logs [50]. This evolution has led to the incorporation of novel methodologies designed to capture a more holistic understanding of students’ learning experiences, including their physiological responses [51]. This progression encompasses a diverse range of data acquisition techniques, such as eye-tracking data that traces the gaze patterns of students, electrodermal activity which measures skin conductance and emotional arousal, EEG (electroencephalogram) recordings that capture brain activity patterns, heartbeat analysis reflecting physiological responses to learning stimuli, and motion detection capturing physical movements during learning activities [52]. These physiological datasets are often combined with other forms of information, such as video recordings (e.g., [50]). Combining various data modalities allows researchers and educators to gain a better understanding of how students engage with educational content and respond to different teaching methodologies [51]. This analysis goes beyond the capabilities of conventional digital learning tools, offering insights into the emotional, cognitive, and physical aspects of learning that might otherwise remain concealed [53, 54]. This synergistic analysis of multiple data sources is often referred to as “multimodal learning analytics”. In Chapter 13, we will cover multi-channel sequence analysis, a method suitable for analyzing several modalities of data at the same time [33]."
  },
  {
    "objectID": "chapters/ch02-data/ch2-data.html#dataset-selection",
    "href": "chapters/ch02-data/ch2-data.html#dataset-selection",
    "title": "2  A Broad Collection of Datasets for Educational Research Training and Application",
    "section": "3 Dataset selection",
    "text": "3 Dataset selection\nThe present section describes a set of curated datasets that will be used throughout the book. In the introductory chapters, the reader will learn how to import datasets in different formats [7], clean and transform data [8], conduct basic statistics [17], and create captivating visualizations [9]. Each of the remaining chapters covers a specific method, which is illustrated in a tutorial-like way using one or more of the datasets described below. All the datasets are available on Github (https://github.com/lamethods/data).\n\n3.1 LMS data from a blended course on learning analytics\n Link to the dataset\nThe first dataset in our book is a synthetic dataset generated from on a real blended course on Learning Analytics offered at the University of Eastern Finland. The course has been previously described in a published article [55] which used the original (non-synthetic) dataset. The lectures in the course provided the bases for understanding the field of learning analytics: the recent advances in the literature, the types of data collected, the methods used, etc. Moreover, the course covered learning theories as well as ethical and privacy concerns related to collecting and using learners’ data. The course had multiple practical sessions which allowed students to become skilled in learning analytics methods such as process mining and social network analysis using real-life datasets and point-and-click software.\nStudents in the course were required to submit multiple assignments; most of them were practical, in which they had to apply the methods learned in the course, but others were focused on discussing learning theories, ethics, and even conducting a small review of the literature. The course had a final project that accounted for 30% of the course final grade in which students had to analyze several datasets in multiple ways and comment and discuss their findings. Moreover, there was a group project in which students had to present an implementation of learning analytics application in an institutional setting, discussing the sources of data collection, the analyses that could be conducted, and how to present and make use of the data and analyses. The course was implemented in a blended format: instruction was face-to-face while the learning materials and assignments were available online, in the Moodle LMS. Discussions among students in the group project also took place online in the LMS forum.\nThe dataset contains four files: a file containing students’ online activities in Moodle, a file containing their grades, a file containing their demographic data, and a file that aggregates all the information. It is shared with a CC BY 4.0 license, which means that anyone is free to share, adapt, and distribute the data as long as appropriate credit is given. The dataset has been used in the introductory chapters of the book to learn the basics of R [7], data cleaning [8], basic statistics [17] and data visualization [9]. Moreover, it has been used in two additional chapters to illustrate to well-known learning analytics methods: sequence analysis [30] and process mining [27]. Below, we provide further details on each of the files of the dataset.\n\n3.1.1 Events\nThe Events.xlsx file contains 95,580 timestamped Moodle logs for 130 distinct students. The activities include viewing the lectures, discussing on forums, and working on individual assignments, as well as discussion in small groups, among other events. The logs were re-coded to balance granularity with meaningfulness, i.e., grouping together logs that essentially represent the same action. For example, the activities related to the group project were all coded as Group_work, log activities related to feedback were coded as Feedback, logs of students’ access to practical resources or assignments were coded as Practicals, social interactions that are unrelated to learning were coded as Social, etc. Below we describe the columns of the dataset and show a preview. In Figure 2.1, we show the distribution of events per student.\n\nEvent.context: Resource of the LMS where the event takes place, for example “Assignment: Literature review”.\nuser: User name in the LMS.\ntimecreated: Timestamp in which each event took place, ranging from September 9th 2019 to October 27th 2019.\nComponent: Type of resource involved in the event. There are 13 distinct entries, such as Forum (39.11%); System (34.33%); Assignment (15.50%) and 10 others.\nEvent.name: Name of the event in Moodle. There are 27 distinct entries, such as Course module viewed (35.89%); Course viewed (26.28%); Discussion viewed (13.77%) and 24 others.\nAction: Column coded based on the combination of the event name and context. There are 12 distinct entries, such as Group_work (34.25%); Course_view (26.45%); Practicals (10.48%) and 9 others.\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 1. Number of actions per student per type\n\n\n\n\n\n\n3.1.2 Demographics\nThe Demographics.xlsx file contains simulated demographic data on the students, including name, date of birth, gender, location (on-campus vs. remote student), and employment status. Below, we describe the columns of the dataset and show a preview of the data.\n\nuser: User identifier in the learning management system, with 130 distinct entries.\nName: User’s first name.\nSurname: User’s last name.\nOrigin: Country of origin.\nGender: User’s gender: F (Female, 50%) or M (Male, 50%).\nBirthdate: Date of birth.\nLocation: Whether the student is on campus or studying remotely, with 2 distinct entries, such as On campus (81.54%); Remote (18.46%).\nEmployment: Whether the student is working part-time, full-time or not at all, with 3 distinct entries, such as None (70.77%); Part-time (25.38%); Full-time (3.85%).\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n3.1.3 Results\nPerformance data is provided in the Results.xlsx file, including grades per assignment and the overall course grade. Below we describe the dataset columns and show a preview of the data (the column names have been abbreviated in the preview). Figure 2.2 shows the distribution of grades per graded item.\n\nuser: User name in the learning management system (it matches the previous files).\nGrade.SNA_1: Grade of the first SNA assignment (0-10).\nGrade.SNA_2: Grade of the second SNA assignment (0-10).\nGrade.Review: Grade of the studies’ review assignment (0-10).\nGrade.Group_self: Individual grade of the group project (0-10).\nGrade.Group_All: Group grade of the group project (0-10).\nGrade.Excercises: Grade of the practical exercises (0-10).\nGrade.Project: Final project grade (0-10).\nGrade.Literature: Grade of the literature review assignment (0-10).\nGrade.Data: Grade of the data analysis assignment (0-5).\nGrade.Introduction: Grade of the introductory assigment (0-10).\nGrade.Theory: Grade of the theory assignment (0-10).\nGrade.Ethics: Grade of the ethics assignment (0-10).\nGrade.Critique: Grade of the critique assignment (0-10).\nFinal_grade: Final course grade (0-10).\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFigure 2. Grade per student and graded item\n\n\n\n\n\n\n3.1.4 AllCombined\nThis file AllCombined.xlsx contains the students’ demographics, grades, and frequency of each action in the LMS. The columns from students’ demographic data and grades remain the same, while the event data has been grouped per student for each type of event (Action column in the Events dataset), i.e., there is a new column for each type of event that contains the number of events of that type that each student had. Moreover, a new column AchievingGroup separates the high achievers from the low achievers. Below we show a preview of the new columns of the data (the column names have been abbreviated) that were not shown in the previous previews and describe them.\n\nFrequency.Applications: Number of events related to the “Applications” resource.\nFrequency.Assignment: Number of events related to the assignments’ submission.\nFrequency.Course_view: Number of visits to the course main page.\nFrequency.Feedback: Number of views of the assignment feedback.\nFrequency.General: Number of events related to general learning resources.\nFrequency.Group_work: Number of events related to the group work.\nFrequency.Instructions: Number of events related to assignment instructions.\nFrequency.La_types: Number of events related to the “LA types” resource.\nFrequency.Practicals: Number of events related to the practicals.\nFrequency.Social: Number of events related to the forum discussion.\nFrequency.Ethics: Number of events related to the “Ethics” resource.\nFrequency.Theory: Number of events related to the “Theory” resource.\nFrequency.Total: Number of events overall.\nAchievingGroup: Categorization as high achievers (top 50% grades) and lows achievers (bottom 50% grades).\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFigure 3. Relationship between total frequency of events and final grade\n\n\n\n\n\n\n\n\n\n\n3.2 LMS data from a higher education institution in Oman\n Link to the dataset\nThe next dataset includes students’ log data enrolled in computing specialization a higher education institution in Oman. The dataset contains data from students enrolled in the sixth semester or beyond. The data was recorded across five modules (Spring 2017-2021) and includes the records of 326 students with 40 features in total, including the students’ academic information (24 features), logs of students’ activities performed on the Moodle LMS (10 features), and students’ video interactions on the eDify mobile application (6 features). The academic data includes demographic data, academic data, study plan, and academic violations. The Moodle activity data includes students’ timestamped activities in Moodle. The eDify data contains video interactions in the eDify mobile application.\nThe dataset has been described in an article by Hasan et al. [56] and was originally made available in the Zenodo repository [57] with a CC BY 4.0 license, which means anyone can share and adapt the dataset but must give credit to the author and cannot apply any further restrictions to the dataset. Besides the raw data, the dataset includes a processed file that contains a series of indicators that can be used for different purposes such as predictive modeling or clustering of students’ profiles. It has been used in several publications with the purpose of predicting student performance using different algorithms and approaches [58, 59]. The main files of the dataset are described below.\n\n3.2.1 Student academic information\nStudents’ academic information downloaded from the institutional information system. The data are spread in 15 files (starting with KMS), but have been combined in a single file (KMSmodule.RDS) for convenience. Below we show a preview of the data and its structure:\n\nfile: Original file name that contains the module offering identifier (e.g., “KMS Module 1 F19.csv”). There are 15 distinct entries (one for each file).\nModuleCode: Module identifier (Module 1-5).\nModuleTitle: Name of the module (Course 1-5).\nSessionName: Class section in which the student has been enrolled (Session-A or Session-B).\nRollNumber: Student identifier (e.g., “Student 83”). There are 306 distinct students.\nCGPA: Students’ cumulative GPA (1-4).\nAttemptCount: Number of attempts per student and module.\nAdvisor: Students’ advisor identifier (e.g., ‘Advisor 16’). There are 50 distinct advisors.\nRemoteStudent: Whether the student is studying remotely. There are 2 possible values: Yes (0.31%) or No (99.69%).\nProbation: Whether the student has previous incomplete modules. There are 2 possible values: Yes (3.36%) or No (96.64%).\nHighRisk: Whether the module has a high risk of failure. There are 2 possible values: Yes (6.73%) or No (93.27%).\nTermExceeded: Whether the student is progressing in their degree plan. There are 2 possible values: Yes (1.83%) or No (98.17%).\nAtRisk: Whether the student has failed two or more modules in the past. There are 2 possible values: Yes (23.55%) or No (76.45%).\nAtRiskSSC: Whether the student has been registed for having any educational deficiencies. There are 2 possible values: Yes (4.59%) or No (95.41%).\nOtherModules: Number of other modules the student is enrolled in on the same semester.\nPrerequisiteModules: Whether the student has enrolled in the prerequistide module.\nPlagiarismHistory: Modules for which the student has a history of plagiarism.\nMalPracticeHistory: Modules for which the student has a history of academic malpractice.\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n3.2.2 Moodle\nThe Moodle data is spread around 15 files (starting with Moodle), which contain all the clicks that students performed in the Moodle LMS in each module. The 15 files have been combined in a single file (moodleEdify.RDS) for convenience. Below is a preview of the data and a description of its structure:\n\ncsv: Module offering identifier. There are 15 distinct entries (one for each file).\nTime: Timestamp in which the event occurred, ranging between January 1st 2018 to December 9th 2020.\nEvent context: Resource of the LMS where the event takes place, for example “File: Lecture 4”.\nComponent: Type of resource involved in the event. There are 16 distinct entries, such as System (51.71%); File (22.30%); Turnitin Assignment 2 (15.04%) and 13 others.\nEvent name: Name of the event in Moodle. There are 70 distinct entries, such as Course viewed (36.59%); Course module viewed (25.98%); List Submissions (11.08%) and 67 others.\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n3.2.3 Activity\nAggregated information per student based on the Moodle data, including the activity on campus and at home. The data are also spread in 15 files (starting with Activity), but has been combined in a single file (ActivityModule.RDS) for convenience. Below we show a preview of the data and its structure:\n\nfile: Original file name that contains the module offering identifier (e.g., “Activity Module 1 F19.csv”). There are 15 distinct entries (one for each file).\nRollNumber: Student identifier (e.g., “Student 208”).\nOnline C: Duration of the activity (in minutes) within campus.\nOnline O: Duration of the activity (in minutes) off campus.\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n3.2.4 Results\nStudent performance data in each module. The data are also spread in 10 files (starting with Result), but has been combined in a single file (ResultModule.RDS) for convenience.Below we show a preview of the data and its structure:\n\nfile: Original file name that contains the module offering identifier (e.g., “Result Module 1 F19.csv”). There are 10 distinct entries (one for each file).\nRollNumber: Student identifier (e.g., “Student 208”). There are 326 distinct students.\nSessionName: Class section, Session-A (84.66%), or Session-B (9.82%).\nCW1: Grade of students’ first assignment (0-100).\nCW2: Grade of students’ second assignment (0-100).\nESE: Grade of students’ end-of-semester examination (0-100).\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n3.2.5 eDify\nStudent aggregated activity data in the eDify mobile application. The data are also spread in 15 files (starting with VL), but has been combined in a single file (VLModule.RDS) for convenience. Below we show a preview of the data and its structure:\n\nfile: Original file name that contains the module offering identifier (e.g., “Result Module 1 F19.csv”). There are 15 distinct entries (one for each file).\nRollNumber: Student identifier (e.g., “Student 208”). There are 326 distinct students.\nPlayed: Number of times the student has played a video.\nPaused: Number of times the student has paused a video.\nLikes: Number of times the student has liked a video.\nSegment: Number of times a student has scrolled to a specific part of the video.\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n3.3 School engagement, academic achievement, and self-regulated learning\n Link to the dataset\nThis dataset includes measures of school engagement, self-regulation and academic performance of a group of primary school students in northern Spain [60]. The data contains responses to the questionnaire from 717 primary education students. The subjects were recruited using convenience sampling from 15 schools (5 privately funded and 10 publicly funded) in northern Spain. The objective of collecting these data was to characterize school engagement and to explore to which extent different engagement profiles are associated with academic performance and self-regulation. In the questionnaire, engagement was assessed with the school engagement measure [61], which allows to differentiate between behavioral, cognitive and emotional engagement. Self-regulation was assessed using the self-regulation strategy inventory [62], which allows measuring students’ approaches to seeking and learning information, maladaptive regulatory behavior, environment management, and time management. The measure used for achievement was students’ self-reported information about their grades in Spanish and mathematics on a scale of 1 (fail) to 5 (outstanding).\nFigure 2.4 summarizes the responses of the students in both subjects, Figure 2.5 presents a set of histograms with the engagement measures, and Figure 2.6 includes a set of histograms with the self-regulation measures. The dataset was analyzed in a previous article [63], where the authors carried out cluster analysis using LPA (Latent Profile Analysis) to identify different groups of students according to their engagement and self-regulation and to compare the performance between these groups. The dataset is used in Chapter 9 [11] of this book, which covers model-based clustering. The dataset is published with a CC BY 4.0 license, which means that you can share, copy and modify this dataset so long as appropriate credit is given. Below we show a preview of the dataset and describe its main variables.\n\nalumno: Student identifier in the school.\nsexo: Student’s gender (1 = Male, 48.40%; 2 = Female, 51.46%).\ncoleg: School identifier (1-15).\ncurso: Grade that students were in 5th grade (62.48%) or 6th grade (37.52%).\ngrup: Class section (1-3).\nren.mat: Mathematics self-reported academic achievement (1-5).\nren.leng: Spanish self-reported academic achievement (1-5).\nEmotion_Engage: Emotional engagement (z-score).\nCognitive_Engage: Cognitive engagement (z-score).\nBehavior_Engage: Behavioural engagement (z-score).\nEnviroment_Manage: Environment management (z-score).\nInformation_help_Manage: Information and help management (z-score).\nMaladapative_Behavior: Maladaptative self-regulation (z-score).\nTime_Manage: Time management self-regulation (z-score).\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 4. Self-reported grades of students in Mathematics and Spanish\n\n\n\n\n\n\n\n\n\n\n\n(a) Emotion engagement z-score\n\n\n\n\n\n\n\n(b) Cognitive engagement z-score\n\n\n\n\n\n\n\n(c) Behavior engagement z-score\n\n\n\n\nFigure 5. School engagement results of the survey\n\n\n\n\n\n\n\n\n\n(a) Environment management z-score\n\n\n\n\n\n\n\n(b) Information help management z-score\n\n\n\n\n\n\n\n\n\n(c) Maladaptative behavior z-score\n\n\n\n\n\n\n\n(d) Time management z-score\n\n\n\n\nFigure 6. Self-regulation results of the survey\n\n\n\n\n3.4 Teacher burnout survey data\n Link to the dataset\nThe next dataset presents the responses collected from a survey about teacher burnout in Indonesia [64]. The survey questionnaire contains 18 items in five different categories. The first category contains five items to assess the teacher self-concept (TSC), from the TSC Evaluation Scale [65]. The second category is teacher efficacy (TE), with 5 items adapted from [66]. The remaining categories are Emotional Exhaustion (EE, 5 items), Depersonalization (DP, 3 items), and Reduced Personal Accomplishment (RPA, 5 items), adapted from the Maslach burnout inventory [67]. The survey items were measured using a 5-point Likert scale, where 1 represents “never”, and 5 represents “always”.\nBelow we show a preview of the dataset with the questions and answers, which can be visualized in Figure 2.7 using a Likert scale chart. The dataset has been analyzed using several statistical methods [68]: Content Validity Index (CVI), Exploratory Factor Analysis (EFA), Confirmatory Factor Analysis (CFA), Covariance-Based SEM (CB-SEM). The main aim was to determine the factors that may be predictors of teacher burnout. In this book, we also make use of this dataset to illustrate Factor Analysis [19] and SEM [20]. The files associated with this dataset are licensed under a CC BY 4.0 license, which means you can share, copy and modify this dataset so long as appropriate credit is given to the authors. The variables of the dataset are described below.\n\nTeacher self-concept\n\nTSC1: Response to the item “I think I have good teaching skills and ability”.\nTSC2: Response to the item “I have a reputation for being an efficient teacher”.\nTSC3: Response to the item “My colleagues regard me as a competent teacher”.\nTSC4: Response to the item “I feel I am a valuable person”.\nTSC5: Response to the item “Generally speaking, I am a good teacher”.\n\nTeacher efficacy\n\nTE1: Response to the item “I help my students value learning”.\nTE2: Response to the item “I motivate students who show low interest in schoolwork”.\nTE3: Response to the item “I improve understanding of students who are failing”.\nTE4: Response to the item “I provide appropriate challenges for very capable students”.\nTE5: Response to the item “I get students the students to follow classroom rules”.\n\nEmotional exhaustion\n\nEE1: Response to the item “I feel emotionally drained from my work”.\nEE2: Response to the item “I feel used up at the end of the workday”.\nEE3: Response to the item “I feel fatigued when I get up in the morning and have to face another day on the job”.\nEE4: Response to the item “I feel burnt out from my work”.\nEE5: Response to the item “Working with people all day is really a strain on me”.\n\nDepersonalization\n\nDE1: Response to the item “I’ve become more callous toward people since I took this job”.\nDE2: Response to the item “I worry that this job is hardening me emotionally”.\nDE3: Response to the item “I feel frustrated by my job”.\n\nReduced Personal Accomplishment\n\nRPA1: Response to the item “I cannot easily create a relaxed atmosphere with my students”.\nRPA2: Response to the item “I do not feel exhilarated after working closely with my students”.\nRPA3: Response to the item “I have not accomplished many worthwhile things in this job”.\nRPA4: Response to the item “I do not feel like I’m at the end of my rope”.\nRPA5: Response to question “In my work, I do not deal with emotional problems very calmly”.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 7. Results of the burnout survey\n\n\n\n\n\n\n3.5 Interdisciplinary academic writing self-efficacy\n\n Link to the dataset\nThis dataset contains the result of nursing students’ responses to the Situated Academic Writing Self-Efficacy Scale (SAWSES) [69] questionnaire for interdisciplinary students (543 undergraduate and 264 graduate). Participating students were recruited from three higher education institutions and from the general public using social media. The survey contains 16 items based on Bandura’s self-efficacy theory [70] and the model proposed by [71].\nThe questionnaire items are related to three dimensions. The first dimension is Writing Essentials, with three items related to synthesis, emotional control, and language. The second dimension is Relational Reflective Writing, with seven items related to relationship building with writing facilitators and the self through reflection. The third dimension is Creative Identity, with six items related to gaps in student achievement of transformative writing. Demographic data for age, gender, years in post-secondary, English language status, English writing status, and writing attitude are also included.\nThe survey has been validated in a published article [72]. We make use of this dataset in Chapter 8 [12], devoted to clustering algorithms. The dataset is published under the CC0 1.0, which means that anyone can copy, modify, distribute it, even for commercial purposes, without asking permission from the authors. The dataset variables are described below.\n\n\n\nFirst, we describe the demographic variables, which can be previewed below.\n\nAge: Age.\nGender: Student’s gender, 1 = male (24.91%), 2 = female (72.12%), 3 = non-binary (2.97%).\nWritingAttitude: Writing attitude, 1 = dislikes writing (44.73%), 2 = somewhere in between (14.37%), or 3 = likes writing (44.73%).\nTypeStudent: Academic level, 1 = undergraduate (67.29%), 2 = graduate (32.71%).\nUgyears: Undergraduate years in the school for undergraduate students.\nGryears: Graduate years in the school for graduate students.\nWriteEnglish: English writing status (1-5).\nSpeakEnglish: English language status (1-5).\n\n\n\n\n\n\n\n\n  \n\n\n\nNext, we present the questionnaire responses for each of the dimensions. The responses are on a scale of 0-100. Below is a preview of the data, and Figure 2.8 presents a box plot with some the response distribution of the questionnaire.\n\nWriting Essentials\n\novercome: Response to the item “Even when the writing is hard, I can find ways to overcome my writing difficulties”.\nwords: Response to the item “I can successfully use scholarly academic words and phrases when writing in my courses”.\nsynthesize: Response to the item “I can combine or synthesize multiple sources I’ve read to create an original product or text”.\n\nRelational Reflective Writing\n\nmeaning: Response to the item “When I write, I can think about my audience and write so they clearly understand my meaning”.\nimprove: Response to the item “When I receive feedback on my writing, no matter how it makes me feel, I can use that feedback to improve my writing in the future”.\nreflect: Response to the item “When I reflect on what I am writing I can make my writing better”.\nideas: Response to the item “When I read articles about my topic, the connections I feel with the ideas of other authors can inspire me to express my own ideas in writing”.\noverall: Response to the item “When I look at the overall picture I’ve presented in my writing, I can assess how all the pieces tell the complete story of my topic or argument”.\nwander: Response to the item “I can recognize when I’ve wandered away from writing what my audience needs to know and have begun writing about interesting, but unrelated, ideas”.\nadapt: Response to the item “With each new writing assignment, I can adapt my writing to meet the needs of that assignment”.\nfeedback: Response to the item “When I seek feedback on my writing, I can decide when that feedback should be ignored or incorporated into a revision in my writing”.\n\nCreative Identity\n\ncreativity: Response to the item “I can use creativity when writing an academic paper”.\nspark: Response to the item “I feel I can give my writing a creative spark and still sound professional”.\nvoice: Response to the item “I feel I can develop my own writing voice (ways of speaking in my writing that are uniquely me)”.\noriginal: Response to the item “Even with very specific assignment guidelines, I can find ways of writing my assignment to make it original or unique”.\ndiscipline: Response to the item “I can comfortably express the concepts, language, and values of my discipline or major in my writing assignments”.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 8. Results of the SAWSES survey\n\n\n\n\n\n\n3.6 Educators’ discussions in a MOOC (SNA)\n Link to the dataset\nThis dataset belongs to two offerings of the MOOC “The Digital Learning Transition in K-12 Schools” [73]. The course was aimed at helping school and district leaders implement digital learning initiatives in K-12 education. The objectives of the course were for participants to understand the advantages of digital learning in schools, to assess the specific goals for their own school, and to devise a plan to achieve such goals. The course consisted of five learning units dealing with the schools of the future, teaching and learning culture, successful digital transition, leading the transition, and crowd sourcing. The MOOCs were offered to American as well as international teachers. There were two offerings of the MOOC, with minor variations regarding duration and groups. The dataset contains the interactions of the MOOC discussion forums and concerns teachers’ communications throughout the courses. The dataset also contains the characteristics of the teachers, e.g., their professional roles, and experience.\nThe dataset is extensively described in a dedicated publication where the authors give details about the context and the courses, the files, and the fields contained in each file [74]. In this book, we use this dataset to illustrate dissimilarity-based clustering [39], Social Network Analysis [39] and Temporal Network Analysis [41]. The dataset is available with a CC0 1.0 license. Therefore, permission to copy, modify, and distribute, even for commercial purposes, is granted. As a standard Social Network Analysis dataset, it comes in two files for each of the courses (four in total), which we describe in detail below.\n\nEdges file: This file defines who interacted (the source or the sender of the communication) with whom (the target or the receiver of the communication). The edges file comes with other metadata, such as time, discussion topic and group. Below is a preview of one of the edge files, and Figure 2.9 shows the network of collaboration between forum contributors.\n\nSender: Source of the communication identifier (1-445).\nReceiver: Target of the communication identifier (1-445).\nTimestamp: Timestamp of the intervention in “m/d/Y H:M” format, ranging from April 10th 2013 to June 8th 2013.\nDiscussion Title: Title of the discussion.\nDiscussion Category: Category of the discussion.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 9. MOOC participants’ network\n\n\n\n\n\nNodes file: The file defines the characteristics of the interacting teachers, their IDs, their professional status and expertise level. Below is a preview of one of the nodes file data.\n\nUID: Teacher identifier (1-445).\nrole1: Role of the teacher.\nexperience: Level of experience (1-3).\nexperience2: Years of experience.\ncountry: Country of origin.\ngender: Teachers’ gender, female (68.09%); male (31.69%).\nexpert: Level of expertise (0-1).\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\nCentralities file: The file contains the centrality measures of the participants which indicate their number of contributions (OutDegree), replies (InDegree), position in the network (Closeness_total), worth of their connections (Eigen), spread of their ideas (Diffusion_degree), and more. For more information on how to calculate centralities from interaction data, refer to Chapter 15 [39].\n\nname: Teacher identifier (1-445).\nInDegree: In-degree centrality. Number of responses received.\nOutDegree: Out-degree centrality. Number of messages sent.\nCloseness_total: Closeness centrality. Position in the network.\nBetweenness: Betweenness centrality. Influential position.\nEigen: Eigen centrality. Worth of connections.\nDiffusion.degree: Diffusion degree centrality [75]. Spread of ideas.\nCoreness: Coreness centrality. Spreading capability.\nCross_clique_connectivity: Cross clique connectivity. Facilitation of information propagation.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n3.7 High school learners’ interactions (SNA)\n Link to the dataset\nThe next dataset [76] concerns a course of interactions among 30 students in a high school in Kenitra, Morocco. The course under examination had a duration of two months and covered topics related to computer science: the computer information system, algorithmis and programming. The course was implemented in the Moodle LMS, using the forum as a discussion space. Students’ interactions were aimed at communicating, discussing and exchanging knowledge among them.\nThe dataset has been analyzed using social network analysis, is briefly described in an article [77], and is shared under Creative Commons license CC BY 4.0, which means that anyone can share, copy and modify this dataset so long as appropriate credit is given. The dataset includes two files described below.\n\nEdges file: The file contains the interactions source, target and weights. Below is a preview of the edge files. Figure 2.10 presents the graph of all the interactions on the network.\n\nsource: Source node identifier (1-21).\nTarget: Target node identifier (1-21).\nW: Weight of the link (the value is always 1).\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFigure 10. High school learners’ network\n\n\n\n\n\nNodes file: contains the characteristics of the interacting students, e.g., gender and age. Below is a preview of the dataset.\n\nID: Student identifier (1-30).\nUsername: Username of the student.\nname: Name of the student.\ngenre: Gender of the student F (n = 23); M (n = 7).\nDate de naissance: Birthdate of the student in format “D/M/Y”.\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n3.8 Interactions in an LMS forum from a programming course (SNA)\n Link to the dataset\nThis dataset includes message board data collected from a programming undergraduate course in a higher education institution in Spain using the Moodle LMS. The most particular characteristic of the course is that it follows the CTMTC (Comprehensive Training Model of the Teamwork Competence) methodology [78], which allows individualized training and assessment of teamwork across all stages of teamwork-based learning: storming, norming, performing, delivery and documentation [79].\nThis is a mandatory course with a workload of 6 ECTS. The data dates back to the first semester of the 2014-2015 academic year. The course offers foundational knowledge on programming and numeric calculus, has a strong focus on the development of algorithms, and a hands-on approach to teaching. The course starts with a two-hour long introductory session; after this session, students work in class and out of class during the whole semester on a team project, following the CTMTC methodology. Individual evidence of teamwork is collected from forum activity, where the work phases are presented, and group evidence of teamwork is collected from Dropbox and wikis.\nThe dataset refers to individual evidence of teamwork; in other words, it contains information about forum interactions during the course. The original dataset, obtained from Moodle logs, was processed with the help of GraphFES [80] to provide condensed information. The output of GraphFES consists of three different datasets: views, or the number of times user a read a message posted by user b; replies, which informs about the number of replies from user a to user b; and messages, which provides a network with the hierarchical structure of messages in the forum. This dataset has been used previously in [81] and is now being publicly released under a CC 4.0 BY-NC-SA license, which means that anyone is free to share, adapt, and distribute the data as long as appropriate credit is given, it is not used for commercial purposes, and the original license is kept. The dataset is used in Chapter 16 [40] of this book, about community detection. This dataset presents the replies network, a directed graph, and consists of an edges file and a nodes file.\n\n\n\n\nEdges file: this file includes information about who (attribute source) interacted with (replied to) whom (attribute target); in other words, the sender and the receiver of the informational exchange, and how many times that exchange happened during the course (attribute weight), considering all messages exchanged. The dataset includes a total of 662 weighed edges. Below is a preview of the edges file data, and Figure 2.11 represents the complete network of interactions among the Moodle users.\n\nsource: Source node identifier (108 distinct values).\ntarget: Target node identifier (108 distinct values).\nweight: Weight of the link.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNodes file: this file contains information about all users with access to the course in the Moodle space, including students, instructors and administrators. The file includes a total of 124 nodes (users); of these, 110 users are students, distributed in 19 groups of between 5 and 7 members each. The file includes an identifier for each user (attribute id), the username (attribute user; after anonymization, all usernames have the format user_id), the number of initial posts, which refers to the number of first posts in a thread (attribute initPosts), the number of replies, or posts that were a reply to another post (attribute replyPosts) and the total number of posts by that user in the forum (attribute totalPosts), which is the sum of initPosts and replyPosts. It is worth noting that user_55, a central node of the network, corresponds to the main instructor of the course. Below is a preview of the nodes file.\n\nUser: User identifier. There are 124 distinct users.\ninitPosts: Number of first posts in a thread.\nreplyPosts: Number of replies to posts in a thread.\ntotalPosts: Total number of posts by a user in the forum.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 11. Interactions of the users in the Moodle discussion forum\n\n\n\n\n\n\n3.9 Engagement and achievement throughout a study program\n Link to the dataset\nThis dataset contains simulated data of students’ online engagement and academic achievement throughout a study program. The dataset has been simulated based on the results of a published article [25]. The article used students’ logs extracted from a university’s Moodle LMS for all the subjects and for all the students who attended the years: 2015, 2016, 2017 and 2018. The logs were used to derive indicators of engagement, such as frequency of performing learning activities (course browsing, forum consumption, forum contribution, and lecture viewing), session count, total session time, active days and regularity of each activity. Regularity of viewing the course main page, for example, was calculated by dividing main page browse actions for the given student over the total main page browse actions over the course duration; the resulting probabilities are used within a Shannon entropy formula to calculate the entropy.\nThen, Latent Class Analysis was used to cluster students into engagement states for each course: Active, Average or Disengaged. Achievement was measured through course final grades, which were divided into tertiles: Achiever, Intermediate and Low. Hence, for each course, students had an engagement state, and an achievement state. The motivation and process of deriving these states from students’ engagement indicators is explained in Chapter 11 [31].\nThe simulated dataset contains the data for 142 students for 8 sequential courses, including their engagement and achievement states, as well as covariate data. It is shared with a CC BY 4.0 license, which means that anyone is free to share, adapt, and distribute the data, as long as appropriate credit is given. The dataset is used in Chapter 13 [33] of this book, to illustrate multi-channel sequence analysis. A preview of the dataset can be seen below. A visual representation of the evolution of engagement and achievement throughout the program is depicted in Figure 2.13. The dataset contains two files:\n\n3.9.1 Longitudinal engagement indicators and grades\nThe file LongitudinalEngagement.csv contains all of the engagement indicators per student and course (frequency, duration and regularity of learning activities) as well as the final grade. Each column is described below and a preview can be seen below and in Figure 2.12.\n\nUserID: User identifier. There are 142 distinct users.\nCourseID: Course identifier. There are 38 distinct courses.\nSequence: Course sequence for the student (1-8).\nFreq_Course_View: Number of views of the course main page.\nFreq_Forum_Consume: Number of views of the forum posts.\nFreq_Forum_Contribute: Number of forum posts created.\nFreq_Lecture_View: Number of lectures viewed.\nRegularity_Course_View: Regularity of visiting the course main page.\nRegularity_Lecture_View: Regularity of visiting the lectures.\nRegularity_Forum_Consume: Regularity of reading forum posts.\nRegularity_Forum_Contribute: Regularity of writing forum posts.\nSession_Count: Number of online learning sessions.\nTotal_Duration: Total activity time online.\nActive_Days: Number of active days (with online activity).\nFinal_Grade: Final grade (0-100).\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 12. Histogram of engagement indicators and final grade throughout the eight courses\n\n\n\n\n\n\n3.9.2 Longitudinal engagement and achievement states\nThe file SequenceEngagementAchievement.xlsx contains students’ engagement and achievement states for each course as well as covariates (their previous grade, their attitude towards learning, and their gender). Engagement states were obtained by applying model-based clustering techniques to the engagement indicators in the previous file. Achievement states were obtained in a similar way for the final grade. The dataset columns are described below and a preview of the data can be seen below and a graphical representation is shown in Figure 2.13.\n\nUserID: User identifier. There are 142 distinct users.\nCourseID: Course identifier. There are 38 distinct courses.\nSequence: Course sequence for the student (1-8).\nEngagement: Engagement state. There are 3 distinct states: Active (29.93%), Average (47.98%), amd Disengaged (22.10%).\nFinal_Grade: Final grade of each student for each course (1-100).\nAchievement: Achievement state calculated using model-based clustering. There are 3 distinct states: Achiever (39.26%), Intermediate (23.33%), and Low (37.41%).\nAchievementNtile: Achievement state calculated using tertiles. There are 3 distinct states: Achiever (33.27%), Intermediate (33.36%), and Low (33.36%).\nPrev_grade: GPA with which the student applied to the program (1-10).\nAttitude: Attitude towards learning (0-20).\nGender: Gender (male/female). There are 44% females and 56% males.\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 13. Sequence and engagement for each student across eight courses\n\n\n\n\n\n\n\n3.10 University students’ basic need satisfaction, self-regulated learning and well-being during COVID-19\n Link to the dataset\nThis dataset contains the results of a survey investigating students’ psychological characteristics related to their well-being during the COVID-19 pandemic. The variables under study are related to the satisfaction of basic psychological needs (relatedness, autonomy, and experienced competence), self-regulated learning, positive emotion and intrinsic learning motivation. Moreover, the dataset contains demographic variables, such as country, gender, and age. The data were collected from 6,071 students from Austria and Finland. There are, however, 564 records with missing responses to at least one item.\nThis dataset has been used in a published study to examine the relationships between the different variables using SEM [82]. The dataset has been used in Chapter 19 [21] of this book, to illustrate the implementation of psychological networks. The dataset has been published under a CC BY 4.0 license, which means that you are free to use and adapt the data but you must give appropriate credit, provide a link to the license, and indicate if changes were made. A summary of the responses is in Figure 2.14. Below, we describe each of the dataset columns and provide a preview of its content.\n\nDemographic data\n\ncountry: Country of the student: 0 = Austria (78.60%), 1 = Finland (21.40%).\ngender: Gender of the student: 1 = Female (70.74%), 2 = Male (28.25%), 3 = Other (0.70%).\nage: Age of the student.\n\n\nBelow we describe the items of the questionnaire for each construct. The possible responses are: 1 = strongly agree, 2 = agree, 3 = somewhat agree, 4 = disagree, 5 = strongly disagree.\n\nBasic Psychological Needs: Relatedness\n\nsr1: Response to the item “Currently, I feel connected with my fellow students”.\nsr2: Response to the item “Currently, I feel supported by my fellow students”.\nsr3: Response to the item “Currently, I feel connected with the people who are important to me (family, friends)”.\n\nBasic Psychological Needs: Competence\n\ncomp1: Response to the item “Currently, I am dealing well with the demands of my studies”.\ncomp2: Response to the item “Currently, I have no doubts about whether I am capable of doing well in my studies”.\ncomp3: Response to the item “Currently, I am managing to make progress in studying for university”.\n\nBasic Psychological Needs: Autonomy\n\nauto1: Response to the item “Currently, I can define my own areas of focus in my studies”.\nauto2: Response to the item “Currently, I can perform tasks in the way that best suits me”.\nauto3: Response to the item “In the current home-learning situation, I seek out feedback when I need it”.\n\nPositive Emotion\n\npa1: Response to the item “I feel good”.\npa2: Response to the item “I feel confident”.\npa3: Response to the item “Even if things are difficult right now, I believe that everything will turn out all right”.\n\nIntrinsic learning motivation\n\nlm1: Response to the item “Currently, doing work for university is really fun”.\nlm2: Response to the item “Currently, I am really enjoying studying and doing work for university”.\nlm3: Response to the item “Currently, I find studying for university really exciting”.\n\nSelf-regulated learning\n\ngp1: Response to the item “In the current home-learning situation, I plan my course of action”.\ngp2: Response to the item “In the current home-learning situation, I think about how I want to study before I start”.\ngp3: Response to the item “In the current home-learning situation, I formulate learning goals that I use to orient my studying”.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nFigure 14. Results of COVID-19 well-being survey"
  },
  {
    "objectID": "chapters/ch02-data/ch2-data.html#discussion",
    "href": "chapters/ch02-data/ch2-data.html#discussion",
    "title": "2  A Broad Collection of Datasets for Educational Research Training and Application",
    "section": "4 Discussion",
    "text": "4 Discussion\nIn this chapter, we have provided an overview of the types of data operationalized in learning analytics research. Our journey encompassed a wide spectrum of data types, ranging from foundational demographic information to the footprints left by the interactions of students with online learning technologies, including clicks, activities, social interactions, and assessment data. We have pointed to some of the most commonly employed analytical techniques for each type of data and we referred the reader to the chapters of the book that have covered each type of analysis. Thereafter, we presented a meticulous curation of illustrative datasets. We have described each dataset in detail, describing and representing the relevant variables. We also acknowledged the ways each dataset have been analyzed throughout the remaining chapters of the book.\nWe must disclose that collecting learners’ data is not an easy endeavor. First and foremost, it is crucial to consider the ethical implications of collecting and using different types of data, and to comply with data protection laws and regulations [14]. Moreover, it is important to ensure the data quality to draw relevant conclusions from the data, especially in scenarios where data come from heterogeneous sources and are provided in large quantities [83], such as in the educational field [84]. These requirements make finding good-quality open datasets online extremely challenging. In this regard, we hope that the selection offered in this chapter is useful for the reader beyond the scope of the book. A few articles have offered other dataset collections suitable for learning analytics [85] or educational data mining [86]. Moreover, the reader is encouraged to consult open data repositories where datasets are continuously published in multiple fields: Zenodo (https://zenodo.org), US Department of Education Open Data Platform (https://data.ed.gov), Harvard Dataverse (https://dataverse.harvard.edu), European Data Portal (https://data.europa.eu), Mendeley Data (https://data.mendeley.com), openICPSR (https://www.openicpsr.org), Google Dataset Search (https://datasetsearch.research.google.com), figshare (https://figshare.com), Open Science Framework (https://osf.io), or data.world (https://data.world)."
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html",
    "href": "chapters/ch03-intro-r/ch3-intor.html",
    "title": "3  Getting started with R for Education Research",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#introduction",
    "href": "chapters/ch03-intro-r/ch3-intor.html#introduction",
    "title": "3  Getting started with R for Education Research",
    "section": "1 Introduction",
    "text": "1 Introduction\nR is a free, versatile, and open source programming language and software environment specifically designed for statistical computing and data analysis. R has a vast library of packages that enable data manipulation, visualization, modeling and machine learning [1]. Such a wealth of packages enable a wide range of functionalities that saves the users time, effort or the need to write complex code. The fact that R is freely available makes all these functionalities accessible to all. R has a large community of developers, users and researchers who support the development of the platforms as well as provide support and shared knowledge on popular sites such as StackExchange. Thereupon, R is becoming an increasingly popular choice for students, researchers and data scientists [2].\nBeing open source and accessible to researchers, several packages are added continuously to expand the possibilities and functions that R offers. Some of the R packages included in this book have been added by researchers during the last few years to address contemporary scientific problems and state-of-the-art innovations [3]. For example, R software packages for the analysis of psychological networks were developed in the past five years and ever since have grown tremendously due to contributions from a large base of researchers [4].\nAlthough many of the methods described in this book can be implemented with other software tools, it is hard to find a comprehensive platform that can be used to perform practically all the existing learning analytics methods with such maturity, performance and range of possibilities. For instance, Social Network Analysis (SNA) can be performed with several programming languages (e.g., Python) and desktop software applications (e.g., Gephi) [5]. However, both options provide limited capabilities compared to what R provides for the analysis of SNA. This includes a wider range of SNA centrality measures, mathematical models, community finding algorithms and generative models. Sequence analysis is another example in which the possibilities offered by R are hard to match with other software solutions.\nThis book does not make any assumptions about the superiority of R over any other platform. Other languages and software platforms are indeed very helpful and have vast capabilities for researchers. For instance, Python has remarkable tools for machine learning and Gephi offers beautiful graphics for the visualization of networks. Oftentimes, readers may need to learn or use other tools to do specific tasks. Put another way, where R offers a rich toolset for researchers, there is a space for other tools that researchers can use to accomplish certain tasks. In summary, investing time in learning R is a worthwhile endeavor that helps interested researchers to perform and expand their research skills and toolset. Since R is a large platform, it represents a doorway to the vast capabilities of its ever expanding repertoire of functions and packages."
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#learning-r",
    "href": "chapters/ch03-intro-r/ch3-intor.html#learning-r",
    "title": "3  Getting started with R for Education Research",
    "section": "2 Learning R",
    "text": "2 Learning R\nThe goal of programming is to write, i.e. code, a program that performs a desired task. A program consists of several commands, each of which does something very simple. In the statistics and data analysis context, R is typically used to write short programs called scripts. R is therefore not intended for developing games or other complicated programs. R is also not a language originally intended for web programming, although with the right packages you can also make web applications with R.\nR is a high-level programming language. This means that there are many ready-made commands in R, which have much more code “underneath” that the R programmer does not have to touch. For example, a statistical t-test requires several mathematical intermediate steps, but an R programmer can perform the test with a single command (t.test) that provides all the necessary computations and information about the test.\nThe best way to learn how to use and program R code is by doing. This text has R code embedded between the text in gray boxes, as in the example below. Lines starting with two hashes, i.e. ##, are not code, but output generated by running the code. Let’s first take the classic “Hello, world!” command as an example:\n\nprint(\"Hello, world!\")\n\n[1] \"Hello, world!\"\n\n\nThe print function prints the given text to the console. It is convenient, for example, for testing the operation of a program and monitoring the progress of a longer program. R can also be used as a calculator. In the example below, we calculate the price of a product after a 35% discount that was originally priced at 80 euros.\n\n80 * (1 - 0.35)\n\n[1] 52\n\n\nHowever, running individual commands is usually not useful unless the results can be saved somewhere. In programming languages, data is stored in variables, which you will become familiar with later."
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#rstudio",
    "href": "chapters/ch03-intro-r/ch3-intor.html#rstudio",
    "title": "3  Getting started with R for Education Research",
    "section": "3 RStudio",
    "text": "3 RStudio\nR has a large array of tools and integrated development environments (IDEs) that make writing and managing code easier and more accessible. The most widely used R IDE is RStudio, which is a free open source software that — similarly to R — runs on all major operating systems [6]. RStudio provides a comprehensive and user-friendly interface for writing, running, and debugging R code, which makes it easier for users to get started and become more productive. Together, R and RStudio allow for the creation of reproducible research. The code and results can be easily shared and replicated, making R and RStudio great tools for collaboration and transparency.\nR and RStudio can be very useful in analyzing and visualizing different types of data. This can help researchers, educators and administrators make data-driven decisions and improve the learning experience for students. Whether you are analyzing student performance, demographic data, or tracking the effectiveness of instructional interventions, R and RStudio provide a flexible and efficient platform for achieving your goal.\nFirst install R (step 1) and then RStudio Desktop for your operating system. R and RStudio are available for many operating systems. The interface of RStudio shown in Figure 3.1 has the following default components:\n\n\n\nFigure 1. RStudio user interface.\n\n\n\nEditor: The editor is used to write files containing R code, i.e., R scripts. Scripts will be introduced later, but they are simply a collection of R commands that carry out a specific task when placed together, for example analyze the data of a research project or draw figures of the finished results. A new script can be opened from the “File” menu by selecting “New File” and then “R script”. The same can be accomplished via a keyboard shortcut by pressing Ctrl + Shift + N on Windows and Linux. On macOS, users can use Cmd + Shift + N. Also, see the “Keyboard Shortcuts Help” under the “Tools” menu for all the shortcuts available. The code written in the editor can be run line by line by pressing Ctrl + Enter at the line. Several lines can also be selected and run at once. The Source button at the top runs all the code in the current file. R scripts can be saved just like other files and their file extension is .R. All the code you use to analyze your data should be written in scripts. When you save your code in this way, the next time you can simply run the script to perform the desired task instead of rewriting the code from scratch.\nConsole: R commands are executed in the console. If the code written in the editor is run, RStudio automatically executes the commands in the console. In the console, just pressing Enter is enough to execute a line of code. You can try to write a calculation in the console, such as 2 * 3 and press Enter, and the result will be printed in the console. You can also write code in the editor and press Ctrl + Enter to accomplish the same result. Possible messages, warnings and errors are also printed into the console. The main difference between the console and the editor is that the commands written in the console are not saved in any file. So if you want to keep your code, it should be written in the editor and saved in a script file. Commands made during the same session can be scrolled in the console with the up and down arrows. In addition, the command history can be viewed in the History tab in RStudio.\nWorkspace: Displays the variables in the current working environment of the R session.\nFiles: Shows the directory structure of the operating system, by default the working directory.\nPlots: Graphics drawn with R appear here.\nPackages: Here you can manage the installed packages (instructions for installing the packages are below).\nHelp: Here you can browse the R manual with instructions for every R command. You can try running the ?print command in the editor or console, which opens the help page for the print function."
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#best-practices-in-programming",
    "href": "chapters/ch03-intro-r/ch3-intor.html#best-practices-in-programming",
    "title": "3  Getting started with R for Education Research",
    "section": "4 Best practices in programming",
    "text": "4 Best practices in programming\nAs the word “script” already suggests, data analysis often requires a “script” of various steps, such as reading measurements from a file, organizing a table, or describing the results of an analysis, for example by calculating the averages of different sample groups and drawing figures. Writing the R commands that perform these steps in a script provides a documentation of how the analysis was done, and it’s also easy to come back to and possibly add a few additional steps, such as performing a t-test. The script file is also easy to share with others who work with similar data analysis tasks, because with small modifications (file names, table structure) the same code also works for different datasets and workflows. Comments are often used inside the code. Comments are text written along the R commands, which are not written in a programming language, and which are ignored when running the code. The purpose of comments is to describe the behavior and purpose of the code. It is good to practice to comment your own code from the beginning, even if the code for the first tasks is very simple. In R, comments are marked with the ## symbol.\n\n## Assign arbitrary numbers to two variables\nx <- 3\ny <- 5\n## Sum of two variables\nz <- x + y\n## Print the results\nz\n\n[1] 8\n\n\n\n4.1 R Markdown\nWhile scripts can only store code and comments, a more comprehensive format called R Markdown is also available in RStudio. R Markdown is an extension of the Markdown markup language that allows users to create dynamic reports and interactive notebooks that can integrate text, code, and visualizations. R Markdown documents are created in a plain text format and can be rendered into various output formats, such as HTML, PDF, Word, or even presentations. To create a new R Markdown document in RStudio, go to the “File” menu, select “New File” and finally “R Markdown”. In the dialog that opens, choose the output format you want to use, and give your document a title. Figure 3.2 shows the RStudio editor panel for a default R Markdown document of R Studio.\n\n\n\nFigure 2. An example R Markdown document in R Studio.\n\n\nThe YAML metadata wrapped between the --- delimiters at the top of the document can be used to customize the output and contents in various ways. For example, you can change the title, author, or add a table of contents. In this example we have defined the title, the output format (HTML) and the date, but many more options are available besides these. Use Markdown syntax to format your text, and enclose your R code in code chunks with the ```{r} and ``` tags which can also be provided other options. For example, our first code chunk has been given the label setup and the following option include = FALSE means that this chunk will be executed but its output will not be printed into the final document. Within the chunk, a global knitr option is set so that all code chunks will print (echo) their output by default. You can also use inline chunks to quickly reference the results of computations or other R objects. Some examples of standard markdown syntax in the document are second level headers marked with ## (note that # is not used for comments in an R Markdown document) and bold text denoted by wrapping the text with **. Once you are satisfied with your document, you can render it into the chosen output format by clicking the “Knit” button in RStudio, or use the keyboard shortcut “Ctrl + Shift + K”. Figure 3.3 shows the corresponding rendered HTML document.\n\n\n\nFigure 3. The example R Markdown document rendered into HTML.\n\n\nThis simple example shows only a fraction of the full features of R Markdown documents and the Markdown syntax. R Markdown is a powerful tool for creating reproducible research reports, teaching materials, or even websites. It allows users to integrate code and output seamlessly into their written work, making it easier to share and reproduce analyses. As an alternative to R Markdown documents, R Studio also supports the creation of R Notebooks, which are in essence interactive R Markdown documents. R Notebooks can be useful for example when the goal is not to produce one comprehensive analysis report but instead to keep track of the code and try out various approaches to a problem interactively. For an in-depth guide to R Markdown, see [7] which is also freely available online at https://bookdown.org/yihui/rmarkdown/.\n\n\n4.2 How is code developed?\nCode development typically follows similar steps:\n\nDesign parts of the code.\nStart by writing a small piece of code.\nTest whether the code you wrote works. If it doesn’t, find out why and fix it.\nGo to the next piece of code and continue accordingly, always testing piece by piece whether your code works.\n\nAlong with this material, many packages include a “Cheat Sheet” as a summary of basic tasks and functions related to the package. Cheat sheets provide a quick and easy reference for checking how something is done in R if you don’t remember it by heart. There are cheat sheets for various R packages and other entities on the Internet e.g., the base R cheat sheet, or the tidyr [8] cheat sheet.\nIn addition to the basic commands presented in this chapter of the book, practical R programming relies to a great extent on the use of various packages developed by the scientific community. Packages are collections of code that contain new functions, classes and data, i.e., they extend R. Most R packages are available from the Comprehensive R Archive Network (CRAN). They can be installed with the install.packages() function, or via RStudio’s installation window which in practice calls the install.packages() function. You can also install several packages at once. The command below installs the dplyr [9] and tidyr packages:\n\ninstall.packages(c(\"dplyr\", \"tidyr\"))\n\nIn order to use the commands contained in an R package, the package must be installed and attached to the R workspace. This is done with the library() command:\n\nlibrary(\"tidyr\")\n\nNow that the tidyr package is loaded, we can use the commands it provides, for example to manage the learning analytics data in data frame format that we will address later. If you don’t want to attach the entire package, you can use individual commands from packages with the format name_of_the_package::name_of_the_command()."
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#basic-operations",
    "href": "chapters/ch03-intro-r/ch3-intor.html#basic-operations",
    "title": "3  Getting started with R for Education Research",
    "section": "5 Basic operations",
    "text": "5 Basic operations\nBasic operations in R consist of arithmetic operations, logical operations, and assignment. In addition, there are several commands that are often helpful when starting a new project or managing the working directory. For example, the current working directory can be obtained with the following command:\n\ngetwd()\n\n[1] \"/home/sonsoles/labook/chapters/ch03-intro-r\"\n\n\n\n5.1 Arithmetic operators\nR can be used to compute basic arithmetic operations such as addition (+), subtraction (-), multiplication (*), division (/), and exponentiation (^). These operations follow standard precedence rules, and additional brackets can be added to control the evaluation order if needed.\n\n1 + 1 ## Addition\n\n[1] 2\n\n2 - 1 ## Subtraction\n\n[1] 1\n\n2 * 4 ## Multiplication\n\n[1] 8\n\n5 / 2 ## Division\n\n[1] 2.5\n\n2 ^ 4 ## Exponentiation\n\n[1] 16\n\n\n\n\n5.2 Relational operators\nRelational operators compare objects or values to other objects or values. These operators are often required for conditional data filtering, for example when selecting a subset of individuals that satisfy some criterion. In R, there are six such operators: smaller than, greater than, smaller or equal to, greater or equal to, equal to, and not equal to. There operators have the following syntax in R:\n\n1 < 2  ## Smaller than\n\n[1] TRUE\n\n3 > 2  ## Greater than\n\n[1] TRUE\n\n2 <= 2 ## Smaller or equal to\n\n[1] TRUE\n\n3 >= 3 ## Greater or equal to\n\n[1] TRUE\n\n5 == 5 ## Equal to\n\n[1] TRUE\n\n1 != 2 ## Equal to\n\n[1] TRUE\n\n\nIn the previous example we used these operators to compare integers, but we may also use them to compare other types of values, such as characters:\n\n\"a\" == \"b\"\n\n[1] FALSE\n\n\n\n\n5.3 Logical operators\nSimilar to relational operators, logical operators are used to evaluate the logical value of a conjunction of two logical values. In R, there are five logical operators: negation, AND, OR, elementwise AND, and elementwise OR. These operators have the following syntax:\n\n!TRUE         ## Negation\n\n[1] FALSE\n\nTRUE && TRUE  ## Logical AND\n\n[1] TRUE\n\nTRUE || FALSE ## Logical OR\n\n[1] TRUE\n\nTRUE & TRUE   ## Elementwise AND\n\n[1] TRUE\n\nTRUE | FALSE  ## Elementwise OR\n\n[1] TRUE\n\n\nThe elementwise operators & and | can be used to compare multiple pairs of logical values simultaneously, for example\n\nc(TRUE, FALSE, TRUE) | c(TRUE, FALSE, FALSE)\n\n[1]  TRUE FALSE  TRUE\n\n\nwhereas the operators && and || only accept single values. In the previous example, we also used one of the most important operations of R: the c() function (the letter ‘c’ is short for “combine”) which we used to combine the logical values into a vector, i.e., an ordered sequence of values of the same type. Vectors and other important data types will be discussed in greater details in the next section.\n\n\n5.4 Special operators\nAnother core functionality in R is the assignment operator <-. Assignment can be used to store the results of computations into variables which can then be used again in other computations without having to redo the original computations. For instance\n\nx <- 5 ## Assign value 5 into variable named x\ny <- 7 ## Assign value 7 into variable named y\nx      ## Access value of x (value is printed into the console in RStudio)\n\n[1] 5\n\ny      ## Access value of y\n\n[1] 7\n\nx + y  ## Compute the sum of x and y\n\n[1] 12\n\nx > y  ## Is x greater than y?\n\n[1] FALSE\n\n\nHere we chose the names x and y for our variables, but the names are arbitrary with the caveat that one should avoid assigning values to objects with names that R already uses internally, such as names of common functions like c(), exp(), or lm() to name a few. Variables currently assigned in the working environment can be displayed with the following command:\n\nls()\n\n[1] \"x\" \"y\" \"z\"\n\n\nIt is also possible to use the equal sign = as the assignment operator, but this is often not recommended, because the equal sign also has other purposes in the R language and may cause confusion if used for assignment. There are also some special instances, where the equals sign does not function identically to <-. Therefore, we recommend always using the standard assignment operator <-.\nWhen constructing vectors, the function c() can be cumbersome in some scenarios. For instance, say we wanted to create a vector that contains all integers from 1 to 100. With c(), we would have to write each value individually. Fortunately, such sequences can be constructed effortlessly using the : (colon) operator:\n\n1:100\n\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100\n\n\nOften we may wish to select only specific values from a vector. Values in a vector can be accessed by their index, starting from 1. This is accomplished by using the subset operator which uses the following square bracket syntax. For example, we could select the first value of a vector x by writing x[1]. For a more involved example, we could simultaneously select the first, 50th and 100th value of the vector 1:100 by writing:\n\nx <- 1:100\nx[c(1, 50, 100)]\n\n[1]   1  50 100\n\n\nEssentially, we write the positions of the values that we wish to select inside the square brackets as a vector. Alternatively, we can create a vector of logical values that has the same length as the vector we are selecting from, and the value TRUE for the values we wish to select, and FALSE otherwise. In the next section, we also show how to select values based on a condition."
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#basic-data-types-and-variables",
    "href": "chapters/ch03-intro-r/ch3-intor.html#basic-data-types-and-variables",
    "title": "3  Getting started with R for Education Research",
    "section": "6 Basic data types and variables",
    "text": "6 Basic data types and variables\nIn the examples of the previous section, we already used several of the most common data types that users are most likely to encounter in practical data analyses. Each object in R has a type, which can be determined with the function typeof(). Types are used to describe what kind of data our variables of interest contain and what kind of operations can be carried out on them. Perhaps the most common type is the numeric type, which describes values that can be interpreted as numbers. Two special instances of the numeric type are integer and double, which correspond to integer values and decimal values, respectively.\n\ntypeof(1.0)\n\n[1] \"double\"\n\ntypeof(1L)\n\n[1] \"integer\"\n\n\nThe capital L on the second line denotes that we mean the integer 1, and not the decimal number 1.0. Text data is often represented by the character type. Unlike in some other programming languages, in R the character type does not necessarily describe individual characters as the name would suggest, but character strings, for instance:\n\ntypeof(\"a\")\n\n[1] \"character\"\n\ntypeof(\"hello world!\")\n\n[1] \"character\"\n\n\nAs we can see, both have the same type.\nThe logical type is used to represent the boolean values TRUE and FALSE. Logical values are typically not as common in actual data where such values may often be represented by the integers 1 and 0 instead. However, their importance lies in forming conditions for data filtering and manipulations that we may wish to carry out based on a criterion that is true for only a subset of subjects in our data. For example, suppose we have the values 1, 2, 3, 4, 5, and we wish to programmatically select only those values that are greater than 2. We can accomplish this as follows:\n\nsome_numbers <- 1:5\nsome_numbers[some_numbers > 2]\n\n[1] 3 4 5\n\n\nLet’s walk through what the code above does step by step. On the first line, we create a vector of numeric values 1 through 5 by using the : operator and assign the result to the variable named some_numbers. On the second line, we use the square brackets (the subset operator) to select only those values of some_numbers for which the condition some_numbers > 2 is true. Here we introduce the vectorization feature of R which applies to a vast majority of arithmetic and relational operators. By writing some_numbers > 2, we actually evaluate the condition for every value in the vector some_numbers:\n\nsome_numbers > 2\n\n[1] FALSE FALSE  TRUE  TRUE  TRUE\n\n\nAll of the aforementioned types are called atomic types, meaning that vectors of such values can only contain values of one specific atomic type. For example, a vector cannot contain values of both character and integer types:\n\nc(0L, \"a\")\n\n[1] \"0\" \"a\"\n\n\nWe see that the result was automatically converted to an atomic character vector.\nAlongside type, objects in R also have a class, which can be viewed with the class() function. For atomic types, their class is the same as their type, but for more complicated types of variables, a class essentially describes a special instance of a type. Objects within a specific class typically have their own set of functions or methods that can only be applied for that specific class. A common example of a class is factor. Factors are a special class of integer vectors designed to represent categorical and ordinal data. Variables that are of the factor class have levels, which differentiates factor variables from ordinary integer variables. For example, a factor could represent group membership in a randomized experiment indicating inclusion in the control group or the treatment group for each individual. The levels of this factor could be called \"control\" and \"treatment\" for example. Supposing that 0 indicates that an individual belongs in the control group and 1 indicates that an individual belongs in the treatment group, we could create such a factor by writing\n\ngroup <- c(0, 0, 1, 0, 1, 0, 1, 1)\nfactor(group, labels = c(\"control\", \"treatment\"))\n\n[1] control   control   treatment control   treatment control   treatment\n[8] treatment\nLevels: control treatment\n\n\nFactors are important when fitting statistical models or when performing statistical tests, because if we would simply use the corresponding integer values, they may be erroneously interpreted as continuous. The levels of the factor also often help to provide more informative output. In the next section we will discuss more complicated data types including data frames, which can contain data of various types simultaneously."
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#basic-r-objects",
    "href": "chapters/ch03-intro-r/ch3-intor.html#basic-r-objects",
    "title": "3  Getting started with R for Education Research",
    "section": "7 Basic R objects",
    "text": "7 Basic R objects\nIn this section we will discuss the concepts of data frame and tibble. Let’s assume that your data can be stored in a two-dimensional array where the columns represent variables and each row represents a case of measurement. In R, a data frame is a concept for storing such a two-dimensional array of data. Let’s first study how data frames work in R and we will then move on to see how another concept called tibble extends the capabilities of a data frame.\nA data frame which has been loaded into R under the name grades and printed in the R console will look as follows.\n\n\n\n\n\n\n\ngrades\n\n\n\n  \n\n\n\nIn the above printout we can see that R prints the data frame just as we would expect the data to look like. One issue with a standard data frame is that if there are very many columns or rows, then the printout may be difficult to read. In RStudio, a neater (and more flexible) way of inspecting the data is by using a command called View():\n\nView(grades)\n\n\nTo go further with the data, we need tools for data manipulation. We begin with the very basic tools which are commonplace in any data analysis workflow. More comprehensive knowledge about data transforming, cleaning etc. can be found in Chapter 4.\nA data frame can be constructed directly in the R by using the function called data.frame() and then listing variable names and their values. The grades data above has two columns, group and grade, and each row represents a student. The variable group is indicates the number of a group in which each person has studied. The variable grade stores the final grade that the student has received from a course."
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#working-with-dataframes",
    "href": "chapters/ch03-intro-r/ch3-intor.html#working-with-dataframes",
    "title": "3  Getting started with R for Education Research",
    "section": "8 Working with dataframes",
    "text": "8 Working with dataframes\nTo extract a column from a data frame, one needs to start with the name of the data frame object and connect the column name with the name of the data frame object by using the dollar symbol ($). Thus, extracting the column group from grades data can be accomplished by writing\n\ngrades$group\n\n[1] 2 2 3 4 4\n\n\nNext, let’s use a couple of basic functions which are often needed when developing R code. First of all, to get a summary of every variable of a data frame, we can call\n\nsummary(grades)\n\n     group       grade      \n Min.   :2   Min.   :2.630  \n 1st Qu.:2   1st Qu.:3.390  \n Median :3   Median :4.670  \n Mean   :3   Mean   :4.496  \n 3rd Qu.:4   3rd Qu.:4.900  \n Max.   :4   Max.   :6.890  \n\n\n\nThe result show us the minimum, maximum, median, mean and quartiles of both variables. You may note that as group was intended to be a categorical variable, so computing its mean value in the data does not make sense. We can change that behavior by converting the group column into a factor.\n\ngrades$group <- as.factor(grades$group)\nsummary(grades)\n\n group     grade      \n 2:2   Min.   :2.630  \n 3:1   1st Qu.:3.390  \n 4:2   Median :4.670  \n       Mean   :4.496  \n       3rd Qu.:4.900  \n       Max.   :6.890  \n\n\nOn the other hand, we might want to calculate the mean or the sample standard deviation of the variable grade. This can be done with functions called mean() and sd(), respectively.\n\nmean(grades$grade)\n\n[1] 4.496\n\nsd(grades$grade)\n\n[1] 1.630178\n\n\nThe functions above can only take numeric vectors as input. If we tried using another type of argument, we would encounter an error message.\n\nsd(grades)\n\nError in is.data.frame(x): 'list' object cannot be coerced to type 'double'\n\n\nThe above message can actually teach us a couple of things. First of all, the error comes from the function is.data.frame(), which is called somewhere in the definition of sd(). Second, the actual error message tells us that the object which we gave to the function as its argument is a list object. List is an object type of R on top of which data.frame objects have been built. We will describe lists in greater detail later. Further, the message tells us that R has tried to coerce the argument object into the double type. This means that the object we supplied to the function is not of the right type and cannot easily be converted into the proper format. \n\n8.1 tibble\nA tibble is an expansion of data.frame objects which is used in the tidyverse programming paradigm [10]. To use tidyverse, we advise to load the tidyverse [11] (meta)package which loads all key tidyverse packages.\n\n## load tidyverse to use as_tibble\nlibrary(\"tidyverse\")\n## convert a data frame as tibble\ngrades2 <- as_tibble(grades)\n\nNext, let’s see what a tibble looks like when printed in the console\n\ngrades2\n\n\n\n  \n\n\n\nTibbles behave similarly compared to data frames when printed, but they also describe the dimensions of the data and the types of the columns right under the column names. For instance, <fct> refers to a factor column and <dbl> refers to a double column. In order to discover the column types when using data frames, one would need to apply the class() or typeof() function to the columns, or write str(grades) to see the types of the columns and the structure of the data. Another useful property of tibble tables is that if a tibble has a large number of observations or variables, then only the rows or the columns which can fit on to the screen are printed.\nTidyverse and tibbles also support so called lazy evaluation, which is useful when your data is stored in a database, for instance. With lazy evaluation, the commands that you use on your data would be evaluated directly in the database (if possible). Without lazy evaluation, the entire data would be downloaded onto your computer only after which the commands would be evaluated. Lazy evaluation can perform many tasks faster and it can also alleviate memory usage of the computer."
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#pipes",
    "href": "chapters/ch03-intro-r/ch3-intor.html#pipes",
    "title": "3  Getting started with R for Education Research",
    "section": "9 Pipes",
    "text": "9 Pipes\nPiping is a fairly recent concept in R and was very rarely used in R code just a few years ago. The concept of a pipe originates from a package called magrittr, and pipes are commonly used under the tidyverse programming paradigm, but the pipe was also later added to base R. The notations for a tidyverse pipe and a native R pipe are %>% and |>, respectively. The idea of a pipe is that you can connect multiple function calls sequentially while keeping the code more readable. Pipes also serve to unnest standard R code which often involves using many nested parentheses, and can quickly become hard to read as one has to read the code based on the order of the operations instead of reading it linearly. For example, consider the following code where we apply a sequence of operations on a numeric vector x.\n\nx <- 1:10\nround(mean(diff(log(x))), digits = 2)\n\n[1] 0.26\n\n\nThis code computes the rounded mean differences of the logarithms of the vector x, however this description does not match the order of operations, where the logarithm is computed first. To accomplish the same result using pipes we would write\n\nx <- 1:10\nx |> log() |> diff() |> mean() |> round(digits = 2)\n\n[1] 0.26\n\n\nHere, the order of operations can be easily read from left to right. Next, we will discuss the use of pipes in more detail.\n\n9.1 magrittr pipe %>%\nLet’s have a look at an example, where we call the summary() function for the grades2 data using the magrittr pipe %>%, and we also define that results should be printed with two digits.\n\ngrades2 %>%\n  summary(digits = 2)\n\n group     grade    \n 2:2   Min.   :2.6  \n 3:1   1st Qu.:3.4  \n 4:2   Median :4.7  \n       Mean   :4.5  \n       3rd Qu.:4.9  \n       Max.   :6.9  \n\n\nIn the code above, the object grades2 is taken by the pipe operator %>% and forwarded to the first argument of the summary() function. The summary() function also has a second argument, which is defined by digits = 2. Thus, the pipe only takes the object mentioned before the pipe operator and forwards it to the function after the pipe as the first free argument. It is very common and recommended to structure R code so that there is only one pipe per row and that a new line is started after each pipe.\nAlthough the above example is easy to understand as we already know the summary() function, there is also a more general way to compute summarized information following the tidyverse style. The function summarise() can be used to compute arbitrary statistics from the data, for example the number of observations (via the function n()) and the mean and the sample standard deviation of the variable grade.\n\n\ngrades2 %>%\n  summarise(\n    n = n(),\n    mean = mean(grade), \n    sd = sd(grade)\n  )\n\n\n\n  \n\n\n\nThe summarise() function also produces a tibble enabling further operations via piping, if desired.\n\n\n9.2 Native pipe |>\nIn R version 4.1, the native pipe |> was introduced to the R language, which does not require any external packages to use. In most scenarios, it does not matter whether the native or the magrittr pipe is used. However, there are two technical differences between the magrittr pipe and the native pipe. First, the magrittr pipe is actually a function\n\nclass(`%>%`)\n\n[1] \"function\"\n\n\nwhile the native pipe is not, and simply converts the written code into a non-piped version, i.e., into a form that one would write without using the pipe:\n\nx <- 1:5\nquote(x |> sum())\n\nsum(x)\n\n\nWe see that providing x to the sum function via the native pipe is exactly the same as writing sum(x) directly. What this means in practice is that the native pipe may have better performance for example when passing a large dataset through a large number of pipes. The reason for this is that the magrittr pipe incurs an additional computational function call overhead each time it is called. The second difference between the pipes is that parentheses have to be provided for function calls when using the native pipe, but they can be omitted when using the magrittr pipe\n\nx %>% sum\n\n[1] 15\n\nx %>% sum()\n\n[1] 15\n\nx |> sum()\n\n[1] 15\n\n\n\nx |> sum ## produces an error\n\nError: The pipe operator requires a function call as RHS (<text>:1:6)"
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#lists",
    "href": "chapters/ch03-intro-r/ch3-intor.html#lists",
    "title": "3  Getting started with R for Education Research",
    "section": "10 Lists",
    "text": "10 Lists\nEarlier, we already briefly mentioned lists in the context of data frames. Lists are one the most common types of data in R and they resemble basic vectors in many aspects. Like vectors, a list is an ordered sequence of elements, but unlike vectors, lists can contain elements of different types simultaneously and may even contain other lists. For example, we could construct a list that contains a logical value, a numeric value and a character value using the list() function as follows.\n\ny <- list(TRUE, 7.2, \"this is a list\")\n\nSubsetting a list object works slightly differently compared to vectors. When single brackets are used, a sublist is selected, i.e., a list object that contains the elements at the supplied indices, for example:\n\ny[1:2]\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] 7.2\n\ntypeof(y[1:2])\n\n[1] \"list\"\n\n\nTo extract an actual element of a list, double brackets should be used:\n\ny[[2]]\n\n[1] 7.2\n\n\nThe elements of a list may also be named, which enables subsetting via the dollar sign operator similar to data frames, or by giving the element name in double brackets instead of the index:\n\nz <- list(bool = TRUE, num = 7.2, description = \"this is another list\")\nz$bool\n\n[1] TRUE\n\nz[[\"description\"]]\n\n[1] \"this is another list\"\n\n\nOne benefit of using the dollar sign is that it is not necessary to provide the full element name, unlike when using the double brackets. It is sufficient to provide a prefix of the element name so that the full name can be uniquely determined from the prefix. Because all the names of our elements in the previous list z start with a different letter, the first letter of the name suffices as the prefix. The same functionality also applies when using the dollar sign to select columns of data frames.\n\nz$b\n\n[1] TRUE\n\nz$n\n\n[1] 7.2\n\nz$d\n\n[1] \"this is another list\""
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#functions",
    "href": "chapters/ch03-intro-r/ch3-intor.html#functions",
    "title": "3  Getting started with R for Education Research",
    "section": "11 Functions",
    "text": "11 Functions\nA function is a set of statements that when organized together perform a specific task. Each function in R has a name, a set of arguments, a body and a return object. The name of the function usually describes the purpose of the function. For example, the base R function mean() computes the arithmetic mean of the argument vector. The result is returned as a numeric value.\n\nx <- 1:5\nmean(x)\n\n[1] 3\n\n\nFunctions are often much more complicated, which is why it is often helpful to view the documentation of a function before using it in practice. To view the documentation pages of a function, one can simply write the name of the function prefixed by a question mark.\n\n?mean\n\nIn RStudio, the documentation will open in the “Help” tab in the bottom right pane by default. Functions will only be executed when they are called, i.e., when arguments are supplied to them. Simply writing the function name without parentheses will instead print the body of the function to the console, meaning the code that the function consists of and which is executed if the function is called.\nIn the previous sections, we’ve already familiarized ourselves with some commonly used basic functions such as c(), sd(), and summary(). Base R has a wide range of function to accomplish common tasks needed in data analysis, which is further extended by tidyverse and other R packages. This means that one does not typically have to write their own functions when programming in R. We will explore several of the functions provided by the tidyverse in later chapters."
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#conditional-statements",
    "href": "chapters/ch03-intro-r/ch3-intor.html#conditional-statements",
    "title": "3  Getting started with R for Education Research",
    "section": "12 Conditional statements",
    "text": "12 Conditional statements\nSometimes we may only wish to execute a piece of code when a certain condition is met. Conditional statements in R can be defined via the if and else clauses. The if clause evaluates a condition, which is an R expression that evaluates to a single logical value, and if this condition evaluates to TRUE, the expression following the clause if executed. Further, if an else clause is also provided, the expression following else will be executed instead if the condition evaluates to FALSE. As R code, the syntax for these clauses is\n\nif (cond) expr\nif (cond) expr else alt_expr\n\nwhere cond is the condition being evaluated, expr is the expression that will be evaluated if cond == TRUE, and alt_expr will be evaluated if cond == FALSE.\nNote that if will only evaluate a single condition. If cond is a vector, an error will be produced:\n\ncond <- c(TRUE, FALSE)\nif (cond) {\n  print(\"This will not be printed\")\n}\n\nError in if (cond) {: the condition has length > 1\n\n\nAs expected, the error message tells us that the condition contained more than one element when it was evaluated. However, there are often scenarios where we may wish to conditionally select or define values based on a vector of conditions. For such instances, the function ifelse() can be used. This function has three arguments: test, yes, and no. When called, the function will pick those elements of the vector yes for which the logical vector test evaluates to TRUE, and those elements of the vector no for which test evaluates to FALSE:\n\ncond <- c(TRUE, FALSE, FALSE, TRUE)\nx <- 1:4\ny <- -(1:4)\nifelse(cond, x, y)\n\n[1]  1 -2 -3  4"
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#looping-constructs",
    "href": "chapters/ch03-intro-r/ch3-intor.html#looping-constructs",
    "title": "3  Getting started with R for Education Research",
    "section": "13 Looping constructs",
    "text": "13 Looping constructs\nIn some cases, we may wish to execute the same piece of code multiple times under varying conditions. Instead of writing the same code multiple times for each condition, we can use a looping construct. There are two types of loops in R: the for loop and the while loop. The main difference between the two loops is that for always executes the code associated with the loop a fixed number of times whereas while will continue executing the code as long as a specific condition remains satisfied. The syntax of these loops is\n\nfor (var in seq) expr\nwhile (cond) expr\n\nIn other words, for will execute the expression expr for every element var in some object seq that can be indexed. For-loops are very general, and can be used to loop over most ordered structures such as vectors and lists. Similarly, while will execute the expression expr as long as the condition cond evaluates to TRUE. Care must be taken when using while-loops to ensure that the condition will eventually evaluate to FALSE, otherwise the loop will simply run indefinitely and the program will be stuck. As an example, we will print the number 1 through 5 to the console using both for and while loops:\n\nx <- 1:5\nfor (i in x) {\n  print(x[i])\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\ni <- 0\nwhile (i < length(x)) {\n  i <- i + 1\n  print(x[i])\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nIn contrast to explicit for and while loops, the so-called apply family of functions can often be a simpler alternative (see ?apply). As the name suggests, these functions apply an operation to each element of a list or a vector (and other more general data structures). For example, the above loop example could also be accomplished with the lapply() function as follows:\n\ny <- lapply(x, print)\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5"
  },
  {
    "objectID": "chapters/ch03-intro-r/ch3-intor.html#discussion-and-other-resources-for-learning-r",
    "href": "chapters/ch03-intro-r/ch3-intor.html#discussion-and-other-resources-for-learning-r",
    "title": "3  Getting started with R for Education Research",
    "section": "14 Discussion and other resources for learning R",
    "text": "14 Discussion and other resources for learning R\nThe main aim behind this chapter was to introduce R to new users. This chapter is, of course, an initial step and can hardly cover all the basics. Interested users are advised to consult other resources e.g., open access books, tutorials, cheat sheets, and package manuals for more information. An introductory book “An Introduction to R” packaged with each R installation can be accessed from the RStudio Help panel by first selecting “Show R Help” and then selecting “An Introduction to R” under “Manuals”. This book covers a wide range of topics on base R programming in great detail. The book “R for Data Science” by Hadley Wicham and Garret Grolemund provides a comprehensive tutorial on using R for data science under the tidyverse paradigm. The book is free to use and readily available online at https://r4ds.had.co.nz/. As we go further, several questions will emerge and the reader will learn by doing and by consulting the literature and help files. In doing so, the reader will build knowledge and experience that helps advance their skills."
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#introduction",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#introduction",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "1 Introduction",
    "text": "1 Introduction\nWhen analyzing data, it is crucial that the data is in a suitable format for the tools you will be using. This makes data wrangling essential. Data preparation and cleaning, such as extracting information from raw data or removing erroneous measurements, must also be done before data is ready for analysis. Data wrangling often takes up the majority of the time spent on analysis, sometimes up to 80 percent. To reduce the amount of work required, it is beneficial to use tools that follow the same design paradigm to minimize the time spent on data wrangling. The tidyverse [1] programming paradigm is currently the most popular approach for this in R.\nThe tidyverse has several advantages that make it preferable over other alternatives such as simply using base R for your data wrangling needs. All packages in the tidyverse follow a consistent syntax, making it intuitive to learn and use new tidyverse packages. This consistency also makes the code more easier to read, and maintain, and reduces the risk of errors. The tidyverse also has a vast range of readily available packages that are actively maintained, reducing the need for customized code for each new data wrangling task. Further, these packages integrate seamlessly with one another, facilitating a complete data analysis pipeline.\n\nTo fully realize the benefits of the tidyverse programming paradigm, one must first understand the key concepts of tidy data and pivoting. Tidy data follows three rules:\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nLet’s consider examples of tidy data. For instance, if you have data from a Moodle course where two attempts of an exam for each student are located in a single  column. This example data violates the first rule, because there are two variables in a single column instead of separate columns for each variable. What is needed to make this data tidy is to pivot it to a longer format. The pivoted data would have two rows for each student, both of which are different observations (exam attempts 1 and 2). Thus the pivoted data would not conflict with second rule.\n\nData can also be too long, but in practice, this is much more rare. This can occur if two or more variables are stored on a single column across multiple rows. A key indicators of this is if the different rows of the same column have different measurements units (e.g. lb vs. kg). It may also occur that your raw data has multiple values in a single cell. In these cases, it is necessary to split the cells to extract the necessary information. In a simple case, where you have two values systematically in a one cell, the values can be easily separated into their own columns.\nOverall, using tidyverse and understanding the key concepts of tidy data and pivoting can streamline the data analysis process and make code easier to work with and maintain. The rest of this chapter will guide readers through the process of data cleaning and wrangling with R in the field of learning analytics. We demonstrate how data can be grouped and summarized, how to select and transform variables of interest, and how data can be rearranged, reshaped and joined with other datasets. We will strongly rely on the tidyverse programming paradigm for a consistent and coherent approach to data manipulation, with a focus on tidy data."
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#reading-data-into-r",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#reading-data-into-r",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "2 Reading data into R",
    "text": "2 Reading data into R\nData files come in many formats, and getting your data ready for analysis can often be a daunting task. The tidyverse offers much better alternatives to base R functions for reading data, especially in terms of simplicity and speed when reading large files. Additionally, most of the file input functions in the tidyverse follow a similar syntax, meaning that the user does not have to master every function for reading every type of data individually.\nOften, just before the data can be read into R, user must specify the location of data files by setting a working directory. Perhaps most useful way to do that is to create a project in RStudio and then create a folder called “data” within the project folder. Data files can be put into that folder and user can refer to those files just by telling R-functions relative path of data file (e.g. “data/Final%20Dataset.csv”) while the project takes care of the rest of the path. A more traditional way of setting this, which also works without RStudio, is by using a command such as setwd(\"/home/Projects/LAproject/data/Final%20Dataset.csv\"). Here, a function called setwd() is used to set up a folder into location mentioned in a character string given as its argument. A getwd() lists current working directory, which can also be seen in RStudio just above the Console output.\nSome of the most common text data formats are comma-separated files or semicolon-separated files, both of which typically have the file extension .csv. These files can be read into R using the readr [2] package and the functions read_csv() and read_csv2(), respectively. For instance, we can read a comma-separated file R as follows\n\n\n\nFunctions in readr provide useful information about how the file was read into R, which can be used to assess if the input was successful and what assumptions about the data were made during the process. In the printout above, the read_csv() function tells us the number of rows and columns in the data and the column specification, i.e., what type of data is contained within each column. In this case, we have 17 columns with character type of data, and 4 columns of double type of data. Functions in readr try to guess the column specification automatically, but it can also be specified manually when using the function. For more information about this dataset, please refer to Chapter 2 in this book [3].\nData from Excel worksheets can be read into R using the import() function from the rio [4] package. We will use synthetic data generated based on a real blended course of learning analytics for the remainder of this chapter. These data consist of three Excel files which we will first read into R.\n\nlibrary(\"rio\")\nurl <- \"https://github.com/lamethods/data/raw/main/1_moodleLAcourse/\"\nevents <- import(paste0(url, \"Events.xlsx\"), setclass = \"tibble\")\nresults <- import(paste0(url, \"Results.xlsx\"), setclass = \"tibble\")\ndemographics <- import(paste0(url, \"Demographics.xlsx\"), setclass = \"tibble\")\n\nThe data files contain information on students’ Moodle events, background information such as their name, study location and employment status, and various grades they’ve obtained during the course. For more information about the dataset, please refer to Chapter 2 in this book [3]. These data are read in the tibble [5] format, a special type of data.frame commonly used by tidyverse packages. We also load the dplyr [6] package which we will use for various tasks throughout this chapter.\n\nlibrary(\"dplyr\")"
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#grouping-and-summarizing-data",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#grouping-and-summarizing-data",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "3 Grouping and summarizing data",
    "text": "3 Grouping and summarizing data\nInstead of individual-level data metrics, we may be interested in specific groups as specified by some combination of data values. For example, we could compute the number of students studying in each location by gender. To accomplish this, we need to start by creating a grouped dataset with the function group_by(). To compute the number of students, we can use the summarise() function which we already used previously in Chapter 1 and the function count(), which simply returns the number of observations in each category of its argument.\n\ndemographics |>\n  group_by(Gender) |> \n  count(Location)\n\n\n\n  \n\n\n\nThe column n lists the number of students in each group. When a tibble that contains grouped data is printed into the console, the grouping variable and the number of groups will be displayed in the console below the dimensions. Next, we will compute the total number of Moodle events of each student, which we will also use in the subsequent sections.\n\nevents_summary <- events |>\n  group_by(user) |> \n  tally() |>\n  rename(Frequency.Total = n)\nevents_summary\n\n\n\n  \n\n\n\nHere, the function tally() simply counts the number of number rows in the data related to each student, reported in the column n which we rename to Frequency.Total with the rename() function. We could also count the number of events by event type for each student\n\nevents |>\n  group_by(user, Action) |>\n  count(Action)"
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#selecting-variables",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#selecting-variables",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "4 Selecting variables",
    "text": "4 Selecting variables\nIn the tidyverse paradigm, selecting columns, i.e., variables from data is done using the select() function. The select() function is very versatile, allowing the user to carry out selections ranging from simple selection of a single variable to highly complicated selections based on multiple criteria. The most basic selection selects only a single variable in the data based on its name. For example, we can select the employment statuses of students as follows\n\ndemographics |> \n  select(Employment)\n\n\n\n  \n\n\n\nNote that using select() with a single variable is not the same as using the $ symbol to select a variable, as the result is still a tibble, select() simply produces a subset of the data, where only the selected columns are present. Select is more similar to subset() in base R, which can accomplish similar tasks as select() and filter() in the tidyverse. However, we do not recommend using subset(), as it may not work correctly when the working environment has variables that have the same name as columns in the data, which can lead to undesired outcomes.\nTo extract the values of the selected column as a vector, we can use the function pull(). We use the head() function here to limit the console output to just the first few values of the vector (default is 6 values).\n\ndemographics |> \n  pull(Employment) |>\n  head()\n\n[1] \"None\"      \"None\"      \"None\"      \"None\"      \"Part-time\" \"Part-time\"\n\n\nThe select() function syntax supports several operations that are similar to base R. We can select ranges of consecutive variables using :, complements using !, and combine selections using c(). The following selections illustrate some of these features:\n\ndemographics |> \n  select(user:Origin)\n\n\n\n  \n\n\ndemographics |> \n  select(!Gender)\n\n\n\n  \n\n\ndemographics |> \n  select(c(user, Surname))\n\n\n\n  \n\n\n\nIn the first selection, we select all variables starting from user on the left to Origin on the right. In the second, we select all variables except Gender. In the third, we select both user and Surname variables.\nSometimes, our selection might not be based directly on the variable names themselves as in the examples above but instead on vectors that contain the names of columns we wish to select. In such cases, we can use the function all_of(). We can consider the intersections or unions of such selections using & and |, respectively.\n\ncols_a <- c(\"user\", \"Name\", \"Surname\")\ncols_b <- c(\"Surname\", \"Origin\")\ndemographics |> \n  select(all_of(cols_a))\n\n\n\n  \n\n\ndemographics |> \n  select(all_of(cols_a) & all_of(cols_b))\n\n\n\n  \n\n\ndemographics |> \n  select(all_of(cols_a) | all_of(cols_b))\n\n\n\n  \n\n\n\nOften the names of data variables follow a similar pattern, and these patterns can be used to construct selections. Selections based on a prefix or a suffix in the variable name can be carried out with the functions starts_with() and ends_with(), respectively. The function contains() is used to look for a specific substring in the names of the variables, and more complicated search patterns can be defined with the function matches() that uses regular expressions (see ?tidyselect::matches for further information).\n\nresults |> \n  select(starts_with(\"Grade\"))\n\n\n\n  \n\n\nresults |> \n  select(contains(\"Data\"))\n\n\n\n  \n\n\n\nSo far, our selections have been based on variable names, but other conditions for selection are also feasible. The general-purpose helper function where() is used to select those variables for which a function provided to it returns TRUE. For example, we could select only those columns that contain character type data or double type data.\n\nresults |> \n  select(where(is.character))\n\n\n\n  \n\n\nresults |> \n  select(where(is.double))"
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#filtering-observations",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#filtering-observations",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "5 Filtering observations",
    "text": "5 Filtering observations\nIn contrast to selection which relates to obtaining a subset of the columns of the data, filtering refers to obtaining a subset of the rows. In the tidyverse, data filtering is carried out with the dplyr package function filter(), which should not be confused with the base R filter() function in the stats package. As we have attached the dplyr package, the base R filter() function is masked, meaning that when we write code that uses filter(), the dplyr version of the function will automatically be called.\nFiltering is often a much simpler operation than selecting variables, as the filtering conditions are based solely on the values of the data variables. Using filter() is analogous to the base R subset operator [, but the filtering condition is given as an argument to the filter() function instead. It is good to remind that in R a single equal sign (=) is merely for arguments of function calls, while double equal sign (==) is needed for comparison of two values. And example of filter:\n\ndemographics |> \n  filter(Origin == \"Bosnia\") |> \n  select(Name, Surname)\n\n\n\n  \n\n\n\nThe code above first filters our student demographics data to only those students whose country of origin is Bosnia. Then, we select their first and last names.\nMultiple filtering conditions can be refined and combined using base R logical operators, such as & and |.\n\ndemographics |>\n  filter(Gender == \"F\" & Location == \"Remote\")\n\n\n\n  \n\n\n\nHere, we filtered our data to female students who are studying remotely. The same result could also be obtained by using the filter() function two times\n\ndemographics |>\n  filter(Gender == \"F\") |> \n  filter(Location == \"Remote\")\n\n\n\n  \n\n\n\nThis type of approach may improve the readability of your code especially when there are several independent filtering conditions to be applied simultaneously.\nFilters can naturally be based on numeric values as well. For example, we could select those students whose final grade is higher than 8.\n\nresults |> \n  filter(Final_grade > 8)\n\n\n\n  \n\n\n\nSimilarly, we could select students based on their total number of Moodle events.\n\nevents_summary |>\n  filter(Frequency.Total > 100 & Frequency.Total < 500)"
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#transforming-variables",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#transforming-variables",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "6 Transforming variables",
    "text": "6 Transforming variables\nIn the best-case scenario, our data is already in the desired format after it has been read into R, but this is rarely the case with real datasets. We may need to compute new variables that were not present in the original data, convert measurements to different units, or transform text data into a numeric form. In the tidyverse, data transformations are carried out by the mutate() function of the dplyr package. This function can be used to transform multiple variables at the same time or to construct entirely new variables. The syntax of the function is the same in both cases: first, the name of the variable should be provided followed by an R expression that defines the variable. The transformed data is not automatically assigned to any variable, enabling transformations to be used as temporary variables within a chain of piped operations.\nAs a simple example, we could convert the students’ locations into a factor variable.\n\ndemographics |> \n  mutate(Location = factor(Location))\n\n\n\n  \n\n\n\nAs we see from the tibble printout, the Location variable is a factor in the transformed data as indicated by the <fct> heading under the variable name. Note that the original demographics data was not changed, as we did not assign the result of the computation.\nThe gender and employment status of the students could also be used as factors, which we could do in a single mutate() call\n\ndemographics |> \n  mutate(\n    Gender = factor(Gender),\n    Location = factor(Location),\n    Employment = factor(Employment)\n  )\n\n\n\n  \n\n\n\nHowever, writing out individual identical transformations manually is cumbersome when the number of variables is large. For such cases, the across() function can be leveraged, which applies a function across multiple columns. This function uses the same selection syntax that we already familiarized ourselves with earlier to define the columns that will be transformed. To accomplish the same three transformations into a factor format, we could write\n\ndemographics |> \n  mutate(across(c(Gender, Location, Employment), factor))\n\n\n\n  \n\n\n\nThe first argument to the across() function is the selection that defines the variables to be transformed. The second argument defines the transformation, in this case, a function, to be used.\nWorking with dates can often be challenging. When we read the student demographic data into R, the variable Birthdate was assumed to be a character type variable. If we would like to use this variable to e.g., compute the ages of the students, we need to first convert it into a proper format using the as.Date function. Since the dates in the data are not in any standard format, we must provide the format manually. Afterwards, we can use the lubridate [7] package to easily compute the ages of the students, which we will save into a new variable called Age. We will also construct another variable called FullName which formats the first and last names of the students as \"Last, First\".\n\nlibrary(\"lubridate\")\ndemographics |> \n  mutate(\n    Birthdate = as.Date(Birthdate, format = \"%d.%m.%Y\"),\n    Age = year(as.period(interval(start = Birthdate, end = date(\"2023-03-12\")))),\n    FullName = paste0(Surname, \", \", Name) \n  ) |> \n  select(Age, FullName)\n\n\n\n  \n\n\n\nThe computation of the ages involves several steps. First, we construct a time interval object with the interval() function from the birthdate to the date for which we wish to compute the ages. Next, the as.period() function converts this interval into a time duration, from which we lastly get the number of years with the year() function.\nSuppose that we would like to construct a new variable AchievingGroup that categorizes the students into top 50% achievers and bottom 50% achievers based on their final grade on the course. We leverage two functions from the dplyr package to construct this new variable: case_when() and ntile(). The function case_when() is used to transform variables based on multiple sequential conditions. The function ntile() has two arguments, a vector x and an integer n, and it splits x into n equal-sized groups based on the ranks of the values in x.\n\nresults <- results |>\n  mutate(\n    AchievingGroup = factor(\n      case_when(\n        ntile(Final_grade, 2) == 1 ~ \"Low achiever\",\n        ntile(Final_grade, 2) == 2 ~ \"High achiever\"\n      )\n    )\n  )\n\nThe syntax of case_when() is very simple: we describe the condition for each case followed by ~ after which we define the value that the case should correspond to. We assign the result of the computation to the results data, as we will be using the AchievingGroup variable in later chapters.\nWe would also like to categorize the students based on their activity level, i.e., the number of total Moodle events. Our goal is to create three groups of equal size consisting of low activity, moderate activity and high activity students. The approach we applied to categorizing the achievement level of the students is also applicable for this purpose. We name our new variable as ActivityGroup, and we assign the result of the computation, as we will also be using this variable in later chapters.\n\nevents_summary <- events_summary |> \n  mutate(\n    ActivityGroup = factor(\n      case_when(\n        ntile(Frequency.Total, 3) == 1 ~ \"Low activity\",\n        ntile(Frequency.Total, 3) == 2 ~ \"Moderate activity\",\n        ntile(Frequency.Total, 3) == 3 ~ \"High activity\"\n      )\n    )\n  )"
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#rearranging-data",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#rearranging-data",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "7 Rearranging data",
    "text": "7 Rearranging data\nSometimes we may want to reorder the rows or columns of our data, for example in alphabetical order based on the names of students on a course. The arrange() function from the dplyr package orders the rows of the by the values of columns selected by the user. The values are sorted in ascending order by default, but the order can be inverted by using the desc() function if desired. The variable order in the selection defines how ties should be broken when duplicate values are encountered in the previous variables of the selection. For instance, the following code would arrange the rows of our demographics data by first comparing the surnames of the students, and then the given names for those students with the same surname. Missing values are placed last in the reordered data.\n\ndemographics |> \n  arrange(Surname, Name)\n\n\n\n  \n\n\n\nA descending order based on both names can be obtained by applying the desc() function.\n\ndemographics |>\n  arrange(desc(Surname), desc(Name))\n\n\n\n  \n\n\n\nColumn positions can be changed with the relocate() function of the dplyr package. Like arrange(), we first select the column or columns we wish to move into a different position in the data. Afterwards, we specify the position where the columns should be moved to in relation to positions of the other columns. In our demographics data, the user ID column user is the first column. The following code moves this column after the Employment column so that the user column becomes the last column in the data.\n\ndemographics |>\n  relocate(user, .after = Employment)\n\n\n\n  \n\n\n\nThe mutually exclusive arguments .before and .after of relocate() specify the new column position in relation to columns that were not selected. These arguments also support the select() function syntax for more general selections."
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#reshaping-data",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#reshaping-data",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "8 Reshaping data",
    "text": "8 Reshaping data\nBroadly speaking, tabular data typically take one of two formats: wide or long. In the wide format, there is one row per subject, where the subjects are identified by an identifier variable, such as the user variable in our Moodle data, and multiple columns for each measurement. In the long format, there are multiple rows per subject, and the columns describe the type of measurement and its value. For example, the events data is in a long format containing multiple Moodle events per student, but the results and demographics data are in a wide format with one row per student.\nIn the previous section, we constructed a summary of the users’ Moodle events in total and of different types. The latter data is also in a long format with multiple rows per subject, but we would instead like to have a column for each event type with one row per user, which means that we need to convert this data into a wide format. Conversion between the two tabular formats is often called pivoting, and the corresponding functions pivot_wider() and pivot_longer() from the tidyr [8] package are also named according to this convention. We will create a wide format data of the counts of different event types using the pivot_wider() function as follows\n\nlibrary(\"tidyr\")\nevents_types <- events |>\n  group_by(user, Action) |>\n  count(Action) |> \n  pivot_wider(\n    names_from = \"Action\", \n    names_prefix = \"Frequency.\",\n    values_from = \"n\",\n    values_fill = 0\n  )\nevents_types\n\n\n\n  \n\n\n\nHere, we first specify the column name that the names of the wide format data should be taken from in the long format data with names_from. In addition, we specify a prefix for the new column names using names_prefix that helps to distinguish what these new columns will contain, but in general, the prefix is optional. Next, we specify the column that contains the values for the new columns with values_from. Because not every student necessarily has events of every type, we also need to specify what the value should be in cases where there are no events of a particular type by using values_fill. As we are considering the frequencies of the events, it is sensible to select 0 to be this value. We save the results to events_types as we will use the event type data in later sections and chapters."
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#joining-data",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#joining-data",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "9 Joining data",
    "text": "9 Joining data\nNow that we have computed the total number of events for each student and converted the event type data into a wide format, we still need to merge these new data with the demographics and results data. Data merges are also called joins, and the dplyr package provides several functions for different kinds of joins. Here, we will use the left_join() function that will preserve all observations of the first argument.\n\nleft_join(demographics, events_summary, by = \"user\")\n\n\n\n  \n\n\n\nIn essence, the above left join adds all columns from events_summary to demographics whenever there is a matching value in the by column. To continue, we can use additional left joins to add the remaining variables from the results data, and the Moodle event counts of different types from events_types to have all the student data in a single object.\n\nall_combined <- demographics |> \n  left_join(events_types, by = \"user\") |>\n  left_join(events_summary, by = \"user\") |> \n  left_join(results, by = \"user\")\nall_combined\n\n\n\n  \n\n\n\nWe will use this combined dataset in the following chapters as well."
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#missing-data",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#missing-data",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "10 Missing data",
    "text": "10 Missing data\nSometimes it occurs that learning analytics data has cells for which the values are missing for some reason. The Moodle event data which we have utilized in this chapter does not naturally contain missing data. Thus, to have an example, we need to create a data which does. Second, handling of missing data is a vast topic of which we can only discuss some of the key points very briefly from a practical perspective. For a more comprehensive overview, we recommend reading [9] and [10] for a hands on approach. A short overview of missingness can be found in [11].\nThe code below will create missing values randomly to each column of events_types data (user column is an exception). To do that, we use the mice [12] package which also has methods for the handling of missing data. Unfortunately, mice is not part of the tidyverse. For more information about mice, a good source is miceVignettes at https://www.gerkovink.com/miceVignettes/. Now, let’s create some missing data.\n\nlibrary(\"mice\")\nset.seed(44)\nevents_types <- events_types |>\n  rename(\n    \"Ethics\" = \"Frequency.Ethics\",\n    \"Social\" = \"Frequency.Social\",\n    \"Practicals\" = \"Frequency.Practicals\"\n  )\nampute_list <- events_types |>\n  ungroup(user) |>\n  select(Ethics:Practicals)|>\n  as.data.frame() |>\n  ampute(prop = 0.3)\nevents_types_mis <- ampute_list$amp |>\n  as_tibble()\nevents_types_mis[2, \"Practicals\"] <- NA\n\nAbove, we also rename the variables that contain the frequencies of Moodle events related to ethics, social and practicals into Ethics, Social and Practicals, respectively. Let’s now see some of the values of events_types_mis\n\nevents_types_mis\n\n\n\n  \n\n\n\nWe can see that now the data contains NA values in some of the cells. These are the cells in which a missing value occurs, meaning that a value for those measurements has not been recorded. A missing data pattern, that is how missing of one variable affects missingness of other variables, can be show as:\n\nmd.pattern(events_types_mis, rotate.names = TRUE)\n\n\n\n\nAbove, each red square indicates a missing value while blue squares stand for observed ones. We can see that there are 95 complete rows, 10 for which Practicals are missing, 17 have missingness on Social and 9 are missing on Ethics. Also, one row has two red squares indicating a missing value on both Social and Practicals.\nLet’s now discuss options of handling missing data briefly. There are four classes of statistical methods for analyzing data with missing values: complete case (CC) methods, weighting methods, imputation methods, and model-based methods. The simplest of these is complete case analysis, which leaves missing values out of the analysis and only uses observations with all variables recorded. This can be done with the tidyr [8] package function drop_na():\n\nevents_types_mis |>\n  drop_na()\n\n\n\n  \n\n\n\nWe can see that after using this method, our data has only 95 rows as those were the rows without any columns having missing values. This made our data much smaller! If there are a lot of missing values, the data may become too small to use for practical purposes.\nA more novel group of methods are imputation methods. One of the options is using single imputation (SI) where the mean of each variable will determine the imputed value. The single mean imputation can be done as follows:\n\nimp <- mice(events_types_mis, method = \"mean\", m = 1, maxit = 1 , print = FALSE)\ncomplete(imp) |> \n  head()\n\n\n\n  \n\n\n\nWe can see from above that the imputed values are not integers anymore. However, if we aim to estimate means or regression coefficients (see Chapter 5 [13] for details) that is not a problem. One of the problems with mean imputation is that the variance and standard error estimates will become downward biased. A mean of Ethics for mean imputation is:\n\nfit <- with(imp, lm(Ethics ~ 1))\nsummary(fit)\n\n\n\n  \n\n\n\nNext, let’s briefly have a look at how we can utilize multiple imputation (MI) which is an improvement over single imputation. The multiple imputation approach generates more than one imputation thus creating many complete data sets for us. For each of these datasets, we can perform any analysis that we are interested in. After the analysis, one must pool the results from the impured datasets to get the final result. Here, we utilize a method called predictive mean matching (method = \"pmm\" in the code below), which uses the neighbour values of data as imputations.\n\nimp2 <- mice(events_types_mis, method = \"pmm\", m = 10, maxit = 100, print = FALSE)\nfit2 <- with(imp2, lm(Ethics ~ Practicals))\npool_fit <- pool(fit2)\n## Multiple imputation\nsummary(pool_fit)\n\n\n\n  \n\n\n## Complete cases\nsummary(lm(Ethics ~ Practicals, events_types_mis))[\"coefficients\"]\n\n$coefficients\n              Estimate Std. Error  t value   Pr(>|t|)\n(Intercept) 2.15459162 2.12235051 1.015191 0.31226283\nPracticals  0.06220884 0.02605001 2.388054 0.01865793\n\n## Without missingness\nsummary(lm(Ethics ~ Practicals, events_types))[\"coefficients\"] \n\n$coefficients\n              Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) 1.05409529 2.17821479 0.4839262 0.6292651258\nPracticals  0.08891892 0.02590313 3.4327482 0.0008053447\n\n\nFrom the results above, we can see that in this particular case the multiple imputation performs well in comparison to CC approach. The regression coefficient for full data without any missing values is \\(0.089\\), and it is \\(0.080\\) for multiple imputation, while complete case analysis gives \\(0.062\\). As all of them have very similar standard errors, this yields that MI and full data give statistically significant p-values for significance level 0.01, while CC does not."
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#correcting-erroneous-data",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#correcting-erroneous-data",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "11 Correcting erroneous data",
    "text": "11 Correcting erroneous data\nLets imagine that our data has an error on the surname variable Surname and that all the names ending with “sen” should end with “ssen”. What we can do is that we can use regular expressions to detect the erroneous rows and we can also use them to replace the values. Let’s first figure out which last names contain a name ending with “sen”. We can use a function str_detect() to return TRUE/FALSE for each row from stringr [14] package within a filter() function call. We define pattern = \"sen$\" where $ indicates the end of the string.\n\nlibrary(\"stringr\")\ndemographics |>\n  filter(str_detect(string = Surname, pattern = \"sen$\")) |>\n  pull(Surname)\n\n[1] \"Nielsen\"  \"Johansen\" \"Joensen\"  \"Jansen\"   \"Olsen\"   \n\n\nAfter pulling the filtered surnames, there seems to be five surnames ending with “sen”. Next, let’s try to replace “sen” with “ssen”. On the next row we filter just as previously to limit output.\n\ndemographics |>\n  mutate(Surname = str_replace(\n    string = Surname, pattern = \"sen$\", replacement = \"ssen\")\n  ) |>\n  filter(str_detect(string = Surname, pattern = \"sen$\")) |>\n  pull(Surname)\n\n[1] \"Nielssen\"  \"Johanssen\" \"Joenssen\"  \"Janssen\"   \"Olssen\"   \n\n\nThus, the following code updates the data so that all the surnames ending with “sen” now end with “ssen” instead.\n\ndemographics <- demographics |>\n  mutate(Surname = str_replace(\n    string = Surname, pattern = \"sen$\", replacement = \"ssen\")\n  )"
  },
  {
    "objectID": "chapters/ch04-data-cleaning/ch4-datacleaning.html#conclusion-and-further-reading",
    "href": "chapters/ch04-data-cleaning/ch4-datacleaning.html#conclusion-and-further-reading",
    "title": "4  An R Approach to Data Cleaning and Wrangling for Education Research",
    "section": "12 Conclusion and further reading",
    "text": "12 Conclusion and further reading\nData wrangling is one of the most important steps in any data analysis pipeline. This chapter introduced the tidyverse, tidy data, and several commonly used R packages for data manipulation and their use in basic scenarios in the context of learning analytics. However, the tidyverse is vast and can hardly be fully covered in a single chapter. We refer the reader to additional resources such as those found on the tidyverse website at and the book “R for Data Science” by Hadley Wicham and Garret Grolemund. The book is free to use and readily available online at https://r4ds.had.co.nz/."
  },
  {
    "objectID": "chapters/ch05-basic-stats/ch5-stats.html",
    "href": "chapters/ch05-basic-stats/ch5-stats.html",
    "title": "5  Introductory Statistics with R for Educational Researchers",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch05-basic-stats/ch5-stats.html#introduction",
    "href": "chapters/ch05-basic-stats/ch5-stats.html#introduction",
    "title": "5  Introductory Statistics with R for Educational Researchers",
    "section": "1 Introduction",
    "text": "1 Introduction\nLearning analytics involves the practical application of statistical methods to quantitative data, which can represent various aspects of the learning process such as student engagement, progress, and outcomes. Thus, knowledge about basic statistical methods is essential. Let’s start with how statistics connects with the research process and how it is also linked with philosophy of science. According to Niiniluoto [1], the research process can be described with the following eight steps:\n\nSetting up a problem.\nDisambiguation of the problem. Building a research strategy.\nCollecting data.\nDescribing the data.\nAnalysis of data.\nInterpreting the analyses.\nWriting the report.\nPublishing the results.\n\nSteps 1 and 2 require knowledge and skills related to the applied field, but also general scientific aptitude. Knowledge about statistics is central in steps 3, 4, 5, and 6. Finally, steps 7 and 8 mostly require skills in writing and communication. Overall, it can be argued that a solid understanding of statistics and statistical methods is crucial for anyone conducting research with quantitative data.\nThis chapter of the book concentrates on steps 4, 5, and 6 of the research process. We start with descriptive statistics, which are statistics that describe the overall features of the data. In contrast, inferential statistics are used to draw conclusions and make inferences about the population under study. Afterwards, we explain the basics of statistical hypothesis testing, which is the most common — although not the only — way to analyze data. The most common statistical tests, such as Student’s t-test, Chi-squared test, Analysis of variance, Levene’s test, and Shapiro-Wilk test are covered in this chapter. We also explain how to interpret the results of each test. We also present the linear regression model, which is not a statistical test but one of the most powerful statistical tools. Basic understanding of linear regression is essential for anyone interested in more advanced regression techniques, including logistic regression which is covered in the final section of this chapter. For a more in-depth view on the statistical tests covered in this chapter, we refer the reader to works such as [2, 3]."
  },
  {
    "objectID": "chapters/ch05-basic-stats/ch5-stats.html#descriptive-statistics",
    "href": "chapters/ch05-basic-stats/ch5-stats.html#descriptive-statistics",
    "title": "5  Introductory Statistics with R for Educational Researchers",
    "section": "2 Descriptive statistics",
    "text": "2 Descriptive statistics\nDescriptive statistics are used to provide a meaningful quantitative overview of data, and to summarize potentially vast amounts of information into more easily comprehensible and manageable quantities. In general, descriptive statistics are used as a first step in a data analysis workflow. In this section we will focus on numeric descriptors while visualizations are the topic of Chapter 6 [4]. For this chapter, we will use the combined Moodle data with students’ demographics, results, and summarized Moodle event activity. For more information about the dataset, please refer to Chapter 2 in this book [5]. We begin by installing all R packages that we will use in this chapter.\n\ninstall.packages(\n  c(\"car\", \"rio\", \"see\", \"dplyr\", \"tidyr\", \n    \"broom\", \"report\", \"correlation\", \"performance\")\n)\n\nWe use the rio [6] package to read the data files into R via the import() function. The Events dataset contains log data on the student’s Moodle activity such as Moodle event types and names. The Demographics dataset contains background information on the students such as their gender and location of study (categorical variables). Finally, the Results data contains grades on various aspects of the Moodle course including the final course grade (numeric variables). We also create a new variable called AchievingGroup that categorizes the students into bottom and top 50% of achievers in terms of the final grade. We will leverage the dplyr [7] and tidyr [8] packages to wrangle the data into a single combined dataset. We begin by reading the data files and by constructing the AchievingGroup variable.\n\nlibrary(\"rio\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nurl <- \"https://github.com/lamethods/data/raw/main/1_moodleLAcourse/\"\nevents <- import(paste0(url, \"Events.xlsx\"), setclass = \"tibble\")\ndemographics <- import(paste0(url, \"Demographics.xlsx\"), setclass = \"tibble\")\nresults <- import(paste0(url, \"Results.xlsx\"), setclass = \"tibble\") |> \n  mutate(\n    AchievingGroup = factor(\n      case_when(\n        ntile(Final_grade, 2) == 1 ~ \"Low achiever\",\n        ntile(Final_grade, 2) == 2 ~ \"High achiever\"\n      )\n    )\n  )\n\nNext, we summarize the student’s engagement based on their Moodle activity into three groups: Low activity, Moderate activity and High Activity.\n\nevents_summary <- events |>\n  group_by(user) |>\n  tally() |>\n  rename(Frequency.Total = n) |>\n  mutate(\n    ActivityGroup = factor(\n      case_when(\n        ntile(Frequency.Total, 3) == 1 ~ \"Low activity\",\n        ntile(Frequency.Total, 3) == 2 ~ \"Moderate activity\",\n        ntile(Frequency.Total, 3) == 3 ~ \"High activity\"\n      )\n    )\n  )\n\nWe also count the different types of Moodle events.\n\nevents_types <- events |>\n  group_by(user, Action) |>\n  count(Action) |> \n  pivot_wider(\n    names_from = \"Action\", \n    names_prefix = \"Frequency.\",\n    values_from = \"n\",\n    values_fill = 0\n  )\n\nFinally, we combine the data.\n\nall_combined <- demographics |>\n  left_join(events_types, by = \"user\") |> \n  left_join(events_summary, by = \"user\") |> \n  left_join(results, by = \"user\")\n\nThe various steps of the combined data construction are discussed in greater detail in Chapter 4 [9].\n\n2.1 Measures of central tendency\nA typical way to summarize a univariate data sample is to describe its “middle point” using an appropriate statistic depending on the measurement scale of the data. The most common statistics to describe such a value are the mean, the median, and the mode.\nFor data on the interval or ratio scales (and sometimes also on the ordinal scale), the most common option is to use the arithmetic mean, which is available via the base R function mean(). This function takes a vector of values as its input.\n\nall_combined |> \n  summarise(\n    mean_grade = mean(Final_grade),\n    mean_total = mean(Frequency.Total)\n  )\n\n\n\n  \n\n\n\nThe means are reported as a tibble with a single column for both variables.\nFor data on the ordinal scale (or interval or ratio scales), the median can be used which is defined as the value that separates the lower half from the bottom half of the data sample, i.e., the 50% quantile. The median can be computed in R using the built-in median() function. Similarly to mean(), this function also takes a vector of values as its input.\n\nall_combined |> \n  summarise(\n    median_grade = median(Final_grade),\n    median_total = median(Frequency.Total)\n  )\n\n\n\n  \n\n\n\nJust like before, the medians are reported as a tibble with each value in its own column.\nFor data on the nominal or ordinal scale, the mode is a suitable choice as it describes the category with the highest number of observations. Unfortunately, there is no readily available function in R to compute the mode, and the reader should take care not to mistakenly use the mode() function, which is used to determine the internal storage mode of a variable (similar to the typeof() function). However, we can easily write our own function to compute the statistical mode as follows:\n\nstat_mode <- function(x) {\n  u <- unique(x)\n  u[which.max(tabulate(match(x, u)))]\n}\n\nFunctions in R are written using the following syntax. First, we define the name of the function, just like we would define the name of a variable when assigning data into it, in this case the name is stat_mode. Next, we assign the function definition, which starts with the keyword function. Next, we describe the function arguments within the parentheses, in this case we call our argument x, which we assume contains the data vector we wish to compute the mode for. Next, we define the body of the function within the braces. The body determines what the function does and what its output should be. Within the body, we first determine the unique values in the data vector x, and assign the result to a variable u. Next, we need to count the number of occurrences of each unique value. To start, we first match each observed value in x to the unique values in u to get the corresponding indices, which we will then count using tabulate. We obtain the index of the value with the highest number of occurrences with the function which.max(), and finally the corresponding unique value by selecting it from u using the subset operator, i.e., the brackets. Our function will now work on all types of data.\n\nall_combined |>\n  summarise(\n    mode_gender = stat_mode(Gender),\n    mode_location = stat_mode(Location)\n  )\n\n\n\n  \n\n\n\nThe output is now similar to the mean and median functions that we used earlier, showing the modes of Gender and Location as a two-column tibble. For nominal variables, it is common to also compute the frequencies of each category. This can easily be done with the base R function table()\n\ntable(all_combined$Gender)\n\n\n F  M \n65 65 \n\ntable(all_combined$Location)\n\n\nOn campus    Remote \n      106        24 \n\n\nThe function outputs the names of the categories and the frequency of each category as an integer vector.\n\n\n2.2 Measures of dispersion\nFor data on the interval and ratio scales (and sometimes also on the ordinal scale), it is also meaningful to describe how clustered or scattered the values in the sample are, i.e., how far apart the values are from one another. Commonly used measures of statistical dispersion include the variance, standard deviation, and the interquartile range. Typically, such measures have the value 0 when all values in the sample are identical, and the value increases as the dispersion in the data grows.\nAll three measures can be readily computed with built-in R functions var(), sd(), and IQR() respectively. Like mean() and median(), these functions accept a vector or numeric values as input.\n\nall_combined |>\n  summarise(\n    var_grade = var(Final_grade),\n    sd_grade = sd(Final_grade),\n    iqr_grade = IQR(Final_grade)\n  )\n\n\n\n  \n\n\n\nThe variance, standard deviation and interquartile range of the final grade are returned as a tibble with three columns.\n\n\n2.3 Covariance and Correlation\nCovariance and correlation measure the linear dependence between two variables. Correlation is a unitless measure between -1 and 1, whereas covariance is not, and its scale depends on the scale of the variables. The sign of both measures indicates the tendency of the relationship. Positive sign means that as the value of one variable increases, the value of the other variable tends to increase as well. Conversely, a negative sign indicates that the value of the second variable tends to decrease as the value of the first variable increases.\nBoth covariance and correlation and be computed directly in R using the functions cov() and cor(), respectively.\n\nall_combined |>\n  summarise(\n    cov_grade_total = cov(Final_grade, Frequency.Total),\n    cor_grade_total = cor(Final_grade, Frequency.Total),\n  )\n\n\n\n  \n\n\n\nWe obtain the covariance and correlation between the final grade and total number of Moodle events. We will familiarize ourselves with correlations in greater depth in Section 5.4.\n\n\n2.4 Other common statistics\nThe extreme values of a data sample can be found using the function range(), which computes the minimum and maximum values of the sample as a vector. These values can also be computed individually with the corresponding functions min() and max(). Because summarise() only allows a single value as output per row, we use the reframe() function instead when computing the range.\n\nall_combined |>\n  reframe(\n    range_grade = range(Final_grade)\n  )\n\n\n\n  \n\n\nall_combined |>\n  summarise(\n    min = min(Final_grade),\n    max = max(Final_grade)\n  )\n\n\n\n  \n\n\n\nWith reframe(), we obtain a tibble with two rows, the first containing the minimum value and the second the maximum value of the final grade. If we instead use summarise() like before, we can only obtain one value per computed variable. The summary() function can also be used to quickly compute several of the most common descriptive statistics for all variables of a dataset.\n\nresults |> \n  select(Grade.SNA_1:Grade.Group_self) |> \n  summary()\n\n  Grade.SNA_1      Grade.SNA_2      Grade.Review    Grade.Group_self\n Min.   : 0.000   Min.   : 0.000   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.: 8.000   1st Qu.: 9.000   1st Qu.: 6.670   1st Qu.: 9.000  \n Median : 9.000   Median :10.000   Median : 8.000   Median :10.000  \n Mean   : 8.346   Mean   : 9.262   Mean   : 7.724   Mean   : 8.085  \n 3rd Qu.:10.000   3rd Qu.:10.000   3rd Qu.: 9.670   3rd Qu.:10.000  \n Max.   :10.000   Max.   :10.000   Max.   :10.000   Max.   :10.000  \n\n\nThe output shows the minimum and maximum values, the quartiles, and the mean of each variable that we selected."
  },
  {
    "objectID": "chapters/ch05-basic-stats/ch5-stats.html#statistical-hypothesis-testing",
    "href": "chapters/ch05-basic-stats/ch5-stats.html#statistical-hypothesis-testing",
    "title": "5  Introductory Statistics with R for Educational Researchers",
    "section": "3 Statistical hypothesis testing",
    "text": "3 Statistical hypothesis testing\nStatistical hypothesis testing aims to evaluate hypotheses about a population of interest using probabilistic inference. The starting point of any statistical test is a so-called null hypothesis (denoted by \\(H_0\\)), which typically corresponds to a scenario, where evidence supporting a specific hypothesis is a result of pure chance. For example, when evaluating whether a new drug is an efficient form of treatment via a randomized controlled trial, the null hypothesis could be that the drug has no effect on the response. A null hypothesis is always associated with an alternative hypothesis (denoted by \\(H_1\\)), which is typically the inverse of the null hypothesis and corresponds to the hypothesis of interest, e.g., the drug has an effect on the response.\nStatistical tests operate by assuming that the null hypothesis is true, and highly unlikely events under this assumption are typically regarded as giving cause for rejecting the null hypothesis. A statistical test is associated with a test statistic, which is a measure of how much the observations deviate from the null hypothesis scenario. The distribution of the test statistic under the null hypothesis and the sample test statistic can be used to compute the probability of obtaining a test statistic as extreme or more extreme than the one observed, assuming that the null hypothesis is true. This probability is known as the p-value, which is often mischaracterized even in scientific literature. For instance, the p-value is not the probability that the null hypothesis is true or that the alternative hypothesis is false. The p-value also does not quantify the size of the observed effect, or its real-world importance.\nTypically, a confidence level is decided before applying a statistical test (usually denoted by \\(\\alpha\\)), and the null hypothesis is rejected if the observed p-value is smaller than this confidence level. If the p-value is greater than the confidence level, the null hypothesis is not rejected. Traditionally, the confidence level is 0.05, but this convention varies by field, and should be understood as being arbitrary, i.e., there is nothing special about the value 0.05. If the p-value falls below the confidence level, the result is regarded as statistically significant.\nHypothesis testing is a powerful tool for drawing conclusions from data, but it is important to use it appropriately and to understand its limitations. Every statistical test is associated with a set of assumptions which are often related to the distribution of the data sample. If these assumptions are violated, then the results of the test may be unreliable. In the following sections, some of the most common statistical tests are introduced. We will take advantage of the report [10] package and the corresponding function report() to showcase the results of the various statistical tests. For more information on the concepts and principles related to statistical hypothesis testing, see e.g., [2, 3].\n\n3.1 Student’s t-test\nStudent’s t-test [11] is one of the most well-known statistical tests. It compares the mean values of variables either between two populations or between a single population and a reference level and is thus applicable to continuous variables. The test assumes homogeneity of variance and that the data originates from a normal distribution. For nonhomogeneous data, the test can still be performed by using an approximation [12]. In R, all variants of the t-test test can be applied using the function t.test().\nOur goal is to compare the students’ Moodle activity with respect to their final grade. For this purpose, we use the binary variable called AchievingGroup which categorizes the students into top and bottom 50% achievers in terms of the final grade.\n\n3.1.1 One-sample t-test\nThe one-sample t-test compares the mean of a data sample against a reference value, typically defined by the null hypothesis. Let us begin by testing the hypothesis that the average number of Moodle events (Frequency.Total) is 600 (\\(H_0 : \\mu = 600\\)). The function t.test() can be used in various ways, but in this example we provide the function with a formula object Frequency.Total ~ 1 as the first argument. The formula syntax is a standard method for defining statistical models and other dependency structures in R. The formula defines that the left-hand side of the ~ symbol is a response variable which is explained by the terms on the right-hand side. Because we’re not conducting the test with respect to any other variable, the right-hand side of the formula is simply 1, which means that it is a constant in the R formula syntax. This does not mean for example, that our null hypothesis would be that the number of Moodle events is 1. The expected value that the test is applied against (i.e., the value we assume \\(\\mu\\) to have under the null hypothesis) is defined via the argument mu, which by default has the value 0 for a one-sample t-test. Argument data defines in which environment the formula should be evaluated. By providing the all_combined data, we do not have to explicitly extract the FrequencyTotal variable from the data in the formula by writing all_combined$Frequency.Total or by using pull(). This is especially useful when the formula contains several variables from the same data.\n\nttest_one <- t.test(Frequency.Total ~ 1, data = all_combined, mu = 600)\nttest_one\n\n\n    One Sample t-test\n\ndata:  Frequency.Total\nt = 3.4511, df = 129, p-value = 0.0007553\nalternative hypothesis: true mean is not equal to 600\n95 percent confidence interval:\n 657.8530 813.3163\nsample estimates:\nmean of x \n 735.5846 \n\n\nAs a result, we obtain the value of the test statistic (t = 3.4511), the degrees of freedom (df = 129), and the p-value of the test (p-value = 0.0007553). Because the p-value is very small (much smaller than the standard 0.05 confidence level), we reject the null hypothesis, which means that the average number of Moodle events is significantly different from 600. The output of the test result object also describes the alternative hypothesis \\(H_1\\) under alternative hypothesis, and the confidence interval of the test statistic.\nWe produce a summarized report of the test results with the report() function.\n\nreport(ttest_one)\n\nWarning: Unable to retrieve data from htest object.\n  Returning an approximate effect size using t_to_d().\n\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe One Sample t-test testing the difference between Frequency.Total (mean =\n735.58) and mu = 600 suggests that the effect is positive, statistically\nsignificant, and small (difference = 135.58, 95% CI [657.85, 813.32], t(129) =\n3.45, p < .001; Cohen's d = 0.30, 95% CI [0.13, 0.48])\n\n\nThis produces a description of the results of the test that is easier to read and interpret than the direct output of the test result object. We note that a warning is also produced which we can safely ignore in this case. The warning occurs because the test result object ttest_one does not retain the original data all_combined which we used to carry out the test. If non-approximate effect sizes are desired, the test should be carried out by supplying the variables being compared directly without using the formula interface of the t.test() function. For more information on the effect size, see e.g., [13, 14].\n\n\n3.1.2 Two-sample t-test\nIn contrast to the one-sample t-test, the two-sample t-test compares the means of two data samples against one another. For example, suppose that we’re interested in the hypothesis that the average number of Moodle events is the same for the top and bottom 50% achievers (\\(H_0: \\mu_1 = \\mu_2\\)). We can once again leverage the formula syntax, but instead of the constant 1 on the right-hand side of the formula, we will now replace it with the variable Achievement which defines the achievement level.\n\nttest_two <- t.test(Frequency.Total ~ AchievingGroup, data = all_combined)\nttest_two\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  Frequency.Total by AchievingGroup\nt = 4.4749, df = 95.988, p-value = 2.102e-05\nalternative hypothesis:\ntrue difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n 182.6427 473.8496\nsample estimates:\nmean in group High achiever  mean in group Low achiever \n                   899.7077                    571.4615 \n\n\nThe contents of the result object are mostly the same as in the case of the one-sample t-test. The result is again statistically significant (using the 0.05 confidence level) meaning that according to the test, the average number of Moodle events is higher for the top 50% achievers. The report() function can be used to produce a similar summary as in the case of the one-sample t-test.\n\nreport(ttest_two)\n\nWarning: Unable to retrieve data from htest object.\n  Returning an approximate effect size using t_to_d().\n\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Welch Two Sample t-test testing the difference of Frequency.Total by\nAchievingGroup (mean in group High achiever = 899.71, mean in group Low achiever =\n571.46) suggests that the effect is positive, statistically significant, and large\n(difference = 328.25, 95% CI [182.64, 473.85], t(95.99) = 4.47, p < .001; Cohen's d\n= 0.91, 95% CI [0.49, 1.33])\n\n\nThis produces the same warning as before in the one-sample case, but we can safely ignore it again.\n\n\n3.1.3 Paired two-sample t-test\nInstead of directly comparing the means of two groups, it may sometimes be of interest to compare differences between pairs of measurements. Such a scenario typically arises in an experiment, where subjects are paired, or two sets of measurements are taken from the same subjects. While our Moodle event data does not contain such measurement pairs, we could still imagine that our data was organized such that each student in the bottom 50% achievers was paired with a student in the top 50% achievers and that there is a one-to-one correspondence between the two achievement groups. This dependency between the two groups is the key difference between the paired test and the two-sample test.\nA more suitable approach for paired data is to test the differences between the pairs, e.g., the differences between the number of Moodle events in our scenario. We supply the t.test() function with the argument paired = TRUE so that it will take the measurement pairs into account. In this test, the null hypothesis is that the mean difference between the student pairs is zero (\\(H_0 : \\mu_d = 0\\)).\n\nttest_paired <- t.test(\n  Frequency.Total ~ AchievingGroup, data = all_combined, paired = TRUE\n)\nttest_paired\n\n\n    Paired t-test\n\ndata:  Frequency.Total by AchievingGroup\nt = 4.3733, df = 64, p-value = 4.599e-05\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 178.3014 478.1910\nsample estimates:\nmean difference \n       328.2462 \n\n\nThe result is once again statistically significant, and we reject the null hypothesis. Because the mean difference between the pairs is positive, this means average number of Moodle events is higher for the top 50% achievers of the pairs.\nPaired two-sample t-test is also supported by report().\n\nreport(ttest_paired)\n\nWarning: Unable to retrieve data from htest object.\n  Returning an approximate effect size using t_to_d().\n\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Paired t-test testing the difference of Frequency.Total by AchievingGroup (mean\ndifference = 328.25) suggests that the effect is positive, statistically\nsignificant, and medium (difference = 328.25, 95% CI [178.30, 478.19], t(64) =\n4.37, p < .001; Cohen's d = 0.55, 95% CI [0.28, 0.81])\n\n\nWe can once again safely ignore the produced warning message.\n\n\n\n3.2 Chi-squared test\nThe chi-squared test is used for the analysis of contingency tables; it tests whether two categorical variables are independent or not [15]. A typical use case for this test is to investigate differences between groups such as student attendance by location or gender. The basic idea of the test is to compare the observed contingency table to a table under the null hypothesis where the variables are independent. The chi-squared test is based on the cell-specific differences between these two tables. As a general rule, the test assumes that the expected value is at least 5 in at least 80% of the cells, and that no expected values are below 1. If these assumptions are violated, the results of the test may not be reliable. In such cases, Fisher’s exact test [16] can be used instead via the function fisher.test(), but it may be computationally slow for large contingency tables. Both the chi-squared test and Fisher’s exact test assume that the data is a random sample from the population.\nWe will use the combined Moodle data to investigate whether the achievement level and the activity level of the students are independent. First, we must create the contingency table from the individual-level data. We use the table() function for this purpose.\n\ntab <- table(all_combined$ActivityGroup, all_combined$AchievingGroup)\ntab\n\n                   \n                    High achiever Low achiever\n  High activity                27           16\n  Low activity                 14           30\n  Moderate activity            24           19\n\n\nThe table shows the observed frequencies in each cell, i.e., for each combination of activity and achievement. Next, we apply the chi-squared test using the chisq.test() function.\n\nXsq_test <- chisq.test(tab)\nXsq_test\n\n\n    Pearson's Chi-squared test\n\ndata:  tab\nX-squared = 9.2135, df = 2, p-value = 0.009984\n\n\nPrinting the test result object shows the test statistic (X-squared), the associated degrees of freedom (df) and the p-value (p-value). The p-value is very small, meaning that we reject the null hypothesis. In other words, the achievement and activity levels of the students are not independent. This is not a surprising result, as more active students are more likely to engage with the course content and perform better in terms of the learning outcomes. We can confirm that the assumptions of the test related to the expected values of the cells were not violated by using the test result object, which contains the expected values of the cells in the element expected.\n\nall(Xsq_test$expected >= 1)\n\n[1] TRUE\n\nmean(Xsq_test$expected >= 5) >= 0.80\n\n[1] TRUE\n\n\nAll expected values were greater than one, and over 80% of the expected values were greater than 5. This means that the assumptions are satisfied for our data and thus the results are reliable. Here, we used the function all() which takes a logical vector as input and returns TRUE if all elements of the vector were TRUE. Otherwise, the function returns FALSE. Unfortunately, the report() function does not support the chi-squared test.\n\n\n3.3 Analysis of variance\nAnalysis of variance (ANOVA) [17] can be viewed as the generalization of Student’s t-test, where instead of one or two groups, the means of a variable are compared across multiple groups simultaneously. The name of the method comes from its test statistic, which is based on a decomposition of the total variance of the variable into variance within the groups and between the groups. ANOVA makes several assumptions: the observations are independent, the residuals of the underlying linear model follow a normal distribution, and that the variance of the variable is the same across groups (homoscedasticity). If these assumptions are violated, the results of the test may not be reliable. One alternative in such instances is to use the non-parametric Kruskal-Wallis test [18] instead, which is available in R via the function kruskal.test(). This test uses the ranks of the observations, and the null hypothesis is that the medians are the same for each group.\nWe use our combined Moodle data to demonstrate ANOVA. Instead of comparing the total number of Moodle events between top and bottom 50% of achievers, this time we will compare the final grade of the students across three activity groups: low activity, moderate activity, and high activity, described by the variable ActivityGroup. Thus the null and alternative hypotheses are in this case:\n\n\\(H_0\\): The expected values of the final grade are the same across the three activity groups (\\(\\mu_1 = \\mu_2 = \\mu_3\\)),\n\\(H_1\\): At least one activity group has a different expected final grade (\\(\\mu_i \\ne \\mu_j\\) for at least one pair \\(i \\ne j\\)).\n\nTo carry out the analysis, we apply the aov function, which uses the same formula syntax to define the response variable and the groups as the t.test() function does. Next, we apply the summary() function to the aov() function return object fit, as the default output of aov() is not very informative.\n\nfit <- aov(Final_grade ~ ActivityGroup, data = all_combined)\nsummary(fit)\n\n               Df Sum Sq Mean Sq F value   Pr(>F)    \nActivityGroup   2  175.7   87.87   25.11 6.47e-10 ***\nResiduals     127  444.4    3.50                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe summary contains the following columns: Df describes the degrees of freedom of the \\(F\\)-distribution associated with the test, Sum Sq reports the sum of squares related to the groups and the residuals, Mean Sq reports the corresponding mean sum of squares, F value is the value of the test statistic, and finally Pr(>F) is the p-value of the test. For this example, the p-value is very small, which means that the null hypothesis is rejected, and there are statistically significant differences in the final grade between the groups according to the test. In the following sections we will learn how to test for the assumptions related to normality and homoscedasticity. The report() function can be used for the output of aov() as well.\n\nreport(fit)\n\nThe ANOVA (formula: Final_grade ~ ActivityGroup) suggests that:\n\n  - The main effect of ActivityGroup is statistically significant and large (F(2,\n127) = 25.11, p < .001; Eta2 = 0.28, 95% CI [0.17, 1.00])\n\nEffect sizes were labelled following Field's (2013) recommendations.\n\n\nThis output also reports the degrees of freedom, the test statistic value and the p-value but in a more easily readable format.\nNote that ANOVA simply measures if there are differences between the groups but does not provide information on how these differences emerge. For example, there could be a single group that is different from all the rest, or two subgroups where the means are similar within each group, but different between the subgroups. Visualizations can be a helpful tool for gaining more insight into the differences, and post-hoc pairwise tests can be carried out to compare the pairs of groups.\n\n\n3.4 Levene’s test\nLevene’s test is used to investigate whether the variance of a variable is the same across two or more groups [19]. Compared to alternatives such as Bartlett’s test [20], Levene’s test is less sensitive to non-normal observations. The test is not available in base R, but it can be found in the car package as the function leveneTest(). The function uses the same formula syntax as t.test() and aov(). We will investigate the homogeneity of the variance of the final grade between the activity groups.\n\nlibrary(\"car\")\nleveneTest(Final_grade ~ ActivityGroup, data = all_combined)\n\n\n\n  \n\n\n\nThe output of leveneTest() is analogous to the output of the ANOVA summary, and it contains the degrees of freedom (Df), the value of the test statistic (F value) and the p-value of the test (Pr(>F)). The p-value is very small, so we reject the null hypothesis meaning that the variance of the final grade is not the same across the groups according to the test. This means that the assumption of homoscedasticity is violated for the analysis of variance of the final grade, and thus the results may not be reliable. The report() function is not supported for leveneTest().\n\n\n3.5 Shapiro-Wilk test\nShapiro-Wilk test tests the null hypothesis that a data sample originated from a normal distribution [21]. The test is available in base R as the function shapiro.test(). Unfortunately, this function does not support the formula syntax unlike the other test functions we have used thus far. The function only accepts a single numeric vector as its argument. Therefore, to test the normality of multiple groups simultaneously, the data must first be split into the groups to be tested. We apply the test to the final grade in each achievement group. With the help of the broom package [22], we wrap the test results into a tidy format.\n\nlibrary(\"broom\")\nall_combined |> \n  ## Performs the computations in each activity group\n  group_by(ActivityGroup) |> \n  ## Apply a function in each group\n  group_modify(~{\n    ## Apply the Shapiro test in each group and create tidy output\n    shapiro.test(.$Final_grade) |> \n      tidy()\n  }) |> \n  ## Selection of variables to keep in the output\n  select(ActivityGroup, statistic, p.value)\n\n\n\n  \n\n\n\nThis is also a great example of the tidyverse paradigm. First, we group the data by ActivityGroup using group_by(), and then apply a function in each group using group_modify(). We apply the shapiro.test() function to the Final_grade variable, and then we convert the test results into a tidy tibble using the tidy() function from the broom package. We also use the special dot notation . to select the final grade variable from the data in each group. Finally, we select the grouping variable (ActivityGroup), the test statistic (statistic) and the p-value (p.value) of each test using select() and print the results. The resulting object is a tibble with three columns: ActivityGroup, statistic and p.value, the last two of which give the test statistic and p-value of the test for the activity group of the first column.\nWe can see that according to the test, the Final_grade variable is not normally distributed in any of the activity groups, as the p-values are very small. As a consequence, the results of the analysis of variance carried out earlier may not be reliable."
  },
  {
    "objectID": "chapters/ch05-basic-stats/ch5-stats.html#sec-correlation",
    "href": "chapters/ch05-basic-stats/ch5-stats.html#sec-correlation",
    "title": "5  Introductory Statistics with R for Educational Researchers",
    "section": "4 Correlation",
    "text": "4 Correlation\nIn Section 5.2.3, we briefly described covariance and correlation, and showcased the base R functions to compute them. However, there are several powerful and user-friendly packages for the analysis and reporting of correlations, such as the correlation [23] package which we will demonstrate in this section.\n\nlibrary(\"correlation\")\n\nFor example, the package can easily compute all pairwise correlations between the numeric variables of the data with the function correlation(). The argument select can be used to compute the correlations only for a subset of the variables.\n\ncorrs <- correlation(\n  all_combined, \n  select = c(\"Frequency.Total\", \"Grade.Theory\", \"Final_grade\")\n)\ncorrs\n\n\n\n  \n\n\n\nThe columns Parameter1 and Parameter2 describe the variables that the correlation was computed for, r is the value of the sample correlation, and the remaining columns report the 95% confidence interval, the value of the test statistic, and the p-value of the test (a t-test for correlations) along with the statistical significance. By default, Pearson’s correlation coefficient is calculated, but the package also supports many alternative correlation measures. The correlation coefficient to be computed can be selected with the argument method that has the value \"pearson\" by default. Selecting for example method = \"spearman\" would compute the Spearman correlation coefficient instead. We can also obtain a correlation matrix by using summary()\n\nsummary(corrs)\n\n\n\n  \n\n\n\nBy default, redundant correlations are omitted, but they can be obtained by setting redundant = TRUE in the call to summary(). A plot of the correlation matrix can be produced with the help of the package see [24].\n\nlibrary(\"see\")\ncorrs |>\n  ## Also include redundant correlations\n  summary(redundant = TRUE) |>\n  plot()\n\n\n\n\nThe plot shows the strength of the correlations where darker colors imply stronger correlations. Visualizations will be covered at greater length in Chapter 6 [4]."
  },
  {
    "objectID": "chapters/ch05-basic-stats/ch5-stats.html#linear-regression",
    "href": "chapters/ch05-basic-stats/ch5-stats.html#linear-regression",
    "title": "5  Introductory Statistics with R for Educational Researchers",
    "section": "5 Linear regression",
    "text": "5 Linear regression\nLinear regression is a statistical tool where one continuous variable is explained by the values of other variables. The variable of interest is said to be a dependent, while the other variables are called predictors. Predictors may also be called explanatory variables, independent variables or covariates depending on the context, applied field, and perspective.\nConsider a very simple case, where we only have one predictor, which happens to be a continuous variable. In this case, fitting a linear regression model is merely the same as fitting a straight line to a scatterplot. It is assumed that deviations from this line are simply a result of random variation. \nNow, let’s go through the formal definition of a linear regression model. Let \\(Y\\) be a dependent variable with measurements \\(y_1 \\ldots, y_n\\), and let \\(X_1, X_2, \\ldots, X_k\\) be predictor variables with measurements \\(x_{1i}, \\ldots, x_{ki}\\) for all \\(i = 1,\\ldots,n\\) where \\(i\\) refers to an individual measurement. Then, the regression equation is \\[\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\cdots + \\beta_k x_{ki} + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0, \\sigma^2), \\quad i = 1,\\ldots,n\n\\] where we have the regression coefficients \\(\\beta_0, \\beta_1, \\ldots \\beta_k\\) and the error variance \\(\\sigma^2\\). The first parameter \\(\\beta_0\\) is called the intercept that models the conditional expectation of \\(Y\\) when all the predictors have the value \\(0\\). From the regression equation, several assumptions become apparent. First, as the name of the model suggests, a linear relationship is assumed between the response and the predictors. Next, the variance \\(\\sigma^2\\) of the errors \\(\\varepsilon_i\\) is constant, and does not depend on the values of the predictors (homoscedasticity). The errors are also assumed independent. The predictor variables are assumed fixed and their values perfectly measured without error.\nLet’s fit a linear regression model that predicts the final grade with the number of Moodle events of different types. To simplify the exposition, we will only use three types of Moodle events as predictors. We use the lm() function which has the same formula interface that we are already familiar with. First, we must define the dependent variable on the left-hand side of the formula, followed by the predictors on the right-hand side separated by a + sign. We must also supply the data argument, which tells the function where the actual values of the variables can be accessed.\n\nfit <- lm(\n  Final_grade ~ Frequency.Applications + Frequency.Assignment +\n    Frequency.La_types,\n  data = all_combined\n)\nsummary(fit)\n\n\nCall:\nlm(formula = Final_grade ~ Frequency.Applications + Frequency.Assignment + \n    Frequency.La_types, data = all_combined)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.0382 -0.8872  0.3665  1.2372  3.4422 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             5.800211   0.405963  14.288  < 2e-16 ***\nFrequency.Applications  0.076516   0.022294   3.432 0.000811 ***\nFrequency.Assignment   -0.005049   0.005734  -0.881 0.380225    \nFrequency.La_types      0.088252   0.027314   3.231 0.001574 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.914 on 126 degrees of freedom\nMultiple R-squared:  0.2559,    Adjusted R-squared:  0.2382 \nF-statistic: 14.45 on 3 and 126 DF,  p-value: 3.798e-08\n\n\nThe summary() function provides a compact overview of the model fit for lm objects. First, a summary of the residuals (i.e., the differences between the observed and predicted values) is provided under Residuals. Next, a summary of the regression coefficients \\(\\beta\\) is provided under Coefficients, including their estimates (Estimate), standard errors (Std. Error), test statistics for t-tests that test whether the coefficients are significantly different from zero (t value), p-values of the tests (Pr(>|t|)) and statistical significance (indicated by the asterisks). For instance, we see that the number of group work events is statistically significant. The notation used for the significance levels of the tests is described following Signif. codes. Estimate of the square root of the error variance \\(\\sigma^2\\) is reported following Residual standard error. The two R-squared values are estimates of the proportion of variance of the data that is explained by the model. Finally, the F-statistic reports the results of ANOVA when applied with the same model formula that was used for the linear regression model.\nThe report() function provides a more comprehensive summary of the model fit and the regression coefficients:\n\nreport(fit)\n\nWe fitted a linear model (estimated using OLS) to predict Final_grade with\nFrequency.Applications, Frequency.Assignment and Frequency.La_types (formula:\nFinal_grade ~ Frequency.Applications + Frequency.Assignment + Frequency.La_types).\nThe model explains a statistically significant and moderate proportion of variance\n(R2 = 0.26, F(3, 126) = 14.45, p < .001, adj. R2 = 0.24). The model's intercept,\ncorresponding to Frequency.Applications = 0, Frequency.Assignment = 0 and\nFrequency.La_types = 0, is at 5.80 (95% CI [5.00, 6.60], t(126) = 14.29, p < .001).\nWithin this model:\n\n  - The effect of Frequency Applications is statistically significant and positive\n(beta = 0.08, 95% CI [0.03, 0.12], t(126) = 3.43, p < .001; Std. beta = 0.32, 95%\nCI [0.13, 0.50])\n  - The effect of Frequency Assignment is statistically non-significant and negative\n(beta = -5.05e-03, 95% CI [-0.02, 6.30e-03], t(126) = -0.88, p = 0.380; Std. beta =\n-0.08, 95% CI [-0.26, 0.10])\n  - The effect of Frequency La types is statistically significant and positive (beta\n= 0.09, 95% CI [0.03, 0.14], t(126) = 3.23, p = 0.002; Std. beta = 0.31, 95% CI\n[0.12, 0.49])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were computed\nusing a Wald t-distribution approximation.\n\n\nAgain, the report output has condensed the information about the model fit into a format that can be read in a straightforward manner.\nThe assumption of normality of the residuals can be assessed with a quantile-quantile plot, or q-q plot for short. The residuals of the model fit can be accessed with the function resid(). The function qqnorm() draws the quantiles of the residuals against the quantiles of the normal distribution. The function qqline() adds a straight line through the plot that passes through the second and third quantiles, by default. Ideally, the residuals should fall on this line, and large deviations indicate that the normality assumption may not hold.\n\n## Draw the quantiles of the residuals and the theoretical quantiles\nqqnorm(resid(fit))\n## Add a line through the theoretical quantiles\nqqline(resid(fit))\n\n\n\n\n\n\n\n\nThe vast majority of residuals fall nicely onto the line for our model. Besides the q-q plot, we can obtain more model diagnostics with the help of the performance [25] package. This package provides a wide array of tools to assess how well models fit to the data. The general-purpose function check_model( provides a visual overview of the model fit using several metrics.\n\nlibrary(\"performance\")\ncheck_model(fit, theme = see::theme_lucid(base_size = 10))\n\n\n\n\nThe functions performs various tests related to the assumptions of the linear regression model. For example, the bottom right panel contains the same q-q plot that we previously constructed using the qqnorm() and qqline() functions. We refer the reader to the documentation of the performance package for more information on the remaining tests."
  },
  {
    "objectID": "chapters/ch05-basic-stats/ch5-stats.html#logistic-regression",
    "href": "chapters/ch05-basic-stats/ch5-stats.html#logistic-regression",
    "title": "5  Introductory Statistics with R for Educational Researchers",
    "section": "6 Logistic regression",
    "text": "6 Logistic regression\nLogistic regression is a similar tool to linear regression, but with a binary outcome instead of a continuous one. Instead of modeling the outcome variable directly, a linear model is constructed for the logarithmic odds of the probability of “success” for the binary outcome, e.g., obtaining a passing grade. There is also no explicit error term \\(\\varepsilon\\) in the model, as the uncertainty in the outcome is already captured by the success probability. Formally, the model is \\[\n\\mbox{logit}\\left(P(y_i = 1)\\right) = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\cdots + \\beta_k x_{ki}, \\quad i = 1,\\ldots,n,\n\\] where the logit-function is defined as \\(\\mbox{logit}(x) = \\log(x/(1-x))\\). Here, the logit-function serves as the so-called link function that connects the expected value of the response to the predictors.\nWe fit a logistic regression model where the outcome variable is the level of achievement (AchievingGroup) and the predictors are the Moodle event counts of each type. The logistic regression model is a generalized linear model: a class of models that extend the linear regression model and that can be fitted in R with the function glm(). The syntax of glm() is analogous to lm(), but we must also specify the distribution of the outcome and the link function via the family argument. We use the function binomial() and supply the argument link = \"logit\" to define that the model should be a logistic regression model (in this case the link argument is optional, as \"logit\" is the default value). Because AchievingGroup is a factor, we must first convert it into a binary response that attains values 1 and 0 (or TRUE and FALSE). We can do this within the formula via the I() function, so that we do not have to modify our data. When used in a formula, this function will first compute its argument expression when evaluated, so that the expression is not mistaken for a variable name in the data (that does not exist). We select the high achievers as the “success” category for the outcome.\n\nfit_logistic <- glm(\n  ## Use the I() function to construct a binary response in the formula\n  I(AchievingGroup == \"High achiever\") ~ Frequency.Applications + \n    Frequency.Assignment + Frequency.La_types,\n  data = all_combined,\n  ## Our response is binary, so we use the binomial family with logit link\n  family = binomial(link = \"logit\")\n)\nsummary(fit_logistic)\n\n\nCall:\nglm(formula = I(AchievingGroup == \"High achiever\") ~ Frequency.Applications + \n    Frequency.Assignment + Frequency.La_types, family = binomial(link = \"logit\"), \n    data = all_combined)\n\nCoefficients:\n                       Estimate Std. Error z value Pr(>|z|)    \n(Intercept)            -0.66272    0.62914  -1.053  0.29217    \nFrequency.Applications  0.30443    0.07778   3.914 9.07e-05 ***\nFrequency.Assignment   -0.04477    0.01402  -3.193  0.00141 ** \nFrequency.La_types      0.12245    0.04710   2.600  0.00933 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 180.22  on 129  degrees of freedom\nResidual deviance: 120.21  on 126  degrees of freedom\nAIC: 128.21\n\nNumber of Fisher Scoring iterations: 6\n\n\nThe summary of a glm() function output is very similar to the output of a lm() summary. First, the Call is reported, which simply restates how the model was fitted. Next, Coefficients reports the estimates of the regression coefficients \\(\\beta\\), and their standard errors and statistical significance. Lastly, two deviance measures and their degrees of freedom are reported. The null deviance is twice the difference between the log-likelihood of the saturated model and the null model, and residual deviance is twice the difference between the saturated model and the model that was fitted. In simpler terms, the saturated model is a perfect model in a sense that there is a parameter for each observation. Conversely, the null model only has the intercept term. The deviance serves as a generalization of the residual sum of squares of the linear regression model, and it can be used to assess the quality of the model fit [26].\nThe report() function is applicable to models fitted with glm().\n\nreport(fit_logistic)\n\nWe fitted a logistic model (estimated using ML) to predict AchievingGroup with\nFrequency.Applications, Frequency.Assignment and Frequency.La_types (formula:\nI(AchievingGroup == \"High achiever\") ~ Frequency.Applications +\nFrequency.Assignment + Frequency.La_types). The model's explanatory power is\nsubstantial (Tjur's R2 = 0.38). The model's intercept, corresponding to\nFrequency.Applications = 0, Frequency.Assignment = 0 and Frequency.La_types = 0, is\nat -0.66 (95% CI [-1.94, 0.56], p = 0.292). Within this model:\n\n  - The effect of Frequency Applications is statistically significant and positive\n(beta = 0.30, 95% CI [0.17, 0.48], p < .001; Std. beta = 1.62e-14, 95% CI\n[-73784.14, 73784.14])\n  - The effect of Frequency Assignment is statistically significant and negative\n(beta = -0.04, 95% CI [-0.07, -0.02], p = 0.001; Std. beta = -9.05e-16, 95% CI\n[-71374.92, 71374.92])\n  - The effect of Frequency La types is statistically significant and positive (beta\n= 0.12, 95% CI [0.04, 0.22], p = 0.009; Std. beta = -2.52e-17, 95% CI [-75547.24,\n75547.24])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were computed\nusing a Wald z-distribution approximation.\n\n\nThe output describes succinctly the model that was fitted, and the effects of the predictors on the response. The performance package is also applicable to models fitted with the glm() function.\n\ncheck_model(fit_logistic)\n\n\n\n\nWe note that a different set of diagnostic checks is carried out for the logistic regression model compared to the linear regression model. For example, there is no assumption of homoscedasticity of variance as there is no explicit error term \\(\\varepsilon\\) in the model. Again, we refer the reader to the documentation of the performance package for more details on these checks."
  },
  {
    "objectID": "chapters/ch05-basic-stats/ch5-stats.html#conclusion",
    "href": "chapters/ch05-basic-stats/ch5-stats.html#conclusion",
    "title": "5  Introductory Statistics with R for Educational Researchers",
    "section": "7 Conclusion",
    "text": "7 Conclusion\nBasic statistics are an essential component of learning analytics. Learning analytics involves the collection, analysis, and interpretation of data related to the learning process, and statistical methods are used to identify patterns and trends in this data and to draw conclusions. Basic descriptive statistics such as measures of central tendency, variability and correlation are crucial for analyzing, interpreting, and visualizing data. Understanding these concepts is important for anyone involved in conducting research with quantitative data in the field of learning analytics. Moreover, mastery of basic statistics can facilitate the comprehension of more advanced statistical methods that are commonly used in learning analytics, such as logistic regression and cluster analysis. Table 10.1 contains a summary of the statistical tests that were introduced in this chapter.\n\n\nTable 1. Summary of the statistical tests, their null hypotheses, and the number of groups they compare simultaneously.\n\n\n\n\n\n\n\n\nTest\nNull Hypothesis\nGroups\nR function\n\n\n\n\nStudent’s t-test\nEqual means\nOne, two or paired\nt.test()\n\n\nChi-squared test\nIndependence\nOne\nchisq.test()\n\n\nFisher’s test\nIndependence\nOne\nfisher.test()\n\n\nANOVA\nEqual means\nTwo or more\naov()\n\n\nKruskal-Wallis test\nEqual medians\nTwo or more\nkruskal.test()\n\n\nLevene’s test\nHomoscedasticity\nTwo or more\nleveneTest()\n\n\nShapiro-Wilk test\nNormality\nOne\nshapiro.test()\n\n\n\n\nWe emphasize that when using any statistical test or a statistical model, it is important to keep the various assumptions related to the chosen method in mind, and to assess them beforehand whenever possible. If the assumptions are violated, the results of the method may not be reliable, and thus suitable alternatives should be considered."
  },
  {
    "objectID": "chapters/ch05-basic-stats/ch5-stats.html#further-reading",
    "href": "chapters/ch05-basic-stats/ch5-stats.html#further-reading",
    "title": "5  Introductory Statistics with R for Educational Researchers",
    "section": "8 Further reading",
    "text": "8 Further reading\nThis chapter scratched the surface of the full features of packages such as correlation, report and performance that can streamline the statistical analysis and reporting process. We refer the reader to the documentation of these packages to gain a more thorough understanding of their features. These packages are part of a package collection called easystats [27] (). There are several other packages in this collection that were not discussed in this chapter that can be useful for R users working with learning analytics. The book “Learning Statistics with R” by Danielle Navarro is freely available online and provides a comprehensive introduction to statistics using R (). For a general introductory text to statistical methods and inference, see e.g., [2]."
  },
  {
    "objectID": "chapters/ch06-data-visualization/ch6-viz.html",
    "href": "chapters/ch06-data-visualization/ch6-viz.html",
    "title": "6  Visualizing and Reporting Educational Data with R",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch06-data-visualization/ch6-viz.html#introduction",
    "href": "chapters/ch06-data-visualization/ch6-viz.html#introduction",
    "title": "6  Visualizing and Reporting Educational Data with R",
    "section": "1 Introduction",
    "text": "1 Introduction\nData visualization can be defined as “the representation and presentation of data that exploits our visual perception abilities in order to amplify cognition” [1]. It has the power to transform complex information into stories that inform and inspire action. Data visualization is an effective tool for learning analytics, as it helps to present learners’ data in a way that is easily understandable and intuitive for students, teachers, researchers, and other stakeholders. Through the use of graphs, charts, and other visual aids, it is possible to quickly identify patterns, trends, and relationships within data that may not be immediately apparent through traditional data analysis methods.\nVisualization in learning analytics has two distinct applications. On the one hand, the use of visual dashboards has become the main vehicle for putting learning analytics into practice. Presenting data in visually appealing and intuitive ways can help promote data literacy among students and other stakeholders, encouraging greater engagement with data and fostering a culture of continuous improvement. On the other hand, learning analytics scientific production heavily relies on data visualization to present research findings in a clear and accessible manner, making it easier for readers from different scholarly backgrounds to understand and act upon research insights. Regardless of the context, the power of visualization in learning analytics lies in its ability to take complex data and turn it into meaningful insights that support better decision-making and drive improvement.\nIn this chapter, the reader will be guided through the process of generating visualizations of different types of data types using well-known R packages. Relevant plots and plot types will be demonstrated with an explanation of their usage and use cases. Furthermore, learning-related examples will be discussed in detail. For instance, readers will learn how to visualize learners’ logs extracted from learning management systems to show how trace data can be used to track students’ learning activities. Other examples of common research scenarios in which learners’ data are visualized will be illustrated throughout the chapter. In addition to creating compelling plots, readers will also be able to generate professional-looking tables with summary statistics to report descriptive statistics."
  },
  {
    "objectID": "chapters/ch06-data-visualization/ch6-viz.html#visualization-in-learning-analytics",
    "href": "chapters/ch06-data-visualization/ch6-viz.html#visualization-in-learning-analytics",
    "title": "6  Visualizing and Reporting Educational Data with R",
    "section": "2 Visualization in learning analytics",
    "text": "2 Visualization in learning analytics\nDeveloping visualizations is a challenging task of balancing the cognitive load of users while not compromising on conveying specific insights from data [2]. Visualizations for practice in learning analytics are mostly developed for two main stakeholders: learners and instructors. Depending on the target group, a visualization or a dashboard (i.e., a collection of visualizations depicting multiple indicators) have different goals.\nLearner-facing visualizations are meant to make learners aware of their own learning and to provide them with actionable feedback on their learning. Visualizations display learners’ performance on a specific metric and compare it with a reference frame: other peers, desirable learning achievement, or their own progress over time [3]. Sense-making questions triggering reflection can be added to a visualization [4, 5], or some elements of the visualizations can be highlighted and described in words using layered storytelling [6, 7]. Another option is to gamify a dashboard, for example, by using badges [8]. To provide feedback to learners, visualizations can be augmented with links to recommended resources [9], information about specific topics to review to close the achievement gap [6], or explanations of the meaning of visualizations and their implications for the learner [10]. Current learner-facing dashboards mostly show resource use and assessment data [11], compare learners to their peers [12], display descriptive analytics rather than predictive or prescriptive analytics [10], and use self-regulated learning theory as their framework [12, 13]. Some reviews found a positive effect on student outcomes [10], while others reported mixed results [11, 14]. Showing visualizations to learners can change their behavior. For example, social network analysis visualizations have resulted in fewer cross-group commenting [15], while a visualization comparing individual submission patterns with the top 25% of students in a class led to earlier homework submissions [16].\nIn comparison, the goal of instructor-facing visualizations is to support teachers and their decision-making process by tracking student progress. Two main types can be distinguished. Mirroring or descriptive visualizations provide insights about the learners on an aggregated or an individual level using either descriptive or comparative data. Advising or prescriptive visualizations show not only information about the learners but also alert the instructor to undertake a pedagogical action [17, 18]. Current instructor-facing visualizations mostly display course-wide information about the learners or track group work [14]. These visualizations can support teachers in facilitating student collaboration [19], planning and collecting student feedback on learning activities [20], or obtaining insights into student interactions within an online environment, such as simulations, virtual labs or online games [21, 22]. However, interpreting dashboard information is a challenging task for instructors. Although some teachers use dashboards as complementary sources of information, others act based only on the dashboard information without further investigation [23].\nA common point of criticism of learning analytics dashboards is that most of them are not grounded in learning theories [13, 14]. Data-driven evaluations of dashboards focused on dashboard acceptance, usefulness, or usability are more prevalent than pedagogically-focused evaluations [24]. Some approaches were developed to mitigate these issues. The model of user-centered learning analytics systems (MULAS) presents a set of recommendations on four interconnected dimensions: theory, design, evaluation, and feedback, and can be used to guide dashboard development [14]. Another approach is an iterative five-stage Learning Awareness Tools – User eXperience (LATUX) workflow, including problem identification, low-fidelity prototyping, high-fidelity prototyping, pilot studies, and classroom use, that can be used to develop visual analytics [25]. Finally, open learner model research could be used as a source of insights while developing learning analytics visualizations, such as dashboards [9]."
  },
  {
    "objectID": "chapters/ch06-data-visualization/ch6-viz.html#generating-plots-with-ggplot2",
    "href": "chapters/ch06-data-visualization/ch6-viz.html#generating-plots-with-ggplot2",
    "title": "6  Visualizing and Reporting Educational Data with R",
    "section": "3 Generating plots with ggplot2",
    "text": "3 Generating plots with ggplot2\nIn the previous section, we have seen how central visualization is to learning analytics. In the remainder of the chapter, we will learn how to create different types of visualizations that are relevant to different types of data related to teaching and learning. We will mostly rely on ggplot2, a popular data visualization package in R that was developed by Hadley Wickham [26]. It is based on the grammar of graphics [27], which is a systematic way of thinking about and constructing visualizations. The ggplot2 library provides a flexible and intuitive framework for creating a wide range of graphics, from basic scatter plots to complex visualizations with multiple layers. It is known for its ability to produce visually appealing and informative graphics with relatively few lines of code. It enables users to define aesthetics, such as color and size, and add layers, such as points and lines, to create customized and interactive plots. In addition, ggplot2 allows for easy customization of plot features, such as titles, axis labels, and legends.\nOverall, ggplot2 is a powerful and versatile tool for data visualization in R, and is widely used by data scientists, statisticians, and researchers in a variety of fields. In this chapter, we will cover the fundamental concepts and techniques of ggplot2, including how to create basic plots, and customize their appearance. We will start by introducing the building blocks of a ggplot2 plot, including aesthetics, layers, and scales. Then, we will create a plot from scratch step by step, showing how to customize the appearance of the plots, including how to change theme, colors, and scales. We will then explore the different types of plots that you can create with ggplot2, such as scatter plots, bar charts, and histograms.\nThroughout this chapter section, we will use datasets of students’ learning data to demonstrate how to create effective visualizations for learning analytics with ggplot2. Please, refer to Chapter 2 of this book [28] to learn more about the datasets used. By the end of this section, you will have a solid foundation in ggplot2 and be able to create basic, yet compelling visualizations to explore your data."
  },
  {
    "objectID": "chapters/ch06-data-visualization/ch6-viz.html#the-ggplot2-grammar",
    "href": "chapters/ch06-data-visualization/ch6-viz.html#the-ggplot2-grammar",
    "title": "6  Visualizing and Reporting Educational Data with R",
    "section": "4 The ggplot2 grammar",
    "text": "4 The ggplot2 grammar\nThe ggplot2 library is based on Wilkinson’s grammar of graphics [27]. The main idea is that every plot can be broken down into a set of components, each of which can be customized and combined in a flexible way. These components are:\n\nData: This is the data we want to visualize. It can be in the form of a dataframe, tibble or any other structured data format.\nAesthetic mapping (aes): It defines how variables in the data are mapped to visual properties of the plot, such as position, color, shape, size, and transparency.\nGeometric object (geom): It represents the actual visual elements of the plot, such as points, lines, bars, and polygons.\nStatistical transformation (stat): It summarizes or transforms the data in some way, such as by computing means, medians, or proportions, or by smoothing or summarizing data, or grouping them into bins.\nScale (scale): It maps values in the data to visual properties of the plot, such as color, size, or position.\nCoordinate system (coord): It defines the spatial or geographic context in which the plot is displayed, such as Cartesian coordinates, polar coordinates, or maps.\nFacet (facet) : It allows to split the data into subsets and display each subset in a separate panel. It often useful for visualizing data with multiple categories or groups.\n\nThrough the combination and customization of these components, we can create a wide variety of complex and informative visualizations in ggplot2. The idea behind the graphics grammar is to provide a consistent framework for constructing plots, allowing users to focus on the data and the message they want to convey, rather than on the technical details of the visualization. In the following section, we will create a plot from scratch step by step to become familiar with the most relevant components."
  },
  {
    "objectID": "chapters/ch06-data-visualization/ch6-viz.html#creating-your-first-plot",
    "href": "chapters/ch06-data-visualization/ch6-viz.html#creating-your-first-plot",
    "title": "6  Visualizing and Reporting Educational Data with R",
    "section": "5 Creating your first plot",
    "text": "5 Creating your first plot\nWe will now create our first plot using ggplot2. Our example deals with a widely studied matter in learning analytics, which is the relationship between online activity and achievement. We will use a bar chart to represent the number of students that have low, moderate and high activity levels in each achievement group (high achievers vs. low achievers). In order to become familiar with the syntax of ggplot2, we will recreate the plot step by step, explaining each of the elements in the plot. Below is the final result we aim at accomplishing:\n\n\n\n\n\nFigure 1: First plot with ggplot2.\n\n\n\n\n\n5.1 Installing ggplot2\nOur first step is installing the ggplot2 library. This is usually the first step in any R script that makes use of external libraries.\n\ninstall.packages(\"ggplot2\")\n\nTo import ggplot2 we just need to use the library command and specify the ggplot2 library:\n\nlibrary(ggplot2)\n\n\n\n5.2 Downloading the data\nNext, we need to import the data that we are going to plot. For this chapter, we are using synthetic data from a blended course on learning analytics. For more details about this dataset, refer to Chapter 2 in this book. The data is in Excel format. We can use the library rio since it makes it easy to read data in several formats. We first install the library:\n\ninstall.packages(\"rio\")\n\nAnd import it so we can use its functions:\n\nlibrary(rio)\n\nNow we can download the data using the import function from rio and assign it to a variable named df (short for dataframe).\n\ndemo_url = \"https://github.com/lamethods/data/raw/main/1_moodleLAcourse/AllCombined.xlsx\"\ndf <- import(demo_url)\n\nWe can use the head command to get an idea of what the dataset looks like. To recreate the plot above we will need the AchievingGroup column —which indicates whether students’ are high achievers (to 50%) or low achievers (bottom 50%), according to their final grade— and the ActivityGroup column —which indicates whether students have a high level of activity (top 33%), moderate activity (middle 33%), or low activity (bottom 33%), according to their total number of events in the LMS.\n\nhead(df)\n\n\n\n# A tibble: 130 × 37\n   User      Name   Gender ActivityGroup AchievingGroup Surname Origin Birthdate\n   <chr>     <chr>  <chr>  <chr>         <chr>          <chr>   <chr>  <chr>    \n 1 00a05cc62 Wan    M      Low activity  Low achiever   Tan     Malay… 12.12.19…\n 2 042b07ba1 Daniel M      High activity Low achiever   Tromp   Aruba  28.5.1999\n 3 046c35846 Sarah  F      Low activity  Low achiever   Schmit  Luxem… 25.4.1997\n 4 05b604102 Lian   F      Low activity  Low achiever   Abdull… Yemen  19.11.19…\n 5 0604ff3d3 Nina   F      Low activity  Low achiever   Borg    Malta  13.6.1994\n 6 077584d71 Moham… M      High activity High achiever  Gamal   Egypt  13.7.1998\n 7 081b100cf Maxim… M      Moderate act… High achiever  Gruber  Austr… 20.12.19…\n 8 0857b3d8e Hugo   M      High activity High achiever  Pérez   Spain  22.12.19…\n 9 0af619e4b Aylin  F      Low activity  Low achiever   Barat   Kazak… 14.8.1995\n10 0ec99ce96 Polina F      Moderate act… Low achiever   Novik   Belar… 9.10.1996\n# ℹ 120 more rows\n# ℹ 29 more variables: Location <chr>, Employment <chr>,\n#   Frequency.Applications <dbl>, Frequency.Assignment <dbl>,\n#   Frequency.Course_view <dbl>, Frequency.Feedback <dbl>,\n#   Frequency.General <dbl>, Frequency.Group_work <dbl>,\n#   Frequency.Instructions <dbl>, Frequency.La_types <dbl>,\n#   Frequency.Practicals <dbl>, Frequency.Social <dbl>, …\n\n\n\n\n5.3 Creating the aesthetic mapping\nNow that we have our data, we can pass it on to ggplot2 as follows:\n\nggplot(df)\n\n\n\n\nFigure 2: Empty plot\n\n\n\n\nWe still do not see anything because we have not selected the type of chart or the variables of the data that we want to plot. First, let us specify that we want to plot the AchievingGroup column (high vs. low achievers) on the x-axis. Assigning columns of our dataset to different elements of the plot is called constructing an aesthetic mapping. We can do it by calling the aes function from ggplot2, specifying that we want to map the AchievingGroup column to the x-axis, and then passing this call to aes to our plot using the second argument of ggplot:\n\n\n5.4 Add the geometry component\n\nggplot(df, aes(x = AchievingGroup)) \n\n\n\n\nFigure 3: Empty plot with AchievingGroup in x-axis labels\n\n\n\n\nWe now see that the x-axis has the two possible values of AchievingGroup: “High achiever” and “Low achiever”. We still need to tell ggplot2 the type of chart we want to use to plot the number of students of each type. To do that we need to add a geometrical (geom) component to our plot in which we specify that we want a bar chart. We do it by adding a + sign after our call to ggplot and calling geom_bar() (the name of the geometry that represents a bar chart).\n\nggplot(df, aes(x = AchievingGroup)) + geom_bar() \n\n\n\n\nFigure 4: Basic bar plot showing students by achievement group\n\n\n\n\nNow the plot can actually be called a plot. Notice that we have not specified what we want to plot in the y-axis. When not specified, ggplot2 assumes that we want to use the count of rows.\nWe also notice that the bars are in the wrong order. By default, ggplot2 orders the values in an ascending way (alphabetically in the case of text values). If we want to enforce our own order, we need to convert the AchievingGroup column of df into a factor and provide the ordered list of values to the levels argument.\n\ndf$AchievingGroup = factor(df$AchievingGroup,\n                           levels = c(\"Low achiever\", \"High achiever\"))\n\nIf we generate our plot again, we see that the bars are now in the order we want them to be:\n\nggplot(df, aes(x = AchievingGroup)) + geom_bar() \n\n\n\n\nFigure 5: Basic bar plot showing students by achievement group after transforming the x-axis variable into a factor\n\n\n\n\n\n\n5.5 Adding the color scale\nWe still need to color our bar chart according to students’ activity level. We do that by mapping the fill aesthetic to the ActivityLevel column inside the aes. When we provide the fill property, ggplot will automatically create the appropriate legend.\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + geom_bar()\n\n\n\n\nFigure 6: Basic bar plot showing students’ activity level by achievement group and colored by activity level\n\n\n\n\nAgain, we need to change the order of our legend so that it follows the logical semantic order for the activity levels (low-moderate-high):\n\ndf$ActivityGroup = factor(df$ActivityGroup,\n                          levels = c(\"Low activity\", \"Moderate activity\", \"High activity\"))\n\nIf we generate the plot again, we see now that the legend is in the right order:\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + geom_bar()\n\n\n\n\nFigure 7: Basic bar plot showing students’ activity level by achievement group and colored by activity level after ordering the legend\n\n\n\n\nHowever, the stacks are still not in the right order, being the low activity students at the top of the bar, and the high activity students at the bottom, which might be counter-intuitive. To change this, we need to reverse the position of the bar using position = position_stack(reverse = TRUE) inside geom_bar:\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE))\n\n\n\n\nFigure 8: Basic bar plot showing students’ activity level by achievement group and colored by activity level after ordering the stacks\n\n\n\n\nWe are getting closer but the color scheme does not quite match our intended result. To add a color scheme to our plot we need to add a scale layer. In this case, the scale is for the fill property, which is the color of the bars in our chart. There are many ways to specify the color scheme. One option is to use sequential colors from the same palette. For that we add a new layer to our plot named scale_fill_brewer and we pass the palette that we want as an argument. For example, palette number 15 would look like this:\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_brewer(palette = 15)\n\n\n\n\nFigure 9: Bar plot showing students’ activity level by achievement group with sequential color scale\n\n\n\n\nAnother option is to provide a manual scale with the colors of our choice. For that we use scale_fill_manual and specify a values vector as an argument. You need to specify as many colors as unique elements in your scale. In this case we have three activity groups (for low, moderate or high activity), so we must provide three colors. There are tons of resources online where you can find or create your own palettes (e.g., Coolors, Adobe Color or Lospec). You have to provide the hexadecimal code of each color or the official color names recognized by R. Below is an example:\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_manual(values = c(\"#ef6461\", \"#7AE7C7\", \"#8E518D\"))  \n\n\n\n\nFigure 10: Bar plot showing students’ activity level by achievement group with manual color scale\n\n\n\n\nLastly, a very common color scale used is Viridis. It is designed to be perceived by viewers with common forms of color blindness. To use it in our plot we just add scale_fill_viridis_d().\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_viridis_d() \n\n\n\n\nFigure 11: Bar plot showing students’ activity level by achievement group with viridis color scale\n\n\n\n\nViridis is the palette we need to replicate our target plot. However, the order of the color needs to be reversed so the most dense color represents the higher activity level. We do this by reversing the direction of the palette as follows:\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_viridis_d(direction = -1) \n\n\n\n\nFigure 12: Bar plot showing students’ activity level by achievement group with viridis color scale\n\n\n\n\n\n\n5.6 Working with themes\nNow that the geometry and color scheme of the bars looks like our initial plot, we notice that there are still some differences. An important one is the grey background of the plot. To change the general appearance of our plot, we may use the ggplot2 themes. Below are some examples:\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_viridis_d(direction = -1) + theme_dark()\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_viridis_d(direction = -1) + theme_classic()\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) +  \n  scale_fill_viridis_d(direction = -1) + theme_void()\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) +  \n  scale_fill_viridis_d(direction = -1) + theme_minimal()\n\n\n\n\n\n\nFigure 13: Bar plot using different themes: theme_dark (top left), theme_classic (top right), theme_void (bottom left), and theme_minimal (bottom right)\n\n\n\n\nWe have theme_dark with a dark background and border, theme_classic with thick axes and no grid lines, theme_void which is completely empty, and theme_minimal with a minimalistic look. There are more available in the ggplot2 documentation and even more third-party implementations. To recreate our goal plot, we select the theme_minimal. To avoid having to add the theme to all of our plots from now on, we can set a default theme for our whole project by using theme_set:\n\ntheme_set(theme_minimal())\n\nNotice how now we get theme_minimal even when we do not specify it in our code:\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_viridis_d(direction = -1)\n\n\n\n\nFigure 14: Bar plot with theme minimal by default\n\n\n\n\n\n\n5.7 Changing the axis ticks\nYou may have not noticed that another difference with our goal plot is the ticks in our y-axis. In the goal plot we count 10 by 10, whereas in our last plot we do so 20 by 20. Just like we modified the scale of the fill aesthetic when we changed the color of our bars, we can also modify the y aesthetic to adjust to our needs. We use the scale_y_continuous layer and we try different number of breaks (n.breaks), until we find what we like best:\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_viridis_d(direction = -1) + \n  scale_y_continuous(n.breaks = 15)\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_viridis_d(direction = -1) + \n  scale_y_continuous(n.breaks = 3)\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_viridis_d(direction = -1) + \n  scale_y_continuous(n.breaks = 7)\n\n\n\n\n\n\nFigure 15: Bar plot with different numbers of y.axis breaks: 15 (left), 3 (middle), and 7 (right)\n\n\n\n\nWe choose 7 breaks to obtain our desired result.\n\n\n5.8 Titles and labels\nOur plot is still missing some slight modifications to be 100% equal to the original one. For instance, the axes’ titles are not the same. To specify the y-axis label, we add a new layer to our plot named ylab and we pass a string with our desired label “Number of students”:\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) +  \n  scale_fill_viridis_d(direction = -1)  +\n  scale_y_continuous(n.breaks = 7) + \n  ylab(\"Number of students\") \n\n\n\n\nFigure 16: Bar plot with y-axis label\n\n\n\n\nWe do the same for the x-axis using xlab, and for the legend using labs:\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) +  \n  scale_fill_viridis_d(direction = -1)  +\n  scale_y_continuous(n.breaks = 7) + \n  ylab(\"Number of students\") +\n  xlab(\"Achievement group\") + \n  labs(fill = \"Activity level\")\n\n\n\n\nFigure 17: Bar plot with all labels\n\n\n\n\nMore importantly, we are missing the overall title of the plot. To add it we use ggtitle and we pass our intended plot title “Activity level by achievement group”. Keep in mind that, whenever possible, it is better to add a caption to the image rather than a title on the plot. A caption is more accessible for visually impaired users since it is compatible with screen readers. In scientific papers, it is also more common to have a Figure caption than a title within the plot. In social media, it is frequent to see the title on the plot as images are often shared without context. However, many social media platforms allow to provide an alternative text which is what screen readers will read as a substitute for the image, and that is also the case in learning analytics dashboards.\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_viridis_d(direction = -1)  +\n  scale_y_continuous(n.breaks = 7) + \n  ylab(\"Number of students\") +\n  xlab(\"Achievement group\") + \n  labs(fill = \"Activity level\") +\n  ggtitle(\"Activity level by achievement group\") \n\n\n\n\nFigure 18: Bar plot with title\n\n\n\n\n\n\n5.9 Other cosmetic modifications\nLastly, we need to do some slight modifications to the overall appearance of the plot. We do this through the generic theme function of ggplot2. We first modify the position of the legend by setting legend.position to “bottom”. We then increase the size of the axes titles, by setting axis.title to element_text(size = 12). Finally, we make the plot title bigger as well and put it in bold by setting plot.title to element_text(size = 15, face = \"bold\")). With these last changes, we have an exact replica of our original plot.\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_viridis_d(direction = -1) +\n  scale_y_continuous(n.breaks = 7) + \n  ylab(\"Number of students\") +\n  xlab(\"Achievement group\") + \n  labs(fill = \"Activity level\") +\n  ggtitle(\"Activity level by achievement group\") + \n  theme(legend.position = \"bottom\", \n        axis.title = element_text(size = 12), \n        plot.title = element_text(size = 15, face = \"bold\"))\n\n\n\n\nFigure 19: Bar plot with theme modifications\n\n\n\n\n\n\n5.10 Saving the plot\nSince we have obtained the desired result, we may now save it as an image to be able to use it elsewhere. For that, we first need to assign the plot to a variable (e.g., myplot).\n\nmyplot <- ggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  geom_bar(position = position_stack(reverse = TRUE)) + \n  scale_fill_viridis_d(direction = -1) +\n  scale_y_continuous(n.breaks = 7) + \n  ylab(\"Number of students\") +\n  xlab(\"Achievement group\") + \n  labs(fill = \"Activity level\") + \n  ggtitle(\"Activity level by achievement group\") + \n  theme(legend.position = \"bottom\", axis.title = element_text(size = 12), \n        plot.title = element_text(size = 15, face = \"bold\"))\n\nWe then use ggsave to save the plot to our filesystem. We need to specify the file path (including the extension, such as PNG, JPEG, etc.) where we want to save the plot (e.g., “bar.png”) as the first argument and pass the variable where we saved our plot (myplot) as a second argument. If we do not do this, ggplot2 assumes we want to save the latest plot that we created. Lastly, we may specify the width, height and resolution (dpi) of our plots. If we are submitting our figure to a scientific journal, we probably need a high resolution image. If we are using the figure in social media, we do not want the resolution to be so high as it would take a long time to load.\n\nggsave(\"bar.png\", myplot, width = 10000, height = 5000, units = \"px\", dpi = 900)\n\nThroughout this section, we have learned how we can create a plot from scratch using only the ggplot2 library and a simple dataset. We have seen the many customization possibilities (theme, scales, titles) that we can achieve using the different plot components without needing to rely on external tools for retouching our final graph. In the next section we will learn about new types of plots that might be more suitable for other types of data and their customization possibilities."
  },
  {
    "objectID": "chapters/ch06-data-visualization/ch6-viz.html#types-of-plots",
    "href": "chapters/ch06-data-visualization/ch6-viz.html#types-of-plots",
    "title": "6  Visualizing and Reporting Educational Data with R",
    "section": "6 Types of plots",
    "text": "6 Types of plots\nThe ggplot2 library offers many types of plots (or geoms) that you can choose from to visualize your data in several ways. In this section, we go over some of the most common types and present examples using students’ learning data.\n\n6.1 Bar plot\nWe have seen how to construct a bar plot in the previous section as an example of how to use ggplot2. But when should we use a bar plot? Bar plots are useful when we want to represent counts or any numerical variable broken down by categories. The y-axis would represent the count (or other continuous numerical variable) and the x-axis would represent the categories. Keep in mind that if the categories follow a natural order, the x-axis should respect it (for example: “Morning”, “Afternoon”, “Evening”; or “Children”, “Adults”, “Elders”). Otherwise, you can just order the x-axis alphabetically or from highest to lowest value in the y-axis.\n\nggplot(df, aes(x = AchievingGroup)) +  geom_bar(position = position_stack(reverse = TRUE)) \n\n\n\n\nFigure 20: Basic bar plot of students by achievement group\n\n\n\n\nRemember that you can add a “third dimension” to the plot by using the fill property. This is known as a ‘stacked’ bar chart and helps highlight the proportion of, in this case, students’ activity level (ActivityGroup).\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  scale_fill_viridis_d(direction = -1) + \n  geom_bar(position = position_stack(reverse = TRUE)) \n\n\n\n\nFigure 21: Basic bar plot of students by achievement group filled by activity level\n\n\n\n\nIf we care more about the actual number rather than the proportion of students with each activity level, instead of a stacked bar chart we can keep each ‘stack’ as a whole bar of their own. This plot is very useful to compare values among categories. We accomplish this by passing the position argument with the value “dodge” to the geom_bar component:\n\nggplot(df, aes(x = AchievingGroup, fill = ActivityGroup)) + \n  scale_fill_viridis_d(direction = -1) + geom_bar(position = \"dodge\") \n\n\n\n\nFigure 22: Basic bar plot of students by achievement group filled by activity level with position dodge instead of stacked\n\n\n\n\nWe can now see that the highest group is represented by the low achievers with low activity, followed by the high achievers with high activity.\n\n\n6.2 Histogram\nHistograms allow us to represent the distribution of a single continuous variable. It is inherently a bar chart, but instead of each bar representing the count of a single category, it represents the count of a range of values in the x-axis (what is known as a bin). Let us, for example, create a histogram for students’ online activity. Specifically, let us see the distribution of the number of accesses to the course main page online.\nIf we look at our dataset, we can see that the name of the variable that we are interested in is Frequency.Course_view:\n\nhead(df)\n\n\n\n# A tibble: 130 × 37\n   User      Name       Surname  Origin     Gender Birthdate Location Employment\n   <chr>     <chr>      <chr>    <chr>      <chr>  <chr>     <chr>    <chr>     \n 1 00a05cc62 Wan        Tan      Malaysia   M      12.12.19… Remote   None      \n 2 042b07ba1 Daniel     Tromp    Aruba      M      28.5.1999 Remote   None      \n 3 046c35846 Sarah      Schmit   Luxembourg F      25.4.1997 On camp… None      \n 4 05b604102 Lian       Abdullah Yemen      F      19.11.19… On camp… None      \n 5 0604ff3d3 Nina       Borg     Malta      F      13.6.1994 On camp… None      \n 6 077584d71 Mohamed    Gamal    Egypt      M      13.7.1998 On camp… Part-time \n 7 081b100cf Maximilian Gruber   Austria    M      20.12.19… On camp… None      \n 8 0857b3d8e Hugo       Pérez    Spain      M      22.12.19… On camp… None      \n 9 0af619e4b Aylin      Barat    Kazakhstan F      14.8.1995 On camp… None      \n10 0ec99ce96 Polina     Novik    Belarus    F      9.10.1996 On camp… None      \n# ℹ 120 more rows\n# ℹ 29 more variables: Frequency.Applications <dbl>,\n#   Frequency.Assignment <dbl>, Frequency.Course_view <dbl>,\n#   Frequency.Feedback <dbl>, Frequency.General <dbl>,\n#   Frequency.Group_work <dbl>, Frequency.Instructions <dbl>,\n#   Frequency.La_types <dbl>, Frequency.Practicals <dbl>,\n#   Frequency.Social <dbl>, Frequency.Ethics <dbl>, Frequency.Theory <dbl>, …\n\n\nTo create a histogram for this variable we may use the geom_histogram feature of ggplot2. We just pass our dataset and map the Frequency.Course_view variable to the x axis, and we add the geometry geom_histogram:\n\nggplot(df, mapping = aes(x = Frequency.Total)) + geom_histogram()\n\n\n\n\nFigure 23: Histogram of students’ course page views\n\n\n\n\nWe can provide our own value to the bins argument in geom_histogram to personalize how many bins we want in our plot:\n\nggplot(df, mapping = aes(x = Frequency.Total)) + \n  geom_histogram(bins = 50)\n\n\n\n\nFigure 24: Histogram of students’ course page view with 50 bins\n\n\n\n\nWe can also personalize the color scheme using fill for the background of the bars:\n\nggplot(df, mapping = aes(x = Frequency.Total)) + \n  geom_histogram(bins = 20,  fill = \"deeppink\" ) +  \n  scale_x_continuous(n.breaks = 10) \n\n\n\n\nFigure 25: Histogram of students’ course page view with color, fill and linewidth\n\n\n\n\nThe histogram allows us to acknowledge that most students had around 400-500 events, with another peak around 900-1000. Students with more than 1000 events were rare.\n\n\n6.3 Line plot\nAnother very widely used type of plot is the line plot. Like the histogram, it is also appropriate when we have both a numerical continuous x-axis and y-axis but it gives us a bit more liberty of what we plot and it is suitable for when we want to plot several series of data together. A very common scenario for a line plot is when we deal with timelines and we wish to visualize the evolution of a certain variable over time. Let us, for instance, plot the students’ daily events in the LMS throughout the course, a common plot in learning analytics dashboards. In the dataset that we have been using, we have the total count of events per user but not the timestamp of each event. We need to import the original event data from the dataset:\n\nev_url <- \"https://github.com/lamethods/data/raw/main/1_moodleLAcourse/Events.xlsx\"\nevents <- import(ev_url)\n\nThe Events.xlsx file contains all the actions that the students enrolled in this course performed in the LMS (Action) with their corresponding timestamp (timecreated): clicking on a lecture file, viewing the assignment instructions, etc.\n\nhead(events)\n\n\n\n# A tibble: 95,626 × 7\n   Event.context     user  timecreated         Component Event.name Log   Action\n   <chr>             <chr> <dttm>              <chr>     <chr>      <chr> <chr> \n 1 Assignment: Fina… 9d74… 2019-10-26 09:37:12 Assignme… Course mo… Assi… Assig…\n 2 Assignment: Fina… 9148… 2019-10-26 09:09:34 Assignme… The statu… Assi… Assig…\n 3 Assignment: Fina… 278a… 2019-10-18 12:05:28 Assignme… Course mo… Assi… Assig…\n 4 Assignment: Fina… 53d6… 2019-10-19 13:28:37 Assignme… The statu… Assi… Assig…\n 5 Assignment: Fina… aab7… 2019-10-15 23:38:13 Assignme… Course mo… Assi… Assig…\n 6 Assignment: Fina… 82ed… 2019-10-18 17:51:43 Assignme… Course mo… Assi… Assig…\n 7 Assignment: Fina… 4178… 2019-10-18 15:22:56 Assignme… Course mo… Assi… Assig…\n 8 Assignment: Fina… 82ed… 2019-10-22 13:46:51 Assignme… The statu… Assi… Assig…\n 9 Assignment: Fina… f2e9… 2019-10-15 14:58:17 Assignme… Submissio… Assi… Assig…\n10 Assignment: Fina… 53d6… 2019-10-19 13:28:38 Assignme… Course mo… Assi… Assig…\n# ℹ 95,616 more rows\n\n\nInstead of mapping timecreated directly to the x aesthetic, we can plot the timeline of the number of events per day by using as.Date(timecreated) and the geom_line geometry from ggplot2. Notice that, unlike geom_bar, if we do not provide a y aesthetic and want ggplot2 to count the number of events per day for us, we need to make it explicit by passing the stat argument with value \"count\" to geom_line.\n\nggplot(events, aes(x = as.Date(timecreated) )) + geom_line(stat = \"count\")\n\n\n\n\nFigure 26: Line plot of number of events per day\n\n\n\n\nThe line plot of students’ events allows us to identify periods of increased activity. We can see that it was low at the very beginning of the course, with some peaks corresponding to the assignment deadlines and one last peak for the final project. When the course is over, activity begins to decrease.\nTo make our plot more aesthetically pleasing, we can customize the color and line width. We do so by tweaking the color and linewidth properties of the geom_line. We can also fix the axes’ titles as we learned before. For example:\n\nggplot(events, aes(x = as.Date(timecreated) )) + \n  geom_line(stat = \"count\", color = \"turquoise\", linewidth = 2) + \n  xlab (\"Date\") + ylab(\"Number of events\")\n\n\n\n\nFigure 27: Line plot of number of events per day with color, linewidth, and custom labels\n\n\n\n\nWe can also add a point to mark each date using geom_point:\n\nggplot(events, aes(x = as.Date(timecreated) )) + \n  geom_line(stat = \"count\", color = \"turquoise\", linewidth = 1.5)  + \n  geom_point(stat = \"count\",  color = \"purple\", size = 2, stroke = 1)  + \n  xlab (\"Date\")  + \n  ylab(\"Number of events\")\n\n\n\n\nFigure 28: Line plot of number of events per hour with points every hour\n\n\n\n\nBesides visualizing the events for all the students of the course, we can pinpoint specific students to follow their progress and offer them personalized support. To do this, we would need to filter our data before handing it over to ggplot2. We can filter the data using the filter function from dplyr, as we learned in Chapter 4 [29]. We first install dplyr if we do not have it:\n\ninstall.packages(\"dplyr\")\n\nThen, we import it as usual:\n\nlibrary(dplyr)\n\nWe can now filter the data and pass it on to ggplot2:\n\nevents |> filter(user == \"9d744e5bf\") |> ggplot(aes(x = as.Date(timecreated) )) + \n  geom_line(stat = \"count\", color = \"turquoise\", linewidth = 2)  + \n  geom_point(stat = \"count\",  color = \"purple\", size = 2, stroke = 1)  + \n  xlab (\"Date\")  + \n  ylab(\"Number of events\")\n\n\n\n\nFigure 29: Line plot of number of events per date for a single student\n\n\n\n\n\n\n6.4 Jitter plots\nIn the previous plots we have seen aggregated information for all the cohort of students as well as information for a single student. However, in some occasions, it is very useful to see the general picture while accounting for possible individual differences. For example, using our original df dataset, we can plot the number of events on the LMS, differentiating between high achievers and low achievers.\nOne option is to use geom_point to represent each students’ count of events as a single point. To do this, we map the Event column to the x aesthetic, the Frequency column to the y aesthetic, and the User column to the group aesthetic:\n\nggplot(df, aes(x = AchievingGroup, y = Frequency.Total)) + \n  geom_point() +   \n  xlab(\"Achieving group\") + \n  ylab(\"Number of events\") +\n  theme(legend.position = \"bottom\", \n        legend.text = element_text(size = 7), \n        legend.title = element_blank())\n\n\n\n\nFigure 30: Jitter plot of number of events per achievement group using geom_point\n\n\n\n\nHowever, there are many points that overlap. If we use geom_jitter instead, we take advantage of the horizontal gap between the event names to spread the points and avoid the overlap:\n\nggplot(df, aes(x = AchievingGroup, y = Frequency.Total)) + \n  geom_jitter() + \n  xlab(\"Achieving group\") + \n  ylab(\"Number of events\") +\n  theme(legend.position = \"bottom\", \n        legend.text = element_text(size = 7), \n        legend.title = element_blank())\n\n\n\n\nFigure 31: Jitter plot of number of events per achievement group using geom_jitter\n\n\n\n\nThe plot shows that students that are high achievers generally have a higher number of events than low achievers.\n\n\n6.5 Box plot\nWhen we have too many data points, it is often more useful to visualize summary statistics instead of all the points. Box plots are very useful in summarizing data distributions. We can create a box plot for the number of events per achievement group using geom_boxplot:\n\nggplot(df, aes(x = AchievingGroup, y = Frequency.Total)) + geom_boxplot() + \n  xlab(\"Achieving group\") + ylab(\"Number of events\") \n\n\n\n\nFigure 32: Box plot of activity per achievement group\n\n\n\n\nThe lower hinge of each box indicates the 25% percentile, the thick middle line is the median, and the top hinge is the top 75% percentile. The upper whisker extends from the hinge up to the maximum value within 1.5 * IQR (inter-quantile range), whereas the lower whisker extends to the minimum value within 1.5 * IQR of the hinge. The points outside the whisker represent outliers in the distribution (i.e., values outside of the 1.5 * IQR range). As the jitter plot already hinted, the median number of events is higher in the high achieving group.\n\n\n6.6 Violin plot\nWe can also visualize the distribution of the number of events for each group using violin plots (geom_violin), but these are recommended when we have a large amount of data:\n\nggplot(df, aes(x = AchievingGroup, y = Frequency.Total)) + geom_violin() + \n  xlab(\"Achieving group\") + ylab(\"Number of events\") \n\n\n\n\nFigure 33: Violin plot of total activity per achievement group\n\n\n\n\n\n\n6.7 Scatter plots\nThe examples we have seen so far have dealt with plotting a single variable alone or divided in categories. Another common scenario is to investigate the direct relationship between two or more variables. Scatter plots are used to visualize how two numerical variables relate to each other. For example, we can use them to see how LMS activity relates to grades.\n\nggplot(df, aes(x = Frequency.Total, y = Final_grade)) + \n  geom_point() + \n  ylab(\"Final grade\") + xlab(\"Number of events\") \n\n\n\n\nFigure 34: Scatter plot of number of events vs. final grade\n\n\n\n\nIn the plot, each point represents a student. Students at the right side of the plot represent students with higher activity, while students closer to the left side of the plot, represent students with lower activity. At the same time, students with low grades are closer to the bottom of the plot, while students with high grades are closer to the top. Overall, se see an upward trend whereby students with higher activity indeed obtain better grades.\nWe can add another dimension by coloring points according to another variable. For example, we can color the points according to high vs. low achievers, so we can now where the division between the two groups is:\n\nggplot(df, aes(x = Frequency.Total, y = Final_grade, color = AchievingGroup)) + \n  geom_point() + \n  ylab(\"Final grade\") + xlab(\"Number of events\") +\n  labs(color = \"Achievement\")\n\n\n\n\nFigure 35: Scatter plot of number of events vs. final grade colored by achievement group\n\n\n\n\nWe can add yet another dimension by mapping the size aesthetic to another variable, for example Frequency.Group_work which represents the number of events related to group work.\n\nggplot(df, aes(x = Frequency.Total, y = Final_grade, \n               fill = AchievingGroup, size = Frequency.Group_work)) +\n  geom_point(color = \"black\", pch = 21) + \n  scale_size_continuous(range = c(1, 7)) +\n  ylab(\"Final grade\") + xlab(\"Number of events\") +\n  labs(size = \"Group work\", fill = \"Achievement\")\n\n\n\n\nFigure 36: Scatter plot of number of events vs. final grade colored by achievement group and sized by frequency of group work"
  },
  {
    "objectID": "chapters/ch06-data-visualization/ch6-viz.html#advanced-features",
    "href": "chapters/ch06-data-visualization/ch6-viz.html#advanced-features",
    "title": "6  Visualizing and Reporting Educational Data with R",
    "section": "7 Advanced features",
    "text": "7 Advanced features\n\n7.1 Plot grids\nSometimes, adding all the information in a single plot can be overwhelming and hard to interpret. For example, take a look at the following line plot that shows the number of events per day for each of the course online components:\n\nggplot(events, aes(x = as.Date(timecreated), color = Action )) + \n  scale_fill_viridis_d() +  \n  geom_line(stat = \"count\") +\n  xlab(\"Date\") + \n  ylab(\"Number of events\")\n\n\n\n\nFigure 37: Multiple series line plot\n\n\n\n\nIf we had only a few (2-5) lines, the plot would probably look very good, but as the number of categories grow, the plot becomes unintelligible. Instead of showing all the lines together, the plot would be easier to understand if each component had their own plot. To do this, instead of mapping the Action column to the color aesthetic, we add a new component to our plot using facet_wrap and we pass the name of the column as a character string (\"Action\"). We can change the geom_line to a geom_area to enhance the visualization.\n\nggplot(events, aes(x = as.Date(timecreated))) +\n  geom_area(stat = \"count\", fill = \"turquoise\", color = \"black\") + \n  facet_wrap(\"Action\") +  \n  xlab(\"Date\") + \n  ylab(\"Number of events\")\n\n\n\n\nFigure 38: Grid of multiple plots\n\n\n\n\n\n\n7.2 Combining multiple plots\nIn the previous example, we saw how to split a plot into multiple plots. But what happens if we want to combine multiple independent plots? For that purpose, we can use the library patchwork. Install it first if you do not have it already:\n\ninstall.packages(\"patchwork\")\n\nWe import the patchwork library:\n\nlibrary(patchwork)\n\nWe have to create the plots that we want to combine and assign each of them to a different variable. We can use previous examples from this chapter and assign them to variables named p1, p2, and p3.\n\np1 <- ggplot(df, aes(x = Frequency.Total, y = Final_grade)) + \n  geom_point() + ylab(\"Grade\") + \n  xlab(\"Total number of events\")\n\np2 <- ggplot(df, aes(x = AchievingGroup, fill = ActivityGroup )) + geom_bar(position = position_fill(reverse = T)) + \n  scale_fill_viridis_d(direction = -1) + \n  xlab(\"Achievement group\") + \n  ylab(\"Number of events\") + \n  labs(fill = \"Activity level\")\np3 <- ggplot(events, aes(x = as.Date(timecreated) )) + \n  geom_line(stat = \"count\", color = \"turquoise\", linewidth = 1.5)  + \n  geom_point(stat = \"count\",  color = \"purple\", size = 2, stroke = 1)  + \n  xlab (\"Date\")  + \n  ylab(\"Number of events\")\n\nNow, if we add the three variables together separated by the + sign, the plots will be placed horizontally next to each other:\n\np1 + p2 + p3\n\n\n\n\nFigure 39: Multiple plots stacked horizontally\n\n\n\n\nIf we use the / character side instead, we lay them out vertically:\n\np1 / p2 / p3\n\n\n\n\nFigure 40: Multiple plots stacked vertically\n\n\n\n\nWe can use combinations of both signs and even leave blank spaces as follows:\n\n(p1 + p2) / ( p3 + plot_spacer())\n\n\n\n\nFigure 41: Multiple plots in a grid\n\n\n\n\nPutting plots side by side can be very useful to compare datasets and discuss the differences. Some publication venues limit the number of figures or pages of their articles, so combining several plots together can be very useful to overcome this limitation."
  },
  {
    "objectID": "chapters/ch06-data-visualization/ch6-viz.html#creating-tables-with-gt",
    "href": "chapters/ch06-data-visualization/ch6-viz.html#creating-tables-with-gt",
    "title": "6  Visualizing and Reporting Educational Data with R",
    "section": "8 Creating tables with gt",
    "text": "8 Creating tables with gt\nWe have seen earlier in this chapter multiple types of visualizations that are suitable for diverse scenarios in learning analytics. However, we must not forget the other main way of reporting results or metrics, i.e., tables. When we display a data frame in Rstudio, it is by default presented as a table, but we need to be able to extract this table and display it in a dashboard, a report or a scientific article. The library gt can help us with this endeavor. First, install it if you do not have it yet:\n\ninstall.packages(\"gt\")\n\nWe then import it, as usual:\n\nlibrary(gt)\n\nLet us create a table, for example, to display the descriptive statistics of students’ events in the LMS. Using the events dataset, we first count the number of events of each type (Event.name) per student (user) using group_by and count from dplyr. We then group by Event.name only and use the summarize function, also from dplyr, to create the mean, and standard deviation of the number of events of each type per student, as we learned in Chapter 5 [30].\n\nevents |> \n  group_by(user, Action) |> \n  count() |> \n  group_by(Action) |>\n  summarize(Mean = mean(n), SD = sd(n))\n\n\n\n  \n\n\n\nNow that we have a data frame with the shape that we like, we can use gt to create the formatted table by simply adding gt to the pipeline of operations:\n\nevents |> \n  group_by(user, Action) |> \n  count() |> \n  group_by(Action) |>\n  summarize(Mean = mean(n), SD = sd(n)) |>\n  gt()\n\n\n\n\n\nTable 1:  Table created with gt \n  \n    \n    \n      Action\n      Mean\n      SD\n    \n  \n  \n    Applications\n11.07143\n9.825022\n    Assignment\n56.68462\n34.129492\n    Course_view\n194.56154\n151.656947\n    Ethics\n11.68182\n10.669050\n    Feedback\n24.71429\n16.243082\n    General\n25.73846\n21.390991\n    Group_work\n251.90769\n162.899810\n    Instructions\n49.80000\n40.272213\n    La_types\n14.54615\n7.583245\n    Practicals\n77.07692\n33.751627\n    Social\n18.10744\n19.034093\n    Theory\n11.10484\n6.922120\n  \n  \n  \n\n\n\n\n\nWe might add some tweaks by forcing the numerical columns to have two decimals and the first column to be aligned left. You can also apply themes to the table using the library gtExtras.\n\nevents |> \n  group_by(user, Action) |> \n  count() |> \n  group_by(Action) |>\n  summarize(Mean = mean(n), SD = sd(n)) |>\n  gt() |> \n  fmt_number(decimals = 2, columns = where(is.numeric)) |>\n  cols_align(align = \"left\", columns = 1)\n\n\n\n\n\nTable 2:  Table created with gt with formatting \n  \n    \n    \n      Action\n      Mean\n      SD\n    \n  \n  \n    Applications\n11.07\n9.83\n    Assignment\n56.68\n34.13\n    Course_view\n194.56\n151.66\n    Ethics\n11.68\n10.67\n    Feedback\n24.71\n16.24\n    General\n25.74\n21.39\n    Group_work\n251.91\n162.90\n    Instructions\n49.80\n40.27\n    La_types\n14.55\n7.58\n    Practicals\n77.08\n33.75\n    Social\n18.11\n19.03\n    Theory\n11.10\n6.92"
  },
  {
    "objectID": "chapters/ch06-data-visualization/ch6-viz.html#discussion",
    "href": "chapters/ch06-data-visualization/ch6-viz.html#discussion",
    "title": "6  Visualizing and Reporting Educational Data with R",
    "section": "9 Discussion",
    "text": "9 Discussion\nThe use of data visualization in the context of learning analytics has the potential to greatly enhance our understanding of student behavior and performance. Using tools such as ggplot2, instructors and researchers can create informative and visually appealing plots that highlight important patterns and trends in student activity, providing insights into factors that may be impacting student success and therefore inform instructional decisions and improve student outcomes.\nAs we have already seen throughout the chapter, we often do not use the same plots when we are dealing with categorical variables or numerical variables; when we are plotting a single variable or two or more, etc. Moreover, on some occasions when we need very detailed information, a table might be needed instead of or in addition to a figure. Table 6.3 summarizes the most commonly used visualization types that we have seen throughout this chapter according to the number of variables and the data type. It also points to the ggplot2 geometry that is used to create each visualization.\n\n\nTable 3: Summary of the types of visualization for each data type and number of variables\n\n\n\n\n\n\n\n\nNumber of variables\nVariable types\nType of visualization\nggplot2 geometry\n\n\n\n\nOne variable\nContinuous\nHistogram\ngeom_hist()\n\n\n\nDiscrete\nBar chart\ngeom_bar()\n\n\nTwo or more variables\nBoth continuous\nScatter plot\ngeom_point()\n\n\n\nOne discrete time and\nLine chart\ngeom_line()\n\n\n\none continuous\nArea chart\ngeom_area()\n\n\n\nOne discrete and one\nBar chart\ngeom_bar()\n\n\n\ncontinuous\nBox plot\ngeom_boxplot()\n\n\n\n\nJitter plot\ngeom_jitter()\n\n\n\n\nViolin plot\ngeom_violin()\n\n\n\nBoth discrete\nStacked bar chart\ngeom_bar()\n\n\n\n\nAnother way to decide which visualization to use is to think what kind of story we want to tell or which aspect of our data we want to highlight. Figure 6.42 shows a flowchart that can help choose the most suitable visualization for your data. There are many other decision charts online made for this purpose. For example, “From Data to Viz”1 leads you to the most appropriate graph for your data and also links to the code to build it and lists common caveats you should avoid.\n\n\n\nFigure 42: Flowchart to decide the most appropriate visualization for your data\n\n\nThroughout the rest of the book, we will see other forms of data visualization that are inherent to specific learning analytics methods. For example, in Chapter 15 [31], we will learn how to represent students’ discussions in the form of social networks, and in Chapter 10 [32], we will represent students’ sequences of activities using sequence analysis. The foundations learned in this chapter are key to understanding more complex visualizations in learning analytics and are, of course, transferable to other fields as well. We encourage readers to further their knowledge of data visualization by referring to the recommended resources in the next section. Especially readers that would like to take their visualizations to the next step should consider using shiny2, a web framework for R that allows creating fully interactive web apps for data analyses such as dashboards. By adding interactivity, instead of creating a static data report, final users can switch between datasets, filter the data in different ways, customize visualizations, etc."
  },
  {
    "objectID": "chapters/ch06-data-visualization/ch6-viz.html#additional-material",
    "href": "chapters/ch06-data-visualization/ch6-viz.html#additional-material",
    "title": "6  Visualizing and Reporting Educational Data with R",
    "section": "10 Additional material",
    "text": "10 Additional material\n\nWilke, Claus. 2019. Fundamentals of Data Visualization. O’Reilly. https://clauswilke.com/dataviz/.\nRahlf, Thomas. 2019 Data visualisation with R: 111 Examples. Springer. https://doi.org/10.1007/978-3-030-28444-2.\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pedersen. 2019. ggplot2: Elegant Graphics for Data Analysis (Use R) https://ggplot2-book.org/index.html.\nSahin, Muhittin and Dirk Ifenthaler. 2021. Visualizations and Dashboards for Learning Analytics. Springer. https://doi.org/10.1007/978-3-030-81222-5.\nDougherty, Jack and Ilya Ilyankou. 2021. Hands-On Data Visualization: Interactive Storytelling from Spreadsheets to Code https://handsondataviz.org/spreadsheet.html.\nFrom Data to Viz. https://www.data-to-viz.com/about.html\nWickham, Hadley. 2021. Mastering shiny. O’Reilly. https://mastering-shiny.org/."
  },
  {
    "objectID": "chapters/ch07-prediction/ch7-pred.html",
    "href": "chapters/ch07-prediction/ch7-pred.html",
    "title": "7  Predictive Modelling in Learning Analytics: A Machine Learning Approach in R",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch07-prediction/ch7-pred.html#introduction",
    "href": "chapters/ch07-prediction/ch7-pred.html#introduction",
    "title": "7  Predictive Modelling in Learning Analytics: A Machine Learning Approach in R",
    "section": "1 Introduction",
    "text": "1 Introduction\nPrediction of students’ performance has been a central theme within the field of learning analytics (LA) since the early days [1]. In fact, the initial conceptualization of the field has highlighted the use of digital data collected from learners to predict their success —among other usages. Such predictions hold the promise to help identify those who are at risk of low achievement, in order to proactively offer early support and appropriate intervention strategies based on insights derived from learners’ data [1, 2]. Nevertheless, the prediction of students’ performance is not unique to LA and was an important theme in related fields even before LA, e.g., academic analytics [3], educational data mining [4], and even far earlier in education research at large [5].\nSuch widespread, longstanding and continuous centrality of early and accurate prediction of students’ performance lends itself to the premise that detection of early signs could allow a timely prevention of, e.g., dropout, low achievement, or undesired outcomes in general [6]. More importantly, identifying the predictors could help inform interventions, explain variations in outcomes and inform educators of why such outcomes happened —a form of predictive modelling that is often referred to as explanatory modelling [7]. Noticeable is the famous example of the Signal system at the University of Purdue, where predictions were based on digital data collected from an online learning platform [8]. Signal produced predictions and classified students into three categories according to “safety” and presented the students with traffic-light-inspired dashboard signs, where at-risk students get a red light. However, the influence of Signal on retention rates is unclear and often debated [9]. Several other systems were designed, built and applied in practice, e.g., OU analyse at the Open University, where the system offers informative dashboards to students and teachers as well as predictive models to forecast students' performance [10].\nSuccessful prediction of students’ performance has been demonstrated repeatedly in several LA studies across the years [11]. In general, the majority of the published studies used features extracted from logged learning trace data (i.e., data about students’ interactions with online learning activities and resources) and achieved accurate predictions for a considerable number of students. Yet, most of such studies examined a single course or what is often referred to as a convenience sample (i.e., a course with a sufficiently large and accessible dataset) [11]. Studies that attempted to apply predictive modelling across several courses have not found similar success [12–15]. For instance, Finnegan et al. [15] examined 22 courses across three academic domains using student log-trace data recorded from the learning management system. The authors found considerable differences among predictive models developed for individual courses regarding their predictive power as well as the significance of features. Similar results were reported by Gašević et al. [12] who used data from nine undergraduate courses in different disciplines to examine how instructional variations affected the prediction of academic success. Gašević et al. [12] found that predictors were remarkably different across courses with no consistent pattern that would allow for having one model applicable across all courses. Similarly, Conijn et al. [13] examined 17 courses across several subjects and confirmed the considerable variability of the indicators and predictive models across courses.\nStudies within the same domain have also found significant differences in predictors and predictive models. For instance, a recent study [14] examined 50 courses with a similar course design and homogeneous pedagogical underpinning. The authors found variations among different offerings of the same course, that is, the same predictor was statistically significantly correlated with performance in one course offering, but not in the same course offered to similar students in the next year. Furthermore, some predictors were more consistent than others e.g., the frequency of online sessions was more consistent than the frequency of lectures. In a similar vein, Jovanović et al. [16] applied mixed-effect linear modelling to data from fifty combined courses and developed several predictive models with different combinations of features. All predictive models in the work by Jovanović et al. [16] were able to explain only a limited proportion of variations in students’ grades. The intraclass correlation coefficient (a measure of source of variability) of all models revealed that the main source of variability were students themselves, that is, students’ specific features not captured in the logged data, pointing to the importance of taking students’ international conditions into account.\nThe goal of this chapter is to introduce the reader to predictive LA. The next section is a review of the existing literature, including the main objectives, indicators and algorithms that have been operationalized in previous works. The remainder of the chapter is a step-by-step tutorial of how to perform predictive LA using R. The tutorial describes how to predict student success using students’ online trace log data extracted from a learning management system. The reader is guided through all the required steps to perform prediction, including the data preparation and exploration, the selection of the relevant indicators (i.e., feature engineering) and the actual prediction of student success."
  },
  {
    "objectID": "chapters/ch07-prediction/ch7-pred.html#sec-2",
    "href": "chapters/ch07-prediction/ch7-pred.html#sec-2",
    "title": "7  Predictive Modelling in Learning Analytics: A Machine Learning Approach in R",
    "section": "2 Predictive modelling: objectives, features, and algorithms",
    "text": "2 Predictive modelling: objectives, features, and algorithms\nExtensive research in the LA field has been devoted to the prediction of different measures of student success, as proven by the existence of multiple reviews and meta-analyses on the topic [17–20]. Among the measures of student success that have been examined in the literature are student retention [21], grades [22], and course completion [23]. Predicting lack of success has also been a common target of predictive analytics, mostly in the form of dropout [24], with special interest in the early prediction of at-risk students [25, 26].\nTo predict student success, numerous indicators from varying data sources have been examined in the literature. Initially, indicators were derived from students’ demographic data and/or academic records. Some examples of such indicators are age, gender, and previous grades [27]. More recent research has focused on indicators derived from students’ online activity in the learning management system (LMS) [17, 20]. Many of such indicators are derived directly from the raw log data such as the number of total clicks, number of online sessions, number of clicks on the learning materials, number of views of the course main page, number of assignments completed, number of videos watched, number of forum posts [13, 14, 28–31]. Other indicators are related to time devoted to learning, rather than to the mere count of clicks, such as login time, login frequency, active days, time-on-task, average time per online session, late submissions, and periods of inactivity [13, 14, 32–35]. More complex indicators are often derived from the time, frequency, and order of online activities, such as regularity of online activities, e.g., regularity of accessing lecture materials [16, 36, 37], or regularity of active days [14, 16]. Network centrality measures derived from network analysis of interactions in collaborative learning settings were also considered, as they compute how interactions relate to each other and their importance [38]. Research has found that predictive models with generic indicators are only able to explain just a small portion of the overall variability in students’ performance [36]. Moreover, it is important to take into account learning design as well as quality and not quantity of learning [17, 20].\nThe variety of predictive algorithms that have been operationalized in LA research is also worth discussing. Basic algorithms, such as linear and logistic regression, or decision trees, have been used for their explainability, which allows teachers to make informed decisions and interventions related to the students “at risk” [37]. Other machine learning algorithms have also been operationalized such as kNN or random forest [39, 40], although their interpretability is less straightforward. Lastly, the most cutting-edge techniques in the field of machine learning have also made their way to LA, such as XGBoost [41] or Neural Networks [42]. Despite the fact that the accuracy achieved by these complex algorithms is often high, their lack of interpretability is often pointed out as a reason for teachers to avoid making decisions based on their outcomes [7, 43].\nIt is beyond the scope of this review to offer a comprehensive coverage of the literature. Interested readers are encouraged to read the cited literature and the literature reviews on the topics [11, 14, 17–20]"
  },
  {
    "objectID": "chapters/ch07-prediction/ch7-pred.html#predicting-students-course-success-early-in-the-course",
    "href": "chapters/ch07-prediction/ch7-pred.html#predicting-students-course-success-early-in-the-course",
    "title": "7  Predictive Modelling in Learning Analytics: A Machine Learning Approach in R",
    "section": "3 Predicting students’ course success early in the course",
    "text": "3 Predicting students’ course success early in the course\n\n3.1 Prediction objectives and methods\nThe overall objective of this section is to illustrate predictive modelling in LA through a typical LA task of making early-in-the-course predictions of the students’ course outcomes based on the logged learning-related data (e.g., making predictions of the learners’ course outcomes after log data has been gathered for the first 2-3 weeks). The course outcomes will be examined and predicted in two distinct ways: i) as success categories (high vs. low achievement), meaning that the prediction task is approached with classification models; ii) as success score (final grades), in which case the development of regression models is required.\nTo meet the stated objectives, the following overall approach will be applied: create several predictive models, each one with progressively more learning trace data (i.e., logged data about the learners’ interactions with course resources and activities), as they become available during the course. In particular, the first model will be built using the learning traces available at the end of the first week of the course; the second model will be built using the data available after the completion of the second week of the course (i.e., the data logged over the first two weeks); then, the next one will be built by further accumulating the data, so that we have learning traces for the first three weeks, and so on. In all these models, the outcome variable will be the final course outcome (high/low achievement for classification models, that is, the final grade for regression models). We will evaluate all the models on a small set of properly chosen evaluation metrics and examine when (that is, how early in the course) we can make reasonably good predictions of the course outcome. In addition, we will examine which learning-related indicators (i.e., features of the predictive models) had the highest predictive power.\n\n\n3.2 Context\nThe context of the predictive modelling presented in this chapter is a postgraduate course on learning analytics (LA), taught at University of Eastern Finland. The course was 6 weeks long, though some assignments were due in the week after the official end of the course. The course covered several LA themes (e.g., Introductory topics, Learning theories, Applications, Ethics), and each theme was covered roughly in one week of the course. Each theme had a set of associated learning materials, mostly slides, and reading resources. The course reading resources included seminal articles, book chapters, and training materials for practical work. The course also contained collaborative project work (referred to as group projects). In the group project, students worked together in small groups to design an LA system. The group project was continuous all over the course and was designed to align with the course themes. For instance, when students learned about LA data collection, they were required to discuss the data collection of their own project. The group project has two grades, one for the group project as a whole and another for the individual contribution to the project. It is important to note here that the dataset is based on a synthetic anonymized version of the original dataset and was augmented to three times the size of the original dataset. For more details on the course and the dataset, please refer to the dataset chapter [44] of the book.\n\n\n3.3 An overview of the required tools (R packages)\nIn addition to a set of tidyverse packages that facilitate general purpose data exploration, wrangling, and analysis tasks (e.g., dplyr, tidyr, ggplot2, lubridate), in this chapter, we will also need a few additional R packages relevant for the prediction modelling tasks:\n\nThe caret (Classification And REgression Training) package [45] offers a wide range of functions that facilitate the overall process of development and evaluation of prediction models. In particular, it includes functions for data pre-processing, feature selection, model tuning through resampling, estimation of feature importance, and the like. Comprehensive documentation of the package, including tutorials, is available online1.\n\n\nThe randomForest package [46] provides an implementation of the Random Forest prediction method [47] that can be used both for the classification and regression tasks.\nThe performance package [48] offers utilities for computing indices of model quality and goodness of fit for a range of regression models. In this chapter, it will be used for estimating the quality of linear regression models. The package documentation, including usage examples, is available online2.\n\n\nThe corrplot package [49] allows for seamless visual exploration of correlation matrices and thus facilitates understanding of connections among variables in high dimensional datasets. A detailed introduction to the functionality the package offers is available online3.\n\n\n\n3.4 Data preparation and exploration\nThe data that will be used for predictive modelling in this chapter originates from the LMS of a blended course on LA. The dataset is publicly available in a GitHub repository4, while its detailed description is given in the book’s chapter on datasets [44]. In particular, we will make use of learning trace data (stored in the Events.xlsx file) and data about the students’ final grades (available in the Results.xlsx file).\nWe will start by familiarising ourselves with the data through exploratory data analysis.\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(rio)\n\nAfter loading the required packages, we will load the data from the two aforementioned data files:\n\nevents = import(\"https://github.com/lamethods/data/raw/main/1_moodleLAcourse/Events.xlsx\")\nresults = import(\"https://github.com/lamethods/data/raw/main/1_moodleLAcourse/Results.xlsx\")\n\nWe will start by exploring the events data, and looking first into its structure:\n\nevents\n\n\n\n  \n\n\n\nSince we intend to build separate predictive models for each week of the course, we need to be able to organise the events data into weeks. Therefore, we will extend the events data frame with additional variables that allow for examining temporal aspects of the course events from the weekly perspective. To that end, we first order the events data based on the events’ timestamp (timecreated) and then add three auxiliary variables for creating the course_week variable: weekday of the current event (wday), weekday of the previous event (prev_wday), and indicator variable for the start of a new week (new_week). The assumption applied here is that each course week starts on Monday and the beginning of a new week (new_week) can be identified by the current event being on Monday (wday==\"Mon\") while the previous one was on any day other than Monday (prev_wday!=\"Mon\") :\n\nevents |>\n  arrange(timecreated) |>\n  mutate(wday = wday(timecreated, \n                     label = TRUE, \n                     abbr = TRUE, \n                     week_start = 1)) |>\n  mutate(prev_wday = lag(wday)) |>\n  mutate(new_week = ifelse((wday == \"Mon\") & (is.na(prev_wday) | prev_wday != \"Mon\"), \n                           yes = TRUE, no = FALSE)) |>\n  mutate(course_week = cumsum(new_week)) -> events\n\nHaving created the variable that denotes the week of the course (course_week), we can remove the three auxiliary variables, to keep our data frame tidy:\n\nevents |> select(-c(wday, prev_wday, new_week)) -> events\n\nWe can now explore the distribution of the events across the course weeks. The following code will give us the count and proportion of events per week (with proportions rounded to the 4th decimal):\n\nevents |>\n  count(course_week) |>\n  mutate(prop = round(n/nrow(events), 4))\n\nThe output of the above lines show that we have data for seven weeks: six weeks of the course plus one more week, right after the course officially ended but students were still able to submit assignments. We can also observe that the level of students’ interaction with course activities steadily increased up until week 5 and then started going down.\nLet us now move to examining the factor variables that represent different types of actions and logged events. First, we can check how many distinct values each of these variables has:\n\nevents |>\n  summarise(across(c(Event.context, Component:Action), n_distinct))\n\n\n\n  \n\n\n\nWe can also examine unique values of each of the four variables, but it is better to examine them together, that will help us better understand how they relate to one another and get a better idea of the semantics of events they denote. For example, we can examine how often distinct Component, Event, and Action values co-occur:\n\nevents |>\n  count(Component,Event.name, Action) |> \n  arrange(Component, desc(n))  \n\n\n\n  \n\n\n\n\n\n\nLikewise, we can explore how Action and Log values are related (i.e., co-occur):\n\nevents |>             \n  count(Action, Log) |> \n  arrange(Action)         \n\n\n\n  \n\n\n\n\n\n\nHaving explored the four categorical variables that capture information about the students’ interactions with course resources and activities, we will select the Action variable as the most suitable one for further analysis. The reason for choosing the Action variable is twofold: i) it is not overly granular (it has 12 distinct values), and thus allows for the detection of patterns in the learning trace data; ii) it captures sufficient information about the distinct kinds of interaction the events refer to. In fact, the Action variable was manually coded by the course instructor to offer a more nuanced way of analysis. The coding was performed to group actions that essentially indicate the same activities under the same label. For instance, logs of viewing feedback from the teacher were grouped under the label feedback. Practical activities (Social network analysis or Process mining) were grouped under the label practicals. In the same way, accessing the group work forums designed for collaboration, browsing, reading others’ comments, or writing were all grouped under the label group_work [50].\nWe will rename some of the Action values to make it clear that they refer to distinct topics of the course materials:\n\ntopical_action <- c(\"General\", \"Applications\", \"Theory\",  \"Ethics\", \"Feedback\", \"La_types\")\n\nevents |>\n  mutate(action = ifelse(test = Action %in% topical_action, \n                         yes = str_glue(\"Materials_{Action}\"), \n                         no = Action), \n         .keep = \"unused\") -> events\n\nLet us now visually examine the distribution of events across different action types and course weeks:\n\n# Compute event counts across action types and course weeks\nevents |>\n  count(course_week, action) |>\n  arrange(course_week, desc(n)) -> action_dist_across_weeks\n\n# Visualise the event distribution\naction_dist_across_weeks |>\n  mutate(Action = as.factor(action)) |>\n  ggplot(aes(x = course_week, y = n, fill = action)) +\n  geom_col(position = position_fill()) +\n  scale_fill_brewer(palette = 'Paired') +\n  scale_x_continuous(breaks = seq(1,7)) +\n  labs(x = \"\\nCourse week\", y = \"Proportion\\n\") +\n  theme_minimal()\n\n\n\n\nFigure 1. Distribution of action types across the course weeks\n\n\n\n\nFrom the plot produced by the above lines of code (Figure 21.1), we can observe, for example, that group work (Group_work) was the most represented type of actions from week 2 till the end of the course (week 6). It is followed by browsing the main page of the course containing the course materials, announcements and updates (Course_view) and working on practical tasks (Practicals). We can also note that the assignment-related actions (Assignment) are present mostly towards the end of the course.\nNow that we have familiarised ourselves with the events data and done some initial data preparation steps, we should do some final ‘polishing’ of the data and store it to have it ready for further analysis.\n\n# Keep only the variables to be used for further analysis and \n# Rename some of the remaining ones to keep naming consistent\nevents |> \n  select(user, timecreated, course_week, action) |>\n  rename(week = course_week, ts = timecreated) -> events\n\n# Save the prepared data in the R native format\ndir.create(\"preprocessed_data\")\nsaveRDS(events, \"preprocessed_data/events.RDS\")\n\nThe next step is to explore the grades data that we previously loaded into the results data frame\n\nresults\n\n\n\n  \n\n\n\nEven though the results dataset includes the students’ grades on individual assignments, we will be able to use just the final grade (Final_grade) since we do not have information when during the course the individual assignment grades became available.\nTo get an overall understanding of the final grade distribution, we will compute the summary statistics and plot the density function for the Final_grade variable:\n\nsummary(results$Final_grade)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   5.666   7.954   7.254   9.006  10.000 \n\n\n\nggplot(results, aes(x = Final_grade)) +\n  geom_density() +\n  labs(x = \"Final grade\", \n       title = \"Distribution of the final grade\") +\n  theme_minimal()\n\n\n\n\nFigure 2. Distribution of the final course grade\n\n\n\n\nWe can clearly notice both in the summary statistics and the distribution plot (Figure 20.2) that the final grade is not normally distributed, but skewed towards higher grade values.\nAs noted in Section 7.3.1, we will build two kinds of prediction models: models that predict the final grade (regression models) as well as models that predict whether a student belongs to the group of high or low achievers (classification models). For the latter group of models, we need to create a binary variable (e.g., Course_outcome) indicating if a student is in the high or low achievement group. Students whose final grade is above the 50th percentile (i.e., above the median) will be considered as being high achievers in this course (High), the rest will be considered as having low course achievement (Low):\n\nresults |>\n  mutate(Course_outcome = ifelse(test = Final_grade > median(Final_grade),\n                                 yes = \"High\", no = \"Low\")) |>\n  mutate(Course_outcome = factor(Course_outcome)) -> results\n\nNow that we have prepared the outcome variables both for regression and classification models (Final_grade and Course_outcome, respectively), we can save them for later use in model building:\n\nresults |>\n  select(user, Final_grade, Course_outcome) |>\n  saveRDS(\"preprocessed_data/final_grades.RDS\")\n\n\n\n3.5 Feature engineering\nAfter the data has been preprocessed, we can focus on feature engineering, that is, the creation of new variables (features) to be used for model development. This step needs to be informed by the course design and any learning theory that underpins the course design, so that the features we create and use for predictive modelling are able to capture relevant aspects of the learning process in the given learning settings. In addition, we should consult the literature on predictive modelling in LA (see Section 7.2), to inform ourselves about the kinds of features that were good predictors in similar learning settings. Following such an approach, we have identified the following event-based features as potentially relevant:\n\nFeatures based on learning action counts\n\nTotal number of each type of learning actions\nAverage number of actions (of any type) per day\nEntropy of action counts per day\n\nFeatures based on learning sessions:\n\nTotal number of learning sessions\nAverage (median) session length (time)\nEntropy of session length\n\nFeatures based on number of active days (= days with at least one learning session)\n\nNumber of active days\nAverage time distance between two consecutive active days\n\n\nIn addition to the course specific features (A1), the feature set includes several course-design agnostic (i.e., not directly related to a specific course design) features (e.g., A2 and A3) that proved as good predictors in similar (blended) learning settings [14, 16, 36, 51]. Furthermore, the chosen features allow for capturing both the amount of engagement with the course activities (features A1, A2, B1, B2, C1) and regularity of engagement (features A3, B3, C2) at different levels of granularity (actions, sessions, days).\nTo compute features based on action counts per day (group A), we need to extend the events dataset with date as an auxiliary variable:\n\nevents |> mutate(date = as.Date(ts)) -> events\n\nTo compute features based on learning sessions, we need to add sessions to the events data. It is often the case that learning management systems and other digital learning platforms do not explicitly log beginning and end of learning sessions. Hence, LA researchers have used heuristics to detect learning sessions in learning events data. An often used approach to session detection consists of identifying overly long periods of time between two consecutive learning actions (of the same student) and considering them as the end of one session and beginning of the next one [14, 16, 36]. To determine such overly long time periods that could be used as “session delimiters”, LA researchers would examine the distribution of time periods between consecutive events in a time-ordered dataset, and set the delimiter to the value corresponding to a high percentile (e.g., 85th or 90th percentile) of the time distance distribution. We will rely on this approach to add sessions to the event data.\nFirst, we need to compute time distance between any two consecutive actions of each student:\n\nevents |>\n  group_by(user) |>\n  arrange(ts) |>\n  mutate(ts_diff = ts - lag(ts)) |>\n  ungroup() -> events\n\nNext, we should examine the distribution of time differences between any two consecutive actions of each student, to set up a threshold for splitting action sequences into sessions:\n\nevents |> pull(ts_diff) -> ts_diff\n\nts_diff_hours = as.numeric(ts_diff, units = 'hours')\n\nsummary(ts_diff_hours)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.      NA's \n  0.00000   0.00028   0.00028   1.40238   0.01694 307.05028       130 \n\n\nAs summary statistics is not sufficiently informative, we should examine the upper percentiles:\n\nquantile(ts_diff_hours, probs = seq(0.8, 1, 0.01), na.rm = TRUE) |> round(3)\n\n    80%     81%     82%     83%     84%     85%     86%     87%     88%     89% \n  0.017   0.017   0.034   0.034   0.034   0.050   0.067   0.084   0.117   0.167 \n    90%     91%     92%     93%     94%     95%     96%     97%     98%     99% \n  0.234   0.350   0.617   1.000   1.834   3.367   7.434  14.300  22.035  39.767 \n   100% \n307.050 \n\n\nConsidering the computed percentile values, on one hand, and the expected length of learning activities in the online part of the course (which included long forums discussions), we will set 1.5 hours (value between 93th and 94th percentile) as the threshold for splitting event sequences into learning sessions:\n\nevents |>\n  mutate(ts_diff_hours = as.numeric(ts_diff, units = 'hours')) |>\n  group_by(user) |>\n  arrange(ts) |>\n  mutate(new_session = (is.na(ts_diff_hours)) | (ts_diff_hours >= 1.5)) |>   \n  mutate(session_nr = cumsum(new_session))|> \n  mutate(session_id = paste0(user,\"_\", \"session_\",session_nr)) |>\n  ungroup() -> events_with_sessions\n\nWe will also add session length variable, which can be computed as the difference between the first and last action in each session, as it will be required for the computation of some of the features (B2 and B3):\n\nevents_with_sessions |>\n  group_by(session_id) |>\n  mutate(session_len = as.numeric(max(ts) - min(ts), units = \"secs\")) |>\n  ungroup() -> events_with_sessions\n\nAfter adding the necessary variables for feature computation, we can tidy up the dataset before proceeding to the feature computation. In particular, we will keep only the variables required for feature computation:\n\nevents_with_sessions <- events_with_sessions |>\n  select(user, ts, date, week, action, session_nr, session_id, session_len)\n\nAll functions for computing the event-based features outlined above are given in the feature_creation R script. In the following, we give a quick overview of those functions, while the script with further explanations is available at the book’s GitHub repository:\n\nthe total_counts_per_action_type function computes the set of features labelled as A1, that is, counts of each type of learning action, up to the current week\nthe avg_action_cnt_per_day function computed feature A2, that is, average (median) number of learning actions per day, up to the current week\nthe daily_cnt_entropy function computes feature A3, namely entropy of action counts per day, up to the current week\nthe session_based_features function computes all session-based features up to the current week: total number of sessions (B1), average (median) session length (B2), and entropy of session length (B3)\nthe active_days_count function computes the number of active days (C1), up to the current week\nthe active_days_avg_time_dist function computes avg. (median) time distance between two consecutive active days (C2), up to the current week\nfinally, the create_event_based_features function makes use of the above functions to compute all event-based features, up to the given week\n\nHaving defined the feature set and functions for feature computation, cumulatively for each week of the course, we can proceed to the development of predictive models. In the next section (Section 7.3.6), we will present the creation and evaluation of models for predicting the overall course success (high / low), whereas Section 7.3.7 will present models for predicting the final course grade.\n\n\n3.6 Predicting success category\nTo build predictive (classification) models, we will use Random Forest [47]. This decision was motivated by the general high performance of this algorithm on a variety of prediction tasks [52] as well as its high performance on prediction tasks specific to the educational domain [43].\nRandom forest (RF) is a very efficient machine learning method that can be used both for classification and regression tasks. It belongs to the group of ensemble methods, that is, machine learning methods that build and combine multiple individual models to do the prediction. In particular, RF builds and combines the output of several decision or regression trees, depending on the task at hand (classification or regression). The way it works can be briefly explained as follows: the method starts by creating a number of bootstrapped training samples to be used for building a number of decision trees (e.g. 100). When building each tree, each time a split in a tree is to be made, instead of considering all predictors, a random sample of predictors is chosen as split candidates from the full set of predictors (typically the size of the sample is set to be equal to the square root of the number of predictors). The reason for choosing a random sample of predictors is to make a diverse set of trees, which has proven to increase the performance. After all the trees have been built, each one is used to generate a prediction, and those predictions are then aggregated into the overall prediction of the RF model. In case of a classification task, the aggregation of predictions is done through majority vote, that is, the class voted (i.e., predicted) by the majority of the classification trees is the final prediction. In case of a regression task, the aggregation is done by averaging predictions of individual trees. For a thorough explanation of the RF method (with examples in R), an interested reader is referred to Chapter 8 of [53].\nWe will first load the additional required R packages as well as R scripts with functions for feature computation and model building and evaluation:\n\nlibrary(caret)\nlibrary(randomForest)\n\nsource(\"feature_creation.R\")\nsource(\"model_develop_and_eval.R\")\n\nThe following code snippet shows the overall process of model building and evaluation, one model for each week of the course, starting from week 1 to week 5. Note that week 5 is set as the last week for prediction purposes since it is the last point during the course when some pedagogical intervention, informed by the model’s output, can be applied by the course instructors.\n\nmodels <- list()\neval_measures <- list()\n\nfor(k in 1:5) {\n  \n  ds <- create_dataset_for_course_success_prediction(events_with_sessions, \n                                                  k, results)\n  set.seed(2023)\n  train_indices <- createDataPartition(ds$Course_outcome, \n                                       p = 0.8, list = FALSE)\n  train_ds <- ds[train_indices,] |> select(-user)\n  test_ds <- ds[-train_indices,] |> select(-user)\n\n  rf <- build_RF_classification_model(train_ds)\n  eval_rf <- get_classification_evaluation_measures(rf, test_ds)\n  \n  models[[k]] <- rf\n  eval_measures[[k]] <- eval_rf\n}\n\nThe process consists of the following steps, each of which will be explained in more detail below:\n\nCreation of a dataset for prediction of the course outcomes, based on the logged events data (events_with_sessions) up to the given week (k) and the available course outcomes data (results)\nSplitting of the dataset intro the part for training the model (train_ds) and evaluating the model’s performance (test_ds)\nBuilding a RF model based on the training portion of the dataset\nEvaluating the model based on the test portion of the dataset\n\nAll built models and their evaluation measures are stored (in models and eval_measures lists) so that they can later be compared.\nGoing now into details of each step, we start with the creation of a dataset to be used for predictive modelling in week k. This is done by first computing all features based on the logged events data (events_data) up to the week k, and then adding the course outcome variable (Course_outcome) from the dataset with course results (grades):\n\ncreate_dataset_for_course_success_prediction <- function(events_data, \n                                                      current_week, \n                                                      grades) {\n  features <- create_event_based_features(events_data, current_week)\n  grades |>\n    select(user, Course_outcome) |>\n    inner_join(features, by = \"user\")\n} \n\nNext, to be able to properly evaluate the performance of the built model, we need to test its performance on a dataset that the model “has not seen”. This requires the splitting of the overall feature set into two parts: one for training the model (training set) and the other for testing its performance (test set). This is done in a way that a larger portion of the dataset (typically 70-80%) is used for training the model, whereas the rest is used for testing. In our case, we use 80% of the feature set for training (train_ds) and 20% for evaluation purposes (test_ds). Since observations (in this case, students) are randomly selected for the training and test sets, to assure that we can replicate the obtained results, we initiate the random process with an (arbitrary) value (set.seed).\nIn the next step, we use the training portion of the dataset to build a RF model, as shown in the code snippet below. We train a model by tuning its mtry hyper-parameter and choose the model with optimal mtry value based on the Area under the ROC curve (AUC ROC) metric. The mtry hyper-parameter defines the number of features that are randomly chosen at each step of tree branching, and thus controls how much variability will be present among the trees that RF will build. It is one of the key hyper-parameters for tuning RF models and its default value (default_mtry) is equal to the square root of the number of features (n_features). Hence, we create a grid that includes the default value and a few values around it.\n\nbuild_RF_classification_model <- function(dataset) {\n  \n  #defining the model hyperparameter (mtry) that we want to tune \n  n_features <- ncol(dataset)-1\n  default_mtry <- round(sqrt(n_features))\n  grid <- expand.grid(.mtry = (default_mtry-1):(default_mtry+1))\n  \n  #setting that we want to train the model through 10-fold cross-validation\n  ctrl <- trainControl(method = \"CV\", \n                       number = 10,\n                       classProbs = TRUE,\n                       summaryFunction = twoClassSummary)\n  \n  # initiating the training process and setting the evaluation measure \n  # (ROC) for choosing the best value of the tuned hyperparameter \n  rf <- train(x = dataset |> select(-Course_outcome),\n              y = dataset$Course_outcome,\n              method = \"rf\",\n              metric = \"ROC\",\n              tuneGrid = grid,\n              trControl = ctrl)\n  \n  rf$finalModel\n}\n\nThe parameter tuning is done through 10-fold cross-validation (CV). K-fold CV is a widely used method for tuning parameters of machine learning models. It is an iterative process, consisting of k iterations, where the training dataset is randomly split into k folds of equal size, and in each iteration, k-1 folds are used for training the model whereas the k-th fold is used for evaluating the model on the chosen performance measure (e.g., ROC AUC, as in our case). In particular, in each iteration, a different fold is used for evaluation purposes, whereas the remaining k-1 folds are used for training. When this iterative process is finished, the models’ performance, computed in each iteration, are averaged, thus giving a more stable estimate of the performance for a particular value of the parameter being tuned. CV is often done in 10 iterations, hence the name 10-fold CV.\nThe final step is to evaluate each model based on the test data. To that end, we compute four standard evaluation metrics for classification models —Accuracy, Precision, Recall, and F1— as shown in the code snippet below. These four metrics are based on the so-called confusion matrix, which is, in fact, a cross-tabulation of the actual and predicted counts for each value of the outcome variable (i.e., class).\n\nget_classification_evaluation_measures <- function(model, test_data) {\n  \n  # use the model to make predictions on the test set\n  predicted_vals <- predict(model, \n                            test_data |> select(-Course_outcome))\n  actual_vals <- test_data$Course_outcome\n  \n  # create the confusion matrix (see Figure 3)\n  cm <- table(actual_vals, predicted_vals)\n  \n  TP <- cm[2,2] \n  TN <- cm[2,2]\n  FP <- cm[1,2]\n  FN <- cm[2,1]\n  \n  # compute evaluation measures based on the confusion matrix\n  accuracy = sum(diag(cm)) / sum(cm)\n  precision <- TP / (TP + FP)\n  recall <- TP / (TP + FN)\n  F1 <- (2 * precision * recall) / (precision + recall)\n  \n  c(Accuracy = accuracy, \n    Precision = precision, \n    Recall = recall, \n    F1 = F1)\n}\n\n\n\n\nFigure 3. Confusion matrix for the prediction of the students’ overall course success\n\n\nIn our case, the confusion matrix has the structure as shown on Figure 7.3. In rows, it has the counts of the actual number of students in the high and low achievement groups, whereas the columns give the predicted number of high and low achievers. We consider low course achievement as the positive class, since we are primarily interested in spotting those students who might benefit from a pedagogical intervention (to prevent a poor course outcome). Hence, TP (True Positive) is the count of students who had low course achievement and were predicted by the model as such. TN (True Negative) is the count of those who were high achieving in the course and the model predicted they would be high achievers. FP (False Positive) is the count of those who were high achievers in the course, but the model falsely predicted that they would have low achievement. Finally, FN (False Negative) is the count of students who were predicted to have high achievement in the course, but actually ended up in the low achievement group. These four count-based values forming the confusion matrix serve as the input for computing the aforementioned standard evaluation measures (Accuracy, Precision, Recall, and F1) based on the formuli given in the code snippet above.\nAfter the predictive models for weeks 1 to 5 are built and evaluated, we combine and compare their performance measures:\n\neval_df <- bind_rows(eval_measures)\neval_df |>\n  mutate(week = 1:5) |>\n  mutate(across(Accuracy:F1, \\(x) round(x, digits = 4))) |>\n  select(week, Accuracy:F1) \n\n\n\n  \n\n\n\n\n\n\nThe results above shows the resulting comparison of the built models. According to all measures, models 2 and 3, that is, models with the data from the first two and first three weeks of the course, are the best. In other words, the students' interactions with the course activities in the first two-three weeks are the most predictive of their overall course success. In particular, the accuracy of these models is 84%, meaning that for 84 out of 100 students, the models will correctly predict if the student would be a high or low achiever in this course. These models have precision of 75%, meaning that out of all the students for whom the models predict will be low achievers in the course, 75% will actually have low course achievement. In other words, the models will underestimate students’ performance in 25% of predictions they make, by wrongly predicting that students would have low course achievement. The two best models have perfect recall (100%), meaning that the models would identify all the students who will actually have low course performance. These models outperform the other three models also in terms of the F1 measure, which was expected considering that this measure combines precision and recall giving them equal relevance. Interestingly, the studies exploring predictive models on weekly basis have found similar high predictive power for models developed around the second week of the course [54]\nRF allows for estimating the relevance of features used for model building. In a classification task, RF estimates feature relevance as the total decrease in the impurity (measured by the Gini index) of leaf nodes from splitting on a particular feature, averaged over all the trees that a RF model builds [46]. We will use this RF’s functionality to compute and plot the importance of features in the best model. The function that does the computation and plotting is given below.\n\ncompute_and_plot_variable_importance <- function(rf_model) {\n  importance(rf_model, type = 2) |> \n    as.data.frame() |> \n    (function(x) mutate(x, variable = rownames(x)))() -> var_imp_df\n  \n  row.names(var_imp_df) <- 1:nrow(var_imp_df)\n  colnames(var_imp_df) <- c(\"importance\",\"variable\")\n  \n  ggplot(var_imp_df, \n         aes(x = reorder(variable, importance), y = importance)) +\n    geom_col(width = 0.35) +\n    labs(x = \"\", y = \"\", title = \"Feature importance\") +\n    coord_flip() +\n    theme_minimal()\n}\n\nThe plot produced by this function for one of the best models (Model 2) is given in Figure 20.4.\n\ncompute_and_plot_variable_importance(models[[2]])\n\n\n\n\nFigure 4. The importance of features in the best course outcome prediction model, as estimated by the RF algorithm\n\n\n\n\nAs Figure 20.4 shows, features denoting the overall level of activity (avg_session_len, session_cnt) are those with the highest predictive power. They are followed by entropy-based features, that is, features reflective of the regularity of study. These findings are in line with the LA literature (e.g., [14, 36, 37]. It should be also noted that the feature reflective of the level of engagement in the group work (Group_work_cnt) is among the top 5 predictors, which can be explained by the prominent role of group work in the course design.\n\n\n3.7 Predicting success score\nTo predict the students’ final grades, we will first try to build linear regression models, since linear regression is one of the most often used regression methods in LA [43]. To that end, we will first load a few additional R packages:\n\nlibrary(performance)\nlibrary(corrplot)\n\nConsidering that a linear regression model can be considered valid only if it satisfies a set of assumptions that linear regression, as a statistical method, is based upon (linearity, homogeneity of variance, normally distributed residuals, and absence of multicollinearity and influential points), we will first examine if our data satisfies these assumptions. In particular, we will compute the features based on the events data from the 1st week of the course, build a linear regression model using the computed features, and examine if the resulting model satisfies the assumptions. Note that we limit our initial exploration to the logged events data over the 1st week of the course since we aim to employ a regression method that can be applied to any number of course weeks; so, if the data from the first course week allow for building a valid linear regression model, we can explore the same method further; otherwise, we need to choose a more robust regression method, that is, method that is not so susceptible to imperfections in the input data.\nHaving created the dataset for final grade prediction based on the week 1 events data, we will split it into training and test sets (as done for the prediction of the course outcome, Section 7.3.6), and examine correlations among the features. The latter step is due to the fact that one of the assumptions of linear regression is the absence of high correlation among predictor variables. In the code below, we use the corrplot function to visualise the computed correlation values (Figure 20.5), so that highly correlated variables can be easily observed.\n\nds <- create_dataset_for_grade_prediction(events_with_sessions, 1, results)\n  \nset.seed(2023)\ntrain_indices <- createDataPartition(ds$Final_grade, p = 0.8, list = FALSE)\ntrain_ds <- ds[train_indices,] |> select(-user)\ntest_ds <- ds[-train_indices,] |> select(-user)\n\n# examine correlations among the variables: for a linear regression model, \n# they must not be highly mutually correlated\ncorrplot(train_ds |> select(-Final_grade) |> cor(), \n         method = \"number\", type = \"lower\",\n         diag = FALSE, order = 'hclust',\n         tl.cex = 0.75, tl.col = 'black', tl.srt = 30, number.cex = 0.65)\n\n\n\n\nFigure 5. Correlations among variables in the feature set\n\n\n\n\nFigure 20.5 indicates that there are a couple of features that are highly mutually correlated. These will be removed before proceeding with the model building. While there is no universal agreement on the correlation threshold above which features should be considered overly correlated, correlation coefficients of 0.75 and -0.75 are often used as the cut-off values [53].\n\ntrain_ds |> select(-c(session_cnt, Course_view_cnt, \n                      active_days_cnt, entropy_daily_cnts)) -> train_ds_sub\n\nWe can now build a model and check if it satisfies the assumptions of linear regression:\n\nlr <- lm(Final_grade ~ ., data = train_ds_sub)\ncheck_model(lr)\n\n\n\n\nFigure 6. The output of the check_model function enables visual verification of the assumptions that linear regression is based upon\n\n\n\n\nThe check_model function from the performance R package [48] allows for seamless, visual verification of whether the assumptions are met. The output of this function when applied to our linear regression model (lr) is shown on Figure 20.6. As the figure shows, two important assumptions of linear models are not met, namely linearity and homoscedasticity (i.e. homogeneity of variance). Therefore, linear regression cannot be used with the given feature set. Instead, we have to use a regression method that does not impose such requirements on the data distribution. Since Random forest is such a method and it has already proven successful with our dataset on the classification task (Section 7.3.6), we will use it to build regression models that predict students’ final grades.\nBefore moving to regression with Random forest, it is worth noting that, in addition to checking all model assumptions at once, using the check_model function, one can also check each assumption individually using appropriate functions from the performance R package. For example, from Figure 20.6, one can not clearly see the X-axis of the collinearity plot and might want to explore this assumption more closely. That can be easily done using the check_collinearity function:\n\ncheck_collinearity(lr)\n\n\n\n  \n\n\n\nFrom the function’s output, we can clearly see the VIF (Variance Inflation Factor) values for all the features and a confirmation that the assumption of the absence of multicollinearity is satisfied. The documentation of the performance 5 package provides the whole list of functions for different ways of checking regression models.\nTo build and compare regression models in each course week, we will follow a similar procedure to the one applied when building classification models (Section 7.3.6); the code that implements it is given below. The differences are in the way that the dataset for grade prediction is built (create_dataset_for_grade_prediction), the way that regression models are built (build_RF_regression_model) and evaluated (get_regression_evaluation_measures), and these will be explained in more detail below.\n\nregression_models <- list()\nregression_eval <- list()\nfor(k in 1:5) {\n  print(str_glue(\"Starting computations for week {k} as the current week\"))\n  \n  ds <- create_dataset_for_grade_prediction(events_with_sessions, k, results)\n  \n  set.seed(2023)\n  train_indices <- createDataPartition(ds$Final_grade, p = 0.8, list = FALSE)\n  train_ds <- ds[train_indices,] |> select(-user)\n  test_ds <- ds[-train_indices,] |> select(-user)\n\n  rf <- build_RF_regression_model(train_ds)\n  eval_rf <- get_regression_evaluation_measures(rf, train_ds, test_ds)\n  \n  regression_models[[k]] <- rf\n  regression_eval[[k]] <- eval_rf\n}\n\nTo create a dataset for final grade prediction in week k, we first compute all features based on the logged events data (events_data) up to the week k, and then add the final grade variable (Final_grade) from the dataset with course results (grades):\n\ncreate_dataset_for_grade_prediction <- function(events_data, current_week, grades) {\n  features <- create_event_based_features(events_data, current_week)\n  grades |> \n    select(user, Final_grade) |>\n    inner_join(features, by = \"user\")\n} \n\nAs can be observed in the code snippet below, building a RF regression model is very similar to building a RF classification model. The main difference is in the evaluation measure that is used for selecting the optimal mtry value in the cross-validation process - here, we are using RMSE (Root Mean Squared Error), which is a standard evaluation measure for regression models [53]. As its name suggests, RMSE is the square root of the average squared differences between the actual and predicted values of the outcome variable on the test set.\n\nbuild_RF_regression_model <- function(dataset) {\n  \n  n_features <- ncol(dataset)-1\n  default_mtry <- round(sqrt(n_features))\n  grid <- expand.grid(.mtry = (default_mtry-1):(default_mtry+1))\n  \n  ctrl <- trainControl(method = \"CV\", \n                       number = 10)\n  \n  rf <- train(x = dataset |> select(-Final_grade),\n              y = dataset$Final_grade,\n              method = \"rf\",\n              metric = \"RMSE\",\n              tuneGrid = grid,\n              trControl = ctrl)\n  \n  rf$finalModel\n}\n\nFinally, to evaluate each model on the test data, we compute three standard evaluation metrics for regression models, namely RMSE, MAE (Mean Absolute Error), and R2. MAE is the average value of the absolute differences between the actual and predicted values of the outcome variable (final grade) on the test set. Finally, R2 (R-squared) is a measure of variability in the outcome variable that is explained by the given regression model. The computation of the three evaluation measures is shown in the code below.\n\nget_regression_evaluation_measures <- function(model, train_ds, test_data) {\n  \n  predicted_vals <- predict(model, \n                            test_data |> select(-Final_grade))\n  actual_vals <- test_data$Final_grade\n  \n  # R2 = 1 - RSS/TSS\n  # RSS - Residual Sum of Squares\n  RSS <- sum((predicted_vals - actual_vals)^2)\n  # TSS - Total Sum of Squares\n  TSS <- sum((median(train_ds$Final_grade) - actual_vals)^2)\n  R2 <- 1 - RSS/TSS\n  \n  # RMSE = sqrt(RSS/N)\n  RMSE <- sqrt(RSS/nrow(test_ds))\n  \n  # MAE = avg(abs(predicted - actual))\n  MAE <- mean(abs(predicted_vals - actual_vals))\n  \n  c(R2 = R2, RMSE = RMSE, MAE = MAE)\n}\n\nAfter the regression models for weeks 1 to 5 are built and evaluated, we combine and compare their performance measures, with the results reported below.\n\nregression_eval_df <- bind_rows(regression_eval)\nregression_eval_df |>\n  mutate(WEEK = 1:5) |>\n  mutate(across(R2:MAE, \\(x) round(x, digits = 4))) |>\n  select(WEEK, R2, RMSE, MAE)\n\n\n\n  \n\n\n\n\n\n\nAs shown above, in this case, we do not have a clear situation as it was with the classification task, since the three evaluation measures point to different models as potentially the best ones. In particular, according to R2, the best model would be model 5 (i.e, the model based on the data from the first 5 weeks), whereas the other two measures point to the 2nd or 3rd model as the best. Considering that i) RMSE and MAE measures are considered more important than R2 when evaluating the predictive performance of regression models [13] and ii) RMSE and MAE values for models 2 and 3 are very close, while the 2nd model is better in terms of R2, we will conclude that the 2nd model, that is, the model based on the logged event data from the first two weeks of the course is the best model. This model explains 84.23% of variability in the outcome variable (final grade), and predicts it with an average absolute error of 0.4531, which can be considered a small value with respect to the value range of the final grade [0-10].\nTo estimate the importance of features in the best regression model, we will again leverage the RF’s ability. We will use the same function as before (compute_and_plot_variable_importance) to estimate and plot feature importance. The only difference will be that the importance function (from the randomForest package) will internally use residual sum of squares as the measure of node impurity when estimating the features importance. Figure 7.7 shows that, as in the case of predicting the overall course success (Figure 20.4), regularity of study features (avg_aday_dist, entropy_daily_cnts, session_len_entropy) are among the most important ones. In addition, the number of learning sessions (session_cnt), as an indicator of overall activity in the course, is also among the top predictors.\n\ncompute_and_plot_variable_importance(regression_models[[2]])\n\n\n\n\nFigure 7. The importance of features in the best final grade prediction model, as estimated by the RF algorithm"
  },
  {
    "objectID": "chapters/ch07-prediction/ch7-pred.html#concluding-remarks",
    "href": "chapters/ch07-prediction/ch7-pred.html#concluding-remarks",
    "title": "7  Predictive Modelling in Learning Analytics: A Machine Learning Approach in R",
    "section": "4 Concluding remarks",
    "text": "4 Concluding remarks\nThe results of predictive modelling presented in the previous section show that, in the examined postgraduate course on LA, we can make fairly accurate predictions of the students’ course outcomes already in the second week of the course. In fact, both classification and regression models, that is, prediction of the students’ overall course success and final grades, proved to be the most accurate when based on the logged learning events data from the first two or three course weeks. That students’ learning behaviour in the first part of the course is highly predictive of their course performance, which is in line with related research on predictive modelling (e.g., [54–56]). It should be also noted that the high performance of the presented predictive models can be partially explained by the well chosen feature set and the used algorithm (Random forest) that generally performs well on prediction tasks. However, it may also be due to the relatively large dataset. As noted in Section 7.3.1, we used a synthetic anonymized version of the original dataset that is three times larger than the original dataset.\nConsidering the features that proved particularly relevant for predicting the students’ course performance, we note that in both kinds of predictive tasks —-course success and final grade prediction— features reflective of regularity of study stand out. In addition, features denoting the overall level of engagement with online learning activities and resources also have high predictive power. It is also worth noting that the highly predictive features are session level features, suggesting that learning session is the right level of granularity (better than actions or active days) for predictive modelling in the given course. In fact, this finding is in line with earlier studies that examined predictive power of a variety of features derived from learning traces [13, 14, 36] . Note that due to the purpose of this chapter to serve as introductory reading to predictive modelling in LA, we based the feature set on relatively simple features and used only one data source for feature creation. For more advanced and diverse feature creation options, interested readers are referred to, for example [57–59].\nThe algorithm used for building predictive models, namely Random forest, offers the advantage of flexibility in terms of the kinds of data it can work with (unlike, for example, linear regression which is based on several assumptions about data distribution) as well as fairly good prediction results it tends to produce. On the other hand, the algorithm is not as transparent as simpler algorithms are (e.g., linear regression or decision trees) and thus its use might raise issues of teachers’ trust and willingness to rely on the models’ output. On the positive side, the algorithm offers an estimate of feature importance thus shedding some light on the underlying “reasoning” process that led to its output (i.e., predictions).\nTo sum up, predictive modelling, as applied in LA, can bring about important benefits in the form of early in the course detection of students who might be struggling with the course and pointing out indicators of learning behaviour that are associated with poor course outcomes. With such insights available, teachers can make better informed decisions as to the students who need support and the kind of support they might benefit from. However, predictive modelling is also associated with challenges, especially practical challenges related to the development and use of such models, including availability and access to the data, interpretation of models and their results, and the associated issue of trust in the models’ output."
  },
  {
    "objectID": "chapters/ch07-prediction/ch7-pred.html#suggested-readings",
    "href": "chapters/ch07-prediction/ch7-pred.html#suggested-readings",
    "title": "7  Predictive Modelling in Learning Analytics: A Machine Learning Approach in R",
    "section": "5 Suggested readings",
    "text": "5 Suggested readings\n\nMax Kuhn & Julia Silge (2022). Tidy Modeling with R: A Framework for Modeling in the Tidyverse. O’Reilly. https://www.tmwr.org/\nBradley Boehmke & Brandon Greenwell (2020). Hands-On Machine Learning with R. Taylor & Francis. https://bradleyboehmke.github.io/HOML/\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning: With Applications in R. 2nd Edition. Springer US. https://doi.org/10.1007/978-1-0716-1418-1"
  },
  {
    "objectID": "chapters/ch08-clustering/ch8-clus.html",
    "href": "chapters/ch08-clustering/ch8-clus.html",
    "title": "8  Dissimilarity-based Cluster Analysis of Educational Data: A Comparative Tutorial using R",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch08-clustering/ch8-clus.html#sec-intro",
    "href": "chapters/ch08-clustering/ch8-clus.html#sec-intro",
    "title": "8  Dissimilarity-based Cluster Analysis of Educational Data: A Comparative Tutorial using R",
    "section": "1 Introduction",
    "text": "1 Introduction\nCluster analysis is a term used to describe a broad range of techniques which have the goal of uncovering groups of observations in a data set. The typical aim of cluster analysis is to categorise observations into groups in such a way that observations within the same group are in some sense more similar to each other, while being relatively dissimilar to those in other groups [1]. In other words, clustering methods uncover group structure in heterogeneous populations by identifying more homogeneous groupings of observations which may represent distinct, meaningful subpopulations. Using machine learning terminology, cluster analysis corresponds to unsupervised learning, whereby groups are identified by relying solely on the intrinsic characteristics (typically dissimilarities), without guidance from unavailable ground truth group labels. Indeed, foreknowledge of the fixed number of groups in a data set is characteristic of supervised learning and the distinct field of classification analysis.\nIt is important to note that there is no universally applicable definition of what constitutes a cluster [2, 3]. Indeed, in the absence of external information in the form of existing “true” group labels, different clustering methods can reveal different things about the same data. There are many different ways to cluster the same data set, and different methods may yield solutions with different assignments, or even differ in the number of groups they identify. Consequently, we present several cluster analysis algorithms in this chapter; namely, \\(K\\)-Means [4] in Section 8.3.1, a generalisation thereof, \\(K\\)-Medoids [5], in Section 8.3.1.2.3, and agglomerative hierarchical clustering [6] in Section 8.3.2. We apply each method in a comparative study in our tutorial using R [7], with applications to a data set about the participants from discussion forum of a massive open online course (MOOC) for teachers.\nThe clustering methods we review in this chapter are designed to find mutually exclusive, non-overlapping groups in a data set, i.e., they recover a hard partition whereby each observation belongs to one group only. This is in contrast to soft clustering approaches under which each observation is assigned a probability of belonging to each group. One such example of soft clustering is the model-based clustering paradigm, which is discussed in the later Chapter 9 [8]. While model-based clustering methods, as the name suggests, are underpinned by the assumption of generative probabilistic models [9], the more traditional dissimilarity-based methods on which this chapter is focused are purely algorithmic in nature and rely on heuristic criteria regarding the pairwise dissimilarities between objects.\nHeuristic clustering algorithms can be further divided into two categories; partitional clustering (e.g., \\(K\\)-Means and \\(K\\)-Medoids) and hierarchical (of which we focus on the agglomerative variant). Broadly speaking, partitional clustering methods start with an initial grouping of observations and iteratively update the clustering until the “best” clustering is found, according to some notion of what defines the “best” clustering. Hierarchical clustering methods, on the other hand, is more sequential in nature; a tree-like structure of nested clusterings is built up via successive mergings of similar observations, according to a defined similarity metric. In our theoretical expositions and our applications in the R tutorial, we provide some guidelines both on how to choose certain method-specific settings to yield the best performance and how to determine the optimal clustering among a set of competing methods.\nThe remainder of this chapter proceeds as follows. In Section 8.2, we review relevant literature in which dissimilarity-based clustering methods have been applied in the realm of educational research. In Section 8.3, we describe the theoretical underpinnings of each method in turn and discuss relevant practical guidelines which should be followed to secure satisfactory performance from each method throughout. Within Section 8.4, we introduce the data set of our case study in Section 8.4.1 and give an overview of some required pre-processing steps in Section 12.4.3, and then present a tutorial using R for each clustering method presented in this chapter in Section 8.4.2, with a specific focus on identifying the optimal clustering solution in Section 8.4.2.4. Finally, we conclude with a discussion and some recommendations for related further readings in Section 8.5, with a particular focus on some limitations of dissimilarity-based clustering which are addressed by other frameworks in the broader field of cluster analysis."
  },
  {
    "objectID": "chapters/ch08-clustering/ch8-clus.html#sec-review",
    "href": "chapters/ch08-clustering/ch8-clus.html#sec-review",
    "title": "8  Dissimilarity-based Cluster Analysis of Educational Data: A Comparative Tutorial using R",
    "section": "2 Clustering in education: review of the literature",
    "text": "2 Clustering in education: review of the literature\nIn education, clustering is among the oldest and most common analysis methods, predating the field of learning analytics and educational data mining by several decades. Such early adoption of clustering is due to the immense utility of cluster analysis in helping researchers to find patterns within data, which is a major pursuit of education research [10]. Interest was fueled by the increasing attention to heterogeneity and individual differences among students, their learning processes, and their approaches to learning [11, 12]. Finding such patterns or differences among students allows teachers and researchers to improve their understanding of the diversity of students and tailor their support to different needs [13]. Finding subgroups within cohorts of students is a hallmark of so-called “person-centered methods”, to which clustering belongs [8].\nA person-centered approach stands in contrast to the variable-centered methods which consider that most students belong to a coherent homogeneous group with little or negligible differences [14]. Variable-centered methods assume that there is a common average pattern that represents all students, that the studied phenomenon has a common causal mechanism, and that the phenomenon evolves in the same way and results in similar outcomes amongst all students. These assumptions are largely understood to be unrealistic, “demonstrably false, and invalidated by a substantial body of uncontested scientific evidence” [15]. In fact, several analytical problems necessitate clustering, e.g., where opposing patterns exist [16]. For instance, in examining attitudes towards an educational intervention using variable-centered methods, we get an average that is simply the sum of negative and positive attitudes. If the majority of students have a positive attitude towards the proposed intervention —combined with a minority against— the final result will imply that students favour such intervention. The conclusions of this approach are not only wrong but also dangerous as it risks generalising solutions to groups to whom it may cause harm.\nTherefore, clustering has become an essential method in all subfields of education (e.g., education psychology, education technology, and learning analytics) having operationalised almost all quantitative data types and been integrated with most of the existing methods [10, 17]. For instance, clustering became an essential step in sequence analysis to discover subsequences of data that can be understood as distinct approaches or strategies of students’ behavior [18]. Similar applications can be found in social network analysis data to identify collaborative roles, or in multimodal data analysis to identify moments of interest (e.g., synchrony). Similarly, clustering has been used with survey data to find attitudinal patterns or learning approaches to mention a few [17]. Furthermore, identifying patterns within students’ data is a prime educational interest in its own right and therefore, has been used extensively as a standalone analysis technique to identify subgroups of students who share similar interests, attitudes, behaviors, or background.\nClustering has been used in education psychology for decades to find patterns within self-reported questionnaire data. Early examples include the work of Beder and Valentine [19] who used responses of a motivational questionnaire to discover subgroups of students with different motivational attitudes. Similarly, Clément et al. [20] used the responses of a questionnaire that assessed anxiety and motivation towards learning English as a second language to find clusters which differentiated students according to their attitudes and motivation. Other examples include the work by Fernandez-Rio et al. [21] who used hierarchical clustering and \\(K\\)-Means to identify distinct student profiles according to their perception of the class climate. With the digitalisation of educational institutions, authors also sought to identify students profiles using admission data and learning records. For instance, Cahapin et al. [22] used several algorithms such as \\(K\\)-Means and agglomerative hierarchical clustering to identify patterns in students’ admission data.\nThe rise of online education opened many opportunities to investigate students’ behavior in learning management systems based on the trace log data that students leave behind them when working on online activities. An example is the work by Saqr et al. [23], who used \\(K\\)-Means to cluster in-service teachers’ approaches to learning in a MOOC according to their frequency of clicks on the available learning activities. Recently, research has placed a focus on the temporality of students’ activities, rather than the mere count. For this purpose, clustering has been integrated within sequence analysis to find subsequences which represent meaningful patterns of students behavior. For instance, Jovanovic et al. [24] used hierarchical clustering to identify distinct learning sequential patterns based on students’ online activities in a learning management system within a flipped classroom, and found a significant association between the use of certain learning sequences and learning outcomes. Using a similar approach, López-Pernas et al. [25] used hierarchical clustering to identify distinctive learning sequences in students’ use of the learning management system and an automated assessment tool for programming assignments. They also found an association between students’ activity patterns and their performance in the course final exam. Several other examples exist for using clustering to find patterns within sequence data [16, 26, 27].\nA growing application of clustering can be seen in the study of computer-supported collaborative learning. Saqr and López-Pernas [28] used cluster analysis to discover students with similar emergent roles based on their forum interaction patterns. Using the \\(K\\)-Means algorithm and students’ centrality measures in the collaboration network, they identified three roles: influencers, mediators, and isolates. Perera et al. [29] used \\(K\\)-Means to find distinct groups of similar teams and similar individual participating students according to their contributions in an online learning environment for software engineering education. They found that several clusters which shared some distinct contribution patterns were associated with more positive outcomes. Saqr and López-Pernas [30] analysed the temporal unfolding of students’ contributions to group discussions. They used hierarchical clustering to identify patterns of distinct students’ sequences of interactions that have a similar start and found a relationship between such patterns and student achievement.\nAn interesting application of clustering in education concerns the use of multimodal data. For instance, Vieira et al. [31] used \\(K\\)-Means clustering to find patterns in students’ presentation styles according to their voice, position, and posture data. Other innovative uses involve students’ use of educational games [32, 33], virtual reality [34], or artificial intelligence [35]. Though this chapter illustrates traditional dissimilarity-based clustering algorithms with applications using R to data on the centrality measures of the participants of a MOOC, along with related demographic characteristics, readers are also encouraged to read Chapter 9 [8] and Chapter 10 [18] in which further tutorials are presented and additional literature is reviewed, specifically in the contexts of clustering using the model-based clustering paradigm and clustering longitudinal sequence data, respectively. We now turn to an explication of the cluster analysis methodologies used in this chapter’s tutorial."
  },
  {
    "objectID": "chapters/ch08-clustering/ch8-clus.html#sec-methods",
    "href": "chapters/ch08-clustering/ch8-clus.html#sec-methods",
    "title": "8  Dissimilarity-based Cluster Analysis of Educational Data: A Comparative Tutorial using R",
    "section": "3 Clustering methodology",
    "text": "3 Clustering methodology\nIn this section, we describe some of the theory underpinning the clustering methods used in the later R tutorial in Section 8.4. We focus on some of the most widely-known heuristic dissimilarity-based clustering algorithms; namely, \\(K\\)-means, \\(K\\)-medoids, and agglomerative hierarchical clustering. In Section 8.3.1, we introduce \\(K\\)-Means clustering by describing the algorithm, outline the arguments to the relevant R function kmeans(), and discuss some of the main limitations and practical concerns researchers should be aware of in order to obtain the best performance when running \\(K\\)-Means. We also discuss the related \\(K\\)-Medoids algorithm and the associated function pam() in the cluster library [36] in R, and situate this method in the context of an extension to \\(K\\)-Means designed to overcome its reliance on squared Euclidean distances. In Section 8.3.2, we introduce agglomerative hierarchical clustering and the related R function hclust(), while outlining various choices available to practitioners and their implications. Though method-specific strategies of choosing the optimal number of clusters \\(K\\) are provided throughout Section 8.3.1 and Section 8.3.2, we offer a more detailed treatment of this issue in Section 8.3.3, particularly with regard to criteria that can guide the choice of clustering solution among multiple competing methodologies, and not only the choice of the number of clusters \\(K\\) for a given method.\n\n3.1 \\(K\\)-Means\n\\(K\\)-means is a widely-used clustering algorithm in learning analytics and indeed data analysis and machine learning more broadly. It is an unsupervised technique that seeks to divide a typically multivariate data set into some pre-specified number of clusters, based on the similarities between observations. More specifically, \\(K\\)-means aims to partition \\(n\\) objects \\(\\mathbf{x}_1,\\ldots,\\mathbf{x}_n\\), each having measurements on \\(j=1,\\ldots,d\\) strictly continuous covariates, into a set of \\(K\\) groups \\(\\mathcal{C}=\\{C_1,\\ldots,C_K\\}\\), where \\(C_k\\) is the set of \\(n_k\\) objects in cluster \\(k\\) and the number of groups \\(K \\le n\\) is pre-specified by the practitioner and remains fixed. \\(K\\)-means constructs these partitions in such a way that the squared Euclidean distance between the row vector for observations in a given cluster and the centroid (i.e., mean vector) of the given cluster are smaller than the distances to the centroids of the remaining clusters. In other words, \\(K\\)-means aims to learn both the cluster centroids and the cluster assignments by minimising the within-cluster sums-of-squares (i.e., variances). Equivalently, this amounts to maximising the between-cluster sums-of-squares, thereby capturing heterogeneity in a data set by partitioning the observations into homogeneous groups. What follows is a brief technical description of the \\(K\\)-Means algorithm in Section 8.3.1.1, after which we describe some limitations of \\(K\\)-Means and discuss practical concerns to be aware of in order to optimise the performance of the method in Section 8.3.1.2. In particular, we present the widely-used \\(K\\)-Medoids extension in Section 8.3.1.2.3.\n\n3.1.1 \\(K\\)-Means algorithm\nThe origins of \\(K\\)-Means are not so straightforward to summarise. The name was initially used by James MacQueen in 1967 [4]. However, the standard algorithm was first proposed by Stuart Lloyd in a Bell Labs technical report in 1957, which was later published as a journal article in 1982 [37]. In order to understand the ideas involved, we must first define some relevant notation. Let \\(\\mu_k^{(j)}\\) denote the centroid value for the \\(j\\)-th variable in cluster \\(C_k\\) by \\[\\mu_k^{(j)} = \\frac{1}{n_k}\\sum_{\\mathbf{x}_i \\in C_k} x_{ij}\\] and the complete centroid vector for cluster \\(C_k\\) by \\[\\boldsymbol{\\mu}_k = \\left(\\mu_k^{(1)}, \\ldots, \\mu_k^{(p)}\\right)^\\top.\\] These centroids therefore correspond to the arithmetic mean vector of the observations in cluster \\(C_k\\). Finding both these centroids \\(\\boldsymbol{\\mu}_1,\\ldots,\\boldsymbol{\\mu}_K\\) and the clustering partition \\(\\mathcal{C}\\) is computationally challenging and typically proceeds by iteratively alternating between allocating observations to clusters and then updating the centroid vectors. Formally, the objective is to minimise the total within-cluster sum-of-squares \\[\n\\sum_{i=1}^n\\sum_{k=1}^K z_{ik} \\lVert \\mathbf{x}_i - \\boldsymbol{\\mu}_k\\rVert_2^2,\n\\tag{1}\\]\nwhere \\(\\lVert\\mathbf{x}_i - \\boldsymbol{\\mu}_k\\rVert_2^2=\\sum_{j=1}^p\\left(x_{ij}-\\mu_k^{(j)}\\right)^2\\) denotes the squared Euclidean distance to the centroid \\(\\boldsymbol{\\mu}_k\\) — such that \\(\\lVert\\cdot\\rVert_2\\) denotes the \\(\\ell^2\\) norm— and \\(\\mathbf{z}_i=(z_{i1},\\ldots,z_{iK})^\\top\\) is a latent variable such that \\(z_{ik}\\) denotes the cluster membership of observation \\(i\\); \\(z_{ik}=1\\) if observation \\(i\\) belongs to cluster \\(C_k\\) and \\(z_{ik}=0\\) otherwise. This latent variable construction implies \\(\\sum_{i=1}^n z_{ik}=n_k\\) and \\(\\sum_{k=1}^Kz_{ik}=1\\), such that each observation belongs wholly to one cluster only. As the total variation in the data, which remains fixed, can be written as a sum of the total within-cluster sum-of-squares (TWCSS) from Equation 8.1 and the between-cluster sum-of-squares as follows\n\\[\\sum_{i=1}^n \\left(\\mathbf{x}_i - \\bar{\\mathbf{x}}\\right)^2=\\sum_{i=1}^n\\sum_{k=1}^K z_{ik} \\lVert \\mathbf{x}_i - \\boldsymbol{\\mu}_k\\rVert_2^2 + \\sum_{k=1}^Kn_k\\lVert\\boldsymbol{\\mu}_k - \\bar{\\mathbf{x}}\\rVert_2^2,\\]\nwhere \\(\\bar{\\mathbf{x}}\\) denotes the overall sample mean vector, minimising the TWCSS endeavours to ensure that observations in the same cluster are maximally similar to observations in the same cluster and maximally dissimilar to those in other clusters.\nUsing the notation just introduced, a generic \\(K\\)-means algorithm would proceed as follows:\n\nInitialise: Select the number of desired clusters \\(K\\) and define \\(K\\) initial centroid vectors \\(\\boldsymbol{\\mu}_1,\\ldots,\\boldsymbol{\\mu}_K\\).\nAllocate: Find the optimal \\(z_{ik}\\) values that minimise the objective, holding the \\(\\mu_k\\) values fixed.\n\nCalculate the squared Euclidean distance between each observation and each centroid vector and allocate each object to the cluster corresponding to the initial centroid to which it is closest in terms of squared Euclidean distance. Looking at the objective function in Equation 8.1 closely and examining the contribution of observation \\(i\\), we need to choose the value of \\(\\mathbf{z}_i\\) which minimises the expression \\(\\sum_{k=1}^K z_{ik} \\lVert \\mathbf{x}_i - \\boldsymbol{\\mu}_k\\rVert_2^2\\). This is achieved by setting \\(z_{ik}=1\\) for the value of \\(k\\) that has smallest \\(\\lVert \\mathbf{x}_i - \\boldsymbol{\\mu}_k\\rVert_2^2\\) and setting \\(z_{ik^\\prime}=0\\:\\forall\\:k^\\prime\\ne k\\) everywhere else.\n\nUpdate: Find the optimal \\(\\boldsymbol{\\mu}_k\\) values that minimise the objective, holding the \\(z_{ik}\\) values fixed.\n\nIf we re-write the objective function in Equation 8.1 as \\[\\sum_{i=1}^n\\sum_{k=1}^K\\sum_{j=1}^p z_{ik} \\left(x_{ij}-\\mu_k^{(j)}\\right)^2,\\] we can use the fact that \\[\\frac{\\partial}{\\partial \\mu_k^{(j)}}\\left(x_{ij}-\\mu_k^{(j)}\\right)^2=-2\\left(x_{ij}-\\mu_k^{(j)}\\right)\\] to obtain \\[\\frac{\\partial}{\\partial \\mu_k^{(j)}}\\sum_{i=1}^n\\sum_{k=1}^K\\sum_{j=1}^p z_{ik} \\left(x_{ij}-\\mu_k^{(j)}\\right)^2=-2\\sum_{i=1}^nz_{ik}\\left(x_{ij}-\\mu_k^{(j)}\\right).\\] Solving this expression for \\(\\mu_k^{(j)}\\) yields \\[\\mu_k^{(j)}=\\frac{\\sum_{i=1}^nz_{ik}x_{ij}}{\\sum_{i=1}^nz_{ik}}=\\frac{1}{n_k}\\sum_{\\mathbf{x}_i \\in C_k} x_{ij}.\\]\n\nIterate: One full iteration of the algorithm consists of an allocation step (Step 2) and an update step (Step 3). Steps 2 and 3 are repeated until no objects can be moved between clusters, at which point the algorithm has converged to at least a local minimum of Equation 8.1.\n\nUpon convergence, we obtain not only the estimated partition \\(\\mathcal{C}=\\{C_1,\\ldots,C_K\\}\\), indicating the cluster membership of each observation, but also the estimated cluster centroids \\(\\boldsymbol{\\mu}_1,\\ldots,\\boldsymbol{\\mu}_K\\) which act as cluster prototypes, efficiently summarising the characteristics of each cluster. The algorithm just described is just one standard variant of \\(K\\)-Means. there have been several algorithms proposed for the same objective which derive their names from the author who proposed them; namely MacQueen [4], Lloyd [37], Forgy [38], and Hartigan and Wong [39]. They differ in some subtle ways, particularly with regard to how initial centroids in Step 1 are chosen and whether Steps 3 and 4 are applied to all \\(n\\) observations simultaneously or whether allocations and centroids are updated for each observation one-by-one (i.e., some variants iterate over Step 2 and Step 3 in a loop over \\(i=1,\\ldots,n\\)). Without going into further details, we note that the default option for kmeans() in R uses the option algorithm=\"Hartigan-Wong\" and this is what we will henceforth adopt throughout.\n\n\n3.1.2 \\(K\\)-Means limitations and practical concerns\nThough \\(K\\)-Means is a useful tool in many application contexts due to its conceptual and computational simplicity —so ubiquitous, in fact, that the kmeans() function in R is available without loading any additional libraries— it suffers from numerous limitations and some care is required in order to obtain reasonable results. We now discuss some of the main limitations in turn, but note that each is addressed explicitly throughout the \\(K\\)-Means application portion of the R tutorial in Section 8.4.2.1. The concerns relate to choosing \\(K\\) (Section 8.3.1.2.1), choosing initial centroids \\(\\boldsymbol{\\mu}_1,\\ldots,\\boldsymbol{\\mu}_K\\) (Section 8.3.1.2.2), and relaxing the reliance on squared Euclidean distances with the more general \\(K\\)-Medoids method (Section 8.3.1.2.3).\n\n3.1.2.1 Fixed \\(K\\) and the elbow method\nThe first major drawback is that the number of clusters \\(K\\) must be pre-specified. This is a key input parameter: if \\(K\\) is too low, dissimilar observations will be wrongly grouped together; if \\(K\\) is too large, observations will be partitioned into many small, similar clusters which may not be meaningfully different. Choosing the optimal \\(K\\) necessitates running the algorithm at various fixed values of \\(K\\) and finding the single value of \\(K\\) which best balances interpretability, parsimony, and fit quality. Fit quality is measured by the TWCSS, i.e., the objective function in Equation 8.1. Increasing \\(K\\) indefinitely will cause the TWCSS to decrease indefinitely, but this is not what we want. Instead, we seek a \\(K\\) value beyond which the decrease in TWCSS is minimal, in order to yield a parsimonious solution with a reasonable number of clusters to interpret, without overfitting the data or merely subdividing the actual groups. Thus, a commonly used heuristic graphical method for determining the optimal \\(K\\) value is to plot a range of \\(K\\) values against the corresponding obtained TWCSS values and look for an “elbow” or kink in the resulting curve. Such a plot will guide the choice of \\(K\\) in the \\(K\\)-Means portion of R tutorial which follows in Section 8.4.2.1.\n\n\n3.1.2.2 Initialisation and \\(K\\)-Means\nAn especially pertinent limitation which must be highlighted is that \\(K\\)-Means is liable to converge to sub-optimal local minima, i.e., it is not guaranteed to converge to the global minimum of the objective function in Equation 8.1. Many practitioners have observed that the performance of \\(K\\)-Means is particularly sensitive to a good choice of initial cluster centroids in Step 1. Indeed, as different initial settings can lead to different clusterings of the same data, good starting values are vital to the success of the \\(K\\)-Means algorithm. Typically, \\(K\\) random vectors are used to define the initial \\(\\boldsymbol{\\mu}_1,\\ldots,\\boldsymbol{\\mu}_K\\) centroids. One means of mitigating (but not completely remedying) the problem is to run \\(K\\)-Means with a suitably large number of random starting values and choose the solution associated with the set of initial centroids which minimise the TWCSS criterion.\nIn order to contextualise this issue, it is prudent to first describe some of the main arguments to the kmeans() R function. The following list is a non-exhaustive list of the available arguments to kmeans():\n\nx: a numeric matrix of data or a data.frame with all numeric columns.\ncenters: either the number of clusters \\(K\\) or a set of \\(K\\) initial (distinct) cluster centroids.\nnstart: if centers is specified as a number, this represents the number of random sets of \\(K\\) initial centroids with which to run the algorithm.\niter.max: the maximum number of allocation/update cycles allowed per set of initial centroids.\n\nThe arguments nstart and iter.max have default values of 1 and 10, respectively. Thus, a user running kmeans() with centers specified as a number will, by default, only use one random set of initial centroids, to which the results are liable to be highly sensitive, and will have the algorithm terminate after just ten iterations, regardless of whether convergence was achieved. It would seem be an improvement, therefore, to increase the values of nstart and iter.max from these defaults. Fortunately, the function automatically returns the single optimal solution according to the random initialisation which yields the lowest TWCSS when nstart exceeds 1.\nThough this will generally lead to better results, this approach can be computationally onerous if the number of observations \\(n\\), number of features \\(d\\), or size of the range of fixed \\(K\\) values under consideration is large. An alternative strategy, which greatly reduces the computational burden and the sensitivity of the final solution to the initial choices of \\(\\boldsymbol{\\mu}_1,\\ldots,\\boldsymbol{\\mu}_k\\) is to choose a suitable set of informed starting values in a data-driven fashion. To this end, the so-called \\(K\\)-Means algorithm was proposed [40] in order to improve the performance of \\(K\\)-Means by replacing Step 1 with an iterative distance-weighting scheme to select the initial cluster centroids. Though there is still randomness inherent in \\(K\\)-Means, this initialisation technique ensures that the initial centroids are well spread out across the data space, which increases the likelihood of converging to the true global minimum. The \\(K\\)-Means algorithm works as follows:\n\nChoose an initial centroid uniformly at random from the rows of the data set.\nFor each observation not yet chosen as a centroid, compute \\(D^2(\\mathbf{x}_i)\\), which represents the squared Euclidean distance between \\(\\mathbf{x}_i\\) and the nearest centroid that has already been chosen.\nRandomly sample new observation as a new centroid vector with probability proportional to \\(D^2(\\mathbf{x}_i)\\).\nRepeat Steps B and C until \\(K\\) centroids have been chosen. If any of the chosen initial centroids are not distinct, add a small amount of random jitter to distinguish the non-unique centroids.\nProceed as per Steps 2–4 of the traditional \\(K\\)-Means algorithm (or one of its variants).\n\nAlthough these steps take extra time, \\(K\\)-Means itself tends to converge very quickly thereafter and thus \\(K\\)-Means actually reduces the computational burden. A manual implementation of the \\(K\\)-Means is provided by the function kmeans_pp() below. Its output can be used as the centers argument when running kmeans().\n\nkmeans_pp <- function(X, # data\n                      K  # number of centroids\n                      ) {\n  \n  # sample initial centroid from distinct rows of X\n  X       <- unique(as.matrix(X))\n  new_center_index <- sample(nrow(X), 1)\n  centers <- X[new_center_index,, drop=FALSE]\n  \n  # let x be all observations not yet chosen as a centroid\n  X <- X[-new_center_index,, drop=FALSE]\n  \n  if(K >= 2) {\n    # loop over remaining centroids\n    for(kk in 2:K) {\n      \n      # calculate distances from all observations to all already chosen centroids\n      distances <- apply(X, 1, function(x) min(sum((x - centers)^2)))\n      \n      # sample new centroid with probability proportional to squared Euclidean distance\n      probabilities <- distances/sum(distances)\n      new_center_index <- sample(nrow(X), 1, prob=probabilities)\n      \n      # record the new centroid and remove it for the next iteration\n      centers <- rbind(centers, X[new_center_index,, drop=FALSE])\n      X <- X[-new_center_index,, drop=FALSE]\n    }\n  }\n  \n  # add random jitter to distinguish non-unique centroids and return\n  centers[duplicated(centers)] <- jitter(centers[duplicated(centers)])\n  return(centers)\n}\n\nHowever, it should be noted that there is still inherent randomness in \\(K\\)-Means —note the use of sample() in lines 7 and 22— and the algorithm is liable to produce different initial centroids in different runs on the same data. In effect, \\(K\\)-Means does not remove the burden of random initialisation; it is merely a way to have more informed random initialisations. Thus, it would be prudent to run \\(K\\)-Means with \\(K\\)-Means initialisation and select the solution which minimises the TWCSS, to transfer the burden of requiring multiple runs of \\(K\\)-Means with random starting values to fewer runs of \\(K\\)-Means followed by \\(K\\)-Means with more informed starting values. We adopt this strategy in the later R tutorial in Section 8.4.2.1.\n\n\n3.1.2.3 \\(K\\)-Medoids and other distances\nThe \\(K\\)-Means objective function in Equation 8.1 explicitly relies on squared Euclidean distances and requires all features in the data set to be strictly continuous. An artefact of this distance measure is that it is generally recommended to standardise all features of have a mean of zero and unit variance prior to running \\(K\\)-Means. In general, standardisation is advisable if the values are of incomparable units (e.g., height in inches and weight in kilogram). More specifically for \\(K\\)-Means, it is desirable to ensure all features have comparable variances to avoid having variables with higher magnitudes and variances dominate the distance calculation and have an undue prominence on the clustering partition obtained. While we employ such normalisation to the data used in our R tutorial when applying some pre-processing steps in Section 12.4.3, we note that this is not sufficient to overcome all shortcomings of relying on squared Euclidean distances.\nFor these reasons and more, \\(K\\)-Medoids —otherwise known as partitioning around medoids (PAM)— was proposed as an extension to \\(K\\)-Means which allows using any alternative dissimilarity measure [5]. The \\(K\\)-Medoids objective function is given by \\[\\sum_{i=1}^n\\sum_{k=1}^K z_{ik} d\\left(\\mathbf{x}_i, \\boldsymbol{\\psi}_k\\right),\\] where \\(d\\left(\\mathbf{x}_i, \\boldsymbol{\\psi}_k\\right)\\) can be any distance measure rather than squared Euclidean and \\(\\boldsymbol{\\psi}_k\\) is used in place of the mean vector \\(\\boldsymbol{\\mu}_k\\). The PAM algorithm works in much the same fashion as \\(K\\)-Means, alternating between an allocation step which assigns each observation to the cluster with the closest \\(\\boldsymbol{\\psi}_k\\) (according to the specified distance measure) and an update step which minimises the within-cluster total distance (WCTD). Notably, when minimising with respect to \\(\\boldsymbol{\\psi}_k\\), the notion of a cluster centroid \\(\\boldsymbol{\\mu}_k\\) is redefined as a cluster medoid \\(\\boldsymbol{\\psi}_k\\), which is selected among the rows of the observed data set, i.e., the medoid is the observation \\(\\mathbf{x}_i\\) from which the distance to all other observations currently allocated to the same cluster, according to the specified distance measure, is minimised. Similar to \\(K\\)-Means, the medoids obtained at convergence again enable straightforward characterisation of a “typical” observation from each cluster and the elbow method from Section 8.3.1.2.1 can be adapted to guide the choice of \\(K\\) in \\(K\\)-Medoids by plotting a range of candidate \\(K\\) values against the within-cluster total distance.\nThis reformulation has three main advantages. Firstly, the distance \\(d\\left(\\mathbf{x}_i, \\boldsymbol{\\psi}_k\\right)\\) is not squared, which diminishes the influence of outliers. As \\(K\\)-Means relies on squared Euclidean distances, which inflates the distances of atypical observations, and defines centroids as means, it is not robust to outliers. Secondly, by defining the medoids as observed rows of the data, rather than finding the value of \\(\\boldsymbol{\\psi}_k\\) that minimises in general, which could potentially be difficult to estimate for complex data types or particularly sophisticated dissimilarity measures, the algorithm can be much more computationally efficient. It requires only a pre-calculated pairwise dissimilarity matrix as input. Finally, the flexibility afforded by being able to modify the dissimilarity measure enables data which are not strictly continuous to be clustered. In other words, \\(K\\)-Medoids is applicable in cases where the mean is undefined. As examples, one could use the Manhattan or general Minkowski distances as alternatives for clustering continuous data, the Hamming [41], Jaccard [42], or Sørensen-Dice [43, 44] distances for clustering binary data, or the Gower distance [45] for clustering mixed-type data with both continuous and categorical features. The closely related \\(K\\)-Modes algorithm [46] has also been proposed specifically for purely categorical data applications, as well as the \\(K\\)-Prototypes algorithm [47] for mixed-type variables applications; neither will considered further in this chapter). As their names suggest, they again redefine the notion of a centroid but otherwise proceed much like \\(K\\)-Means and \\(K\\)-Medoids.\nThe function pam() in the cluster library in R provides an implementation of \\(K\\)-Medoids, with options for implementing many recent additional speed improvements and improved initialisation strategies [48]. We will discuss these in the \\(K\\)-Medoids portion of the later R tutorial in Section 8.4.2.2. Most dissimilarity measures we will use are implement in the base-R function dist(), with the exception of the Gower distance which is implemented in the daisy() function in the cluster library.\n\n\n\n\n3.2 Agglomerative hierarchical clustering\nHierarchical clustering is another versatile and widely-used dissimilarity-based clustering paradigm. Though also dissimilarity-based, hierarchical clustering differs from partitional clustering methods like \\(K\\)-Means and \\(K\\)-Medoids in that it typically doesn’t avail of the notion of computing distances to a central prototype, be that a centroid mean vector or a medoid, but instead greedily builds a hierarchy of clusters based on dissimilarities between observations themselves and sets of observations. Consequently, a hierarchical clustering solution provides a set of partitions, from a single cluster to as many clusters as observations, rather than the single partition obtained by \\(K\\)-Means and \\(K\\)-Medoids. The results of a hierarchical clustering are usually presented in the form of a dendrogram visualisation, which illustrates the arrangement of the set of partitions visited and can help guide the decision of the optimal single clustering partition to extract. However, hierarchical clustering shares some of the advantages \\(K\\)-Medoids has over \\(K\\)-Means. Firstly, any valid measure of distance can be used, so it is not restricted to squared Euclidean distances and not restricted to clustering purely continuous data. Secondly, hierarchical clustering algorithms do not require the data set itself as input; all that is required is a matrix of pairwise distances.\nBroadly speaking, there are two categories of hierarchical clustering:\n\nAgglomerative: Starting from the bottom of the hierarchy, begin with each observation in a cluster of its own and successively merged pairs of clusters while moving up the hierarchy, until all observations are in one cluster. This approach is sometimes referred to as agglomerative nesting (AGNES; [6]).\nDivisive: Starting from the top of the hierarchy, with all observations in one cluster, recursively split clusters while moving down the hierarchy, until all observations are in a cluster of their own. This approach is sometimes referred to as divisive analysis (DIANA; [49]).\n\nHowever, divisive clustering algorithms such as DIANA are much more computationally onerous for even moderately large data sets. Thus, we focus here on the agglomerative variant of hierarchical clustering, AGNES, which is is implemented in both the agnes() function in the cluster library and the hclust() function in base R. We adopt the latter in the hierarchical clustering portion of the R tutorial in Section 8.4.2.3. That being said, even agglomerative hierarchical clustering has significant computation and memory burdens when \\(n\\) is large [50, 51].\nThere are three key decisions practitioners must make when employing agglomerative hierarchical clustering. The first of these, the distance measure, has already been discussed in the context of \\(K\\)-Medoids. We now discuss the other two in turn; namely, the so-called linkage criterion for quantifying the distances between merged clusters as the algorithm moves up the hierarchy, and the criterion used for cutting the resulting dendrogram to produce a single partition.\n\n3.2.1 Linkage criteria\nAgglomerative hierarchical clustering employs two different notions of dissimilarity. There is the distance measure, \\(d\\), such as Euclidean, Manhattan, or Gower distance, which is used to quantify the distance between pairs of single observations in the data set. Different choices of distance measure can lead to markedly different clustering results and it is thus common to run the hierarchical clustering algorithm with different choices of distance measure and compare the results. However, in the agglomerative setting, individual observations are successively merged into clusters. In order to decide which clusters should be combined, it is necessary to quantify the dissimilarity of sets of observations as a function of the pairwise distances of observations in the sets. This gives rise to the notion of a linkage criterion. At each step, the two clusters separated by the shortest distance are combined; the linkage criteria is precisely the definition of ‘shortest distance’ which differentiates different agglomerative approaches. Again, the choice of linkage criterion can have a substantial impact on the result of the clustering so multiple solutions with different combinations of distance measure and linkage criterion should be evaluated.\nThere are a number of commonly-used linkage criteria which we now describe. A non-exhaustive list of such linkages follows —only those which we use in the later R tutorial in Section 8.4— in which we let \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) denote two sets of observations, \\(\\lvert\\cdot\\rvert\\) denote the cardinality of a set, and \\(d(a, b)\\) denote the distance between observations in those corresponding sets according to the chosen distance measure. We note that ties for the maximum or minimum distances for complete linkage and single linkage, respectively, are broken at random.\n\nComplete linkage: Define the dissimilarity between two clusters as the distance between the two elements (one in each cluster) which are furthest away from each other according to the chosen distance measure \\(d\\): \\[\\max_{a \\in \\mathcal{A}, b \\in \\mathcal{B}} d(a, b).\\]\nSingle linkage: Define the dissimilarity between two clusters as the distance between the two elements (one in each cluster) which are closest to each other according to the chosen distance measure \\(d\\): \\[\\min_{a \\in \\mathcal{A}, b \\in \\mathcal{B}} d(a, b).\\]\nAverage linkage: Define the dissimilarity between two clusters as the average distance according to the chosen distance measure \\(d\\) between all pairs of elements (on in each cluster): \\[\\frac{1}{\\lvert\\mathcal{A}\\rvert \\times \\lvert\\mathcal{B}\\rvert}\\sum_{a \\in \\mathcal{A}}\\sum_{b \\in \\mathcal{B}}d_(a, b).\\]\nCentroid linkage: Define the dissimilarity between two clusters \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) as the distance, according to the chosen distance measure \\(d\\), between their corresponding centroid vectors \\(\\boldsymbol{\\mu}_{\\mathcal{A}}\\) and \\(\\boldsymbol{\\mu}_{\\mathcal{B}}\\): \\[d(\\boldsymbol{\\mu}_{\\mathcal{A}}, \\boldsymbol{\\mu}_{\\mathcal{B}}).\\]\nWard linkage [52]: Instead of measuring the dissimilarity between clusters directly, define the dissimilarity as the cost of merging two clusters as the increase in total within-cluster variance after merging. In other words, minimise the total within-cluster sum-of-squares by finding the pair of clusters at each step which leads to minimum increase in total within-cluster variance after merging, where \\(\\mathcal{A} \\cup \\mathcal{B}\\) denotes the cluster obtained after merging, with corresponding centroid \\(\\boldsymbol{\\mu}_{\\mathcal{A}\\cup\\mathcal{B}}\\): \\[\\frac{\\lvert\\mathcal{A}\\rvert \\times \\lvert \\mathcal{B}}{\\lvert \\mathcal{A} \\cup \\mathcal{B}\\rvert}\\lVert\\boldsymbol{\\mu}_{\\mathcal{A}} - \\boldsymbol{\\mu}_{\\mathcal{B}}\\rVert_2^2 = \\sum_{x\\in \\mathcal{A}\\cup\\mathcal{B}}\\lVert x - \\boldsymbol{\\mu}_{\\mathcal{A}\\cup\\mathcal{B}}\\rVert_2^2 - \\sum_{x \\in \\mathcal{A}}\\lVert x - \\boldsymbol{\\mu}_{\\mathcal{A}}\\rVert_2^2 - \\sum_{x \\in \\mathcal{B}}\\lVert x - \\boldsymbol{\\mu}_{\\mathcal{B}}\\rVert_2^2.\\]\n\nThe Ward and centroid linkage criteria differ from the other linkage criteria in that they are typically meant to be used only when the initial pairwise distances between observations are squared Euclidean distances. All of the above linkage criteria are implemented in the function hclust(), which is available in R without requiring any add-on libraries to be loaded and specifically performs agglomerative hierarchical clustering. Its main arguments are d, a pre-computed pairwise dissimilarity matrix (as can be created from the function dist()), and method, which specifies the linkage criterion (e.g., \"complete\", \"single\", \"average\", and \"centroid\"). Special care must be taken when employing Ward’s linkage criterion as two options are available: \"ward.D\", which assumes that the initial pairwise distance matrix already consists of squared Euclidean distances, and \"ward.D2\", which assumes the distances are merely Euclidean distances and performs the squaring internally [53].\n\n\n3.2.2 Cutting the dendrogram\nOne might notice that when calling hclust(), the number of clusters \\(K\\) is not specified in advance, as it is when calling kmeans() or pam(). Instead, hclust() returns an object which describes the hierarchy of the tree produced by the clustering process. A visualisation of such a tree is referred to as a dendrogram, which can be thought of as a representation of a set of candidate partitions. In a dendrogram representation of an agglomerative hierarchical clustering solution, each observation is initially in a singleton cluster on its own, along the x-axis, according to their similarities. Thereafter, each observation, and subsequently each set of observations, are merged along the y-axis in a nested fashion. The scale along the y-axis is proportional to the distance, according to the chosen linkage criterion, at which two clusters are combined. In the end, the groups formed towards the bottom of the graph are close together, whereas those at the top of the graph are far apart.\nObtaining a single hard partition of objects into disjoint clusters is obtained by cutting the dendrogram horizontally at the corresponding height. In other words, observations are allocated to clusters by cutting the tree at an appropriate height. Generally, the lower this height, the greater the number of clusters (theoretically, there can be as many clusters as there are observations, \\(n\\)), while the greater the height, the lower the number of clusters (theoretically, there can be as few as only one cluster, corresponding to no group structure in the data). Thus, an advantage of hierarchical clustering is that the user need not know \\(K\\) in advance; the user can manually select \\(K\\) after the fact by examining the constructed tree and fine-tuning the output to find clusters with a desired level of granularity. There is no universally applicable rule for determining the optimal height at which to cut the tree, but it is common to select a height in the region where there is the largest gap between merges, i.e., where there is a relatively wide range of distances over which the number of clusters in the resulting partition does not change. This is, of course, very much guided by the visualisation itself.\nIn R, one can visualise the dendrogram associated with a particular choice of distance measure and linkage criterion by calling plot() on the output from hclust(). Thereafter, the function cutree() can be used to obtain a single partition. This function takes the arguments tree, which is the result of a call to hclust(), h which is the height at which the tree should be cut, and k which more directly allows the desired number of clusters to be produced. Specifying k finds the corresponding height which yields k clusters and overrides the specification of h.\nHowever, it is often the case that certain combinations of dissimilarity measure and linkage criterion produce undesirable dendrograms. In particular, complete linkage is known to perform poorly in the presence of outliers, given its reliance on maximum distances, and single linkage is known to produce a “chaining” effect on the resulting dendrogram, whereby, due to its reliance on minimum distances, observations tend to continuously join increasingly larger, existing clusters rather than being merged with other observations to form new clusters. A negative consequence of this phenomenon is lack of cohesion: observations at opposite ends of the same cluster in a dendrogram could be quite dissimilar. These limitations can also be attributed to hierarchical clustering —regardless of the linkage employed— optimising a local criterion for each merge, unlike \\(K\\)-Means and \\(K\\)-Medoids which endeavour to optimise global objectives.\n\n\n\n3.3 Choosing the number of clusters\nDetermining the number of clusters in a data set is a fraught task. Throughout Section 8.3.1 and Section 8.3.2, method-specific strategies for guiding the choice of \\(K\\) were presented. However, they are not without their limitations. For \\(K\\)-Means and \\(K\\)-Medoids, the elbow method is somewhat subjective and unreliable. Often, the presence of an elbow is not so clear at a single value of \\(K\\). Likewise, for agglomerative hierarchical clustering, choosing a height at which to cut the dendrogram as the criterion for choosing \\(K\\) has also been criticised as an overly subjective method. Moreover, these strategies are only capable of identifying the best \\(K\\) value conditional on the chosen method and do not help to identify the overall best solution among multiple competing methods. Moreover, we are required to choose more than just the optimal \\(K\\) value when using \\(K\\)-Medoids (for which different solutions with different dissimilarity measures and different \\(K\\) values can be obtained) and agglomerative hierarchical clustering (for which different solutions can be obtained using different dissimilarity measures and linkage criteria).\nOvercoming these ambiguities and identifying a more general strategy for comparing the quality of clustering partitions is a difficult task for which many criteria have been proposed. Broadly speaking, cluster quality measures fall into two categories:\n\nComparing of the uncovered partition to a reference clustering (or known grouping labels).\nMeasuring of internal cluster consistency without reference to ground truth labels.\n\nThe first is typically conducted using the Rand index [54] or adjusted Rand index [55], which measure the agreement between two sets of partitions. However, as we will be exploring clustering in exploratory, unsupervised settings, using data for which there is no assumed “true” group structure, we will instead focus on a quality measure of the latter kind. As previously stated, a large number of such criteria have been proposed in the literature: several are summarised in Table 7 of Chapter 10 of this book [18], where they are used to guide the choice of \\(K\\) for agglomerative hierarchical clustering in the context of sequence analysis. Here, however, for the sake of brevity, we describe only one commonly used criterion which we later employ in the R tutorial in Section 8.4 —which is itself a dissimilarity-based measure and is thus universally applicable to all clustering algorithms we employ— even if in most applications it would be wise to inform the choice of \\(K\\) with several such quantitative criteria. Moreover, the practitioner’s own subject matter expertise and assessment of the interpretability of the obtained clusters should also be used to inform the choice of \\(K\\).\nThe quantitative criterion we employ is referred to as the average silhouette width (ASW) criterion [56], which is routinely used to assess the cohesion and separation of the clusters uncovered by dissimilarity-based methods. Cohesion refers to the tendency to group similar objects together and separation refers to the tendency to group dissimilar objects apart in non-overlapping clusters. As the name implies, the ASW is computed as the average of observation-specific silhouette widths. Under the assumption that \\(K>1\\), silhouette widths and the ASW criterion are calculated as follows:\n\nLet \\(a(i)\\) be the average dissimilarity from observation \\(i\\) to the other members of the same cluster to which observation \\(i\\) is assigned.\nCompute the average dissimilarity from observation \\(i\\) to the members of all \\(K-1\\) other clusters: let \\(b(i)\\) be the minimum such distance computed.\nThe silhouette for observation \\(i\\) is then defined to be \\[\\begin{align*}\n  s(i) &= \\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}}\\\\\n  &= \\begin{cases} 1 - \\frac{a(i)}{b(i)} & \\mbox{if}\\:a(i) < b(i)\\\\\n  0& \\mbox{if}\\:a(i)=b(i)\\\\\n  \\frac{b(i)}{a(i)} - 1 & \\mbox{if}\\:a(i) > b(i),\n  \\end{cases}\n  \\end{align*}\\] unless observation \\(i\\) is assigned to a cluster of size \\(1\\), in which case \\(s(i)=0\\). Notably, \\(a(i)\\) and \\(b(i)\\) need not be calculated using the same dissimilarity measure with which the data were clustered; it is common to adopt the Euclidean distance.\nDefine the ASW for a given partition \\(\\mathcal{C}\\) as: \\(\\mbox{ASW}\\left(\\mathcal{C}\\right)=\\frac{1}{n}\\sum_{i=1}^n s_i.\\)\n\nGiven that \\(-1 \\le s(i) \\le 1\\), the interpretation of \\(s(i)\\) is that a silhouette close to \\(1\\) indicates that the observation has been well-clustered, a silhouette close to \\(-1\\) indicates that the observation would be more appropriately assigned to another cluster, and a silhouette close to zero indicates that the observation lies on the boundary of two natural clusters. If most values of \\(s(i)\\) are high, then the clustering solution can be deemed appropriate. This occurs when \\(a(i) \\ll b(i)\\), meaning that observation \\(i\\) must be well-matched its own cluster and poorly-matched to all other clusters. Conversely, if most \\(s(i)\\) values are low or even negative, this provides evidence that \\(K\\) may be too low or too high.\nThe values of \\(s(i)\\) can be averaged over all observations assigned to the same cluster, as a measure of how tightly grouped the observations in the given cluster are. The ASW criterion itself is simply the mean silhouette width over all observations in the entire data set. Generally, clustering solutions with higher ASW are to be preferred, however it is also prudent to dismiss solutions with many negative silhouettes or particularly low cluster-specific mean silhouettes. A silhouette plot can help in this regard; such a plot depicts all values of \\(s(i)\\), grouped according to the corresponding cluster and in decreasing order within a cluster. In the R tutorial which follows, ASW values and silhouette plots will guide the choice of an optimal clustering solution in a comparison of multiple methods in Section 8.4.2.4."
  },
  {
    "objectID": "chapters/ch08-clustering/ch8-clus.html#sec-tutR",
    "href": "chapters/ch08-clustering/ch8-clus.html#sec-tutR",
    "title": "8  Dissimilarity-based Cluster Analysis of Educational Data: A Comparative Tutorial using R",
    "section": "4 Tutorial with R",
    "text": "4 Tutorial with R\nIn this section, we will learn how to perform clustering using the R programming language [7], using all methods described throughout Section 8.3. We start by loading the necessary libraries. We will use cluster [36] chiefly for functions related to \\(K\\)-Medoids and silhouettes. As per other chapters in this book, we use tidyverse [57] for data manipulation and rio [58] for downloading the data: see Section 12.4.3 for details the on the data pre-processing steps employed.\n\nlibrary(cluster)\nlibrary(rio)\nlibrary(tidyverse)\n\nWe note that hierarchical clustering and \\(K\\)-Means are implemented in base R and thus no dedicated libraries need to be loaded.\n\n4.1 The data set\nOur case study will be to identify different groups of participants that have a similar role in the discussion forum of a massive open online course (MOOC) for teachers. For that purpose, we will rely on the centrality measures of the participants which indicate their number of contributions (OutDegree), replies (InDegree), position in the network (Closeness_total), worth of their connections (Eigen), spread of their ideas (Diffusion_degree), and more. For more details about the data set, please refer to the data chapter of the book (Chapter 2; [59]). To learn more about centrality measures and how to calculate them, refer to the social network analysis chapter (Chapter 15; [60]). We will henceforth refer to these data as “the MOOC centralities data set”. We can download and preview the data with the following commands:\n\nURL <- \"https://github.com/lamethods/data/raw/main/6_snaMOOC/\"\ndf <- import(paste0(URL, \"Centralities.csv\"))\ndf\n\n\n\n  \n\n\n\nAdditionally, a number of categorical variables pertaining to demographic characteristics are available for the same participants. Again, please refer to the data chapter of the book (Chapter 2; [59]) for more details on these variables. With an appropriate distance measure, namely the Gower distance measure, we will incorporate some of these variables in our applications of \\(K\\)-Medoids and agglomerative hierarchical clustering (but not \\(K\\)-Means) in addition to the continuous variables contained in df, largely for the purpose of demonstrating clustering methods which are capable of clustering variables of mixed type. We can download and preview the auxiliary categorical data set with the commands below. Note that we select only the following variables:\n\nexperience (coded as a level of experience, 1–3),\ngender (female or male),\nregion (Midwest, Northeast, South, and West U.S.A., along with International),\n\nfor the sake of simplifying the demonstration of mixed-type variables clustering and reducing the computational burden. We also extract the UID column, a user ID which corresponds to the name column in df, which will be required for later merging these two data sets. We also ensure that all but this leading UID column is formatted as a factor.\n\ndemog <- import(paste0(URL, \"DLT1%20Nodes.csv\"))\ndemog <- demog |> \n  select(UID, experience, gender, region) |> \n  mutate_at(vars(-UID), as.factor)\ndemog\n\n\n\n  \n\n\n\n\n4.1.1 Pre-processing the data\nIn df, the first column (name) is the student identifier, and the remaining columns are each of the centrality measures calculated from students’ interactions in the MOOC forum. We will eventually discard the name column from future analyses; we retain it for the time being so that df and demog can be merged appropriately. The data also contain a small number of observations —only three rows of df— with missing values for the variable Closeness_total, as seen by\n\ndf |> is.na() |> which(arr.ind=TRUE)\n\n    row col\n441 441   4\n442 442   4\n443 443   4\n\n\nGiven that none of the clustering methods described in this chapter are capable of accommodating missing data, we remove these three observations for future analyses too. Furthermore, one of the rows in demog has NULL recorded for the gender variable. We remove all invalid rows from both df and demog. The function complete.cases() constructs a completely observed data set by extracting the rows which contain one or more missing values and we augment the index of fully observed rows with an index of non-NULL gender values. Finally, we drop the redundant NULL level from the factor variable gender.\n\nobs_ind <- complete.cases(df) & demog$gender != \"NULL\"\ndf$name[!obs_ind] # indices of observations with missing values\n\n[1] 439 441 442 443\n\ndf <- df |> filter(obs_ind)\ndemog <- demog |> filter(obs_ind) |> droplevels()\n\nBefore proceeding any further, it would be prudent to explore the complete data visually, which we do via the matrix of pairwise scatter plots, excluding the name column, in Figure 8.1:\n\npairs(df[,-1])\n\n\n\n\n\n\nFigure 1. Matrix of pairwise scatter plots for all variables in the MOOC centralities data set.\n\n\n\n\nFrom the plots in Figure 8.1, we can see that there are clearly two quite extreme outliers. Simple exploratory analyses (not shown) confirms that these are the final two rows of the complete data set. These observations are known to correspond to the two instructors who led the discussion. They have been marked using a red cross symbol in Figure 8.1. Though we have argued that \\(K\\)-Medoids is a more robust clustering method than \\(K\\)-Means, for example, we also remove these observations in order to avoid their detrimental effects on the \\(K\\)-Means output. The rows must be removed from both df and demog so that they can later be merged. With these observations included, \\(K\\)-Means for instance would likely add one additional cluster containing just these two observations, about whom we already know their role differs substantially from the other observations, as they are quite far from the bulk of the data in terms of squared Euclidean distance. That is not to say, however, that there will not still be outliers in df after removing these two cases.\n\nkeep_ind <- 1:(nrow(df) - 2)\n\ndf <- df |> slice(keep_ind)\ndemog <- demog |> slice(keep_ind)\n\nAs is good practice when using dissimilarity-based clustering algorithms, we pre-process the purely continuous data by normalising each variable to have a mean of 0 and a variance of 1, by constructing the scaled data frame sdf using the function scale(), again excluding the name column.\n\nsdf <- scale(df[,-1], center=TRUE, scale=TRUE)\n\nThe object sdf can be used for all clustering methods described herein. To also accommodate the categorical demographic variables, we use merge() to combine both the scaled continuous data and the categorical data. This requires some manipulation of the name column, with which the two data sets are merged, but we ultimately discard the superfluous name column, which we do not want to contribute to any pairwise distance matrices or clustering solutions, from both merged_df and sdf.\n\nmerged_df <- data.frame(name=df$name, sdf) |>\n  merge(demog, by=1) |> \n  select(-name)\n\nFinally, before proceeding to apply various clustering methods to these data, we present summaries of the counts of each level of the categorical variables experience, gender, and region. These variables imply groupings of size three, two, and five, respectively, and it will be interesting to see if this is borne out in any of the mixed-type clustering applications.\n\ntable(merged_df$experience)\n\n\n  1   2   3 \n118 150 171 \n\ntable(merged_df$gender)\n\n\nfemale   male \n   299    140 \n\ntable(merged_df$region)\n\n\nInternational       Midwest     Northeast         South          West \n           32            77           110           168            52 \n\n\n\n\n\n4.2 Clustering applications\nWe now show how each of the clustering methods described above can be implemented in R, using these data throughout and taking care to address all practical concerns previously raised. For each method —\\(K\\)-Means in Section 8.4.2.1, \\(K\\)-Medoids in Section 8.4.2.2, and agglomerative hierarchical clustering in Section 8.4.2.3— we show clustering results following the method-specific guidelines for choosing \\(K\\). However, we conclude by comparing results across different methods using the average silhouette width criterion to further guide the choice of \\(K\\) in Section 8.4.2.4. We refrain from providing an interpretation of the uncovered clusters until Section 8.4.2.5, after the optimal clustering solution is identified.\nBefore we proceed, we set the seed to ensure that results relying on random number generation are reproducible.\n\nset.seed(2024)\n\n\n4.2.1 \\(K\\)-Means application\nWe begin by showing a naive use of the kmeans() function, supplying only the scaled data sdf and the pre-specified number of clusters \\(K\\) via the centers argument. For now, we assume for no particular reason that there are \\(K=3\\) clusters, just to demonstrate the use of the kmeans() function and its arguments. A number of aspects of the results are printed when we examine the resulting km1 object.\n\nkm1 <- kmeans(sdf, centers=3)\nkm1\n\nK-means clustering with 3 clusters of sizes 48, 129, 262\n\nCluster means:\n    InDegree  OutDegree Closeness_total Betweenness       Eigen\n1  2.2941694  1.9432559       1.0923551   1.9697769  2.00747791\n2 -0.4088814 -0.4311073      -1.4086380  -0.3975783 -0.55451137\n3 -0.2189864 -0.1437536       0.4934399  -0.1651209 -0.09475944\n  Diffusion.degree   Coreness Cross_clique_connectivity\n1        1.9078646  2.1923592                 2.0066265\n2       -1.1038258 -0.5622732                -0.3094092\n3        0.1939543 -0.1248092                -0.2152835\n\nClustering vector:\n  [1] 1 2 2 3 1 1 1 1 3 1 1 3 1 1 1 2 1 3 1 2 2 1 2 1 3 1 1 2 1 1 2 3 3 1 1 1 2\n [38] 3 3 2 1 3 2 1 3 3 2 3 1 1 3 3 1 1 2 3 3 1 3 1 1 1 1 1 3 3 1 1 3 3 3 3 2 3\n [75] 3 3 3 3 2 2 3 3 3 2 3 2 3 1 2 3 3 1 2 3 3 3 2 1 3 1 3 3 3 3 3 2 3 2 3 3 2\n[112] 3 3 3 1 1 2 2 2 3 3 2 2 3 2 3 3 3 3 2 3 2 3 3 2 3 1 3 3 2 3 3 3 3 2 3 3 3\n[149] 2 3 2 3 2 3 3 2 3 3 2 3 3 3 3 2 3 3 3 2 3 3 3 3 3 2 2 3 3 3 3 2 3 2 3 3 3\n[186] 3 3 3 3 3 2 3 3 3 3 3 2 1 3 3 3 3 3 3 3 3 3 2 2 2 3 3 2 3 2 2 3 3 1 2 3 2\n[223] 1 2 2 3 2 2 2 2 2 3 2 1 3 3 2 2 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3\n[260] 2 3 2 2 2 3 2 3 2 2 2 3 2 2 2 2 3 2 3 3 3 3 2 2 3 2 2 2 2 2 2 2 3 2 2 3 2\n[297] 3 3 3 3 3 3 3 3 3 2 2 3 2 1 2 2 2 3 2 2 3 3 2 3 2 3 2 3 3 3 2 2 3 3 3 2 3\n[334] 2 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 3 2 2 2 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 2\n[371] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[408] 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 1 2 2 2 2 2 2 3\n\nWithin cluster sum of squares by cluster:\n[1] 858.90295  72.01655 414.30502\n (between_SS / total_SS =  61.6 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nAmong other things, this output shows the estimated centroid vectors \\(\\boldsymbol{\\mu}_1,\\ldots,\\boldsymbol{\\mu}_K\\) upon convergence of the algorithm ($centers), an indicator vector showing the assignment of each observation to one of the \\(K=3\\) clusters ($cluster), the size of each cluster in terms of the number of allocated observations ($size), the within-cluster sum-of-squares per cluster ($withinss), and the ratio of the between-cluster sum-of-squares ($betweenss) to the total sum of squares ($totss). Ideally, this ratio should be large, if the total within-cluster sum-of-squares is minimised. We can access the TWCSS quantity by typing\n\nkm1$tot.withinss\n\n[1] 1345.225\n\n\nHowever, these results were obtained using the default values of ten maximum iterations (iter.max=10, by default) and only one random set of initial centroid vectors (nstart=1, by default). To increase the likelihood of converging to the global minimum, it is prudent to increase iter.max and nstart, to avoid having the algorithm terminate prematurely and avoid converging to a local minimum, as discussed in Section 8.3.1.2.2. We use nstart=50, which is reasonably high but not so high as to incur too large a computational burden.\n\nkm2 <- kmeans(sdf, centers=3, nstart=50, iter.max=100)\nkm2$tot.withinss\n\n[1] 1339.226\n\n\nWe can see that the solution associated with the best random starting values achieves a lower TWCSS than the earlier naive attempt. Next, we see if an even lower value can be obtained using the \\(K\\)-Means initialisation strategy, by invoking the kmeans_pp() function from Section 8.3.1.2.2 with K=3 centers and supplying these centroid vectors directly to kmeans(), rather than indicating the number of random centers to use.\n\nkm3 <- kmeans(sdf, centers=kmeans_pp(sdf, K=3), iter.max=100)\nkm3$tot.withinss\n\n[1] 1343.734\n\n\nIn this case, using the \\(K\\)-Means initialisation strategy did not further reduce the TWCSS; in fact it is worse than the solution obtained using nstart=50. However, recall that \\(K\\)-Means is itself subject to randomness and should therefore also be run several times, though the number of \\(K\\)-Means runs need not be so high as 50. In the code below, we use replicate() to invoke both kmeans_pp() and kmeans() itself ten times in order to obtain a better solution.\n\nKMPP <- replicate(10, list(kmeans(sdf, iter.max=100,\n                                  centers=kmeans_pp(sdf, K=3))))\n\nAmong these ten solutions, five are identical and achieve the same minimum TWCSS value.\n\nTWCSS <- sapply(KMPP, \"[[\", \"tot.withinss\")\nTWCSS\n\n [1] 1345.225 1339.226 1339.226 1339.226 1340.457 1339.226 1345.225 1345.225\n [9] 1345.225 1339.226\n\n\nThereafter, we can extract a solution which minimises the tot.withinss as follows:\n\nkm4 <- KMPP[[which.min(TWCSS)]]\nkm4$tot.withinss\n\n[1] 1339.226\n\n\nFinally, this approach resulted in an identical solution to km2 being obtained — with just ten runs of \\(K\\)-Means and \\(K\\)-Means rather than nstart=50 runs of the \\(K\\)-Means algorithm alone— which is indeed superior to the solution obtained with just one uninformed random start in km1. We will thus henceforth adopt this initialisation strategy always.\nTo date, the \\(K\\)-Means algorithm has only been ran with the fixed number of clusters \\(K=3\\), which may be suboptimal. The following code iterates over a range of \\(K\\) values, storing both the kmeans() output itself and the TWCSS value for each \\(K\\). The range \\(K=1,\\ldots,10\\) notably includes \\(K=1\\), corresponding to no group structure in the data. The reason for storing the kmeans() output itself is to avoid having to run kmeans() again after determining the optimal \\(K\\) value; such a subsequent run may not converge to the same minimised TWCSS and having to run the algorithm again would be computationally wasteful.\n\nK <- 10 # set upper limit on range of K values\n\nTWCSS <- numeric(K) # allocate space for TWCSS estimates\n\nKM <- list() # allocate space for kmeans() output\n\nfor(k in 1:K) { # loop over k=1,...,K\n  \n  # Run K-means using K-Means++ initialisation: \n  # use the current k value and do so ten times if k > 1\n  KMPP    <- replicate(ifelse(k > 1, 10, 1), \n                       list(kmeans(sdf, iter.max=100,\n                                   centers=kmeans_pp(sdf, K=k))))\n  \n  # Extract and store the solution which minimises the TWCSS\n  KM[[k]] <- KMPP[[which.min(sapply(KMPP, \"[[\", \"tot.withinss\"))]]\n  \n  # Extract the TWCSS value for the current value of k\n  TWCSS[k] <- KM[[k]]$tot.withinss\n}\n\nAs previously stated in Section 8.3.1.2.1, the so-called “elbow method” consists of plotting the range of \\(K\\) values on the x-axis against the corresponding obtained TWCSS values on the y-axis and looking for an kink in the resulting curve.\n\nplot(x=1:K, y=TWCSS, type=\"b\",\n     xlab=\"K\", ylab=\"Total Within-Cluster\\n Sum-of-Squares\")\n\n\n\n\n\n\nFigure 2. Elbow plot showing a range of \\(K\\) values against the corresponding obtained TWCSS for \\(K\\)-Means applied to the MOOC centralities data set using \\(K\\)-Means with \\(K\\)-Means++ initialisation.\n\n\n\n\nHere, it appears that \\(K=4\\) would produce the best results: beyond \\(K=4\\), the decrease in TWCSS is minimal, which suggests that an extra cluster is not required to model the data well; between \\(K=3\\) and \\(K=4\\), there is a more substantial decrease in TWCSS, which suggests that the fourth cluster is necessary. This method is of course highly subjective, and we will further inform our choice of \\(K\\) for \\(K\\)-Means using silhouette widths and the ASW criterion in Section 8.4.2.4.\nFor now, we can interrogate the \\(K\\)-Means solution with \\(K=4\\) by examining the sizes of the clusters and their centroid vectors, using the corresponding fourth element of the list KM by setting K <- 4.\n\nK <- 4\n\nKM[[K]]$size\n\n[1] 127  57   8 247\n\nKM[[K]]$centers\n\n    InDegree  OutDegree Closeness_total Betweenness      Eigen Diffusion.degree\n1 -0.4086612 -0.4381057      -1.4220978  -0.3994123 -0.5596480       -1.1138889\n2  1.5706416  1.1372352       0.9247804   1.3859592  1.0781087        1.4216179\n3  4.1035975  5.0512503       1.5201941   3.4673658  5.4946785        3.1631065\n4 -0.2852445 -0.2007813       0.4685522  -0.2267743 -0.1390054        0.1422138\n    Coreness Cross_clique_connectivity\n1 -0.5672245                -0.3102236\n2  1.7423739                 0.9615041\n3  3.4821417                 5.5033583\n4 -0.2232184                -0.2406243\n\n\nHowever, these centroids correspond to the scaled version of the data created in Section 12.4.3. Interpretation can be made more straightforward by undoing the scaling transformation on these centroids, so that they are on the same scale as the data df rather than the scaled data sdf which was used as input to kmeans(). The column-wise means and standard deviations used when scale() was invoked are available as the attributes \"scaled:center\" and \"scaled:scale\", respectively and are used in the code below. We show these centroids rounded to four decimal places in Table 8.1.\n\nrescaled_centroids <- t(apply(KM[[K]]$centers, 1, function(r) {\n                        r * attr(sdf, \"scaled:scale\") + attr(sdf, \"scaled:center\") \n                        } ))\nround(rescaled_centroids, 4)\n\n\n\n\n\nTable 1. Centroids from the \\(K=4\\) \\(K\\)-Means solution on the original data scale.\n\n\n\n\n\n\n\n\n\n\n\n\nInDegree\nOutDegree\nCloseness_total\nBetweenness\nEigen\nDiffusion.degree\nCoreness\nCross_clique_connectivity\n\n\n\n\n1.1024\n1.7480\n0.0008\n20.7010\n0.0071\n144.1732\n2.6378\n4.6850\n\n\n15.3684\n14.8421\n0.0010\n849.9328\n0.0996\n1370.1053\n16.1053\n157.5789\n\n\n33.6250\n47.3750\n0.0011\n1816.6608\n0.3492\n2212.1250\n26.2500\n703.6250\n\n\n1.9919\n3.7206\n0.0010\n100.8842\n0.0308\n751.5061\n4.6437\n13.0526\n\n\n\n\n\n\nNow, we can more clearly see that the first cluster, with \\(n_{k=1}=127\\) observations, has the lowest mean value for all \\(d=8\\) centrality measures, while the last cluster, the largest with \\(n_{k=4}=247\\) observations, has moderately larger values for all centrality measures. The two smaller clusters, cluster two with \\(n_{k=2}=57\\) observations and cluster three with only \\(n_{k=3}=8\\) observations have the second-largest and largest values for each measure, respectively. As previously stated, we defer a more-detailed interpretation of uncovered clusters to Section 8.4.2.5, after the optimal clustering solution has been identified.\n\n\n4.2.2 \\(K\\)-Medoids application\nThe function pam() in the cluster library implements the PAM algorithm for \\(K\\)-Medoids clustering. By default, this function requires only the arguments x (a pre-computed pairwise dissimilarity matrix, as can be created from the functions dist(), daisy(), and more) and k, the number of clusters. However, there are many options for many additional speed improvements and initialisation strategies [48]. Here, we invoke a faster variant of the PAM algorithm which necessitates specification of nstart as a number greater than one, to ensure the algorithm is evaluated with multiple random initial medoid vectors, in a similar fashion to kmeans(). Thus, we call pam() with variant=\"faster\" and nstart=50 throughout.\nFirstly though, we need to construct the pairwise dissimilarity matrices to be used as input to the pam() function. Unlike \\(K\\)-Means, we are not limited to squared Euclidean distances. It is prudent, therefore, to explore \\(K\\)-Medoids solutions with several different dissimilarity measures and compare the different solutions obtained for different measures. Each distance measure will yield different results at the same \\(K\\) value, thus the value of \\(K\\) and the distance measure must be considered as a pair when determining the optimal solution.\nWe begin by calculating the Euclidean, Manhattan, and Minkowski (with \\(p=3\\)) distances on the scaled continuous data in sdf:\n\ndist_euclidean <- dist(sdf, method=\"euclidean\")\n\ndist_manhattan <- dist(sdf, method=\"manhattan\")\n\ndist_minkowski3 <- dist(sdf, method=\"minkowski\", p=3)\n\nSecondly, we avail of another principal advantage of \\(K\\)-Medoids; namely, the ability to incorporate categorical variables in mixed-type data sets, by calculating pairwise Gower distances between each row of merged_df, using daisy().\n\ndist_gower <- daisy(merged_df, metric=\"gower\")\n\nAs per the \\(K\\)-Means tutorial in Section 8.4.2.1, we can produce an elbow plot by running the algorithm over a range of \\(K\\) values and extracting the minimised within-cluster total distance achieved upon convergence for each value of \\(K\\). We demonstrate this for the dist_euclidean input below.\n\nK <- 10\nWCTD_euclidean <- numeric(K)\npam_euclidean <- list()\nfor(k in 1:K) {\n  pam_euclidean[[k]] <- pam(dist_euclidean, k=k, variant=\"faster\", nstart=50)\n  WCTD_euclidean[k] <- pam_euclidean[[k]]$objective[2]\n}\n\nEquivalent code chunks for the dist_manhattan, dist_minkowski3, and dist_gower inputs are almost identical, so we omit them here for the sake of brevity. Suffice to say, equivalent lists pam_manhattan, pam_minkowski3, and pam_gower, as well as equivalent vectors WCTD_manhattan, WCTD_minkowski3, and WCTD_gower, can all be obtained. Using these objects, corresponding elbow plots for all four dissimilarity measures are showcased in Figure 8.3.\nSome of the elbow plots in Figure 8.3 are more conclusive than others. As examples, there are reasonably clear elbows at \\(K=3\\) for the Euclidean and Minkowski distances, arguably an elbow at \\(K=4\\) for the Manhattan distance, and no clear, unambiguous elbow under the Gower distance. In any case, the elbow method only helps to identify the optimal \\(K\\) value for a given dissimilarity measure; we defer a discussion of how to choose the overall best \\(K\\)-Medoids clustering solution to later in this tutorial.\n\n\n\n\n\nFigure 3. Elbow plots for \\(K\\)-Medoids clustering evaluated with different dissimilarity measures over a range of \\(K\\) values. In clockwise order, beginning with the top-left panel, these are the Euclidean distance, Manhattan distance, Minkowski distance and, for the merged data with additional categorical covariates, the Gower distance.\n\n\n\n\nFor now, let’s interrogate the \\(K=3\\) solution obtained using the Euclidean distance. Recall that the results are already stored in the list pam_euclidean, so we merely need to access the third component of that list by setting K <- 3. Firstly, we can examine the size of each cluster by tabulating the cluster-membership indicator vector as follows:\n\nK <- 3\n\ntable(pam_euclidean[[K]]$clustering)\n\n\n  1   2   3 \n 67 122 250 \n\n\nExamining the medoids which serve as prototypes of each cluster is rendered difficult by virtue of the input having been a distance matrix rather than the data set itself. Though \\(K\\)-Medoids defines the medoids to be the rows in the data set from which the distance to all other observations currently allocated to the same cluster, according to the specified distance measure, is minimised, the medoids component of the output instead gives the indices of the medoids within the data set.\n\npam_euclidean[[K]]$medoids\n\n[1]  88 272  45\n\n\nFortunately, these indices within sdf correspond to the same, unscaled observations within df. Allowing for the fact that observations with name greater than the largest index here were removed due to missingness, they are effectively the values of the name column corresponding to the medoids. Thus, we can easily examine the medoids on their original scale. In Table 8.2, they include the name column, which was not used when calculating the pairwise distance matrices, and are rounded to four decimal places.\n\ndf |> \n  slice(as.numeric(pam_euclidean[[K]]$medoids)) |>\n  round(4)\n\n\n\n\n\nTable 2. Medoids for the \\(K=3\\) \\(K\\)-Medoids solution obtained using the Euclidean distance on the original data scale, with the corresponding observation index in the column.\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nInDegree\nOutDegree\nCloseness_total\nBetweenness\nEigen\nDiffusion.degree\nCoreness\nCross_clique_connectivity\n\n\n\n\n88\n16\n14\n0.0011\n675.5726\n0.1096\n1415\n18\n157\n\n\n272\n1\n1\n0.0007\n0.0000\n0.0047\n41\n2\n2\n\n\n45\n1\n2\n0.0010\n29.4645\n0.0194\n684\n3\n5\n\n\n\n\n\n\nThus, we can see that there is a small cluster with \\(n_{k=1}=67\\) observations which has the largest values for all \\(d=8\\) centrality measures, a slightly larger cluster with \\(n_{k=2}=122\\) observations and the lowest values for all variables, and the largest cluster with \\(n_{k=3}=250\\) and intermediate values for all variables. The cluster sizes of the \\(K\\)-Medoids solution being more evenly balanced than the earlier \\(K\\)-Means solution is an artefact of \\(K\\)-Medoids being less susceptible to outlying observations by virtue of not squaring the distances. We can explore this by cross-tabulating the clusters obtained by \\(K\\)-Means with \\(K=4\\) and \\(K\\)-Medoids with \\(K=3\\) and the Euclidean distance in Table 8.3.\n\ntable(pam_euclidean[[3]]$clustering,\n      KM[[4]]$cluster)\n\n\n\n\n\nTable 3. Cross-tabulation of the clusters obtained by \\(K\\)-Means with \\(K=4\\) (along the columns) and \\(K\\)-Medoids with \\(K=3\\) and the Euclidean distance (along the rows).\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n1\n\n0\n57\n8\n2\n\n\n2\n\n122\n0\n0\n0\n\n\n3\n\n5\n0\n0\n245\n\n\n\n\n\n\nFrom the cross-tabulation in Table 8.3, we can see that the \\(n_k=8\\) observations in the smallest \\(K\\)-Means cluster were absorbed into a larger cluster under the \\(K\\)-Medoids solution, thereby demonstrating the robustness of \\(K\\)-Medoids to outliers. Otherwise, both solutions are broadly in agreement.\n\n\n4.2.3 Agglomerative hierarchical clustering application\nPerforming agglomerative hierarchical clustering is straightforward now that the distance matrices have already been created for the purposes of running pam(). All that is required is to specify the distance matrix and an appropriate linkage criterion as the method when calling hclust(). We demonstrate this below for a subset of all possible distance measure and linkage criterion combinations among those described in Section 8.3.2.1. Recall that for the Ward criterion, the underlying distance measure is usually assumed to be squared Euclidean and that \"ward.D2\" is the correct method to use when the Euclidean distances are not already squared. For method=\"centroid\", we manually square the Euclidean distances.\n\nhc_minkowski3_complete <- hclust(dist_minkowski3, method=\"complete\")\n\nhc_manhattan_single <- hclust(dist_manhattan, method=\"single\")\n\nhc_gower_average <- hclust(dist_gower, method=\"average\")\n\nhc_euclidean_ward <- hclust(dist_euclidean, method=\"ward.D2\")\n\nhc_euclidean2_centroid <- hclust(dist_euclidean^2, method=\"ward.D2\")\n\nPlotting the resulting dendrograms is also straightforward. Simply calling plot() on any of the items above will produce a dendrogram visualisation. We do so here for four of the hierarchical clustering solutions constructed above —the undepicted hc_euclidean2_centroid dendrogram is virtually indistinguishable from that of hc_euclidean_ward— while suppressing the observation indices along the x-axis for clarity by specifying labels=FALSE.\n\nplot(hc_minkowski3_complete, labels=FALSE, \n     main=\"\", xlab=\"Minkwoski Distance (p=3) with Complete Linkage\")\nplot(hc_manhattan_single, labels=FALSE, \n     main=\"\", xlab=\"Manhattan Distance with Single Linkage\")\nplot(hc_gower_average, labels=FALSE, \n     main=\"\", xlab=\"Gower Distance with Average Linkage\")\nplot(hc_euclidean_ward, labels=FALSE, \n     main=\"\", xlab=\"Euclidean Distance with the Ward Criterion\")\n\n\n\n\n\n\nFigure 4. Dendrograms obtained by agglomerative hierarchical clustering with a selection of dissimilarity measures and linkage criteria.\n\n\n\n\nAs previously alluded to in Section 8.3.2.2, some combinations of dissimilarity measure and linkage criterion are liable to produce undesirable results. The susceptibility to outliers of complete linkage clustering is visible in the top-left panel of Figure 8.4, where just two observations form a cluster at a low height and are never again merged. The tendency of single linkage clustering to exhibit a “chaining” effect whereby all observations are successively merged into just one ever-larger cluster is evident in the top-right panel of Figure 8.4, and similar behaviour is observed for the Gower distance with average linkage depicted in the bottom-left panel. The most reasonable results appear to arise from using the Ward criterion in conjunction with Euclidean distances.\nTaking the set of candidate partitions in hc_euclidean_ward, for the reasons outlined above, all that remains is to cut this dendrogram at an appropriate height. Practitioners have the freedom to explore different levels of granularity in the final partition. Figure 8.5 shows the dendrogram from the bottom-right panel of Figure 8.4 cut horizontally at different heights, indicated by lines of different colours, as well as the corresponding implied \\(K\\) values.\n\n\n\n\n\nFigure 5. Dendrogram obtained using the Euclidean distance and Ward criterion cut at different heights with the corresponding implied \\(K\\) indicated.\n\n\n\n\nThereafter, one simply extracts the resulting partition by invoking cutree() with the appropriate height h. For example, to extract the clustering with \\(K=3\\), which we choose here because of the wide range of heights at which a \\(K=3\\) solution could be obtained:\n\nhc_ward2 <- cutree(hc_euclidean_ward, h=45)\n\nThe object hc_ward2 is now simply an vector indicating the cluster-membership of each observation in the data set. We show only the first few, for brevity, and then tabulate this vector to compute the cluster sizes. However, interpretation of these clusters is more difficult than in the case of \\(K\\)-Means and \\(K\\)-Medoids, as there is no centroid or medoid prototype with which to characterise each cluster.\n\nhead(hc_ward2)\n\n[1] 1 2 2 2 1 1\n\ntable(hc_ward2)\n\nhc_ward2\n  1   2 \n 49 390 \n\n\nIn this section, we have not presented an exhaustive evaluation of all possible pairs of dissimilarity measure and linkage criterion, but note that the code to do so is trivial. In any case, much like \\(K\\)-Means and \\(K\\)-Medoids, we must turn to other cluster quality indices to guide the choice of the best overall solution, be that choosing the best distance and linkage settings for agglomerative hierarchical clustering, or choosing the best clustering method in general among several competing methods. We now turn to finding the optimal clustering solution among multiple competing methods, guided by silhouette widths and plots thereof.\n\n\n4.2.4 Identifying the optimal clustering solution\nIn our application of \\(K\\)-Means, the elbow method appeared to suggest that \\(K=4\\) yielded the best solution. In our applications of \\(K\\)-Medoids, the elbow method suggested different values of \\(K\\) for different distance metrics. Finally, in our applications of hierarchical clustering, we noted that visualising the resulting dendrogram could be used to guide the choice of the height at which to cut to produce a single hard partition of \\(K\\) clusters. Now, we must determine which method yields the overall best solution. Following Section 8.3.3, we employ silhouettes for this purpose.\nFor \\(K\\)-Means and agglomerative hierarchical clustering, silhouettes can be computed using the silhouette function in the cluster library, in which case the function requires two arguments; the integer vector containing the cluster membership labels and an appropriate dissimilarity matrix. Thus, for instance, silhouettes could be obtained easily for the following two examples (using kmeans() and hclust(), respectively).\n\nkmeans_sil <- silhouette(kmeans(sdf, centers=kmeans_pp(sdf, K=4),\n                                iter.max=100)$cluster, \n                         dist_euclidean)\n\n\nhclust_sil <- silhouette(cutree(hclust(dist_euclidean, method=\"ward.D2\"), k=2), \n                         dist_euclidean)\n\nFor \\(K\\)-Medoids, it suffices only to supply the output of pam() itself, from which the cluster membership labels and appropriate dissimilarity matrix will be extracted internally, e.g.,\n\npam_sil <- silhouette(pam(dist_euclidean, k=3, variant=\"faster\", nstart=50))\n\nThereafter, plot() can be called on kmeans_sil, hclust_sil, or pam_sil to produce a silhouette plot. For an example based on hclust_sil, see Figure 8.6. Note that as \\(K=2\\) here, col=2:3 colours the silhouettes according to their cluster.\n\nplot(hclust_sil, main=\"\", col=2:3)\n\n\n\n\n\n\nFigure 6. Silhouette plot for the \\(K=2\\) hierarchical clustering solution obtained using the Ward criterion with Euclidean distances.\n\n\n\n\nFigure 8.6 shows that most silhouette widths are positive under this solution, indicating that most observations have been reasonably well-clustered. Cluster 2 appears to be the most cohesive, with a cluster-specific average silhouette width of \\(0.70\\), while cluster 1 appears to be the least cohesive, with a corresponding average of just \\(0.24\\). The overall ASW is \\(0.65\\), as indicated at the foot of the plot.\nGiven that we adopted a range of \\(K=1,\\ldots,10\\) when using \\(K\\)-Means and \\(K\\)-Medoids, and four different dissimilarity measures when using \\(K\\)-Medoids, we have \\(50\\) non-hierarchical candidate solutions to evaluate, of which some seem more plausible than others according to the respective elbow plots. For agglomerative hierarchical clustering, an exhaustive comparison over a range of \\(K\\) values, for each dissimilarity measure and each linkage criterion, would be far too extensive for the present tutorial. Consequently, we limit our evaluation of silhouettes to the \\(K\\)-Means and \\(K\\)-Medoids solutions already present in the objects KM, pam_euclidean, pam_manhattan, pam_minkowski3, and pam_gower, and the hierarchical clustering solutions which employ the Ward criterion in conjunction with Euclidean distances (of which we consider a further \\(10\\) solutions, again with \\(K=1,\\ldots,10\\), by considering the \\(10\\) possible associated heights at which the dendrogram can be cut). We limit the hierarchical clustering solutions to those based on the Ward criterion given that single linkage and complete linkage have been shown to be susceptible to chaining and sensitive to outliers, respectively. We use the corresponding pre-computed dissimilarity matrices dist_euclidean, dist_manhattan, dist_minkowski3, and dist_gower, where appropriate throughout.\nThough the ASW associated with hclust_sil is given on the associated silhouette plot in Figure 8.6, we can calculate ASW values for other solutions —which we must do to determine the best solution— without producing individiual silhouette plots. To show how this can be done, we examine the structure of the hclust_sil object, showing only its first few rows.\n\nhead(hclust_sil)\n\n     cluster neighbor sil_width\n[1,]       1        2 0.4408007\n[2,]       2        1 0.7416708\n[3,]       2        1 0.7401549\n[4,]       2        1 0.4696606\n[5,]       1        2 0.2494557\n[6,]       1        2 0.1915306\n\n\nThe columns relate to the cluster to which object \\(i\\) is assigned, the cluster for which the corresponding \\(b(i)\\) was minimised, and the \\(s(i)\\) score itself, respectively. Calculating mean(hclust_sil[,3]) will return the ASW. Though the code is somewhat tedious, we calculate the ASW criterion values for all \\(60\\) candidate solutions —that is, six methods evaluated over \\(K=1,\\ldots,10\\)— using a small helper function to calculate the ASW for the sake of tidying the code.\n\nK <- 10\nASW <- function(x) mean(x[,3])\nsilhouettes <- data.frame(K=2:K,\n  kmeans=sapply(2:K, function(k) ASW(silhouette(KM[[k]]$cluster, dist_euclidean))),\n  kmedoids_euclidean=sapply(2:K, function(k) ASW(silhouette(pam_euclidean[[k]]))),\n  kmedoids_manhattan=sapply(2:K, function(k) ASW(silhouette(pam_manhattan[[k]]))),\n  kmedoids_minkowski3=sapply(2:K, function(k) ASW(silhouette(pam_minkowski3[[k]]))),\n  kmedoids_gower=sapply(2:K, function(k) ASW(silhouette(pam_gower[[k]]))),\n  hc_euclidean_ward=sapply(2:K, function(k) ASW(silhouette(cutree(hc_euclidean_ward, \n                                                             k), dist_euclidean))))\n\nIn Figure 8.7, we plot these silhouettes against \\(K\\) using matplot(), omitting the code to do so for brevity.\n\n\n\n\n\nFigure 7. ASW criterion values plotted against \\(K\\) for \\(K\\)-Means, \\(K\\)-medoids (with the Euclidean, Manhattan, Minkowski (\\(p=3\\)), and Gower distances), and agglomerative hierarchical clustering based on Euclidean distance and the Ward criterion.\n\n\n\n\nAccording to Figure 8.7, there is generally little support for \\(K > 5\\) across almost all methods considered, as most method’s ASW values begin to decline after this point. The ASW values also make clear that incorporating the additional categorical demographic variables in a mixed-type clustering using the Gower distance does not lead to reasonable partitions, for any number of clusters \\(K\\). Overall, the most promising solutions in terms of having the highest ASW are \\(K\\)-Means, Ward hierarchical clustering, and \\(K\\)-Medoids with the Manhattan distance, all with \\(K=2\\), and \\(K\\)-Medoids with the Euclidean and Minkowski distances, each with \\(K=3\\). However, it would be wise to examine silhouette widths in more detail, rather than relying merely on the average. The silhouettes for this hierarchical clustering solution are already depicted in Figure 8.6, so Figure 8.8 shows individual silhouette widths for the remaining solutions.\n\n\n\n\n\nFigure 8. Silhouette plots showing silhouette widths for a numbering of promising solutions, coloured according to cluster membership.\n\n\n\n\nIt is notable that the silhouettes and ASW of the \\(K=2\\) \\(K\\)-Means solution (top-left panel of Figure 8.8) and the Ward hierarchical clustering solution (Figure 8.6) appear almost identical (if one accounts for the clusters being relabelled and associated colours being swapped). Indeed, according to a cross-tabulation of their partitions (not shown), their assignments differ for just \\(4\\) out of \\(n=439\\) observations. Despite having the highest ASW, we can justify dismissing these solutions given that \\(K=2\\) was not well-supported by its corresponding elbow plot in Figure 8.2. Similar logic suggests that the \\(K=3\\) \\(K\\)-Means solution and the \\(K=2\\) \\(K\\)-Medoids solution based on the Manhattan distance can also be disregarded. Although we stress again that an ideal analysis would more thoroughly determine an optimal solution with reference to additional cluster quality measures and note that various clustering solutions can be legitimate, for potentially different clustering aims, on the same data set [2, 3], we can judge that —among the two \\(K=3\\) \\(K\\)-Medoids solutions— the one based on the Euclidean distance is arguably preferable, for two reasons. Firstly, its ASW is quite close to that of the Minkowski distance solution:\n\nsilhouettes |> \n  filter(K == 3) |> \n  select(kmedoids_euclidean, kmedoids_minkowski3)\n\n\n\n  \n\n\n\nSecondly, the first cluster has a higher cluster-specific average silhouette width under the solution based on the Euclidean distance. Indeed, this solution has fewer negative silhouette widths also:\n\nsum(silhouette(pam_euclidean[[3]])[,3] < 0)\n\n[1] 27\n\nsum(silhouette(pam_minkowski3[[3]])[,3] < 0)\n\n[1] 28\n\n\n\n\n4.2.5 Interpreting the optimal clustering solution\nBy now, we have identified that the \\(K=3\\) solution obtained using \\(K\\)-Medoids and the Euclidean distance is optimal. Although aspects of this solution were already discussed in Section 8.4.2.2 —in particular, Table 8.2 has already shown the \\(K=3\\) medoid vectors obtained at convergence— we now turn to examining this output in greater detail, in order to provide a fuller interpretation of each of the uncovered clusters. We extract this solution for ease of manipulation.\n\nfinal_pam <- pam_euclidean[[3]]\n\nRecall that this method yielded three clusters of sizes \\(n_1=67\\), \\(n_2=122\\), and \\(n_3=250\\). Although the categorical variables were not used by this clustering method, additional interpretative insight can be obtained by augmenting the respective medoids in Table 8.2 with these cluster sizes and the experience values of the corresponding rows of the merged_df data set, which includes the additional demographic features.\n\ndf |> \n  slice(as.numeric(final_pam$medoids)) |> \n  round(4) |> \n  mutate(size=table(final_pam$clustering)) |>\n  left_join(slice(demog |> \n                    select(UID, experience), \n                  as.numeric(final_pam$medoids)), \n            by=join_by(name == UID)) |>\n  select(-name)\n\n\n\n\n\nTable 4. Medoids for the \\(K=3\\) \\(K\\)-Medoids solution obtained using the Euclidean distance on the original data scale, augmented with the cluster sizes and the corresponding values of the variable (which was not directly used by the clustering algorithm).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInDegree\nOutDegree\nCloseness_total\nBetweenness\nEigen\nDiffusion.degree\nCoreness\nCross_clique_connectivity\nsize\nexperience\n\n\n\n\n16\n14\n0.0011\n675.5726\n0.1096\n1415\n18\n157\n67\n1\n\n\n1\n1\n0.0007\n0.0000\n0.0047\n41\n2\n2\n122\n3\n\n\n1\n2\n0.0010\n29.4645\n0.0194\n684\n3\n5\n250\n2\n\n\n\n\n\n\nInterpretation and labeling of the clustering results is the step that follows, with a focus only on the medoid values of the centrality scores (had the optimal solution been obtained by kmeans(), we could instead examine its centroids in its $centers component, i.e., mean vectors). We will follow the example papers that we used as a guide for choosing the centrality measures [28] and [61]. Both papers have used traditional centrality measures (e.g., degree, closeness, and betweenness) as well as diffusion centralities (diffusion degree and coreness) to infer students’ roles. Furthermore, the second paper has an extended review of the roles and how they have been inferred from centrality measures, so readers are encouraged to read this review.\nAs the data shows, the first cluster has the highest degree centrality measures (InDegree and OutDegree), highest betweenness centrality, as well as the highest values of the diffusion centralities (Diffusion_degree and Coreness). These values are concordant with students who were actively engaged, received multiple replies, had their contributions discussed by others, and achieved significant diffusion. All of such criteria are concordant with the role of leaders. It stands to reason that the leaders cluster would be the smallest, with \\(n_1=67\\).\nThe third cluster has intermediate values for the degree centralities, high diffusion centrality values, as well as relatively high values of betweenness centrality. Such values are concordant with the role of an active participant who participates and coordinates the discussion. Therefore, we will use the label of coordinators.\nFinally, the second cluster has the lowest values for all centrality measures (though its diffusion values are still fairly reasonable). Thus, this cluster could feasibly be labelled as an isolates cluster, gathering participants whose role in the discussions is peripheral at best. Overall, the interpretations of this \\(K=3\\) solution are consistent with other findings in the existing literature, e.g., [28].\nWe can now label the clusters accordingly to facilitate more informative cluster-specific summaries. Here, we also recall the size of each cluster with the new labels invoked, to demonstrate their usefulness.\n\nfinal_pam$clustering <- factor(final_pam$clustering, \n                               labels=c(\"leaders\", \"coordinators\", \"isolates\"))\ntable(final_pam$clustering)\n\n\n     leaders coordinators     isolates \n          67          122          250 \n\n\nAs an example, we can use these labels to guide a study of the mean vectors of each cluster(bearing in mind that these are not centroid centroid vectors obtained by \\(K\\)-Means, but rather mean vectors obtained calculated for each group defined by the \\(K\\)-Medoids solution), for which the interpretation of the leader, coordinator, and isolate labels are still consistent with the conclusions drawn from the medoids in Table 8.2. Note that, for the sake of readability, the group-wise summary below is transposed.\n\ndf |>\n  group_by(clusters=final_pam$clustering) |> \n  select(-name) |> \n  summarise_all(mean) |>\n  mutate_if(is.numeric, round, 2) |> \n  pivot_longer(cols=-clusters, \n               names_to=\"centrality\") |>   \n  pivot_wider(names_from=clusters, \n              values_from=value) \n\n\n\n  \n\n\n\nFrom Table 8.4, we can also see that each observation which corresponds to a cluster medoid contains low, high, and medium levels of experience, respectively. However, one should be cautious not to therefore conclude that the clusters neatly map to experience levels, as the following cross-tabulation indicates little-to-no agreement between the groupings of experience levels in the data and the uncovered clusters.\n\ntable(final_pam$clustering, \n      merged_df$experience,\n      dnn=c(\"Clusters\", \"Experience\"))\n\n              Experience\nClusters        1  2  3\n  leaders      17 23 27\n  coordinators 40 34 48\n  isolates     61 93 96\n\n\nFinally, we can produce a visualisation of the uncovered clusters in order to better understand the solution. Visualising multivariate data with \\(d>2\\) is challenging and consequently such visualisations must resort to either plotting the first two principal components or mapping the pairwise dissimilarity matrix to a configuration of points in Cartesian space using multidimensional scaling. The latter is referred to as a “CLUSPLOT” [62] and is implemented in the clusplot() function in the same cluster library as pam() itself. This function uses classical (metric) multi-dimensional scaling [63] to create a bivariate plot displaying the partition of the data. Observations are represented by points in the scatter plot an ellipse spanning the smallest area containing all points in the given cluster is drawn around each cluster. In the code below, only clusplot(final_pam) is strictly necessary to produce such a plot for the optimal \\(K=3\\) Euclidean distance \\(K\\)-Medoids solution; all other arguments are purely for cosmetic purposes for the sake of the resulting Figure 8.9 and are described in ?clusplot.\nFigure 8.9 shows that the leaders cluster —the smallest cluster with the highest value for all centrality measures— is quite diffuse. Conversely, the larger coordinators cluster, with the smallest values for all centrality measures, and the isolates cluster, the largest of all, with intermediate values for all centrality measures, are more compact. This is consistent with the cluster-specific average silhouette widths shown in the bottom-right panel of Figure 8.8. That being said, the large span of the leaders cluster again affirms the relative robustness of \\(K\\)-Medoids to outliers, of which some (all of which are leaders) still remain despite the earlier pre-processing steps.\n\nclusplot(final_pam,                                    # the pam() output\n         main=\"\", sub=NA,            # remove the main title and subtitle\n         lines=0,                     # do not draw any lines on the plot\n         labels=4,                              # only label the ellipses\n         col.clus=\"black\",               # colour for ellipses and labels\n         col.p=as.numeric(final_pam$clustering) + 1, # colours for points\n         cex.txt=0.75,                      # control size of text labels\n         xlim=c(-4, 17)          # expand x-axis to avoid trimming labels\n         )\n\n\n\n\n\n\nFigure 9. Two-dimensional clustering plot for the final \\(K=3\\) Euclidean distance \\(K\\)-Medoids solution obtained via classical multidimensional scaling. The ellipses around each cluster are given the associated labels of , , and and the points are coloured according to cluster membership."
  },
  {
    "objectID": "chapters/ch08-clustering/ch8-clus.html#sec-disc",
    "href": "chapters/ch08-clustering/ch8-clus.html#sec-disc",
    "title": "8  Dissimilarity-based Cluster Analysis of Educational Data: A Comparative Tutorial using R",
    "section": "5 Discussion & further readings",
    "text": "5 Discussion & further readings\nThe present analysis of the MOOC centralities data set incorporated three of the most commonly used dissimilarity-based clustering approaches; namely, \\(K\\)-Means, \\(K\\)-Medoids, and agglomerative hierarchical clustering. Throughout the applications, emphasis was placed on the sensitivity of the results to various choices regarding algorithmic inputs available to practitioners, be that the choice of how to choose initial centroids for \\(K\\)-Means, the choice of dissimilarity measure for \\(K\\)-Medoids, or the choice of linkage criterion for hierarchical clustering, as examples. Consequently, our analysis considered different values of \\(K\\), different distances, different linkage criteria, different combinations thereof, and indeed different clustering algorithms entirely, in an attempt to uncover the “best” clustering solution for the MOOC centralities data set. We showed how elbow plots and other graphical tools can guide the choice of \\(K\\) for a given method but ultimately identified —via the average silhouette width criterion— an optimal solution with \\(K=3\\), using \\(K\\)-Medoids in conjunction with the Euclidean distance measure. Although we note that there are a vast array of other cluster quality measures in the literature which target different definitions of what constitutes a “good” clustering [3], the solution we obtained seems reasonable, in that the uncovered clusters which we labelled as leaders, coordinators, and isolates are consistent, from an interpretative point of view, with existing educational research. Indeed, several published studies have uncovered similar patterns of three groups which can be labelled in the same way [28, 64, 65].\nNonetheless, there are some limitations to our applications of dissimilarity-based clustering methods in this tutorial which are worth mentioning. Firstly, we note firstly that the decision to standardise the variables —in particular, to normalise them by subtracting their means and dividing by their standard deviations— was applied across the board to each clustering method we explored. Although the standardised data were only used as inputs to each algorithm (i.e., the output was always interpreted on the original scale), we note that different groups are liable to be uncovered with different standardisation schemes. In other words, not standardising the data, or using some other standardisation method (e.g., rescaling to the \\([0,1]\\) range) may lead to different, possibly more or less meaningful clusters.\nA second limitation is that all variables in the data were used as inputs (either directly in the case of \\(K\\)-Means, or indirectly as inputs to the pairwise dissimilarity matrix calculations required for \\(K\\)-Medoids and hierarchical clustering). As well as increasing the computational burden, using all variables can be potentially problematic in cases where the clustering structure is driven by some \\(d^\\star< d\\) variables, i.e., if the data can be separated into homogeneous subgroups along fewer dimensions. In such instances where some of the variables are uninformative in terms of explaining the variability in the data, variable selection strategies tailored to the unsupervised paradigm may be of interest and again may lead to more meaningful clusters being found [66, 67].\nAlthough dissimilarity-based clustering encompasses a broad range of flexible methodologies which can be utilised in other, diverse settings —for example, dissimilarity-based clustering is applied in the context of longitudinal categorical data in the chapter on sequence analysis methods [Chapter 10; [18]]— there are other clustering paradigms which may be of interest in similar or alternative settings as the data used in the present tutorial, even if they are not yet widely adopted in educational research. We now briefly introduce some of these alternative clustering frameworks for readers interested in expanding their knowledge of the topic of clustering beyond the dissimilarity-based framework detailed herein.\nA first alternative to dissimilarity-based clustering is the density-based clustering framework, most famously exemplified by the DBSCAN clustering algorithm [68, 69]. Density-based clustering broadly defines clusters as areas of higher density than the remainder of the data set, where objects are closely packed together with many nearby neighbours, while objects in the sparse areas which separate clusters are designated as outliers. This has been identified as one main advantage of DBSCAN by authors who applied it an education research context [70] —insofar as the capability to easily and effectively separate individual exceptionally poor or exceptionally outstanding students who require special attention from teachers is desirable— along with DBSCAN obviating the need to pre-specify the number of clusters \\(K\\).\nAnother alternative is given by the model-based clustering paradigm, which is further discussed in Chapter 9 [8]. Although we direct readers to that chapter for a full discussion of model-based clustering using finite Gaussian mixture models, the relationship between this approach and latent profile analysis, and a tutorial in R using the mclust package [71], there are some aspects and advantages which are pertinent to discuss here. Firstly, as model-based clustering is based on an underlying generative probability model, rather than relying on dissimilarity-based heuristics, it admits the use of principled, likelihood-based model-selection criteria such as the Bayesian information criterion [72], thereby eliminating the subjectivity of elbow plots and other graphical tools for guiding the choice of \\(K\\). Secondly, such models can be extended to allow covariates to guide the construction of the clusters [73], thereby enabling, for example, incorporation of the categorical demographic variables associated with the MOOC centralities data set used in the present tutorial.\nA third advantage of model-based clustering is that it returns a “soft” partition, whereas dissimilarity-based approaches yield either a single “hard” partition (with each observation placed in one group only), under partitional methods like \\(K\\)-Means or \\(K\\)-Medoids, or a set of nested hard partitions from which a single hard partition can be extracted, under hierarchical clustering. Specifically, model-based clustering assigns each observation a probability of belonging to each cluster, such that observations can have a non-negative association with more than one cluster and the uncertainty of the cluster memberships can be quantified. This has the effect of diminishing the effect of outliers or observations which lie on the boundary of two or more natural clusters, as they are not forced to wholly belong to one cluster. In light of these concerns, another clustering paradigm of potential interest is that of fuzzy clustering, which is notable for allowing for “soft” cluster-membership assignments while still being dissimilarity-based [74, 75]. Indeed, there are fuzzy variants of the \\(K\\)-Means and \\(K\\)-Medoids algorithms which relax the assumption that the latent variable \\(\\mathbf{z}_i\\) encountered in Equation 8.1, for example, is binary. They are implemented in the functions FKM() and FKM.med(), respectively, in the fclust R package [76].\nAnother advantage of model-based clustering over the dissimilarity-based paradigm is the flexibility it affords in relation to the shapes, orientations, volumes, and sizes of the clusters it uncovers. At their most flexible, finite Gaussian mixture models can find clusters where all of these characteristics differ between each cluster, but intermediary configurations —whereby, as but one example, clusters can be constrained to have equal volume and orientation but varying shape and size— are permissible. This is achieved via different parsimonious parameterisations of the cluster-specific covariance matrices which control the geometric characteristics of the corresponding ellipsoids; see Chapter 9 [8] for details. By contrast, \\(K\\)-Means is much more restrictive. The algorithm assumes that clusters are of similar size, even if the estimated clusters can vary in size upon convergence. Moreover, by relying on squared Euclidean distances to a mean vector, with no recourse to modelling covariances, \\(K\\)-Means implicitly assumes that all clusters are spherical in shape, have equal volume, and radiate around their centroid. This can have the damaging consequence, in more challenging applications, that a a larger number of spherical clusters may be required to fit the data well, rather than a more parsimonious and easily interpretable mixture model with fewer non-spherical components. Finally, in light of these concerns, we highlight the spectral clustering approach [77], implemented via the function specc() in the kernlab package in R, which is, like model-based clustering, capable of uncovering clusters with more flexible shapes, particularly when the data are not linearly separable, and shares some connections with kernel \\(K\\)-Means [78], another flexible generalisation of the standard \\(K\\)-Means algorithm adopted in this tutorial.\nOverall, we encourage readers to further explore the potential of dissimilarity-based clustering methods, bear in mind the limitations and practical concerns of each algorithm discussed in this tutorial, and remain cognisant of the implications thereof for results obtained in educational research applications. We believe that by paying particular attention to the guidelines presented for choosing an optimal partition among multiple competing methodologies, with different numbers of clusters and/or different dissimilarity measures and/or different linkage criteria, more meaningful and interpretable patterns of student behaviour can be found. Finally, we hope that the additional references provided to other clustering frameworks will inspire a broader interest in the topic of cluster analysis among practitioners in the field of education research."
  },
  {
    "objectID": "chapters/ch09-model-based-clustering/ch9-model.html",
    "href": "chapters/ch09-model-based-clustering/ch9-model.html",
    "title": "9  An Introduction and R Tutorial to Model-based Clustering in Education via Latent Profile Analysis",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch09-model-based-clustering/ch9-model.html#introduction",
    "href": "chapters/ch09-model-based-clustering/ch9-model.html#introduction",
    "title": "9  An Introduction and R Tutorial to Model-based Clustering in Education via Latent Profile Analysis",
    "section": "1 Introduction",
    "text": "1 Introduction\nEducation research is commonly performed with variable-centered methods (e.g., correlation, regression, comparison of means e.g., t-test) using a sample from the population to devise or compare central tendency measures or an “average” (i.e., mean or median). The average is assumed to represent the population under study and therefore could be generalised to the population at large [1, 2]. Put another way, the statistical findings of variable-centered methods are thought to apply to all learners in the same way. In doing so, variable-centered methods ignore the individual differences that are universal across all domains of human function [3]. Learners are not an exception; students vary in their behavior, attitude, and dispositions, and they rarely —if at all— conform to a common pattern or an average behavior [2, 4]. An “average” is thus a poor simplification of learners’ heterogeneity; consequently, methods to capture individual differences have started to gain popularity with the increasing attention to patterns and differences among students. Our focus in this chapter is on such person-centered methods [1–3, 5].\nPerson-centered methods can be broadly grouped into two categories; i) heuristic, dissimilarity-based clustering algorithms (e.g., agglomerative hierarchical clustering, and partitional clustering algorithms like \\(k\\)-means) on one hand and ii) model-based clustering (MBC) approaches (e.g., finite Gaussian mixture models) on the other. Though we focus here on the MBC paradigm, we note that —contrary to variable-centered methods— person-centered methods are generally concerned with capturing heterogeneity by uncovering the latent (e.g., unobserved or hidden) patterns within data into subgroups of “clusters” or “profiles”, which are assumed to be homogeneous [1, 2]. Modelling the unobserved patterns within the data could reveal the qualitative differences between learners. For instance, where students may have different patterns of approaching their learning, capturing such patterns would make sense as each different approach may benefit from a certain course design, set of instructional methods, or scaffolding approach [2]. Similarly, dispositions such as engagement, motivation, and achievement are multidimensional and vary across students; capturing such differences requires a method that could handle such nonlinear multidimensional dispositions and identify the different patterns.\nThis chapter deals with one of the person-centered methods; namely, latent profile analysis (from the perspective of model-based clustering via finite Gaussian mixture models). To clarify, we note that the terms finite mixture models and latent profile analysis can be used interchangeably, with the former being more common in the statistical literature and the latter being more common in the education literature and social sciences literature more broadly. The equivalence of both terminologies is further elaborated in Section 9.2.1. In any case, this framework represents a probabilistic approach to statistical unsupervised learning that aims at discovering clusters of observations in a data set [6]. In other words, the MBC paradigm is referred to as model-based by virtue of being based on a generative probabilistic model, unlike heuristic clustering algorithms based on dissimilarity criteria.\nWe also offer a walkthrough tutorial for the analysis of a data set on school engagement, academic achievement, and self-regulated learning using the popular mclust package [7] for R [8] which implements the approach. Whereas mixture models and mclust have received growing attention within social science, they have not garnered widespread utilisation in the field of educational research and their adoption in learning analytics research has been relatively slow."
  },
  {
    "objectID": "chapters/ch09-model-based-clustering/ch9-model.html#literature-review",
    "href": "chapters/ch09-model-based-clustering/ch9-model.html#literature-review",
    "title": "9  An Introduction and R Tutorial to Model-based Clustering in Education via Latent Profile Analysis",
    "section": "2 Literature review",
    "text": "2 Literature review\nExamples of mixture models being applied in educational research settings are relatively few compared to other methods of clustering. Some notable examples exist which address patterns in students’ online learning [9] or patterns in students’ disposition [10] or collaborative roles [11]. Most studies in education research that applied mixture models used latent profile analysis (LPA) to identify students’ profiles from self-reported data. For example, [10] performed LPA on a data set of 318 students’ survey responses about emotional self-efficacy, motivation, self-regulation, and academic performance, and identified four profiles: “low”, “average”, “above average with a low ability to handle the emotions of others”, and “high emotional self-efficacy”. In the work by [12], the authors analyzed 615 vocational education students’ achievement emotions in online learning environments, and found three profiles: “blends of negative emotions”, “nonemotional”, and “pure positive emotion”. [13] employed LPA on self-report data on classroom engagement from 413 first-year university students in Vietnam and found three profiles: “highly engaged”, “moderately engaged”, and “minimally engaged”. [14] collected survey responses from 2,339 engineering undergraduates about 28 noncognitive and affective factors using a survey instrument and using Gaussian mixture models found four very distinct profiles of students.\nThe analysis of online trace log data —which is at the core of learning analytics data— with mixture models is even less common. In the study by [15], the authors applied LPA to variables related to debugging derived from students’ programming problems submission traces. They found a profile with higher debugging accuracy and coding speed, another profile with lower debugging performance in runtime and logic errors, and a third profile with lower performance in syntactic errors who tended to make big changes in every submission. Studies covering online collaborative learning are even more scarce. A rare example is the study by [11], in which the authors used latent profile analysis to identify students’ roles in collaboration based on their centrality measures. The mixture models identified three collaborative roles that represented a distinct pattern of collaboration: leaders, who contribute the most to the discussion, whose arguments are more likely to spread; mediators, who bridge others and moderate the discussions; as well as isolates, who are socially idle and do not contribute to the discussion.\nA considerable number of studies that applied mixture models further investigate the association between profile membership and academic achievement. For example, in the aforementioned study by [10], students with high emotional self-efficacy had higher academic performance than the other profiles. In the study by [15], the authors found that higher debugging accuracy was related to higher scores in all exams, whereas there were no differences between the two other identified profiles. By the same token, researchers have attempted to find reasons why a certain profile emerged, or what are the variables that are more associated with one profile more than the other. For example, [13] found that peer support, provision of choice, and task relevance are the factors more likely to predict classroom engagement profile membership. [10] found that self-regulation and motivation played significant roles in determining profile membership.\nClearly, there are plenty of opportunities for further exploration and investigation in this area that could augment our knowledge of learning, learners’ behavior, and the variabilities of learning processes [2]. This is especially true given the numerous advantages of the MBC paradigm over more traditional, heuristic clustering algorithms, which we imminently describe. Subsequently, in the rest of this chapter we elaborate on the theoretical underpinnings of the family of Gaussian parsimonious clustering models implemented in the mclust R package and additionally explore some advanced features of the package, which we employ in an analysis of a real educational research application thereafter. Finally, we conclude with a brief discussion.\n##Model-based clustering As stated above, clustering methods, in a general sense, are used to uncover group structure in heterogeneous populations and identify patterns in a data set which may represent distinct subpopulations. While there is no universally applicable definition of what constitutes a cluster [16], it is commonly assumed that clusters should be well separated from each other and cohesive in an ideal analysis [17]. Conversely, objects within a cluster should be more similar to each other in some sense, in such a way that an observation has a defined relationship with observations in the same cluster, but not with observations from other clusters.\nTraditional clustering approaches, like the aforementioned \\(k\\)-means algorithm, and agglomerative hierarchical clustering, use dissimilarity-based heuristics to ultimately produce a “hard” partition of cases into groups, such that each observation is associated with exactly one cluster only. As such approaches are not underpinned by a statistical model, assessment of the optimal number of clusters is often a fraught task, lacking the guidance of principled statistical model selection criteria. However, we note that \\(k\\)-means can be recasted as a clustering model assuming a Gaussian mixture with equal proportions and diagonal equal covariance matrix across groups. Moreover, some (but not all) agglomerative hierarchical clustering models can be rooted in a statistical model also, as discussed in [18].\nConversely, the MBC paradigm typically assumes that data arise from a (usually finite) mixture of probability distributions, whereby each observation is assumed to be generated from a specific cluster, characterised by an associated distribution in the mixture [19]. Ideally, mixtures of distributions are supposed to provide a good model for the heterogeneity in a data set; that is, once an observation has been assigned to a cluster, it is assumed to be well-represented by the associated distribution. As such, MBC methods are based on a formal likelihood and seek to estimate parameters (e.g., means, variances, and covariances, which may or may not differ across groups) which best characterise the different distributions. Rather than yielding only a “hard” partition, each observation is assigned a probability of being associated with each mixture component —such that observations can have non-negative association with more than one cluster— from which a hard partition can be constructed. These probabilities are treated as weights when estimating the component parameters, which brings the advantages of minimising the effect of observations lying near the boundary of two natural clusters (e.g., a student with an ambiguous learning profile) and being able to quantity the uncertainty in the cluster assignment of a particular observation to provide a sense of cases for which further investigation may be warranted. Compared to other approaches, the other main advantages of this statistical modelling framework are its ability to use statistical model selection criteria and inferential procedures for evaluating and assessing the results obtained.\nInference for finite mixture models is routinely achieved by means of the expectation-maximisation (EM) algorithm [20], under which each observation’s component membership is treated as a “missing” latent variable which must be estimated. This formulation assumes that the data are conditionally independent and identically distributed, where the conditioning is with respect to a latent variable representation of the data in which the latent variable indicates cluster membership. Given the relative familiarity of latent class and latent profile terminology in the social sciences, we now explicitly cast MBC methods in the framework of latent variable modelling.\n\n2.1 Latent variable models\nLatent variable models are statistical models that aim to explain the relationships between observed variables by introducing one or more unobserved or latent variables. The idea behind latent variable models is that some of the underlying constructs or concepts we are interested in cannot be measured directly, but only through their effects on observable variables. Latent variable modelling has a relatively long history, dating back from the measure of general intelligence by factor analysis [21], to the structural equation modelling approach [22], from topic modelling, such as the latent Dirichlet allocation algorithm [23], to hidden Markov models for time series [24] and longitudinal data [25]. Latent variable models are widely used in various fields, including psychology, sociology, economics, and biology, to name a few. They are particularly useful when dealing with complex phenomena that cannot be easily measured or when trying to understand the underlying mechanisms that drive the observed data.\nWhen discussing latent variable modelling, it is useful to consider the taxonomy presented by [26]. This can be particularly helpful, as the same models are sometimes referred to by different names in different scientific disciplines. [26, Table 1.3] considered a cross-classification of latent variable methods based on the type of variable (manifest or latent) and its nature (metrical or categorical). If both the manifest and latent variables are metrical, the model is called a factor analysis model. If the manifest variables are categorical and the latent variables are metrical, the model is called a latent trait model or item response theory model. If the manifest variables are metrical and the latent variables are categorical, the model is called a latent profile analysis model. If both the manifest and latent variables are categorical, the model is called a latent class model.\nIn this scheme, finite Gaussian mixture models described in this chapter assume that the observed variables are continuous and normally distributed, while the latent variable, which represents the cluster membership of each observation, is categorical. Therefore, Gaussian mixtures belong to the family of latent profile analysis models. This connection is made apparent by the tidyLPA R package [27], which leverages this equivalence to provide an interface to the well-known mclust R package [7] used throughout this chapter, using tidyverse syntax and terminology which is more familiar in the LPA literature.\n\n\n2.2 Finite Gaussian mixture models\nAs described above, finite mixture models (FMMs) provide the statistical framework for model-based clustering and allow for the modelling of complex data by combining simpler distributions. Specifically, a FMM assumes that the observed data are generated from a finite mixture of underlying distributions, each of which corresponds to a distinct subgroup or cluster within the data. Gaussian mixture models (GMMs) are a particularly widespread variant of FMMs which specifically assume that each of the underlying distributions is a (multivariate) Gaussian distribution. This means that the data within each cluster are normally distributed, but with potentially different means and covariance matrices. The relevance of GMMs stems from the well-established fact that mixtures of Gaussians can provide an accurate approximation to any continuous density.\nIn FMMs, the latent variable represents the cluster assignment for each observation in the data. It is a categorical variable assuming one of a finite set of possible values that correspond to different clusters. Alternatively, it can be encoded using a set of indicator variables, which take the value of 1 for the cluster to which the observation belongs, and 0 for all other clusters.\nTo estimate the parameters of a GMM with the associated latent variable for cluster membership, a likelihood-based approach is typically used. The likelihood function expresses the probability of observing the data, given the parameter values and the latent variable. The maximum likelihood estimation (MLE) method is commonly used to estimate the parameters and the latent variable which maximise the likelihood function. Usually, the number of clusters in a GMM is also unknown, and it is determined through a process known as model selection, which involves comparing models with different numbers of clusters and parameterisations and selecting the one which best fits the data.\nIn summary, model-based clustering from the perspective of latent variable modelling assumes that the data is generated from a probabilistic model with a specific number of clusters. A likelihood-based approach can be used to estimate the parameters of the model and the latent variable that represents the cluster assignment for each observation in the data, and guide the selection of the number of clusters. A GMM is a common framework for model-based clustering which assumes the data in each cluster is generated from a Gaussian distribution."
  },
  {
    "objectID": "chapters/ch09-model-based-clustering/ch9-model.html#gaussian-parsimonious-clustering-models",
    "href": "chapters/ch09-model-based-clustering/ch9-model.html#gaussian-parsimonious-clustering-models",
    "title": "9  An Introduction and R Tutorial to Model-based Clustering in Education via Latent Profile Analysis",
    "section": "3 Gaussian parsimonious clustering models",
    "text": "3 Gaussian parsimonious clustering models\nFor a continuous feature vector \\(\\boldsymbol{x}\\in \\mathbb{R}^{d}\\), the general form of the density function of a Gaussian mixture model (GMM) with \\(K\\) components can be written as \\[\nf(\\boldsymbol{x}) = \\sum_{k=1}^K \\pi_k \\phi_d(\\boldsymbol{x};\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k),\n\\tag{1}\\] where \\(\\pi_k\\) represents the mixing probabilities, i.e., the marginal probability of belonging to the \\(k\\)-th cluster, such that \\(\\pi_k > 0\\) and \\(\\sum_{k=1}^K\\pi_k=1\\), and \\(\\phi_d(\\cdot)\\) is the \\(d\\)-dimensional multivariate Gaussian density with parameters \\((\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k)\\) for \\(k=1,\\ldots,K\\). Clusters described by such a GMM are ellipsoidal, centered at the means \\(\\boldsymbol{\\mu}_k\\), and with other geometric characteristics (namely volume, shape, and orientation) determined by the covariance matrices \\(\\boldsymbol{\\Sigma}_1, \\ldots, \\boldsymbol{\\Sigma}_K\\). Parsimonious parameterisations of covariance matrices can be controlled by imposing some constraints on the covariance matrices through the following eigen-decomposition [28, 29]: \\[\n\\boldsymbol{\\Sigma}_k = \\lambda_k \\boldsymbol{U}_k \\boldsymbol{\\Delta}_k \\boldsymbol{U}{}^{\\!\\top}_k,\n\\tag{2}\\] where \\(\\lambda_k = \\lvert\\boldsymbol{\\Sigma}_k\\rvert^{1/d}\\) is a scalar which controls the volume, \\(\\boldsymbol{\\Delta}_k\\) is a diagonal matrix whose entries are the normalised eigenvalues of \\(\\boldsymbol{\\Sigma}_k\\) in decreasing order, such that \\(\\lvert\\boldsymbol{\\Delta}_k\\rvert = 1\\), which controls the shape of the density contours, and \\(\\boldsymbol{U}_k\\) is an orthogonal matrix whose columns are the eigenvectors of \\(\\boldsymbol{\\Sigma}_k\\), which controls the orientation of the corresponding ellipsoid. The size of a cluster is distinct from its volume and is proportional to \\(\\pi_k\\) [29].\nGMMs with unconstrained covariance matrices are quite flexible, but require the estimation of several parameters. To obtain a balance between model complexity and accuracy of parameter estimates, a parsimonious model parameterisation can be adopted. Constraining the geometric characteristics of cluster covariances to be equal across clusters can greatly reduce the number of estimable parameters, and is the means by which GMMs obtain intermediate covariance matrices between homoscedasticity and heteroscedasticity. A list of the 14 resulting parameterisations available in the mclust package [7] for R [8] is included in Table 2.1 of [30]. Of particular note is the nomenclature adopted by mclust whereby each model has a three-letter name with each letter pertaining to the volume, shape, and orientation, respectively, denoting whether the given component is equal (E) or free to vary (V) across clusters. Some model names also use the letter I in the third position to indicate that the covariance matrices are diagonal and two particularly parsimonious models have the letter I in the second position to indicate that the covariance matrices are isotropic. Thus, as examples, the fully unconstrained VVV model is one for which the volume, shape, and orientation are all free to vary across clusters, the EVE model constrains the clusters to have equal volume and orientation but varying shape, and the VII model assumes isotropic covariance matrices with cluster-specific volumes. The flexibility to model clusters with different geometric characteristics by modelling correlations according to various parameterisations represents another advantage over heuristic clustering algorithms. Taking the \\(k\\)-means algorithm as an example, a larger number of circular, Euclidean distance-based clusters may be required to fit the data well, rather than a more parsimonious and easily interpretable mixture model with fewer non-spherical components.\nGiven a random sample of observations \\(\\{ \\boldsymbol{x}_1, \\boldsymbol{x}_2, \\ldots, \\boldsymbol{x}_n \\}\\) in \\(d\\) dimensions, the log-likelihood of a GMM with \\(K\\) components is given by \\[\n\\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\log\\left\\{ \\sum_{k=1}^K \\pi_k \\phi_d(\\boldsymbol{x}_i ; \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\right\\},\n\\tag{3}\\] where \\(\\boldsymbol{\\theta}= (\\pi_1, \\ldots, \\pi_{K-1}, \\boldsymbol{\\mu}_1, \\ldots, \\boldsymbol{\\mu}_K, \\boldsymbol{\\Sigma}_1, \\ldots, \\boldsymbol{\\Sigma}_K)\\) denotes the collection of parameters to be estimated.\nMaximizing the log-likelihood function in Equation 9.3 directly is often complicated, so maximum likelihood estimation (MLE) of \\(\\boldsymbol{\\theta}\\) is usually performed using the EM algorithm [20] by including the component membership as a latent variable. The EM algorithm consists of two steps: the E-step (Expectation step) and the M-step (Maximisation step). In the E-step, the algorithm calculates the expected membership probabilities of each data point to each of the mixture components based on the current estimates of the model parameters. Thus, though the latent variable indicating cluster membership is assumed to be categorical and represented by indicator variables taking the values \\(0\\) or \\(1\\), the model estimates assignment probabilities in the range \\([0,1]\\) at this step. In the M-step, the algorithm updates the model parameters by maximizing the likelihood of the observed data given the estimated membership probabilities. These two steps are repeated until convergence or a maximum number of iterations is reached. Details on the use of EM algorithm in finite mixture modelling is provided by [19], while a thorough treatment and further extensions can be found in [31]. For the GMM case, see Sec. 2.2 of [30].\nFollowing the fitting of a GMM and the determination of the MLEs of parameters, the maximum a posteriori (MAP) procedure can be used to assign the observations into the most likely cluster and recover a “hard” partition. For an observation \\(\\boldsymbol{x}_i\\) the posterior conditional probability of coming from the mixture component \\(k\\) is given by \\[\n\\widehat{p}_{ik} = \\frac{\\widehat{\\pi}_k \\phi_d(\\boldsymbol{x}_i; \\widehat{\\boldsymbol{\\mu}}_k, \\widehat{\\boldsymbol{\\Sigma}}_k)}{\\displaystyle\\sum_{g=1}^K \\widehat{\\pi}_g \\phi_d(\\boldsymbol{x}; \\widehat{\\boldsymbol{\\mu}}_g, \\widehat{\\boldsymbol{\\Sigma}}_g)}.\n\\tag{4}\\] Then, an observation is assigned to the cluster with the largest posterior conditional probability, i.e., \\(\\boldsymbol{x}_i \\in \\mathcal{C}_{k^\\star}\\) with \\(k^\\star = \\mathop{\\mathrm{arg\\,max}}_k \\widehat{p}_{ik}\\).\n\n3.1 Model selection\nGiven that a wide variety of GMMs in Equation 9.1 can be estimated by varying the number of mixture components and the covariance decompositions in Equation 9.2, selecting the appropriate model represents a crucial decision. A popular option consists in choosing the “best” model using the Bayesian information criterion [BIC, 32], which, for a given model \\(\\mathcal{M}\\), is defined as \\[\n\\text{BIC}_{\\mathcal{M}} = 2\\ell_{\\mathcal{M}}(\\widehat{\\boldsymbol{\\theta}}) - \\nu_{\\mathcal{M}} \\log(n),\n\\] where \\(\\ell_{\\mathcal{M}}(\\widehat{\\boldsymbol{\\theta}})\\) stands for the maximised log-likelihood of the data sample of size \\(n\\) under model \\(\\mathcal{M}\\), and \\(\\nu_{\\mathcal{M}}\\) for the number of independent parameters to be estimated. Another option available in clustering is the Integrated Complete Likelihood [ICL, 33] criterion given by \\[\n\\text{ICL}_{\\mathcal{M}} = \\text{BIC}_{\\mathcal{M}} + 2 \\sum_{i=1}^n\\sum_{k=1}^K c_{ik} \\log(\\widehat{p}_{ik}),\n\\] where \\(\\widehat{p}_{ik}\\) is the conditional probability that \\(\\boldsymbol{x}_i\\) arises from the \\(k\\)-th mixture component from Equation 9.4, and \\(c_{ik} = 1\\) if the \\(i\\)-th observation is assigned to cluster \\(\\mathcal{C}_k\\) and 0 otherwise.\nBoth criteria evaluate the fit of a GMM to a given set of data by considering both the likelihood of the data given the model and the complexity of the model itself, represented by the number of parameters to be estimated. Compared to the BIC, the ICL introduces a further entropy-based penalisation for the overlap of the clusters. For this reason, the ICL tends to select models with well-separated clusters.\nWhereas there is no consensus of a standard criteria for choosing the best model, there are guidelines that the researcher could rely on. To decide on the optimal model, examining the fit indices (such as the BIC and ICL), model interpretability, and conformance to theory can be of great help. The literature recommends estimating a 1-cluster solution for each model that serves as a comparative baseline and then increasing the number of clusters one by one, evaluating if adding another cluster yields a better solution in both statistical and conceptual terms [34]. Among all fit indices, lower BIC values seems to be the preferred method for selecting the best model. However, examining other indices (e.g., AIC, ICL) is also useful. Oftentimes, fit indices do not converge to a certain model. In such cases, the interrelation between the selected models, such as whether one model is an expanded version of another, should also be taken into consideration, as well as the stability of the different models, including the relative sizes of the emergent profiles (each profile should comprise more than 5-8% of the sample) [34]. Furthermore, the elbow method could be helpful in cases where no clear number of clusters can be easily determined from the fit indices (e.g., the BIC continues to decrease consistently when increasing the number of clusters). This entails plotting the BIC values and finding an elbow shape where a drop in BIC is less noticeable with increasing numbers of clusters or roughly an elbow followed by a relatively flat line. The choice of the best number of clusters can and probably should be guided by theory; that is, in cases where previous research reported a certain number of clusters or profiles, it is recommended to take this guidance into account. For instance, research on engagement has repeatedly reported three levels of engagement. Once we have chosen the most suitable model, it is suggested to compute model diagnostics (e.g., entropy and average posterior probability) to evaluate the selected model. These diagnostics are covered in Section 9.3.3.3.\n\n\n3.2 mclust R package\nmclust is an R package [8] for model-based cluster analysis, classification, and density estimation using Gaussian finite mixture models [30, 35]. It is widely used in statistics, machine learning, data science, and pattern recognition. One of the key features of mclust is its flexibility in modelling quantitative data with several covariance structures and different numbers of mixture components. Additionally, the package provides extensive graphical representations, model selection criteria, initialisation strategies for the EM algorithm, bootstrap-based inference, and Bayesian regularisation, among other prominent features. mclust also represents a valuable tool in educational settings because it provides a powerful set of models that allows students and researchers to quickly and easily perform clustering and classification analyses on their data. We focus here on the use of mclust as a tool for unsupervised model-based clustering, though the package does also provide functions for supervised model-based classification.\nThe main function implementing model-based clustering is called Mclust(), which requires a user to provide at least the data set to analyze. In the one-dimensional case, the data set can be a vector, while in the multivariate case, it can be a matrix or a data frame. In the latter case, the rows correspond to observations, and the columns correspond to variables.\nThe Mclust() function allows for further arguments, including the optional argument G to specify the number of mixture components or clusters, and modelNames to specify the covariance decomposition. If both G and modelNames are not provided, Mclust() will fit all possible models obtained using a number of mixture components from 1 to 9 and all 14 available covariance decompositions, and it will select the model with the largest BIC. Notably, if the data set is univariate, only 2 rather than 14 models governing the scalar variance parameters are returned; that they are equal or unequal across components. Finally, computing the BIC and ICL criteria can be done by invoking the functions mclustBIC() and mclustICL(), respectively.\n\n\n3.3 Other practical issues and extensions\nPrior to commencing the cluster analysis of a data set on school engagement, academic achievement, and self-regulated learning measures, we first provide some theoretical background on some extensions of practical interest which will be explored in the analysis.\n\n3.3.1 Bayesian regularisation\nIncluding a prior distribution over the mixture parameters is an effective way to avoid singularities and degeneracies in maximum likelihood estimation. Furthermore, this can help to prevent overfitting and improve model performance. In situations where the variables of interest are discrete or take on only a few integer values, including a prior distribution can help to regularise the model.\n[36] proposed using weekly informative conjugate priors to regularise the estimation process. The EM algorithm can still be used for model fitting, but maximum likelihood estimates (MLEs) are replaced by maximum a posteriori (MAP) estimates. A slightly modified version of BIC can be used for model selection, with the maximised log-likelihood replaced by the log-likelihood evaluated at the MAP or posterior mode.\nThe prior distributions proposed by [36] are:\n\na uniform prior on the simplex for the mixture weights \\((\\pi_1, \\ldots, \\pi_K)\\);\na Gaussian prior on the mean vector (conditional on the covariance matrix), i.e., \\[\\begin{align}\n\\boldsymbol{\\mu}\\mid \\boldsymbol{\\Sigma}\n& \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{\\scriptscriptstyle \\mathcal{P}}, \\boldsymbol{\\Sigma}/\\kappa_{\\scriptscriptstyle \\mathcal{P}}) \\\\\n& \\propto\n\\left|\\boldsymbol{\\Sigma}\\right|^{-1/2}\n\\exp\\left\\{ -\\frac{\\kappa_{\\scriptscriptstyle \\mathcal{P}}}{2}\n            \\mathop{\\mathrm{tr}}\\left(\\left(\\boldsymbol{\\mu}- \\boldsymbol{\\mu}_{\\scriptscriptstyle \\mathcal{P}}\\right){}^{\\!\\top}\n                     \\boldsymbol{\\Sigma}^{-1}\n                     \\left(\\boldsymbol{\\mu}- \\boldsymbol{\\mu}_{\\scriptscriptstyle \\mathcal{P}}\\right)\n                \\right)\n    \\right\\},\n\\label{eqn:multivariatePriorMean}\n\\end{align}\\] with \\(\\boldsymbol{\\mu}_{\\scriptscriptstyle \\mathcal{P}}\\) and \\(\\kappa_{\\scriptscriptstyle \\mathcal{P}}\\) being the hyperparameters controlling, respectively, the mean vector and the amount of shrinkage applied;\nan inverse Wishart prior on the covariance matrix, i.e., \\[\\begin{align}\n\\boldsymbol{\\Sigma}\n& \\sim \\mathcal{IW}(\\nu_{\\scriptscriptstyle \\mathcal{P}}, \\boldsymbol{\\Lambda}_{\\scriptscriptstyle \\mathcal{P}})\n\\nonumber\\\\\n& \\propto\n\\left|\\boldsymbol{\\Sigma}\\right|^{-(\\nu_{\\scriptscriptstyle \\mathcal{P}}+d+1)/2}\n\\exp\\left\\{ -\\frac{1}{2}\n            \\mathop{\\mathrm{tr}}\\left(\\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\Lambda}_{\\scriptscriptstyle \\mathcal{P}}^{-1}\n               \\right)\n    \\right\\},\n\\label{eqn:multivariatePriorVar}\n\\end{align}\\] with the hyperparameters \\(\\nu_{\\scriptscriptstyle \\mathcal{P}}\\) and the matrix \\(\\boldsymbol{\\Lambda}_{\\scriptscriptstyle \\mathcal{P}}\\) controlling the degrees of freedom and scale of the prior distribution, respectively.\n\nAdding a prior to GMMs estimated using the mclust R package is easily obtained by adding an optional prior argument when calling some of the fitting functions, such as mclustBIC() and Mclust(). Specifically, setting prior = priorControl(functionName = \"defaultPrior\") allows to adopt the conjugate priors described above with the following default values for the hyperparameters:\n\nmean vector \\(\\boldsymbol{\\mu}_{\\scriptscriptstyle \\mathcal{P}}= \\bar{\\boldsymbol{x}}\\), the sample mean of each variable;\nshrinkage \\(\\kappa_{\\scriptscriptstyle \\mathcal{P}}= 0.1\\);\ndegrees of freedom \\(\\nu_{\\scriptscriptstyle \\mathcal{P}}= d+2\\);\nscale matrix \\(\\boldsymbol{\\Lambda}_{\\scriptscriptstyle \\mathcal{P}}= \\boldsymbol{S}/(K^{2/d})\\), where \\(\\boldsymbol{S}\\) is the sample covariance matrix.\n\nRationale for the above default values for the prior hyperparameters, together with the corresponding MAP estimates of the GMM parameters, can be found in [36, Table 2]. These values should suffice for most applications, but experienced users who want to tune the hyperparameters can refer to the documentation available in the help pages for priorControl and defaultPrior(). Further details about specifying different hyperparameter values can be found in [30].\n\n\n3.3.2 Bootstrap inference\nLikelihood-based inference in mixture models is complicated because asymptotic theory applied to mixture models require a very large sample size [19], and standard errors derived from the expected or the observed information matrix tend to be unstable [37]. For these reasons, resampling approaches based on the bootstrap are often employed [38].\nThe bootstrap [39] is a general, widely applicable, powerful technique for obtaining an approximation to the sampling distribution of a statistic of interest. The bootstrap distribution is approximated by drawing a large number of samples, called bootstrap samples, from the empirical distribution. This can be obtained by resampling with replacement from the observed data (nonparametric bootstrap), or from a parametric distribution with unknown parameters substituted by the corresponding estimates (parametric bootstrap). A Bayesian version of the bootstrap, introduced by [40], allows posterior samples to be obtained by resampling with weights for each observation drawn from a uniform Dirichlet distribution. A strictly related technique is the weighted likelihood bootstrap [41], where a statistical model is repeatedly fitted using weighted maximum likelihood with weights obtained as in Bayesian bootstrap.\nLet \\(\\widehat{\\boldsymbol{\\theta}}\\) be the estimate of a set of GMM parameters \\(\\boldsymbol{\\theta}\\) for a given model \\(\\mathcal{M}\\), determined by the adopted covariance parameterisation and number of mixture components. The bootstrap distribution for the parameters of interest is obtained as follows:\n\ndraw a bootstrap sample of size \\(n\\) using one of the resampling techniques described above to form the bootstrap sample \\((\\boldsymbol{x}^\\star_1, \\ldots, \\boldsymbol{x}^\\star_n)\\);\nfit a the GMM \\(\\mathcal{M}\\) to get the bootstrap estimates \\(\\widehat{\\boldsymbol{\\theta}}^\\star\\);\nreplicate the previous steps a large number of times, say \\(B\\).\n\nThe bootstrap distribution for the parameters of interest, \\(\\widehat{\\boldsymbol{\\theta}}^\\star_1, \\widehat{\\boldsymbol{\\theta}}^\\star_2, \\ldots, \\widehat{\\boldsymbol{\\theta}}^\\star_B\\), can then be used for computing the bootstrap standard errors (as the square root of the diagonal elements of the bootstrap covariance matrix) or the bootstrap percentile confidence intervals. More details can be found in [30].\nFrom a practical point of view, bootstrap resampling can be conducted in mclust by means of the function MclustBootstrap(). This function takes as arguments the fitted model object returned from e.g., Mclust() or mclustBIC(), the optional argument type, which allows to specify the type of bootstrap samples to draw (\"bs\" for nonparametric bootstrap, \"pb\" for parametric bootstrap, and \"wlbs\" for weighted likelihood bootstrap), and the optional argument nboot, which sets the number of bootstrap samples. At least \\(999\\) samples should be drawn if confidence intervals are needed.\n\n\n3.3.3 Entropy and average posterior probabilities\nThe definition of entropy in information theory [42] refers to the average amount of information provided by a random variable. Following this definition, [43] defines the entropy of a finite mixture model as follows \\[\nE_{\\text{FMM}} = - \\sum_{i=1}^n \\sum_{k=1}^K \\widehat{p}_{ik} \\log(\\widehat{p}_{ik}),\n\\] where \\(\\widehat{p}_{ik}\\) is the estimated posterior probability of case \\(i\\) to belong to cluster \\(k\\) (see Equation 9.4). If the mixture components are well separated, \\(\\widehat{p}_{ik} \\approx 1\\) for the assigned cluster \\(\\mathcal{C}_k\\) and \\(0\\) otherwise. Consequently, the entropy of the mixture model in this case is \\(E_{\\text{FMM}} = 0\\) (note that \\(0\\log(0)=0\\) by convention). On the contrary, in the case of maximal assignment uncertainty, \\(\\widehat{p}_{ik} = 1/K\\) for all clusters \\(\\mathcal{C}_k\\) (\\(k=1,\\ldots,K\\)). As a result, the entropy of the mixture model is \\(E_{\\text{FMM}} = n\\log(K)\\).\nIn latent class and latent profile analysis, a slightly different definition of entropy is used as a diagnostic statistic to assess how well the fitted model assigns individuals to the identified clusters based on their response patterns. Thus, a normalised version of the entropy is defined as follows \\[\nE = 1 - \\frac{E_{\\text{FMM}}}{n \\log(K)} = 1 + \\dfrac{\\sum_{i=1}^n \\sum_{k=1}^K \\widehat{p}_{ik} \\log(\\widehat{p}_{ik})}{n \\log(K)}.\n\\] Entropy takes values on the range \\([0,1]\\), with higher entropy values indicating that the model has less uncertainty in assigning cases to their respective latent classes/profiles. Thus, high entropy values typically indicate a better model which is able to distinguish between the latent components and that the components are relatively distinct. An entropy value close to 1 is ideal, while values above \\(0.6\\) are considered acceptable, although there is no agreed upon optimal cutoff for entropy.\nThe contribution of each observation to the overall total entropy can be defined as \\[\nE_i = 1 + \\frac{\\sum_{k=1}^K \\widehat{p}_{ik} \\log(\\widehat{p}_{ik})}{\\log(K)},\n\\] so that the overall total entropy is obtained by averaging over the individual contributions, i.e., \\(E = \\sum_{i=1}^n E_i/n\\). The individual contributions \\(E_i\\) can also be used to compute the average entropy of each latent component, which indicates how accurately the model defines components. Average posterior probabilities (AvePP) are a closely related performance assessment measure, given by the average posterior membership probabilities \\(\\widehat{p}_{ik}\\) for each component for the observations most probably assigned to that component, for which a cutoff of \\(0.8\\) has been suggested to indicate acceptably high assignment certainty and well-separated clusters [34]. The analysis below presents the necessary code to calculate entropies and average posterior probabilities thusly from a fitted mclust model."
  },
  {
    "objectID": "chapters/ch09-model-based-clustering/ch9-model.html#application-school-engagement-academic-achievement-and-self-regulated-learning",
    "href": "chapters/ch09-model-based-clustering/ch9-model.html#application-school-engagement-academic-achievement-and-self-regulated-learning",
    "title": "9  An Introduction and R Tutorial to Model-based Clustering in Education via Latent Profile Analysis",
    "section": "4 Application: School engagement, academic achievement, and self-regulated learning",
    "text": "4 Application: School engagement, academic achievement, and self-regulated learning\nA group of 717 primary school students from northern Spain were evaluated in terms of their school engagement, self-regulation, and academic performance through the use of various measures. The school engagement measure (SEM) was employed to assess their engagement, while their self-regulation was evaluated with the self-regulation strategy inventory—self-report. The measure for academic achievement was based on the students’ self-reported grades in Spanish and mathematics, which were rated on a scale of 1 to 5. This data set can be used to identify clusters of students based on their engagement and self-regulation. These clusters would represent distinct patterns or “profiles” of engagement. Finding such profiles would allow us to understand individual differences but more importantly to stratify support according to different engagement profiles.\n\n4.1 Preparing the data\nWe start by loading the packages required for the analysis. We note in particular that version 6.0.0 of mclust is employed here, the latest release at the time of writing.\n\nlibrary(ggplot2)\nlibrary(ggridges)\nlibrary(mclust)\nlibrary(rio)\nlibrary(tidyverse)\n\nThen, we read the data set from an online comma-separated-value (CSV) file, followed by some data cleaning and formatting to prepare the data for subsequent analysis. Note that the CSV file to be read is not in standard format, so we have to explicitly set the separator field using the optional argument sep = \";\".\n\n\n\n\n\n# read the data\ndata <- import(\"https://github.com/lamethods/data/raw/main/3_engSRLach/\n               Manuscript_School%20Engagment.csv\", \n               sep = \";\")\n\n\n# select the variables to be analyzed\nvars <- c(\"PRE_ENG_COND\", \"PRE_ENG_COGN\", \"PRE_ENG_EMOC\")\nx <- select(data, all_of(vars)) |> \n  as_tibble() |>\n  rename(\"BehvEngmnt\" = \"PRE_ENG_COND\",  # Behavioral engagement\n         \"CognEngmnt\" = \"PRE_ENG_COGN\",  # Cognitive engagement\n         \"EmotEngmnt\" = \"PRE_ENG_EMOC\")  # Emotional engagement\n\n# print the data set used in the subsequent analysis\nx\n\n\n\n  \n\n\n\nA table of summary statistics for the data set can be obtained as follows:\n\nx |> pivot_longer(cols = colnames(x),\n                  names_to = \"Variable\",\n                  values_to = \"Value\") |>\n  group_by(Variable) |>\n  summarize(N = n(),\n            Nunq = n_distinct(Value),\n            Mean = mean(Value),\n            SD = sd(Value),\n            Min = min(Value),\n            Median = median(Value),\n            Max = max(Value))\n\n\n\n  \n\n\n\n\n\n4.2 Model estimation and model selection\nTo begin our latent profile analysis, we first fit a number of candidate GMMs with different numbers of latent components and covariance parameritations, and compute the Bayesian Information Criterion (BIC) to select the “optimal” model. This model selection criterion jointly takes into account both the covariance decompositions and the number of mixture components in the model.\nAs mentioned earlier, given the characteristics of the data, which consists of a small number of unique values relative to the number of observations, a prior is used for regularisation. We invoke the default priors described above, summarise the BIC values of the three best models, and visualise the BIC values of all fitted models.\n\nBIC <- mclustBIC(x, prior = priorControl())\nsummary(BIC)\n## Best BIC values:\n##              VVI,3        VVI,4       VVV,3\n## BIC      -4521.213 -4526.905884 -4533.57166\n## BIC diff     0.000    -5.693183   -12.35896\nplot(BIC)\n\n\n\n\nBIC traces for the estimated GMMs with default priors.\n\n\n\n\nThe selected model is a three-component GMM with diagonal covariance matrices of varying volume and shape, with axis-aligned orientation, indicated as (VVI,3). Thus, the variables are independent within each cluster.\n\n\n4.3 Examining model output\nThe fit of the optimal model is obtained using:\n\nmod <- Mclust(x, modelNames = \"VVI\", G = 3, prior = priorControl())\nsummary(mod, parameters = TRUE)\n## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust VVI (diagonal, varying volume and shape) model with 3 components: \n## \n## Prior: defaultPrior() \n## \n##  log-likelihood   n df       BIC       ICL\n##       -2194.856 717 20 -4521.213 -4769.174\n## \n## Clustering table:\n##   1   2   3 \n## 184 119 414 \n## \n## Mixing probabilities:\n##         1         2         3 \n## 0.2895147 0.1620776 0.5484078 \n## \n## Means:\n##                [,1]     [,2]     [,3]\n## BehvEngmnt 3.704041 4.713234 4.257355\n## CognEngmnt 2.287057 3.699530 3.017293\n## EmotEngmnt 2.738969 4.733899 3.737286\n## \n## Variances:\n## [,,1]\n##            BehvEngmnt CognEngmnt EmotEngmnt\n## BehvEngmnt  0.5022148  0.0000000  0.0000000\n## CognEngmnt  0.0000000  0.3909235  0.0000000\n## EmotEngmnt  0.0000000  0.0000000  0.7674268\n## [,,2]\n##            BehvEngmnt CognEngmnt EmotEngmnt\n## BehvEngmnt  0.0737948  0.0000000 0.00000000\n## CognEngmnt  0.0000000  0.4150514 0.00000000\n## EmotEngmnt  0.0000000  0.0000000 0.05540526\n## [,,3]\n##            BehvEngmnt CognEngmnt EmotEngmnt\n## BehvEngmnt  0.2048374  0.0000000  0.0000000\n## CognEngmnt  0.0000000  0.3327557  0.0000000\n## EmotEngmnt  0.0000000  0.0000000  0.2795838\n\nThe shown output reports some basic information about the fit, such as the maximised log-likelihood (log-likelihood), the number of observations (n), the number of estimated parameters (df), the BIC criterion (BIC), and the clustering table based on the MAP classification. The latter indicates that the clusters also vary in terms of size. The optional argument parameters = TRUE in the summary() function call additionally prints the estimated parameters. Observe that the VVI model allows variance to vary across components while fixing all covariance parameters to zero.\nA plot showing the classification provided by the estimated model can be drawn as follows:\n\nplot(mod, what = \"classification\") \n\n\n\n\nScatterplot matrix of engagement features with data points marked and coloured by the identified clusters, and ellipses corresponding to projections of the estimated cluster covariances.\n\n\n\n\nThe estimated model identifies three clusters of varying size. The third group (shown as filled green triangles) accounts for more than 50% of the observations, while the first (shown as blue filled points) and the second (shown as red open squares) account for approximately 29% and 16%, respectively. The smallest cluster is also the group with the largest engagement scores.\nThe different engagement behaviour of the three identified clusters can be shown using a latent profiles plot of the estimated means with point sizes proportional to the estimated mixing probabilities (see Figure 9.1):\n\n# collect estimated means\nmeans <- data.frame(Profile = factor(1:mod$G),\n                    t(mod$parameters$mean)) |>\n  pivot_longer(cols = -1,\n               names_to = \"Variable\",\n               values_to = \"Mean\")\n\n# convert variable names to factor\nmeans$Variable <- factor(means$Variable, \n                         levels = colnames(mod$data))\n\n# add mixing probabilities corresponding to profiles\nmeans <- means |> \n  add_column(MixPro = mod$parameters$pro[means$Profile])\nmeans\n\n\n\n\n\n\nFigure 1. Latent profiles plot showing estimated means with point sizes proportional to estimated mixing probabilities.\n\n\n\n# plot the means of the latent profiles\nggplot(means, aes(x = Variable, y = Mean,\n                  group = Profile, \n                  shape = Profile, \n                  color = Profile)) +\n  geom_point(aes(size = MixPro)) +\n  geom_line(linewidth = 0.5) +\n  labs(x = NULL, y = \"Latent profiles means\") +\n  scale_color_manual(values = mclust.options(\"classPlotColors\")) +\n  scale_size(range = c(1, 3), guide = \"none\") +\n  theme_bw() +\n  theme(legend.position = \"top\")\n\n\n\n\nFigure 2. Latent profiles plot showing estimated means with point sizes proportional to estimated mixing probabilities.\n\n\n\n\nThe smallest cluster (Profile 2) has the highest engagement scores for all three variables. All three scores are lower for the largest cluster (Profile 3), which are all in turn lower for Profile 1. All three profiles exhibit the lowest mean scores for the cognitive engagement attribute. For Profile 2, behavioural engagement and emotional engagement scores are comparable, whereas for the other two profiles, the mean scores for this attribute are lower than those for the behaviour engagement attribute. Taken together, we could characterise profiles 1, 3, and 2 as “low”, “medium”, and “high” engagement profiles, respectively.\nTo provide a more comprehensive understanding of the results presented in the previous graph, it would be beneficial to incorporate a measure of uncertainty for the estimated means of the latent profiles. This can be achieved by resampling using the function MclustBootstrap() as described above:\n\nboot <- MclustBootstrap(mod, type = \"bs\", nboot = 999)\n\nThe bootstrap distribution of the mixing weights can be visualised using histograms with the code\n\npar(mfcol = c(1, 3), mar = c(2, 4, 1, 1), mgp = c(2, 0.5, 0))\nplot(boot, what = \"pro\", xlim = c(0, 1))\n\n\n\n\nFigure 3. Bootstrap distribution of GMM mixture weights.\n\n\n\n\nwhile the bootstrap distribution of the components means can be plotted with the code\n\npar(mfcol = c(3, 3), mar = c(2, 4, 1, 1), mgp = c(2, 0.5, 0))\nplot(boot, what = \"mean\", conf.level = 0.95)\n\n\n\n\nFigure 4. Bootstrap distribution of GMM component means.\n\n\n\n\nThe resulting graphs are reported in Figure 9.3 and Figure 9.4, where the GMM estimates are shown as dashed vertical lines, while the horizontal segments represent the percentile confidence intervals at the 95% confidence level.\nNumerical output of the resampling-based bootstrap distributions is given by:\n\nsboot <- summary(boot, what = \"ci\")\nsboot\n## ---------------------------------------------------------- \n## Resampling confidence intervals \n## ---------------------------------------------------------- \n## Model                      = VVI \n## Num. of mixture components = 3 \n## Replications               = 999 \n## Type                       = nonparametric bootstrap \n## Confidence level           = 0.95 \n## \n## Mixing probabilities:\n##               1          2         3\n## 2.5%  0.1275702 0.09255033 0.4761326\n## 97.5% 0.3872065 0.23592891 0.6845753\n## \n## Means:\n## [,,1]\n##       BehvEngmnt CognEngmnt EmotEngmnt\n## 2.5%    3.479985   1.872176   2.082263\n## 97.5%   3.822700   2.441357   2.942343\n## [,,2]\n##       BehvEngmnt CognEngmnt EmotEngmnt\n## 2.5%    4.628745   3.532034   4.527568\n## 97.5%   4.898591   3.929510   4.878574\n## [,,3]\n##       BehvEngmnt CognEngmnt EmotEngmnt\n## 2.5%    4.097707   2.836241   3.533040\n## 97.5%   4.366235   3.145216   3.887315\n## \n## Variances:\n## [,,1]\n##       BehvEngmnt CognEngmnt EmotEngmnt\n## 2.5%   0.4090113  0.2286101  0.5467998\n## 97.5%  0.7845491  0.4913383  0.9885883\n## [,,2]\n##       BehvEngmnt CognEngmnt EmotEngmnt\n## 2.5%   0.0159611  0.3256681 0.01817132\n## 97.5%  0.1042114  0.5728663 0.14713752\n## [,,3]\n##       BehvEngmnt CognEngmnt EmotEngmnt\n## 2.5%   0.1587247  0.2842579  0.2297413\n## 97.5%  0.2788433  0.3974127  0.4077299\n\nThe information above can then be used to plot the latent profile means accompanied by 95% confidence intervals represented as vertical bars, as illustrated in Figure 9.5. The confidence intervals for the cognitive and emotional engagement attributes are noticeably wider for the “low” engagement profile.\n\nmeans <- means |> \n  add_column(lower = as.vector(sboot$mean[1,,]),\n             upper = as.vector(sboot$mean[2,,]))\nmeans\n\n\n\n\n\n\nFigure 5. Latent profiles plot showing estimated means with 95% bootstrap confidence intervals.\n\n\n\nggplot(means, aes(x = Variable, y = Mean, group = Profile, \n                  shape = Profile, color = Profile)) +\n  geom_point(aes(size = MixPro)) +\n  geom_line(linewidth = 0.5) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), \n                linewidth = 0.5, width = 0.1) +\n  labs(x = NULL, y = \"Latent profiles means\") +\n  scale_color_manual(values = mclust.options(\"classPlotColors\")) +\n  scale_size(range = c(1, 3), guide = \"none\") +\n  theme_bw() +\n  theme(legend.position = \"top\")\n\n\n\n\nFigure 6. Latent profiles plot showing estimated means with 95% bootstrap confidence intervals.\n\n\n\n\nFinally, the entropy of the estimated partition, average entropy of each latent component, and average posterior probabilities are obtained via:\n\nprobs <- mod$z                    # posterior conditional probs\nprobs_map <- apply(probs, 1, max) # maximum a posteriori probs\nclusters <- mod$classification    # cluster assignment for each obs\nn <- mod$n                        # number of obs\nK <- mod$G                        # number of latent profiles\n\n# Entropy:\nE <- 1 + sum(probs * log(probs))/(n * log(K))\nE\n## [1] 0.6890602\n\n# Case-specific entropy contributions:\nEi <- 1 + rowSums(probs * log(probs))/log(K)\nsum(Ei)/n\n## [1] 0.6890602\n\ndf_entropy  <- data.frame(clusters = as.factor(clusters), entropy = Ei)\n\n\ndf_entropy |>\n  group_by(clusters) |>\n  summarise(count = n(),\n            mean = mean(entropy),\n            sd = sd(entropy),\n            min = min(entropy),\n            max = max(entropy))\n\n\n\n\n\n\nFigure 7. Entropy contributions by cluster and total entropy (dashed line).\n\n\n\nggplot(df_entropy, aes(y = clusters, x = entropy,  fill = clusters)) +\n  geom_density_ridges(stat = \"binline\", bins = 21,\n                      scale = 0.9, alpha = 0.5) +\n  scale_x_continuous(breaks = seq(0, 1 ,by=0.1), \n                     limits = c(0, 1.05)) +\n  scale_fill_manual(values = mclust.options(\"classPlotColors\")) +\n  geom_vline(xintercept = E, lty = 2) +\n  labs(x = \"Case-specific entropy contribution\", \n       y = \"Latent profile\") +\n  theme_ridges(center_axis_labels = TRUE) +\n  theme(legend.position = \"none\",\n        panel.spacing = unit(1, \"lines\"),\n        strip.text.x = element_text(size = 8))\n\n\n\n\nFigure 8. Entropy contributions by cluster and total entropy (dashed line).\n\n\n\n\nWe note that, as shown in Figure 9.7 and Figure 9.9, all entropy and AvePP quantities appear satisfactory from the point of view of indicating reasonably well-separated clusters.\n\n# Average posterior probabilities by cluster:\ndf_AvePP <- data.frame(clusters = as.factor(clusters), pp = probs_map)\n\ndf_AvePP |>\n  group_by(clusters) |>\n  summarise(count = n(),\n            mean = mean(pp),\n            sd = sd(pp),\n            min = min(pp),\n            max = max(pp))\n\n\n\n\n\n\nFigure 9. Average posterior probabilities by cluster.\n\n\n\nggplot(df_AvePP, aes(y = clusters, x = pp,  fill = clusters)) +\n  geom_density_ridges(stat = \"binline\", bins = 21,\n                      scale = 0.9, alpha = 0.5) +\n  scale_x_continuous(breaks = seq(0, 1, by=0.1), \n                     limits = c(0, 1.05)) +\n  scale_fill_manual(values = mclust.options(\"classPlotColors\")) +\n  labs(x = \"MAP probabilities\", y = \"Latent profile\") +\n  theme_ridges(center_axis_labels = TRUE) +\n  theme(legend.position = \"none\",\n        panel.spacing = unit(1, \"lines\"),\n        strip.text.x = element_text(size = 8)) \n\n\n\n\nFigure 10. Average posterior probabilities by cluster."
  },
  {
    "objectID": "chapters/ch09-model-based-clustering/ch9-model.html#discussion",
    "href": "chapters/ch09-model-based-clustering/ch9-model.html#discussion",
    "title": "9  An Introduction and R Tutorial to Model-based Clustering in Education via Latent Profile Analysis",
    "section": "5 Discussion",
    "text": "5 Discussion\nUsing a person-centered method (finite Gaussian mixture model), the present analysis uncovered the heterogeneity within the SEM engagement data by identifying three latent or unobserved clusters: low, medium, and high engagement clusters. Uncovering the latent structure could help understand individual differences among students, identify the complex multidimensional variability of a construct —engagement in our case— and possibly help personalise teaching and learning. Several studies have revealed similar patterns of engagement which —similar to the current analysis— comprise three levels that can be roughly summarised as high, moderate, and low [9, 44, 45]. The heterogeneity of engagement has been demonstrated in longitudinal studies, in both face-to-face settings as well as online engagement [9]. Furthermore, the association between engagement and performance has been demonstrated to vary by achievement level, time of the year, as well as engagement state; that is, high achievers may at some point in their program descend to lower engagement states and still continue to have higher achievement [3]. Such patterns, variability, and individual differences are not limited to engagement, but has been reported for almost every major disposition in education psychology [2].\nOn a general level, heterogeneity has been a hot topic in recent educational literature. Several calls have been voiced to adopt methods that capture different patterns or subgroups within students’ behavior or functioning. Assuming that there is “an average” pattern that represents the entirety of student populations requires the measured construct to have the same causal mechanism, same development pattern, and affect students in exactly the same way. The average assumption is of course impossible and has been proven inaccurate across a vast number of studies (e.g., [2] and [4]). Since heterogeneity is prevalent in psychological, behavioral, and physiological human data, person-centered methods will remain a very important tool for researchers [46].\nPerson-centered methods can be grouped into algorithmic clustering methods on one hand and the model-based clustering paradigm on the other, with the former being more traditional and the latter being more novel in the learning analytics literature. The analysis of the SEM data centered here on the model-based approach, specifically the finite Gaussian mixture model framework. The mclust package enabled such models to be easily fitted and this framework exhibits many advantages over traditional clustering algorithms which rely on dissimilarity-based heuristics. Firstly, the likelihood-based underpinnings enable the selection of the optimal model using principled statistical model selection criteria. In particular, it is noteworthy in the present analysis that the model selection procedure was not limited to three-cluster solutions: mixtures with fewer or greater than three clusters were evaluated and the three-cluster solution —supported by previous studies in education research— was identified as optimal according to the BIC. Secondly, the parsimonious modelling of the covariance structures provides the flexibility to model clusters with different geometric characteristics. In particular, the clusters in the present analysis, whereby each group is described by a single Gaussian component with varying volume and shape, but the same orientation aligned with the coordinate axes are more flexible than the spherical, Euclidean distance-based clusters obtainable under the \\(k\\)-means algorithm. Thirdly, the models relax the assumption that each observation is associated with exactly one cluster and yields informative cluster-membership probabilities for each observation, which can be used to compute useful diagnostics such as entropies and average posterior probabilities which are unavailable under so-called “hard” clustering frameworks. Finally, the mclust package facilitates simple summaries and visualisations of the resulting clusters and cluster-specific parameter estimates.\nThat being said, there are a number of methodological limitations of the GMM framework to be aware of in other settings. Firstly, and most obviously, such models are inappropriate for clustering categorical or mixed-type variables. For clustering longitudinal categorical sequences, such as those in Chapter 10 [47], model-based approaches are provided by the mixtures of exponential-distance models framework of [48] (and related MEDseq R package) and the mixtures of hidden Markov models framework of [49] (and related seqHMM package; see Chapter 12 [50]). Regarding mixed-type variables, [51] provide a model-based framework (and the related clustMD package).\nSecondly, the one-to-one correspondence typically assumed between component distributions and clusters is not always the case [52]. This is only true if the underlying true component densities are Gaussian. When the assumption of component-wise normality is not satisfied, the performance of such models will deteriorate as more components are required to fit the data well. However, even for continuous data, GMMs tend to overestimate the number of clusters when the assumption of normality is violated. Two strategies for dealing with this are provided by the mclust package, one based on combining Gaussian mixture components according to an entropy criterion, and one based on a adding a so-called “noise component” —represented by a uniform distribution— to the mixture. The noise component captures outliers which do not fit the prevailing patterns of Gaussian clusters, which would otherwise be assigned to (possibly many) small clusters and minimises their deleterious effect on parameter estimation for the other, more defined clusters. Further details of combining components and adding a noise component can be found in [30]. Alternatively, mixture models which depart from normality have been an active area of research in model-based clustering in recent years. Such approaches —some of which are available in the R package mixture [53]— replace the underlying Gaussian component distributions with e.g., generalised hyperbolic distributions, the multivariate \\(t\\) distribution, and the multivarate skew-\\(t\\) distribution.\nA third main limitation of GMMs is their ineffectiveness in high-dimensional settings, when the data dimension \\(d\\) is comparable to or even greater than \\(n\\). Among the 14 parsimonious parameterisations available in mclust, only models with diagonal covariance structures are tractable when \\(n \\le p\\). Incorporating factor-analytic covariance decompositions in so-called finite Gaussian mixtures of factor analysers have been proposed for addressing this issue [54, 55]. Imposing constraints on the parameters of such factor-analytic structures in the component covariance matrices in the spirit of mclust leads to another family of parsimonious Gaussian mixture models [56], which are implemented in the R package pgmm. Model selection becomes increasingly difficult with such models, given the need to choose both the optimal number of mixture components and the optimal number of latent factors (as well as the covariance parameterisation, in the case of pgmm). Infinite mixtures of infinite factor analysers —implemented in the R package IMIFA— are a recent, Bayesian extension which enable automatic inference of the number of components and the numbers of cluster-specific latent factors [57].\nAnother recent extension, building directly on the 14 models from mclust, is the MoEClust model family of [58] and the associated MoEClust R package, which closely mimics its syntax. MoEClust effectively embeds Gaussian parsimonious clustering models in the mixtures of experts framework, enabling additional sources of heterogeneity in the form of covariates to be incorporated directly in the clustering model, to guide the construction of the clusters. Either, neither, or both the mixing proportions and/or component mean parameters can be modelled as functions of these covariates. The former is perhaps particularly appealing, given its analogous equivalence to latent profile regression [59]. Hypothetically, assuming information on the gender and age of the students in the present analysis was available, such covariates would influence the probabilities of cluster membership under such a model, while the correspondence thereafter between the parameters of the component distributions and the clusters would have the same interpretation as per standard LPA models."
  },
  {
    "objectID": "chapters/ch10-sequence-analysis/ch10-seq.html",
    "href": "chapters/ch10-sequence-analysis/ch10-seq.html",
    "title": "10  Sequence Analysis in Education: Principles, Technique, and Tutorial with R",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch10-sequence-analysis/ch10-seq.html#introduction",
    "href": "chapters/ch10-sequence-analysis/ch10-seq.html#introduction",
    "title": "10  Sequence Analysis in Education: Principles, Technique, and Tutorial with R",
    "section": "1 Introduction",
    "text": "1 Introduction\nPatterns exist everywhere in our life, from the sequence of genes to the order of steps in cooking recipes. Discovering patterns, variations, regularities, or irregularities is at the heart of scientific inquiry and, therefore, several data mining methods have been developed to understand patterns. Sequence analysis —or sequence mining— was developed almost four decades ago to address the increasing needs for pattern mining [1]. Ever since, a wealth of applications, algorithms, and statistical tools have been developed, adapted, or incorporated into the array of sequence analysis. Since sequence mining has been conceptualized, it has grown in scale of adoption and range of applications across life and social sciences [5] and education research was no exception (e.g., [7]). As a data mining technique, sequence mining has been commonly implemented to identify hidden patterns that would otherwise be missed using other analytical techniques and find interesting subsequences (parts of the sequence) that have practical significance or unexpected sequences that we did not know existed [9]. For instance, by mining sequences of collaborative dialogue, we could identify which sequences are followed by more conductive to more argumentative interactions, and what sequences are crucial to the collaborative process. A literature review of the common applications follows in the next section.\nLearning is a process that unfolds in time, a process that occurs in sequences of actions, in repeated steps, in patterns that have meanings and value for understanding learners’ behavior [10]. The conceptualization of learning as a process entails two important criteria: process as a sequence of states that unfold in time and process as a transformative mechanism that drives the change from one state to another [11]. Thereupon, methods such as sequence mining have gained increasing grounds and amassed a widening repertoire of techniques in the field of education to study the learning process. In particular, sequence mining has been used to harness the temporal unfolding of learners’ behavior using digital data, transcripts of conversations, or behavioral states [12]. Nevertheless, sequence mining can be used to study non-temporal sequences such as protein sequences and other types of categorical data [4].\nWhat makes sequences in education interesting is that they have patterns of repeated or recurrent sequences. Finding such patterns has helped typify learners’ behaviors and identify which patterns are associated with learning and which are associated with unfavorable outcomes [7]. Sequence mining can also describe a pathway or a trajectory of events, for example, how a student proceeds from enrolment to graduation [14], and help to identify the students who have a stable trajectory, who are turbulent, and who are likely to falter along their education [7]."
  },
  {
    "objectID": "chapters/ch10-sequence-analysis/ch10-seq.html#review-of-the-literature",
    "href": "chapters/ch10-sequence-analysis/ch10-seq.html#review-of-the-literature",
    "title": "10  Sequence Analysis in Education: Principles, Technique, and Tutorial with R",
    "section": "2 Review of the literature",
    "text": "2 Review of the literature\nIn recent years, sequence analysis has become a central method in learning analytics research due to its potential to summarize and visually represent large amounts of student-related data. In this section we provide an overview of some of the most representative studies in the published literature. A summary of the studies reviewed in this section can be seen in Table 10.1. A common application of sequence analysis is the study of students’ log data extracted from their interactions with online learning technologies (mosty learning management systems, LMSs) throughout a learning session [15–17]. In some studies, the session is well-delimited, such as the duration of a game [18] or moving window [19], but in most cases it is inferred from the data, considering a session as an uninterrupted sequence of events [15, 16]. Few are the studies in which longer sequences are studied, covering a whole course or even a whole study program [7, 14, 20]. In such studies, the sequences are not composed of instantaneous interactions but rather of states that aggregate students’ information over a certain period, for example, we can study students’ engagement [14], learning strategies [7], or collaboration roles [20] for each course in a study program.\nMost of the existing research has used clustering techniques to identify distinct groups of similar sequences. Agglomerative Hierarchical Clustering (AHC) has been the most recurrently used technique, with a wealth of distance measures such as Euclidean [7, 15], Longest Common Subsequence [18], Longest Common Prefix [21], and Optimal Matching [16]. Other works have relied on Markovian Models [13, 20] or differential sequence mining [6]. Throughout the remainder of the book, we provide an introduction to sequence analysis as a method, describing in detail the most relevant concepts for its application to educational data. We provide a step-by-step tutorial of how to implement sequence analysis in a data set of student log data using the R programming language.\n\n\nTable 1. Summary of the reviewed articles about sequence analysis in learning analytics\n\n\n\n\n\n\n\n\n\n\nRef.\nContext\nTime scheme\nActor\nAlphabet\nClustering algorithm\n\n\n\n\n[6]\n40 students\nLearning activity (5 days)\nStudent\nLMS events\nDifferential sequence mining (core algorithm)\n\n\n[19]\n1 middle school class (40 students)\nLearning activity (5 days)\nStudent\nLMS events (e.g., Read, Linkadd)\nDifferential sequence mining (core algorithm, SPAMc)\n\n\n[16]\n1 university course (290 students)\nSession\nStudent-session\nLMS events\nAHC (Optimal matching)\n\n\n\n\nCourse\nStudent-course\nTactics obtained from previous clustering\nAHC (Euclidean distance)\n\n\n[15]\n3 courses: one university course with 3 course offerings (1135 students), another university course with 2 course offerings (487 students), and a MOOC with a single offering (368 students)\nSession\nStudent-session\nLMS events (e.g., content_access, download)\nFirst Order Markov Model\n\n\n\n\nCourse\nStudent-course\nTactics obtained from previous clustering\nAHC (Euclidean distance)\n\n\n[14]\n15 university courses (106 students)\nStudy program (15 courses)\nStudent\nEngagement state (e.g., Active, Average)\nHidden Markov Models\n\n\n[18]\n1 educational escape room game in a university course (96 students)\nEscape room game (1h 45m)\nTeam\nGame activity (e.g., hint obtained, puzzle solving)\nAHC (Longest Common Subsequence)\n\n\n[20]\n10 university courses (329 students)\nStudy program (10 courses)\nStudent\nRoles in the group (e.g., Leaders, Mediators)\nMixture Hidden Markov Models\n\n\n[7]\n10 university courses (135 students)\nSession\nStudent-session\nLMS event (e.g., Course main view, Forum consume)\nMixture Hidden Markov Models\n\n\n\n\nCourse\nStudent-course\nTactics obtained from previous clustering (e.g., Lecture read, Forum read)\nAHC (Euclidean distance)\n\n\n\n\nStudy program (10 courses)\nStudent\nCourse-level strategies from previous clustering (e.g., Light interactive, Moderate interactive)\nAHC (Euclidean distance)\n\n\n[21]\n1 university courses, 4 Course offerings (200 students)\nWeek\nGroup of students\nInteraction type on forum (e.g., Discuss, Argue)\n\n\n\n\n\nSession\nStudent-session\nInteraction type on forum (e.g., Discuss, Argue)\nAHC (Longest Common Prefix)"
  },
  {
    "objectID": "chapters/ch10-sequence-analysis/ch10-seq.html#basics-of-sequences",
    "href": "chapters/ch10-sequence-analysis/ch10-seq.html#basics-of-sequences",
    "title": "10  Sequence Analysis in Education: Principles, Technique, and Tutorial with R",
    "section": "3 Basics of sequences",
    "text": "3 Basics of sequences\nSequences are ordered lists of discrete elements (i.e., events, states or categories). Such elements are discrete (in contrast to numerical values such as grades) and are commonly organized chronologically. Examples include sequence of activities, sequence of learning strategies, or sequence of behavioral states [23]. A sequence of learning activities may include (play video - solve exercise - taking quiz - access instructions) [13], other examples include sequence of game moves e.g., (solve puzzle - request hint - complete game) [18], or collaborative roles, for instance, (leader - mediator - isolate) [20].\nBefore going into sequence analysis, let’s discuss a basic example of a sequence inspired by [14]. Let’s assume we are tracking the engagement states of students from a course to the next and for a full year that has five courses. The engagement states can be either engaged (when the student is fully engaged in their learning), average (when the student is moderately engaged), and disengaged (when the student is barely engaged). Representing the sequence of engagement states of two hypothetical students may look like the example on Table 10.2.\n\n\n\nTable 2. An example sequence\n\n\n\n\n\n\n\n\n\n\nActor\nCourse 1\nCourse 2\nCourse 3\nCourse 4\nCourse 5\n\n\n\n\nStudent 1\nAverage\nEngaged\nEngaged\nEngaged\nEngaged\n\n\nStudent 2\nAverage\nDisengaged\nDisengaged\nDisengaged\nDisengaged\n\n\n\n\n\nThe first student starts in course 1 with an Average engagement state, in Course 2, the student is engaged, and so in all the subsequent courses Course 3, Course 4, and Course 5. The student in row 2 has a Disengaged state in course 2 onwards. As we can see from the two sequences here, there is a pattern that repeats in both sequences (both students stay 4 consecutive courses in the same state). In real-life examples, sequences are typically longer and in larger numbers. For instance, the paper by [14] contains 106 students for a sequence of 15 courses. Finding repeated patterns of engaged states similar to the first student or repeated patterns of disengaged states like the other student would be interesting and helpful to understand how certain subgroups of students proceed in their education and how that relates to their performance.\n\n3.1 Steps of sequence analysis\nSeveral protocols exist for sequence analysis that vary by discipline, research questions, type of data, and software used. In education, sequence analysis protocol usually follows steps that include preparing the data, finding patterns, and relating these patterns to other variables e.g., performance e.g., [16]. The protocol which will be followed in this manual includes six steps:\n\nIdentifying (or coding) the elements of the sequence, commonly referred to as alphabet\nSpecifying the time window or epoch (time scheme) or sequence alignment scheme\nDefining the actor and building the sequence object\nVisualization and descriptive analysis\nFinding similar groups or clusters of sequences,\nAnalyzing the groups and/or using them in subsequent analyses.\n\n\n3.1.1 The alphabet\nThe first step of sequence analysis is defining the alphabet which are the elements or the possible states of the sequence [23]. This process usually entails “recoding” the states to optimize the granularity of the alphabet. In other words, to balance parsimony versus granularity and detail of the data. Some logs are overly detailed and therefore would require a careful recoding by the researcher [4]. For instance, the logs of Moodle (the LMS) include the following log entries for recoding students’ access to the quiz module: quiz_attempt, quiz_continue_attempt, quiz_close_attempt, quiz_view, quiz_view_all, quiz_preview. It makes sense here to aggregate (quiz_attempt, quiz_continue_attempt, quiz_close_attempt) into one category with the label attempt_quiz and (quiz_view, quiz_view all, quiz preview) to a new category with the label view_quiz. Optimizing the alphabet into a reasonable number of states also helps reduce complexity and facilitates interpretation. Of course, caution should be exercised not to aggregate meaningfully distinct states to avoid masking important patterns within the dataset.\n\n\n3.1.2 Specifying the time scheme\nThe second step is to define a time scheme, time epoch or window for the analysis. Sometimes the time window is fairly obvious, for instance, in case a researcher wants to study students’ sequence of courses in a program, the window can be the whole program e.g., [14]. Yet, oftentimes, a decision has to be taken about the time window which might affect the interpretation of the resulting sequences. For example, when a researcher is analyzing the sequence of interactions in a collaborative task, he/she may consider the whole collaborative task as a time window or may opt to choose segments or steps within the task as time epochs. In the same way, analyzing the sequence of tasks in a course, one would consider the whole course to be the time window for analysis or analyze the sequence of steps in each course task e-g., [24].\nIn online learning, the session has been commonly considered the time window e.g., [13, 16]. A session is an uninterrupted sequence of online activity which can be inferred from identifying the periods of inactivity as depicted in Figure 10.1. As can be seen, a user can have multiple sessions across the course. There is no standard guideline for what time window a researcher should consider, however, it is mostly defined by the research questions and the aims of analysis.\n\n\n\nFigure 1. Every line represents a click, a sequence of successive clicks is a session, and the session is defined by the inactivity period\n\n\n\n\n3.1.3 Defining the actor\nThe third important step is to define the actor or the unit of analysis of the sequences (see the actor in Table 10.3 or User in Table 10.4). The actor varies according to the type of analysis. When analyzing students’ sequences of actions, we may choose the student to be the actor and build a sequence of all student actions e.g., [24]. In online learning, sequence mining has always been created for “user sessions” i.e., each user session is represented as a sequence e.g., [13, 25] and therefore, a user typically has several sessions along the course. In other instances, you may be interested in the study of the sequences of the students’ states, for example engagement states in [14] where the student was the actor, or a group of collaborating students’ interactions as a whole such as [21] where the whole group is the actor. In the review of the literature, we have seen examples of such decisions and how varied they can be.\n\n\n3.1.4 Building the sequences\nThis step is specific to the software used. For example, in TraMineR the step includes specifying the dataset on which the building of the sequences is based and telling TraMineR the alphabet, the time scheme, and the actor id variable, as well as other parameters of the sequence object. This step will be discussed in detail in the analysis section.\n\n\n3.1.5 Visualizing and exploring the sequence data\nThe fourth step is to visualize the data and perform some descriptive analysis. Visualization allows us to summarize data easily and to see the full dataset at once. TraMineR includes several functions to plot the common visualization techniques, each one showing a different perspective.\n\n\n3.1.6 Calculating the dissimilarities between sequences\nThe fifth step is calculating dissimilarities or distances between pairs of sequences. Dissimilarity measures are a quantitative estimation of how different —or similar— the sequences are. Since there are diverse contexts, analysis objectives and sequence types, it is natural that there are several methods to compute the dissimilarities based on different considerations.\nOptimal matching (OM) may be the most commonly used dissimilarity measure used in social sciences and possibly also in education [8]. Optimal matching represents what it takes to convert or edit a sequence to become identical to another sequence. These edits may involve insertion, deletion (together often called indel operations) or substitution. For instance, in an example in Table 10.3, where we see a sequence of five students’ engagement states, we can edit Vera’s sequence and substitute the disengaged state with an average state; Vera’s sequence will become identical with Luis’ sequence. That is, editing Vera’s sequence takes one substitution to convert her sequence to that of Luis. We can also see that it will take four substitutions to convert Anna’s sequence to Maria’s sequence. In other words, Anna’s sequence is highly dissimilar to Maria. Different types of substitutions can be given different costs depending on how (dis)similar the two states are viewed (referred to as substitution costs). For example, the cost of substituting state engaged with state average might have a lower cost than substituting engaged with disengaged, since being disengaged is regarded most dissimilar to being engaged while average engagement is more similar to it. Since contexts differ, there are different ways of defining or computing the pairwise substitution costs matrix.\nOptimal matching derives from bioinformatics where transformations such as indels and substitutions are based on actual biological processes such as the evolution of DNA sequences. In many other fields such a transformation process would be unrealistic. In social sciences, [8] outlined five socially meaningful aspects and compared dissimilarity measures to determine how sensitive they are to the different aspects. These similarities are particularly relevant since learning, behavior, and several related processes e.g., progress in school or transition to the labor market are essentially social processes. We explain these aspects based on an example using fictional data in Table 10.3 following [14].\n\n\n\nTable 3. A sequence of engagement states using fictional data\n\n\n\n\n\n\n\n\n\n\nActor\nCourse 1\nCourse 2\nCourse 3\nCourse 4\nCourse 5\n\n\n\n\nMaria\nEngaged\nEngaged\nEngaged\nEngaged\nAverage\n\n\nVera\nDisengaged\nDisengaged\nAverage\nEngaged\nEngaged\n\n\nAnna\nAverage\nDisengaged\nDisengaged\nAverage\nAverage\n\n\nLuis\nDisengaged\nAverage\nAverage\nEngaged\nEngaged\n\n\nBob\nEngaged\nEngaged\nAverage\nEngaged\nEngaged\n\n\n\n\n\n\nExperienced states: how similar are the unique states forming the sequence. For instance, Maria and Bob in Table 10.3 have both experienced the same states (engaged and average).\nDistribution of the states: how similar is the distribution of states. We can consider that two sequences are similar when students spend most of their time in the same states. For instance, Bob and Maria have 80% engaged states and 20% average states.\nTiming: the time when each state occurs. For instance, two sequences can be similar when they have the same states occurring at the same time. For instance, Vera and Luis start similarly in a disengaged state, visit the average state in the middle, and finish in the engaged state.\nDuration: the durations of time spent continuously in a specific state (called spells) e.g., the durations of engaged states shared by the two sequences. For instance, Vera and Anna both had spells of two successive states in the disengaged state while Bob had two separate spells in the engaged state (both of length 2).\nSequencing: The order of different states in the sequence, for instance, Vera and Luis had similar sequences starting as disengaged, moving to average and then finishing as engaged.\n\nOf the aforementioned aspects, the first two can be directly determined from the last three. Different dissimilarity measures are sensitive to different aspects, and it is up to the researcher to decide which aspects are important in their specific context. Dissimilarity measures can be broadly classified in three categories [8]:\n\ndistance between distributions,\ncounting common attributes between sequences, and\nedit distances.\n\nCategory 1 includes measures focusing on the distance between distributions including, e.g., Euclidean distance and \\(\\chi^2\\)-distance that compare the total time spent in each state within each sequence. The former is based on absolute differences in the proportions while the latter is based on weighted squared differences.\nCategory 2 includes measures based on counting common attributes. For example, Hamming distances are based on counting the (possibly weighted) sum of position wise mismatches between the two sequences, the length of the longest common subsequence (LCS) is the number of shared states between two sequences that occur in the same order in both, while the subsequence vector representation -based metric (SVRspell) is counted as the weighted number of matching subsequences.\nCategory 3 includes edit distances that measure the costs of transforming one sequence to another by using edit operations (indels and substitutions). They include (classic) OM with different cost specifications as well as variants of OM such as OM between sequences of spells (OMspell) and OM between sequences of transitions (OMstran).\nStuder and Ritschard [8] give recommendations on the choice of dissimilarity measure based on simulations on data with different aspects. If the interest is on distributions of states within sequences, Euclidean and \\(\\chi^2\\)-distance are good choices. When timing is of importance, the Hamming distances are the most sensitive to differences in timing. With specific definitions also the Euclidean and \\(\\chi^2\\)-distance can be made sensitive to timing – the latter is recommended if differences in rare events are of particular importance. When durations are of importance, then OMspell is a good choice, and also LCS and classic OM are reasonable choices. When the main interest is in sequencing, good choices include OMstran, OMspell, and SVRspell with particular specifications. If the interest is in more than one aspect, the choice of the dissimilarity measure becomes more complex. By altering the specifications in measures such as OMstran, OMspell, and SVRspell the researcher could find a balance between the desired attributes. See [8] for more detailed information on the choice of dissimilarity measures and their specifications.\nDissimilarities are hard to interpret as such (unless the data are very small), so further analyses are needed to decrease the complexity. The most typical choice is to use cluster analysis for finding groups of individuals with similar patterns [22]. Other distance —or dissimilarity—based techniques include visualizations with multidimensional scaling [27], finding representative sequences [28], and ANOVA-type analysis of discrepancies [29].\n\n\n3.1.7 Finding similar groups or clusters of sequences\nThe sixth step is finding similar sequences, i.e., groups or patterns within the sequences where sequences within each group or cluster are as close to each other as possible and as different from other patterns in other clusters as possible. For instance, we can detect similar groups of sequences that show access patterns to online learning which are commonly referred to as tactics e.g., [7]. Such a step is typically performed using a clustering algorithm which may –or may not– require dissimilarity measures as an input [22, 26]. Common clustering algorithms that use a dissimilarity matrix are the hierarchical clustering algorithms. Hidden Markov models are among the most non-distance based cluster algorithms. See the remaining chapters about sequence analysis for examples of these algorithms [30–32].\n\n\n3.1.8 Analyzing the groups and/or using them in subsequent analyses\nAnalysis of the identified patterns or subgroups of sequences is an important research question in many studies and oftentimes, it is the guiding research question. For instance, researchers may use log data to create sequences of learning actions, identify subgroups of sequences, and examine the association between the identified patterns and performance e.g., [6, 7, 13], associate the identified patterns with course and course delivery [15], examine how sequences are related to dropout using survival analysis [14], or compare sequence patterns to frequencies [21].\n\n\n\n3.2 Introduction to the technique\nBefore performing the actual analysis with R code, we need to understand how the data is processed for analysis. Four important steps that require more in-depth explanation will be clarified here, those are: defining the alphabet, the timing scheme, specifying the actor, and visualization. Oftentimes, the required information to perform the aforementioned steps are not readily obvious in the data and therefore some preparatory steps need to be taken to process the file.\nThe example shown in Table 10.4 uses fictional log trace data similar to those that come from LMSs. To build a sequence from the data in Table 10.4, we can use the Action column as an alphabet. If our aim here is to model the sequence of students’ online actions, this is a straightforward choice that requires no preparation. Since the log trace data has no obvious timing scheme, we can use the session as a time scheme. To compute the session, we need to group the actions that occur together without a significant delay between actions (i.e., lag) that can be considered as an inactivity (see Section 10.3.1.2). For instance, Layla’s actions in Table 10.4 started at 18:44 and ended at 18:51. As such, all Layla’s actions occurred within 7 minutes. As Table 10.4 also shows, the first group of Layla’s actions occur within 1 to 2 minutes of lag. The next group of actions by Layla occur after almost one day, an hour and six minutes (1506 minutes) which constitutes a period of inactivity long enough to divide Layla’s actions into two separate sessions. Layla’s actions on the first day can be labeled Layla-session1 and her actions on the second day are Layla-session2. The actor in this example is a composite of the student (e.g., Layla) and the session number. The same for Sophia and Carmen: their actions occurred within a few minutes and can be grouped into the sessions. Given that we have the alphabet (Action), the timing scheme (session), and the actor (user-session), the next step is to order the alphabet chronologically. In Table 10.4, the actions were sequentially ordered for every actor according to their chronological order. The method that we will use in our guide requires the data to be in so-called “wide format”. This is performed by pivoting the data, or creating a wide form where the column names are the order and the value of the Action column is sequentially and horizontally listed as shown in Table 10.5.\n\n\nTable 4. The first three columns are simulated sequence data and the three gray columns are computed.\n\n\n\n\n\n\n\n\n\n\nUser\nAction\nTime\nLag\nSession    \nOrder\n\n\n\n\nLayla\nCalendar\n9.1.2023 18:44\n-\nLayla session 1\n1\n\n\nLayla\nLecture\n9.1.2023 18:45\n1\nLayla session 1\n2\n\n\nLayla\nInstructions\n9.1.2023 18:47\n2\nLayla session 1\n3\n\n\nLayla\nAssignment\n9.1.2023 18:49\n2\nLayla session 1\n4\n\n\nLayla\nLecture\n9.1.2023 18:50\n1\nLayla session 1\n5\n\n\nLayla\nVideo\n9.1.2023 18:51\n1\nLayla session 1\n6\n\n\nSophia\nLecture\n9.1.2023 20:08\n-\nSophia session 1\n1\n\n\nSophia\nInstructions\n9.1.2023 20:12\n4\nSophia session 1\n2\n\n\nSophia\nAssignment\n9.1.2023 20:14\n2\nSophia session 1\n3\n\n\nSophia\nAssignment\n9.1.2023 20:18\n4\nSophia session 1\n4\n\n\nSophia\nAssignment\n9.1.2023 20:21\n3\nSophia session 1\n5\n\n\nCarmen\nLecture\n10.1.2023 10:08\n-\nCarmen session 1\n1\n\n\nCarmen\nVideo\n10.1.2023 10:11\n3\nCarmen session 1\n2\n\n\nLayla\nInstructions\n10.1.2023 19:57\n1506\nLayla session 2\n1\n\n\nLayla\nVideo\n10.1.2023 20:01\n4\nLayla session 2\n2\n\n\nLayla\nLecture\n10.1.2023 20:08\n7\nLayla session 2\n3\n\n\nLayla\nAssignment\n10.1.2023 20:14\n6\nLayla session 2\n4\n\n\n\n\n\n\nTable 5. A sequence of engagement states using fictional data\n\n\n\n\n\n\n\n\n\n\n\nActor\n1\n2\n3\n4\n5\n6\n\n\n\n\nLayla session1\nCalendar\nLecture\nInstructions\nAssignment\nLecture\nVideo\n\n\nSophia session1\nLecture\nInstructions\nAssignment\nAssignment\nAssignment\n\n\n\nCarmen session1\nLecture\nVideo\n\n\n\n\n\n\nLayla session2\nInstructions\nVideo\nLecture\nAssignment\n\n\n\n\n\n\nThe following steps are creating a sequence object using sequence mining software and using the created sequence in analysis. In our case, we use the TraMineR framework which has a large set of visualization and statistical functions. Sequences created with TraMineR also work with a large array of advanced tools, R packages, and extensions. However, it is important to understand sequence visualizations before delving into the coding part.\n\n\n3.3 Sequence Visualization\nTwo basic plots are important here and therefore will be explained in detail. The first is the index plot (Figure 10.2) which shows the sequences of stacked colored bars representing spells, with each token represented by a different color. For instance, if we take Layla’s actions (in session1) and represent them as an index plot, they will appear as shown in Figure 10.2 (see the arrow). Where the Calendar is represented as a purple bar, the Lecture as a yellow bar, and instructions as an orange bar etc. Figure 10.2 also shows the visualization of sequences in Table 10.5 and you can see each of the session sequences as stacked colored bars following their order in the table. Nevertheless, sequence plots commonly include a large number of sequences that are of the order of hundreds or thousands of sequences and may be harder to read than the one presented in the example (see examples in the next sections).\n\n\n\nFigure 2. A table with ordered sequences of actions and the corresponding index plot; the arrow points to Layla’s actions.\n\n\nThe distribution plot is another related type of sequence visualization. Distribution plots —as the name implies— represent the distribution of each alphabet at each time point. For example, if we look at Figure 10.3 (top) we see 15 sequences in the index plot. At time point 1, we can count eight Calendar actions, two Video actions, two Lecture actions and one Instruction action. If we compute the proportions: we get 8/15 (0.53) of Calendar actions; for Video, Assignment, and Lecture we get 2/15 (0.13) in each case, and finally Instructions actions account for 1/15 (0.067). Figure 10.3 (bottom) shows these proportions. At time point 1, we see the first block Assignment with 0.13 of the height of the bar, followed by the Calendar which occupies 0.53, then a small block (0.067) for the Instructions, and finally two equal blocks (0.13) representing the Video and Lecture actions.\nSince the distribution plot computes the proportions of activities at each time point, we see different proportions at each time point. Take for example, time point 6, we have only two actions (Video and Assignment) and therefore, the plot has 50% for each action. At the last point 7, we see 100% for Lecture. Distribution plots need to be interpreted with caution and in particular, the number of actions at each time point need to be taken into account. One cannot say that at the 7th time point, 100% of actions were Lecture, since it was the only action at this time point. Furthermore, distribution plots do not show the transitions between sequences and should not be interpreted in the same way as the index plot.\n\n\n\nFigure 3. Index plot (top) and distribution plot (bottom)"
  },
  {
    "objectID": "chapters/ch10-sequence-analysis/ch10-seq.html#analysis-of-the-data-with-sequence-mining-in-r",
    "href": "chapters/ch10-sequence-analysis/ch10-seq.html#analysis-of-the-data-with-sequence-mining-in-r",
    "title": "10  Sequence Analysis in Education: Principles, Technique, and Tutorial with R",
    "section": "4 Analysis of the data with sequence mining in R",
    "text": "4 Analysis of the data with sequence mining in R\n\n4.1 Important packages\nThe most important package and the central framework that we will use in our analysis is the TraMineR package. TraMineR is a toolbox for creating, describing, visualizing and analyzing sequence data. TraMineR accepts several sequence formats, converts to a large number of sequence formats, and works with other categorical data. TraMineR computes a large number of dissimilarity measures and has several integrated statistical functions. TraMineR has been mainly used to analyze live event data such as employment states, sequence of marital states, or other life events. With the emergence of learning analytics and educational data mining, TraMineR has been extended into the educational field [33]. In the current analysis we will also need the packages TraMineRextras, WeightedCluster, and seqhandbook, which provide extra functions and statistical tools. The first code block loads these packages. In case you have not already installed them, you may need to install them.\n\nlibrary(TraMineR)\nlibrary(TraMineRextras)\nlibrary(WeightedCluster)\nlibrary(seqhandbook)\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(cluster)\nlibrary(MetBrewer)\nlibrary(reshape2)\n\n\n\n4.2 Reading the data\nThe example that will be used here is a Moodle log dataset that includes three important fields: the User ID (user), the time stamp (timecreated), and the actions (Event.context). Yet, as we mentioned before, there are some steps that need to be performed to prepare the data for analysis. First, the Event.context is very granular (80 different categories) and needs to be re-coded as mentioned in the basics of sequence mining section. We have already prepared the file with a simpler coding scheme where, for example, all actions intended as instructions were coded as instruction, all group forums were coded as group_work, and all assignment work was coded as Assignment. Thus, we have a field that we can use as the alphabet titled action. The following code reads the original coded dataset.\n\nSeqdatas <- \n  import(\"https://github.com/lamethods/data/raw/main/1_moodleLAcourse/Events.xlsx\")\n\n\n\n\n\n  \n\n\n\n\n\n4.3 Preparing the data for sequence analysis\nTo create a time scheme, we will use the methods described earlier in the basis of the sequence analysis section. The timestamp field will be used to compute the lag (the delay) between actions, find the periods of inactivity between the actions, and mark the actions that occur without a significant lag together as a session. Actions that follow with a significant delay will be marked as a new session. The following code performs these steps. First, the code arranges the data according to the timestamp for each user (see the previous example in the basics section), this is why we use arrange(timecreated, user). The second step (#2) is to group_by(user) to make sure all sessions are calculated for each user separately. The third step is to compute the lag between actions. Step #4 evaluates the lag length; if the lag exceeds 900 seconds (i.e., a period of inactivity of 900 seconds), the code marks the action as the start of a new session. Step #5 labels each session with a number corresponding to its order. The last step (#6) creates the actor variable, by concatenating the username with the string “Session_” and the session number; the resulting variable is called session_id.\n\nsessioned_data <- Seqdatas |>\n  arrange(timecreated, user) |> # Step 1\n  group_by(user) |> # Step 2\n  mutate(Time_gap = timecreated - (lag(timecreated))) |> # Step 3\n  mutate(new_session = is.na(Time_gap) | Time_gap > 900) |> # Step 4\n  mutate(session_nr = cumsum(new_session)) |> # Step 5\n  mutate(session_id = paste0 (user, \"_\", \"Session_\", session_nr)) # Step 6\n\nAn important question here is what is the optimum lag or delay that we should use to separate the sessions. Here, we used 900 seconds (15 minutes) based on our knowledge of the course design. In the course where the data comes from, we did not have activities that require students to spend long periods of idle time online (e.g., videos). So, it is reasonable here to use a relatively short delay (i.e., 900 seconds) to mark new sessions. Another alternative is to examine the distribution of lags or compute the percentiles.\n\nquantile(sessioned_data$Time_gap, \n         c(0.10, 0.50, 0.75, 0.80, 0.85, 0.90, 0.95, 1.00), na.rm = TRUE)\n\nTime differences in secs\n    10%     50%     75%     80%     85%     90%     95%    100% \n      1       1      61      61     181     841   12121 1105381 \n\n\nThe previous code computes the proportion of lags at 10% to 100% and the results show that at 90th percentile, the length of lags is equal to 841 seconds, that is very close to the 900 seconds we set. The next step is to order the actions sequentially (i.e., create the sequence in each session) as explained in Section 10.3.1.2 and demonstrated in Table 10.4. We can perform such ordering using the function seq_along(session_id) which creates a new field called sequence that chronologically orders the action (the alphabet).\n\nsessioned_data <- sessioned_data |>\n  group_by(user, session_nr) |>\n  mutate(sequence = seq_along(session_nr)) |>\n  mutate(sequence_length = length(sequence))\n\nSome sessions are outliers (e.g., extremely long or very short) – there are usually very few– and therefore, we need to trim such extremely long sessions. We do so by calculating the percentiles of session lengths.\n\nquantile(sessioned_data$sequence_length, \n         c(0.05, 0.1, 0.5, 0.75, 0.90, 0.95, 0.98, 0.99, 1.00),  na.rm = TRUE)\n\n  5%  10%  50%  75%  90%  95%  98%  99% 100% \n   3    4   16   29   42   49   59   61   62 \n\n\nWe see here that 95% of sequences lie within 49 states long and therefore, we can trim these long sessions as well as sessions that are only one event long.\n\nsessioned_data_trimmed <- sessioned_data |>\n  filter(sequence_length > 1 & sequence <= 49)\n\nThe next step is to reshape or create a wide format of the data and convert each session into a sequence of horizontally ordered actions. For that purpose, we use the function dcast from the reshape2 package. For this function, we need to specify the ID columns (the actor) and any other properties for the users can be specified here also. We selected the variables user and session_id. Please note that only session_id is necessary (actor) but it is always a good idea to add variables that we may use as weights, as groups, or for later comparison. We also need to specify the sequence column and the alphabet (action) column. The resulting table is similar to Table 10.5.\nThe last step is creating the sequence object using the seqdef function from the TraMineR package. To define the sequence, we need the prepared file from the previous step (similar to Table 10.5) and the beginning and end of the columns to consider i.e., the start of the sequence. We have started from the fourth column since the first three columns are meta-data (user, session_id, and session_nr). To include all columns in the data we use the ncol function to count the number of columns in the data. Creating a sequence object enables the full potential of sequence analysis.\n\ndata_reshaped <- dcast(user + session_id + session_nr ~ sequence, \n                       data = sessioned_data_trimmed, \n                       value.var = \"Action\")\nSeqobject <- seqdef(data_reshaped, 4:ncol(data_reshaped))\n\n [>] found missing values ('NA') in sequence data\n [>] preparing 9383 sequences\n [>] coding void elements with '%' and missing values with '*'\n [>] 12 distinct states appear in the data: \n     1 = Applications\n     2 = Assignment\n     3 = Course_view\n     4 = Ethics\n     5 = Feedback\n     6 = General\n     7 = Group_work\n     8 = Instructions\n     9 = La_types\n     10 = Practicals\n     11 = Social\n     12 = Theory\n [>] 9383 sequences in the data set\n [>] min/max sequence length: 2/49\n \nAn optional —yet useful— step is to add a color palette to create a better looking plot. Choosing an appropriate palette with separable colors improves the readability of the plot by helping easily identify different alphabets.\n\nNumber_of_colors <- length(alphabet(Seqobject))\ncolors <- met.brewer(name = \"VanGogh2\", n = Number_of_colors)\ncpal(Seqobject) <- colors\n\n\n\n4.4 Statistical properties of the sequences\nA simple way to get the properties of the sequences is through the function summary(). The functions show the total number of sequences in the object, the number of unique sequences, and lists the alphabet. A better way to dig deeper into the sequence properties is to use the seqstatd() function which returns several statistics, most notably the relative frequencies, i.e., the proportions of each state at each time point or the numbers comprising the distribution plot. The function also returns the valid states, that is, the number of valid states at each time point as well as the transversal entropy, which is a measure of diversity of states at each time point [34]. The code in the next section computes the sequence statistics and then displays the results.\n\nsummary(Seqobject)\nseq_stats <- seqstatd(Seqobject)\nseq_stats$Frequencies \nseq_stats$Entropy \nseq_stats$ValidStates \n\n\nFrequenciesEntropyValid states\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n4.5 Visualizing sequences\nVisualization has a summarizing power that allows researchers to have an idea about a full dataset in one visualization. TraMineR allows several types of visualizations that offer different perspectives. The most common visualization type is the distribution plot (described earlier in Figure 10.3). To plot a distribution plot one can use the powerful seqplot function with the argument type=\"d\" or simply seqdplot().\n\nseqplot(Seqobject, type = \"d\")\n\n\n\n\nFigure 4. Sequence distribution plot\n\n\n\n\nThe default distribution plot has an y-axis that ranges from 0 to 1.0 corresponding to the proportion and lists the number of sequences which in our case is 9383 However, the default output of the seqdplot() function is rarely satisfactory and we need to use the function arguments to optimize the resulting plot. The help file contains a rather detailed list of arguments and types of visualizations that can be consulted for more options, which can be obtained like any other R function by typing ?seqplot. In this chapter we will discuss the most basic options. In Figure 10.4, we use cex.legend argument to optimize the legend text size, we use the ncol argument to make the legend spread over six columns, the argument legend.prop to make the legend a bit far away from the main plot so they do not overlap and we use the argument border=NA to remove the borders from the plot. With such small changes, we get a much cleaner and readable distribution plot. Please note, that in each case, you may need to optimize the plot according to your needs. It is important to note here that in the case of missing data or sequences with unequal lengths like ours —which is very common— the distribution plot may show results that are made of fewer sequences at later time points. As such, the interpretation of the distribution plot should take into account the number of sequences, missing data, and timing. An index plot may be rather more informative in cases where missing data is prevalent.\n\nseqplot(Seqobject, type = \"d\", cex.legend = 0.9, ncol = 6, cex.axis = 0.7, \n        legend.prop = 0.1, border = NA)\n\n\n\n\nFigure 5. Sequence distribution plot with customized arguments\n\n\n\n\nThe index plot can be plotted in the same way using seqplot() with the argument type=\"I\" or simply using seqIplot. The resulting plot (Figure 10.6) has each sequence of the 9383 plotted as a line of stacked colored bars. One of the advantages of index plots is that they show the transitions between states in each sequence spell. Of course, plotting more than nine thousand sequences results in very thin lines that may not be very informative. Nevertheless, index plots are very informative when the number of sequences is relatively small. Sorting the sequences could help improve the visualization. On the right side, we see the index plot using the argument “sortv =”from.start”, under which sequences are sorted by the elements of the alphabet at the successive positions starting from the beginning of the time window.\n\nseqplot(Seqobject, type = \"I\", cex.legend = 0.6, ncol = 6, cex.axis = 0.6, \n        legend.prop = 0.2, border = NA)\n\nseqplot(Seqobject, type = \"I\", cex.legend = 0.6, ncol = 6, cex.axis = 0.6, \n        legend.prop = 0.2, border = NA, sortv = \"from.start\")\n\n\n\n\n\n\nFigure 6. Sequence index plots\n\n\n\n\nThe last visualization type we discuss here is the mean time plot, which plots the total time of every element of the alphabet across all time, i.e., a frequency distribution of all states regardless of their timing. As the plot in Figure 10.7 shows, group work seems to be the action that students performed the most, followed by course view.\n\nseqplot(Seqobject, type = \"mt\", cex.legend = 0.7, ncol = 6, cex.axis = 0.6, \n        legend.prop = 0.15, border = NA, ylim = c(0, 5))\n\n\n\n\nFigure 7. Mean time plot\n\n\n\n\n\n\n4.6 Dissimilarity analysis and clustering\nHaving prepared the sequences and explored their characteristics, we can now investigate if they have common patterns, recurrent sequences, or groups of similar sequences. This is a two-stage process; we will need to compute dissimilarities (along with associated substitution costs) and then perform cluster analysis on the resulting matrix. In the case of log trace data, clustering has always been performed to find learning tactics or sequences of students’ actions that are similar to each other or, put another way, patterns of similar behavior (e.g., [7, 16]). For more details on the clustering technique, please refer to Chapter 8 [35]. For the present analysis, we begin with the most common method for computing the dissimilarity matrix, that is Optimal Matching (OM). OM computes the dissimilarity between two sequences as the minimal cost of converting a sequence to the other. OM requires some steps that include specifying a substitution cost matrix, indel cost. Later, we use a clustering algorithm to partition the sequences according to the values returned by the OM algorithm [2, 36].\nA possible way to compute substitutions cost that has been commonly used – yet frequently criticized – in the literature is the TRATE method [37]. The TRATE method is data-driven and relies on transition rates; it assumes that pairs of states with frequent transitions between them should have “lower cost” of substitution (i.e., they are seen as being more similar). Thus, if we replace an action with another action that occurs often, it has a lower cost. This may be useful in some course designs, where some activities are very frequently visited and others are rare. The function seqsubm() is used to compute substitution costs with the TRATE method via:\n\nsubstitution_cost_TRATE <- seqsubm(Seqobject, method = \"TRATE\")\n\nIf we print the substitution cost matrix, we see that, for instance, the cost of replacing Applications with Applications is 0, whereas the cost of replacing Applications with Assignment (and vice versa) is higher (1.94). Since Course_view is the most common transition, replacing any action with Course_view tends to be the lowest in cost, which makes sense. Please note that the TRATE method is presented here for demonstration only. In fact, we do not recommend it to be used by default; readers should choose carefully what cost method best suits their data.\n\n\n\n\n  \n\n\n\nNevertheless, the most straightforward way of computing the cost is to use a constant cost; that is, to assume that the states are equally distant from one another. To do so, we can use the function seqsubm() and supply the argument method=\"CONSTANT\". In the following example, we assign a common cost of 2 (via the argument cval). We also refer below to other optional arguments which are not strictly necessary for the present application but nonetheless worth highlighting as options.\n\nsubstitution_cost_constant <- seqsubm(\n  Seqobject,            # Sequence object\n  method = \"CONSTANT\",  # Method to determine costs\n  cval = 2,             # Substitution cost\n  time.varying = FALSE, # Does not allow the cost to vary over time\n  with.missing = TRUE,  # Allows for missingness state\n  miss.cost = 1,        # Cost for substituting a missing state\n  weighted = TRUE)      # Allows weights to be used when applicable\n\nTo compute the OM dissimilarity matrix, the indel argument needs to be provided and we will use the default value 1 which is half of the highest substitution cost (2). We also need to provide the substitution cost matrix (sm). We opt for the matrix of constant substitution costs created above, given its straightforward interpretability.\n\ndissimilarities <- seqdist(Seqobject, method = \"OM\", indel = 1, \n                           sm = substitution_cost_constant)\n\nIn the resulting pairwise dissimilarity matrix, every sequence has a dissimilarity value with every other sequence in the dataset, and therefore, the dissimilarity matrix can be large and resource intensive in larger matrices. In our case, the dissimilarity matrix is 9383 * 9383 (i.e., 88,040,689) in size. With these dissimilarities between sequences as input, several distance-based clustering algorithms can be applied to partition the data into homogeneous groups. In our example, we use the hierarchical clustering algorithm from the package stats by using the function hclust(), but note that the choice of clustering algorithm can also affect results greatly and should be chosen carefully by the reader. For more details on the clustering technique, please refer to Chapter 8 [35]. The seq_heatmap() function is used to plot a dendrogram of the index plot which shows a hierarchical tree of different levels of subgrouping and helps choose the number of clusters visually.\n\nclusters_sessionsh <- hclust(as.dist(dissimilarities), method = \"ward.D2\") \nseq_heatmap(Seqobject, clusters_sessionsh)\n\n\n\n\nFigure 8. Visualization of clusters using a dendrogram\n\n\n\n\nTo do the actual clustering, we use the function cutree() and with the argument k = 3 to cluster the sequence into three clusters according to the groups highlighted in Figure 10.8. The cutree function produces a vector of cluster numbers, we can create more descriptive labels as shown in the example and assign the results to an R object called Groups. Visualizations of the clustering results can be performed in a similar fashion to the earlier visualizations of the entire set of sequences: via seqplot(), with the desired type of plot, and the addition of the argument group. Readers have to choose the arguments and parameters according to contexts, research questions, and the nature of their data.\n\nCuts <- cutree(clusters_sessionsh, k = 3)\nGroups <- factor(Cuts, labels = paste(\"Cluster\", 1:3))\nseqplot(Seqobject, type = \"d\", group = Groups, cex.legend = 0.8, ncol = 2, \n        cex.axis = 0.6, legend.prop = 0.2, border = NA)\n\n\n\n\nFigure 9. Sequence distribution plot for the k=3 cluster solution\n\n\n\n\nHowever, the resulting clusters might not be the best solution and we need to try other dissimilarity measures and/or clustering algorithms, evaluate the results, and compare their fit indices. TraMineR provides several distance measures, the most common of which are:\n\nEdit distances: Optimal matching \"OM\" or optimal matching with sensitivity to certain factors, e.g., optimal matching with sensitivity to spell sequence (\"OMspell\") or with sensitivity to transitions (\"OMstran\").\nShared attributes: Distance based on the longest common subsequence (\"LCS\"), longest common prefix (\"LCP\"; which prioritizes sequence common initial states), or the subsequence vectorial representation distance (\"SVRspell\"; based on counting common subsequences).\nDistances between distributions of states: Euclidean (\"EUCLID\") distance or Chi-squared (“CHI2”).\n\nDetermining the distance may be done based on the research hypothesis, context, and the nature of the sequences. For instance, a researcher may decide to group sequences based on their common starting points (e.g., [21]) where the order and how a conversation starts matter. TraMineR allows the computation of several dissimilarities. The following code computes some of the most common dissimilarities and stores each in a variable that we can use later.\n\n# Edit distances and sequences\ndissimOMstran <- seqdist(Seqobject, method = \"OMstran\", otto = 0.1, \n                         sm = substitution_cost_constant, indel = 1)\ndissimOMspell <- seqdist(Seqobject, method = \"OMspell\", expcost = 0, \n                         sm = substitution_cost_constant, indel = 1)\ndissimSVRspell <- seqdist(Seqobject, method = \"SVRspell\", tpow = 0)\ndissimOM <- seqdist(Seqobject, method = \"OM\", otto = 0.1, \n                    sm = substitution_cost_constant, indel = 1)\n\n\n# Distances between state distributions\ndissimCHI2 <- seqdist(Seqobject, method = \"CHI2\", step = 1)\ndissimEUCLID <- seqdist(Seqobject, method = \"EUCLID\", step = 49)\n\n\n# Distances based on counts of common attribute e.g., duration (spell lengths)\ndissimOMspell <- seqdist(Seqobject, method = \"OMspell\", expcost = 1, \n                         sm = substitution_cost_constant, indel = 1)\ndissimLCS <- seqdist(Seqobject, method = \"LCS\")\ndissimLCP <- seqdist(Seqobject, method = \"LCP\")\ndissimRLCP <- seqdist(Seqobject, method = \"RLCP\")\n\nWe can then try each dissimilarity with varying numbers of clusters and compute the clustering evaluation measures. The function as.clustrange from the WeightedCluster package computes several cluster quality indices including, among others, the Average Silhouette Width (ASW) which is commonly used in cluster evaluation to measure the coherence of the clusters. A value above 0.25 means that the data has some structure or patterns, whereas a value below 0.25 signifies the lack of structure in the data. The function also computes the R^2 Value which represents the ratio of the variance explained by the clustering solution. The results can be plotted and inspected. We can see that four clusters seem to be a good solution. Table 10.6 and Figure 10.10 show that the ASW and CHsq measures are maximized for the four-cluster solution, for which other parameters such as R^2 are also relatively good. Thus, we can use the four cluster solution. We note the use of the norm=“zscoremed” argument which improves the comparability of the various metrics in Figure 10.10 by standardizing the values to make it easier to identify the maxima. Table 10.6, however, presents the values on their original scales. Finally, the ranges and other characteristics of each cluster quality metric are summarized in Table 10.7. For brevity, we proceed with only the Euclidean distance matrix.\n\ndissimiarities_tested <- dissimEUCLID\nClustered <- hclust(as.dist(dissimiarities_tested), method = \"ward.D2\")\nClustered_range <- as.clustrange(Clustered, diss = dissimiarities_tested, \n                                 ncluster = 10)\nplot(Clustered_range, stat = \"all\", norm = \"zscoremed\", lwd = 2)\n\n\n\n\nFigure 10. Cluster performance metrics. X-axis represents the number of clusters, Y-axis represents the fit index standardized value.\n\n\n\n\n\nClustered_range[[\"stats\"]]\n\n\n\n\n\n\n\nTable 6.  Cluster performance metrics \n  \n    \n    \n      PBC\n      HG\n      HGSD\n      ASW\n      ASWw\n      CH\n      R2\n      CHsq\n      R2sq\n      HC\n    \n  \n  \n    0.281\n0.336\n0.335\n0.268\n0.268\n2,230.420\n0.192\n3,565.535\n0.275\n0.319\n    0.417\n0.489\n0.488\n0.307\n0.307\n1,992.266\n0.298\n3,553.441\n0.431\n0.246\n    0.507\n0.614\n0.614\n0.333\n0.333\n1,952.437\n0.384\n3,985.571\n0.560\n0.190\n    0.491\n0.635\n0.634\n0.272\n0.272\n1,717.644\n0.423\n3,524.149\n0.601\n0.184\n    0.511\n0.677\n0.677\n0.287\n0.288\n1,580.850\n0.457\n3,301.251\n0.638\n0.165\n    0.534\n0.736\n0.735\n0.308\n0.309\n1,449.482\n0.481\n3,178.407\n0.670\n0.138\n    0.557\n0.812\n0.812\n0.324\n0.325\n1,362.368\n0.504\n3,110.913\n0.699\n0.104\n    0.559\n0.831\n0.830\n0.327\n0.327\n1,341.529\n0.534\n3,124.571\n0.727\n0.096\n    0.571\n0.865\n0.865\n0.329\n0.330\n1,274.303\n0.550\n3,085.893\n0.748\n0.080\n  \n  \n  \n\n\n\n\n\n\n\nTable 7. Measures of the quality of a partition. Note: Table is based on [22] with permission from the author [22]\n\n\n\n\n\n\n\n\n\nName\nAbrv.\nRange\nMin/Max\nInterpretation\n\n\n\n\nPoint Biserial Correlation\nPBC\n[-1;1]\nMax\nMeasure of the capacity of the clustering to reproduce the distances.\n\n\nHubert’s Gamma\nHG\n[-1;1]\nMax\nMeasure of the capacity of the clustering to reproduce the distances (order of magnitude).\n\n\nHubert’s Somers’ D\nHGSD\n[-1;1]\nMax\nMeasure of the capacity of the clustering to reproduce the distances (order of magnitude) taking into account ties in distances.\n\n\nHubert’s C\nHC\n[0;1]\nMin\nGap between the partition obtained and the best partition theoretically possible with this number of groups and these distances.\n\n\nAverage Silhouette Width\nASW\n[-1;1]\nMax\nCoherence of assignments. High coherence indicates high between-group distances and strong within-group homogeneity.\n\n\nAverage Silhouette Width (weighted)\nASWw\n[-1;1]\nMax\nAs previous, for floating point weights.\n\n\nCalinski-Harabasz index\nCH\n[0; \\(+\\infty\\)[\nMax\nPseudo F computed from the distances.\n\n\nCalinski-Harabasz index\nCHsq\n[0; \\(+\\infty\\)[\nMax\nAs previous, but using squared distances.\n\n\nPseudo R2\nR2\n[0;1]\nMax\nShare of the discrepancy explained by the clustering solution (only to compare partitions with identical number of groups).\n\n\nPseudo R2\nR2sq\n[0;1]\nMax\nAs previous, but using squared distances.\n\n\n\n\nTo get the cluster assignment, we can use the results from the Clustered_range object and plot the clusters using the previously shown distribution, index, and mean time plot types.\n\ngrouping <- Clustered_range$clustering$cluster4\nseqplot(Seqobject, type = \"d\", group = grouping, cex.legend = 0.9, ncol = 6, \n        cex.axis = 0.6, legend.prop = 0.2, border = NA)\n\n\n\n\nFigure 11. Sequence distribution plot for the four clusters\n\n\n\n\n\nseqplot(Seqobject, type = \"I\", group = grouping, cex.legend = 0.9, ncol = 6, \n        cex.axis = 0.6, legend.prop = 0.2, border = NA)\n\n\n\n\nFigure 12. Sequence index plot for the four clusters\n\n\n\n\n\nseqplot(Seqobject, type = \"mt\", group = grouping, cex.legend = 1, ncol = 6, \n        cex.axis = .5, legend.prop = 0.2, ylim = c(0, 10))\n\n\n\n\nFigure 13. Mean time plot for the four clusters\n\n\n\n\nGiven the clustering structure, we also use a new plot type: the implication plot from the TraMineRextras package. Such a plot explicitly requires a group argument; in each of theseplots, at each time point, “being in this group implies to be in this state at this time point”. The strength of the rule is represented by a plotted line and a 95% confidence interval. Put another way, the more likely states have higher implicative values, which are more relevant when higher than the 95% confidence level.\n\nimplicaton_plot <- seqimplic(Seqobject, group = grouping)\nplot(implicaton_plot, conf.level = 0.95, cex.legend = 0.7)\n\n\n\n\nFigure 14. Implication plot for the four clusters\n\n\n\n\nGiven the implication plot as well as the other plots, the first cluster seems to be a mixed cluster with no prominent activity. Cluster 2 is dominated by practical activities, Cluster 3 is dominated by group work activities, Cluster 4 is dominated by assignments. Researchers usually give these clusters a label e.g., for Cluster 1, one could call it a diverse cluster. See some examples here in these papers [7, 15, 16]."
  },
  {
    "objectID": "chapters/ch10-sequence-analysis/ch10-seq.html#more-resources",
    "href": "chapters/ch10-sequence-analysis/ch10-seq.html#more-resources",
    "title": "10  Sequence Analysis in Education: Principles, Technique, and Tutorial with R",
    "section": "5 More resources",
    "text": "5 More resources\nSequence analysis is a rather large field with a wealth of methods, procedures, and techniques. Since we have used the TraMineR software in this chapter, a first place to seek more information about sequence analysis would be to consult the TraMineR manuals and guides [22, 26, 38]. More tools for visualization can be found in the package ggseqplot [39]. The ggseqplot package reproduces similar plots to TraMineR with the ggplot2 framework as well as other interesting visualizations [40]. This allows further personalisation using the ggplot2 grammar, as we have learned in Chapter 6 of this book on data visualization [41]. Another important sequence analysis package is seqHMM [42], which contains several functions to fit hidden Markov models. In the next chapter, we see more advanced aspects of sequence analysis for learning analytics [30–32].\nTo learn more about sequence analysis in general, you can consult the book by Cornwell (2015), which is the first general textbook on sequence analysis in the context of social sciences. Another valuable resource is the recent textbook by Raab and Struffolino [39], which introduces the basics of sequence analysis and some recent advances as well as data and R code."
  },
  {
    "objectID": "chapters/ch10-sequence-analysis/ch10-seq.html#acknowledgements",
    "href": "chapters/ch10-sequence-analysis/ch10-seq.html#acknowledgements",
    "title": "10  Sequence Analysis in Education: Principles, Technique, and Tutorial with R",
    "section": "6 Acknowledgements",
    "text": "6 Acknowledgements\nThis paper has been co-funded by the Academy of Finland (decision numbers 350560 and 331816), the Academy of Finland Flagship Programme (decision number 320162), the Strategic Research Council (SRC), FLUX consortium (decision numbers: 345130 and 345130), and the Swiss National Science Foundation (project “Strengthening Sequence Analysis”, grant No.: 10001A_204740)."
  },
  {
    "objectID": "chapters/ch11-vasstra/ch11-vasstra.html",
    "href": "chapters/ch11-vasstra/ch11-vasstra.html",
    "title": "11  Modeling the Dynamics of Longitudinal Processes in Education. A tutorial with R for The VaSSTra Method",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch11-vasstra/ch11-vasstra.html#introduction",
    "href": "chapters/ch11-vasstra/ch11-vasstra.html#introduction",
    "title": "11  Modeling the Dynamics of Longitudinal Processes in Education. A tutorial with R for The VaSSTra Method",
    "section": "1 Introduction",
    "text": "1 Introduction\nModeling a longitudinal process brings a lot of variability over time. The modeling procedure becomes even harder when we use multivariate continuous variables to model a single construct. For example, in education research we might model students’ online behavioral engagement through their number of clicks, time spent online, and frequency of interactions [1]. Most human behavioral constructs are an amalgam of interrelated features with complex fluctuations over time. Modeling such processes requires a method that takes into account the multidimensional nature of the examined construct as well as the temporal evolution. Nevertheless, despite the rich boundless information in the quantitative data, discrete patterns can be captured, modeled, and traced using appropriate methods. Such discrete patterns represent an archetype or a “state” that is typical of behavior or function [2]. For instance, a combination of frequent consumption of online course resources, long online time, interaction with colleagues and intense interactions in cognitively challenging collaborative tasks can be coined as an “engaged state” [3] The same student that shows an “engaged state” at one point, can transition to having few interactions and time spent online at the next time point, i.e., a “disengaged state”.\nCapturing a multidimensional construct into qualitative discrete states has several advantages. First, it avoids information overload where the information at hand is overwhelmingly hard to process accurately because of the multiplicity and lack of clarity of how to interpret small changes and variations. Second, it allows an easy way of communicating the information; it is understandable that communicating a state such as “engaged” is easier than telling the values of several activity variables. Third, it is easier to trace or track. As we are interested in significant shifts overtime, fine-grained changes in the activities are less meaningful. We are rather interested in significant shifts between behavioral states, e.g., from engaged to disengaged. Besides, such a shift is also actionable. As [4] puts it, “reliability can sometimes be improved by tuning grain size of data so it is neither too coarse, masking variance within bins, nor too fine-grained, inviting distinctions that cannot be made reliably.” More importantly, capturing the states is more tethered to reality of human nature and function. In fact, many psychological, physiological or disease constructs have been described as states with defining criteria, e.g., motivation, depression or migraine.\nExisting methods for longitudinal analysis are often limited to the study of a single variable’s evolution over time [5]. Some examples of such methods are longitudinal k-means [6], group-based trajectory modeling (GBTM) [7], or growth models [8]. However, when multivariate data is the target of analysis, these methods cannot be used. Multivariate methods are usually limited to one step or another of the analysis e.g., clustering of multivariate data into categorical variables (e.g., states), or chart the succession of categories into sequences. The method presented in this chapter provides an ensemble of methods and tools to effectively model, visualize and statistically analyse the longitudinal evolution of multivariate data. As such, modeling the temporal evolution of latent states, as we propose in this chapter, may not be entirely new and has been performed — at least in part— using several models, algorithms and software platforms [9–11]. For instance, the package lmfa can capture latent states in multivariate data, model their trajectories as well as transition probabilities [12]. Nevertheless, most of the existing methods are concerned with modeling disease states, time-to event (survival models), time-to-failure models [11] or lack a sequential analysis.\nThe VaSSTra method described in this chapter allows to summarize multiple variables into states that can be analyzed using sequence analysis across time. Then, using life-event methods, distinct trajectories of sequences that undergo a similar evolution can be analyzed in detail. VaSSTra consists of three steps: (1) capturing the states or patterns (from variables); (2) modeling the temporal process (from states); and (3) capturing the patterns of longitudinal development (similar sequences are grouped in trajectories). As such, the method described in this chapter is a combination of several methods. First, a person-centered method (latent class or latent profile analysis) is used to capture the unobserved “states” within the data. The states are then used to construct a “sequence of states”, where a sequence represents a person’s ordered states for each time point. The construction of “sequence of states” unlocks the full potential of sequence analysis visually and mathematically. Later, the longitudinal modeling of sequences is performed using a clustering method to capture the possible trajectories of progression of states. Thus, the name of the method is “from variables to states”, “from states to sequences” and “from sequences to trajectories” VaSSTra [5].\nThroughout the chapter, we discuss how to derive states from different variables related to students, how to construct sequences from students’ longitudinal progression of states, and how to identify and study distinct trajectories of sequences that undergo a similar evolution. We also cover some advanced properties of sequences that can help us analyze and compare trajectories. In the next section, we explain the VaSSTra method in detail. Next, we review the existing literature that has used the method. After that, we present a step-by-step tutorial on how to implement the method using a dataset of students’ engagement indicators across a whole program."
  },
  {
    "objectID": "chapters/ch11-vasstra/ch11-vasstra.html#vasstra-from-variables-to-states-from-states-to-sequences-from-sequences-to-trajectories",
    "href": "chapters/ch11-vasstra/ch11-vasstra.html#vasstra-from-variables-to-states-from-states-to-sequences-from-sequences-to-trajectories",
    "title": "11  Modeling the Dynamics of Longitudinal Processes in Education. A tutorial with R for The VaSSTra Method",
    "section": "2 VaSSTra: From variables to states, from states to sequences, from sequences to trajectories",
    "text": "2 VaSSTra: From variables to states, from states to sequences, from sequences to trajectories\nIn Chapter 10, we went through the basics of sequence analysis in learning analytics [13]. Specifically, we learned how to construct a sequence from a series of ordered student activities in a learning session, which is a very common technique in learning analytics (e.g., [14]). In the sequences we studied, each time point represents a single instantaneous event or action by the students. In this advanced chapter, we take a different approach, where sequences are not built from a series of events but rather from states. Such states represent a certain construct (or cluster of variables) related to students (e.g., engagement, motivation) in a certain period (e.g., a week, a semester, a course). The said states are derived from a series of data variables related to the construct under study over the stipulated time period. Analyzing the sequence of such states over several sequential periods allows us to summarize large amounts of longitudinal information and to study complex phenomena across longer timespans [5]. This approach is known as the VaSSTra method. VaSSTra utilizes a combination of person-based methods (to capture the latent states) along with life events methods to model the longitudinal process. In doing so, VaSSTra effectively leverages the benefits of both families of methods in mapping the patterns of longitudinal temporal dynamics. The method has three main steps that can be summarized as (1) identifying latent States from Variables, (2) modeling states as Sequences, and (3) identifying Trajectories within sequences. The three steps are depicted in Figure 11.1 and described in detail below:\n\n\n\nFigure 1. Summary of the three steps of the VaSSTra method\n\n\n\nStep 1. From variables to states: In the first step of the analysis, we identify the “states” within the data using a method that can capture latent or unobserved patterns from multidimensional data (variables). The said states represent a behavioral pattern, function or a construct that can be inferred from the data. For instance, engagement is a multidimensional construct and is usually captured through several indicators. e.g., students’ frequency and time spent online, course activities, cognitive activities and social interactions. Using an appropriate method, such as person-based clustering in our case, we can derive students’ engagement states for a given activity or course. For instance, the method would classify students who invest significant time, effort and mental work are “engaged.” Similarly, students who are investing low effort and time in studying would be classified as “diseganged.” Such powerful summarization would allow us to use the discretized states in further steps. An important aspect of such states is that they are calculated for a specific timespan. Therefore, in our example we could infer students’ engagement states per activity, per week, per lesson, per course, etc. Sometimes, such time divisions are by design (e.g., lessons or courses), but in other occasions researchers have to establish a time scheme according to the data and research questions (e.g., weeks or days). Computing states for multiple time periods is a necessary step to create time-ordered state sequences and prepare the data for sequence analysis.\nStep 2. From states to sequences: Once we have a state for each student at each time point, we can construct an ordered sequence of such states for each student. For example, if we used the scenario mentioned above about measuring engagement states, a sequence of a single student’s engagement states across a six-lesson course would be like the one below. When we convert the ordered states to sequences, we unlock the potential of sequence analysis and life events methods. We are able to plot the distribution of states at each time point, study the individual pathways, the entropy, the mean time spent at each state, etc. We can also estimate how frequently students switch states, and what is the likelihood they finish their sequence in a “desirable” state (i.e., “engaged”).\n\n\nStep 3. From sequences to trajectories: Our last step is to identify similar trajectories —sequences of states with a similar temporal evolution— using temporal clustering methods (e.g., hidden Markov models or hierarchical clustering). Covariates (i.e., variables that could explain cluster membership) can be added at this stage to help identify why a trajectory has evolved in a certain way. Moreover, sequence analysis can be used to study the different trajectories, and not only the complete cohort. We can compare trajectories according to their sequence properties, or to other variables (e.g., performance)."
  },
  {
    "objectID": "chapters/ch11-vasstra/ch11-vasstra.html#review-of-the-literature",
    "href": "chapters/ch11-vasstra/ch11-vasstra.html#review-of-the-literature",
    "title": "11  Modeling the Dynamics of Longitudinal Processes in Education. A tutorial with R for The VaSSTra Method",
    "section": "3 Review of the literature",
    "text": "3 Review of the literature\nThe VaSSTra method has been used to study different constructs related to students’ learning (Table 11.1), such as engagement [3, 15, 16], roles in computer-supported collaborative learning (CSCL) [17], and learning strategies [18, 19]. Several algorithms have been operationalized to identify latent states from students’ online data. Some examples are: Agglomerative Hierarchical Clustering (AHC) [19], Latent Class Analysis (LCA) [3, 15, 16], Latent Profile Analysis (LPA) [17], and mixture hidden Markov models (MHMM) [18].\nMoreover, sequences of states have mostly been used to represent each course in a program [3, 15–18], but also smaller time spans, such as each learning session in a single course [18, 19]. Different algorithms have also been used to cluster sequences of states into trajectories including HMM [3, 19], mixture hidden Markov models (MHMM) [15, 17], and AHC [16, 18]. Moreover, besides the basic aspects of sequence analysis discussed in the previous chapter, previous work have explored advanced features of sequence analysis such as survival analysis [3, 16], entropy [3, 17, 18], sequence implication [3, 18], transitions [3, 17, 18], covariates [17], discriminating subsequences [16] or integrative capacity [17]. Other studies have made used of multi-channel sequence analysis [15, 19], which is covered in Chapter 13 [20].\n\n\n\nTable 1. Previous literature in which VaSSTra has been used\n\n\n\n\n\n\n\n\n\n\n\n\n\nRef.\nVariables\nStates\nAlphabet\nActor\nTime unit\nMethod Step 1\nMethod Step 3\nAdvance sequence methods\n\n\n\n\n[3]\nFrequency, time spent and regularity of online activities\nEngage- ment\nActive, Average, Disengaged, Withdraw\nStudy program\nCourse\nLCA\nHMM\nEntropy, sequence implication, survival, transitions\n\n\n[15]\nFrequency, time spent and regularity of online activities. Grades.\nEngage- ment, achievement\nActive, Average, Disengages.\nAchiever, Intermediate, Low.\nStudy program\nCourse\nLCA\nMHMM\nMulti-channel sequence analysis\n\n\n[16]\nFrequency, time spent and regularity of online activities\nEngage- ment\nActively engaged, Averagely engaged, Disengaged, Dropout\nStudy program\nCourse\nLCA\nAHC\nSurvival analysis, discriminating subsequences, mean time\n\n\n[17]\nCentrality measures\nCSCL roles\nInfluencer, Mediator, Isolate\nStudy program\nCourse\nLPA\nMHMM\nCovariate analysis, spell duration, transitions, entropy, integrative capacity\n\n\n[18]\nOnline activities\nLearning strategies\nDiverse, Forum read, Forum interact, Lecture read.\nIntense diverse, Light diverse, Light interactive, Moderate interactive.\nCourse. Study program.\nLearning session. Course.\nMHMM\nAHC\nEntropy, implication, transitions\n\n\n[19]\nOnline activities\nLearning strategies\nVideo-oriented instruction, Slide-oriented instruction, Help-seeking.\nAssignment struggling, Assignment approaching, Assignment succeeding.\nCourse\nLearning session\nAHC\nHMM\nMulti-channel sequence analysis"
  },
  {
    "objectID": "chapters/ch11-vasstra/ch11-vasstra.html#vasstra-with-r",
    "href": "chapters/ch11-vasstra/ch11-vasstra.html#vasstra-with-r",
    "title": "11  Modeling the Dynamics of Longitudinal Processes in Education. A tutorial with R for The VaSSTra Method",
    "section": "4 VassTra with R",
    "text": "4 VassTra with R\nIn this section we provide a step-by-step tutorial on how to implement VaSSTra with R. To illustrate the method, we will conduct a case study in which we examine students’ engagement throughout all the courses of the first two years of their university studies, using variables derived from their usage of the learning management system.\n\n4.1 The packages\nIn order to conduct our analysis we will need several packages besides the basic rio (for reading and saving data in different extensions), tidyverse (for data wrangling), cluster (for clustering features), and ggplot2 (for plotting). Below is a brief summary of the rest of the packages needed:\n\nBBmisc: A package with miscellaneous helper functions [21]. We will use its normalize function to normalize our data across courses to remove the differences in students’ engagement that are due to different course implementations (e.g., larger number of learning resources).\ntidyLPA: A package for conducting Latent Profile Analysis (LPA) with R [22]. We will use it to cluster students’ variables into distinct clusters or states.\nTraMineR: As we have seen in Chapter 10 about sequence analysis [13], this package helps us construct, analyze and visualize sequences from time-ordered states or events [23].\nseqhandbook: This package complements TraMineR by providing extra analyses and visualizations [24].\nGmisc: A package with miscellaneous functions for descriptive statistics and plots [25]. We will use it to plot transitions between states.\nWeightedCluster: A package for clustering sequences using hierarchical cluster analysis [26]. We will use it to cluster sequences into similar trajectories.\n\nThe code below imports all the packages that we need. You might need to install them beforehand using the install.packages command:\n\nlibrary(rio)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(cluster)\nlibrary(BBmisc)\nlibrary(tidyLPA)\nlibrary(TraMineR)\nlibrary(seqhandbook)\nlibrary(Gmisc)\nlibrary(WeightedCluster)\n\n\n\n4.2 The dataset\nFor our analysis, we will use a dataset of students’ engagement indicators throughout eight courses, corresponding to the first two years of a blended higher education program. The dataset is described in detail in the data chapter. The indicators or variables are calculated from students’ log data in the learning management system, and include the frequency (i.e., number of times) with which certain actions have been performed (e.g., view the course lectures, read forum posts), the time spent in the learning management system, the number of sessions, the number of active days, and the regularity (i.e., consistency and investment in learning). These variables represent students’ bahavioral engagement indicators. The variables are described in detail in Chapter 2 about the datasets of the book [27] and Chapter 7 about predictive learning analytics [28] and in previous works [3]. Below we use rio’s import function to read the data.\n\nLongitudinalData <- \n  import(\"https://github.com/lamethods/data/raw/main/9_longitudinalEngagement/LongitudinalEngagement.csv\")\nLongitudinalData\n\n\n\n  \n\n\n\n\n\n4.3 From variables to states\nThe first step in our analysis is to detect latent states from the multiple engagement-related variables in our dataset (e.g., frequency of course views, frequency of forum posts, etc.). For this purpose, we will use LPA, a person-based clustering method, to identify each student’s engagement state at each course. We first need to standardize the variables, to account for the possible differences in course implementations (e.g., each course has a different number of learning materials and slightly different duration). This way, the mean value of each indicator will be always 0 regardless of the course. Any value above the mean will be always positive, and any value below will be negative. As such the engagement is measures on the same scale. To standardize the data we first group it by CourseID using tidyverse’s group_by and then we apply the normalize function from the BBmisc package to all the columns that contain the engagement indicators using mutate_at and specifying the range of columns. If we inspect the data now, we will see that all variables are centered around 0.\n\nLongitudinalData |> group_by(CourseID) |>  \n  mutate_at(vars(Freq_Course_View:Active_Days), \n            function(x) normalize(x, method = \"standardize\")) |> \n  ungroup() -> df\ndf\n\n\n\n  \n\n\n\nNow, we need to subset our dataset and choose only the variables that we need for clustering. That is, we exclude the metadata about the user and course, and keep only the variables that we believe are relevant to represent the engagement construct.\n\nto_cluster <- dplyr::select(df, Freq_Course_View, Freq_Forum_Consume, \n                                Freq_Forum_Contribute, Freq_Lecture_View, \n                                Regularity_Course_View, Session_Count, \n                                Total_Duration, Active_Days)\n\nBefore we go on, we must choose a seed so we can obtain the same results every time we run the clustering algorithm. We can now finally use tidyLPA to cluster our data. We try from 1 to 10 clusters and different models that enforce different constraints on the data. For example, model 3 takes equal variances and equal covariances, whereas model 6 takes varying variances and varying covariances. You can find out more about this in the tidyLPA documentation [22]. Be aware that running this step may take a while. For more details about LPA, consult the model-based clustering chapter.\n\nset.seed(22294)\nMclustt <- to_cluster |> ungroup() |> \n  single_imputation() |> \n  estimate_profiles(1:10, models = c(1, 2, 3, 6)) \n\n\n\n\nOnce all the possible cluster models and numbers have been calculated, we can calculate several statistics that will help us choose which is the right model and number of clusters for our data. For this purpose, we use the compare_solutions function from tidyLPA and we use the results of calling this function to plot the BIC and the entropy of each model for the range of cluster numbers that we have tried (1-10). In the model-based clustering chapter you can find out more details about how to choose the best cluster solution.\n\ncluster_statistics <- Mclustt |>  \n  compare_solutions(statistics = c(\"AIC\", \"BIC\",\"Entropy\"))\nfits <- cluster_statistics$fits |> mutate(Model = factor(Model))\n\nfits |>\n  ggplot(aes(x = Classes,y = Entropy, group = Model, color = Model)) + \n  geom_line() +\n  scale_x_continuous(breaks = 0:10) +\n   geom_point() +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nfits |>\n  ggplot(aes(x = Classes,y = BIC, group = Model, color = Model)) + \n  geom_line() +\n  scale_x_continuous(breaks = 0:10) +\n  geom_point() +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\nFigure 2. Choosing the number of clusters by plotting statistics\n\n\n\n\nAlthough, based on the BIC values, Model 6 with 3 classes would be the best fit, the entropy for this model is quite low. Instead, Models 1 and 2 have a higher overall entropy and quite a large fall in BIC when increasing from 2 to 3 classes. Taken together, we choose Model 1 with 3 classes which shows better separation of clusters (high entropy) and a large drop in BIC value (elbow). We add the cluster assignment back to the data so we can compare the different variables between clusters and use the cluster assignment for the next steps. Now, for each student’s course enrollment, we have assigned a state (i.e., cluster) that represents the student’s engagement during that particular course.\n\ndf$State <- Mclustt$model_1_class_3$model$classification\n\nWe can plot the mean variable values for each of the three clusters to understand what each of them represents:\n\ndf |> pivot_longer(Freq_Course_View:Active_Days) |> \n  mutate(State = factor(State)) |>\n  filter(name %in% names(to_cluster)) |>\n  mutate(name = gsub(\"_\", \" \", name)) |>\n  group_by(name, State) |>\n  summarize_if(is.double, mean) -> long_mean\n\nlong_mean |>\n  ggplot(aes(fill = name, y = value, x = State)) +\n  geom_col(position = \"dodge\") + \n  scale_fill_brewer(\"\", type = \"qual\", palette=8) +\n  theme_minimal() +\n  ylab (\"Mean value\") + \n  theme(legend.position = \"bottom\")\n\n\n\n\nFigure 3. Mean value of each variable for each cluster\n\n\n\n\nWe clearly see that the first cluster represents students with low mean levels of all engagement indicators; the second cluster represents students with average values, and the third cluster with high values. We can convert the State column of our dataset to a factor to give the clusters an appropriate descriptive label:\n\nengagement_levels = c(\"Disengaged\", \"Average\", \"Active\")\ndf_named <- df |> \n  mutate(State = factor(State, levels = 1:3, labels = engagement_levels)) \n\n\n\n4.4 From states to sequences\nIn the previous step, we turned a large amount of variables representing student engagement in a given course into a single state: Disengaged, Average, or Active. Each student has eight engagement states: one per each course (time point in our data) in the first two years of the program. Since the dataset includes the order of each course for each student, we can construct a sequence of the engagement states throughout all courses for each student. To do that we first need to transform our data into a wide format, in which each row represents a single student, and each column represents the student’s engagement state at a given course:\n\nclus_seq_df <- df_named |> arrange(UserID, Sequence) |>\n  pivot_wider(id_cols = \"UserID\", names_from = \"Sequence\", values_from = \"State\")\n\nNow we can use TraMineR to construct the sequence and assign colors to represent each of the engagement states:\n\ncolors <- c(\"#28a41f\", \"#FFDF3C\", \"#e01a4f\")\nclus_seq <- seqdef(clus_seq_df , 2:9, \n                   alphabet = sort(engagement_levels), cpal = colors)\n\nWe can use the sequence distribution plot from TraMineR to visualize the distribution of each state at each time point (Figure 11.4). We see that the distribution of states is almost constant throughout the eight courses. The ‘Average’ state takes the largest share, followed by the ‘Engaged’ state, and the ‘Disengaged’ state is consistently the least common. For more hints on how to interpret the sequence distribution plot, refer to Chapter 10 [13].\n\nseqdplot(clus_seq, border = NA, use.layout = TRUE, \n         with.legend = T, ncol = 3, legend.prop = 0.2)\n\n\n\n\nFigure 4. Sequence distribution plot of the course states\n\n\n\n\nWe can also visualize each of the individual students’ sequences of engagement states using a sequence index plot (Figure 11.5). In this type of visualization, each horizontal bar represents a single student, and each of the eight colored blocks along the bar represents the students’ engagement states. We can order the students’ sequences according to their similarity for a better understanding. To do this, we calculate the substitution cost matrix (seqsubm) and the distance between the sequences according to this cost (seqdist). Then we use an Agglomerative Nesting Hierarchical Clustering algorithm (agnes) to group sequences together according to their similarity (see Chapter 10 [13]). We may now use the seq_heatmap function of seqhandbook to plot the sequences ordered by their similarity. From this plot, we already sense the existence of students that are mostly active, students that are mostly disengaged, and students that are in-between, i.e., mostly average.\n\nsm <- seqsubm(clus_seq, method = \"CONSTANT\")\nmvad.lcss <- seqdist(clus_seq, method = \"LCS\", sm = sm, with.missing = T)\nclusterward2 <- agnes(mvad.lcss, diss = TRUE, method = \"ward\")\nseq_heatmap(clus_seq, clusterward2)\n\n\n\n\nFigure 5. Sequence index plot of the course states ordered by sequence distance\n\n\n\n\n\n\n4.5 From sequences to trajectories\nIn the previous step we constructed a sequence of each student’s engagement states throughout eight courses. When plotting these sequences, we observed that there might be distinct trajectories of students that undergo a similar evolution of engagement. In this last step, we use hierarchical clustering to cluster the sequences of engagement states into distinct trajectories of similar engagement patterns. For more details on the clustering technique, please refer to Chapter 8 [29]. To perform hierarchical clustering we first need to calculate the distance between all the sequences. As we have seen in the Sequence Analysis chapter, there are several algorithms to calculate the distance. We choose LCS (Longest Common Subsequence), implemented in the TraMineR package which calculates the distance based on the longest common subsequence.\n\ndissimLCS <- seqdist(clus_seq, method = \"LCS\")\n\nNow we can perform the hierarchical clustering. For this purpose, we use the hclust function of the stats package\n\nClustered <- hclust(as.dist(dissimLCS), method = \"ward.D2\")\n\nWe create partitions for clusters ranging from 2 to 10 and we plot the cluster statistics to be able to select the most suitable cluster number.\n\nClustered_range <- as.clustrange(Clustered, diss = dissimLCS, ncluster = 10)\nplot(Clustered_range, stat = \"all\", norm = \"zscoremed\", lwd = 2)\n\n\n\n\nFigure 6. Cluster statistics for the hierarchical clustering\n\n\n\n\nThere seems to be a maximum for most statistics at three clusters, so we save the cluster assignment for three clusters in a variable named grouping.\n\ngrouping <- Clustered_range$clustering$cluster3\n\nNow we can use the variable grouping to plot the sequences for each trajectory using the sequence index plot (Figure 11.7):\n\nseqIplot(clus_seq, group = grouping, sortv = \"from.start\")\n\n\n\n\nFigure 7. Sequence index plot of the course states per trajectory\n\n\n\n\nIn Figure 11.7, we see that the first trajectory corresponds to mostly average students, the second one to mostly active students, and the last one to mostly disengaged students. We can rename the clusters accordingly.\n\ntrajectories_names = c(\"Mostly average\", \"Mostly active\", \"Mostly disengaged\")\ntrajectories = trajectories_names[grouping]\n\nWe can plot the sequence distribution plot to see the overall distribution of the sequences for each trajectory (Figure 11.8).\n\nseqdplot(clus_seq, group = trajectories)\n\n\n\n\nFigure 8. Sequence distribution plot of the course states per trajectory\n\n\n\n\n\n\n4.6 Studying trajectories\nThere are many aspects that we can study about our trajectories. For example, we can use the mean time plot to compare the time spent in each engagement state for each trajectory. This plot summarizes the time distribution plot across all time points (Figure 11.9). As expected, we see that the mostly active students spend most of their time in an ‘Active’ state, the mostly average students in an Average state, and the mostly disengaged students in a ‘Disengaged’ state, although they spend quite some time in an ‘Average’ state as well.\n\nseqmtplot(clus_seq, group = trajectories)\n\n\n\n\nFigure 9. Mean time plot of the course states per trajectory\n\n\n\n\nAnother very useful plot is the sequence frequency plot (Figure 11.10), that shows the most common sequences in each trajectory and the percentage of all the sequences that they represent. We see that, for each trajectory, the sequence that has all equal engagement states is the most common. The mostly active has, of course, sequences dominated by ‘Active’ states, with sparse average states, and one ‘Disengaged’ state. The mostly disengaged shows a similar pattern dominated by disengaged states with some average and one active state. The mostly average, although it is dominated by ‘Average’ states, shows diversity of shifts to active or disengaged.\n\nseqfplot(clus_seq, group = trajectories)\n\n\n\n\nFigure 10. The 10 most frequent sequences in each trajectory\n\n\n\n\nTo measure the stability of engagement states for each trajectory at each time point, we can use the between-study entropy. Entropy is lowest when all students have the same engagement state at the same time point and highest when the heterogeneity is maximum. We can see that the “Mostly active” and “Mostly disengaged” trajectories have a slightly lower entropy compared to the “Mostly average” one, which is a sign that the students in this trajectory are the least stable (Figure 11.11).\n\nseqHtplot(clus_seq, group = trajectories)\n\n\n\n\nFigure 11. Transversal entropy plot of each trajectory\n\n\n\n\nAnother interesting aspect to look into is the difference in the most common subsequences among trajectories (Figure 11.12). We first search the most frequent subsequences overall and then compare them among the three trajectories. Interestingly enough, the most frequent subsequence is remaining ‘Active’, and remaining ‘Disengaged’ is number five. Remaining average is not among the top 10 most common subsequences, but rather the subsequences containing the ‘Average’ state always include transitions to other states.\n\nmvad.seqe <- seqecreate(clus_seq)\nfsubseq <- seqefsub(mvad.seqe, pmin.support = 0.05, max.k = 2)\ndiscr <- seqecmpgroup(fsubseq, group = trajectories, method = \"chisq\")\nplot(discr[1:10])\n\n\n\n\nFigure 12. Most discriminating subsequences per trajectory\n\n\n\n\nThere are other sequence properties that we may need compare among the trajectories. The function seqindic calculates sequence properties for each individual sequence (Table 11.2). Some of these indices need additional information about the sequences, namely, which are the positive and the negative states. In our case, we might consider the Active state to be positive and the Disengaged state to be negative. Below we discuss some of the most relevant measures:\n\nTrans: Number of transitions. It represents the number of times there has been a change of state. If a sequence maintains the same state throughout its whole length, the value of Trans would be zero; if there were two shifts of state, the value would be 2.\nEntr: Longitudinal entropy or within-student entropy is a measure of the diversity of the sequence states. In contrast with the transversal or between-students entropy that we saw earlier (Figure 11.11), which is calculated per time point, longitudinal entropy is calculated per sequence (i.e., which represents a student’s engagement through the courses in a program in this case). Longitudinal entropy is calculated using Shannon’s entropy formula. Sequences that remain in the same state most of the time have a low entropy whereas sequences that shift states continuously with great diversity have a high entropy.\nCplx: The complexity index is a composite measure of a sequence’s complexity based on the number of transitions and the longitudinal entropy. It measures the variety of states within a sequence, as well as the frequency and regularity of transitions between them. In other words, a sequence with a high complexity index is characterized by many different and unpredictable states or events, and frequent transitions between them.\nPrec: Precarity is a measure of the (negative) stability or predictability of a sequence. It measures the proportion of time that a sequence spends in negative or precarious states, as well as the frequency and duration of transitions between positive and negative states. A sequence with a high precarity index is characterized by a high proportion of time spent in negative or precarious states, and frequent transitions between positive and negative states.\nVolat: Objective volatility represents the average between the proportion of states visited and the proportion of transitions (state changes). It is measure of the variability of the states and transitions in a sequence. A sequence with high volatility would be characterized by frequent and abrupt changes in the states or events, while a sequence with low volatility would have more stable and predictable patterns.\nIntegr: Integrative capacity (potential) is the ability to reach a positive state and then stay in a positive state. Sequences with a high integrative capacity not only include positive states but also manage to stay in such positive states.\n\n\nIndices <- seqindic(clus_seq,\n            indic = c(\"trans\",\"entr\",\"cplx\",\"prec\",\"volat\",\"integr\"), \n            ipos.args = list(pos.states = c(\"Active\")), \n            prec.args = list(c(\"Disengaged\")))\n\n\n\n\n\n\n\nTable 2.  Sequence indicators \n  \n    \n    \n      \n      Trans\n      Entr\n      Volat\n      Cplx\n      Integr\n      Prec\n    \n  \n  \n    1\n2.000\n0.343\n0.393\n0.313\n0.000\n0.600\n    2\n2.000\n0.343\n0.393\n0.313\n0.000\n0.600\n    3\n4.000\n0.512\n0.536\n0.541\n0.000\n0.964\n    4\n0.000\n0.000\n0.000\n0.000\n1.000\n0.000\n    5\n4.000\n0.631\n0.536\n0.600\n0.000\n1.159\n    6..141\n\n\n\n\n\n\n    142\n2.000\n0.343\n0.393\n0.313\n0.917\n0.006\n  \n  \n  \n\n\n\n\n\nWe can compare the distribution of these indices between the different trajectories to study their different properties (Figure 11.13). Below is an example for precarity and integrative capacity. We clearly see how the Mostly disengaged trajectory has the highest value of precarity, whereas the Mostly active students have the highest integrative capacity. Beyond a mere visual representation, we could also conduct statistical tests to compare whether these properties differ significantly from each other among trajectories.\n\nIndices$Trajectory = trajectories\n\nIndices |> ggplot(aes(x = Trajectory, y = Prec, fill = Trajectory)) + \n  geom_boxplot() + scale_fill_manual(values = colors) + \n  theme_minimal() + theme(legend.position = \"none\")\n\nIndices |> ggplot(aes(x = Trajectory, y = Integr, fill = Trajectory)) + \n  geom_boxplot() + scale_fill_manual(values = colors) + \n  theme_minimal() + theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 13. Comparison of sequence indicators between trajectories\n\n\n\n\nAs we have mentioned, an important aspect of the study of students’ longitudinal evolution is looking at the transitions between states. We can calculate the transitions using seqtrate from TraMineR and plot them using transitionPlot from Gmisc.\n\ntransition_matrix = seqtrate(clus_seq, count = T)  \n\nFrom the transition matrix and Figure 11.14 we can see how most transitions are between one state and the same (no change). The most unstable state is ‘Average’ with frequent transitions both to ‘Active’ and ‘Disengaged’. Both ‘Active’ and ‘Disengaged’ had occasional transitions to ‘Average’ but rarely from one another.\n\ntransitionPlot(transition_matrix, \n                              fill_start_box = colors,\n                              txt_start_clr = \"black\",\n                              cex = 1,\n                              box_txt = rev(engagement_levels))\n\n\n\n\nFigure 14. Transition plot between states"
  },
  {
    "objectID": "chapters/ch11-vasstra/ch11-vasstra.html#discussion",
    "href": "chapters/ch11-vasstra/ch11-vasstra.html#discussion",
    "title": "11  Modeling the Dynamics of Longitudinal Processes in Education. A tutorial with R for The VaSSTra Method",
    "section": "5 Discussion",
    "text": "5 Discussion\nIn this chapter, the VaSSTra method is presented as a person-centered approach for the longitudinal analysis of complex behavioral constructs over time. In the step-by-step tutorial, we have analyzed students’ engagement states throughout all the courses in the first two years of program. First, we have clustered all the indicators of student engagement into three engagement states using model-based clustering: active, average and disengaged. This step allowed us to summarize eight continuous numerical variables representing students’ online engagement indicators of each course into a single categorical variable (state). Then, we constructed a sequence of engagement states for each student, allowing us to map the temporal evolution of engagement and make use of sequence analysis methods to visualize and investigate such evolution. Lastly, we have clustered students’ sequences of engagement states into three different trajectories: a mostly active trajectory which is dominated by engaged students who are stable throughout time, a mostly average trajectory with averagely engaged students who often transition to engaged or disengaged states, and a mostly disengaged trajectory with inactive students that fail to catch up and remain disengaged most of the program. As such, VaSSTra offers several advantages over the existing longitudinal methods for clustering (such as growth models or longitudinal k-means) which are limited to a single continuous variable [30–32], instead of taking advantage of multiple variables in the data. Through the summarizing power of visualization, VaSSTra is able to represent complex behaviors captured through several variables using a limited number of states. Moreover, through sequence analysis, we can study how the sequences of such states evolve over time and differ from one another, and whether there are distinct trajectories of evolution.\nSeveral literature reviews of longitudinal studies (e.g., [33]) have highlighted the shortcomings of existing research for using variable-centered methods or ignoring the heterogeneity of students’ behavior. Ignoring the longitudinal heterogeneity means mixing trends of different types, e.g., an increasing trend in a subgroup and a decreasing trend in another subgroup exist. Another limitation of the existing longitudinal clustering methods is that cluster membership can not vary with time so one student is assigned to a single longitudinal cluster, which makes it challenging to study variability and transitions.\nAs we have seen in the literature review section, the VaSSTra method can be adapted to various scenarios beyond engagement, such as collaborative roles, attitudes, achievement, or self-efficacy, and can be used with different time points such as tasks, days, weeks, or school years. The reader should refer to Chapter 8 [29] and Chapter 9 [34] about clustering to learn other clustering techniques that may be more appropriate for transforming different types of variables into states (that is, conducting the first step of VaSSTra). Moreover, in Chapter 10 [13], the basics of sequence analysis are described, including how to cluster sequences into trajectories using different distance measures that might be more appropriate in different situations. The next chapter (Chapter 12) [35] presents Markovian modeling, which constitutes another way of clustering sequences into trajectories according to their state transitions. Lastly, Chapter 13 [20] presents multi-channel sequence analysis, which could be used to extend VaSSTra to study several parallel sequences (of several constructs) at the same time."
  },
  {
    "objectID": "chapters/ch12-markov/ch12-markov.html",
    "href": "chapters/ch12-markov/ch12-markov.html",
    "title": "12  A Modern Approach to Transition Analysis and Process Mining with Markov Models in Education",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch12-markov/ch12-markov.html#introduction",
    "href": "chapters/ch12-markov/ch12-markov.html#introduction",
    "title": "12  A Modern Approach to Transition Analysis and Process Mining with Markov Models in Education",
    "section": "1 Introduction",
    "text": "1 Introduction\nIn the previous two chapters, we have learned about sequence analysis [1, 2] and its relevance to educational research. This chapter presents a closely-related method: Markovian models. Specifically, we focus on a particular type of Markovian model, where the data are assumed to be categorical and observed at discrete time intervals, as per the previous chapters about sequence analysis, although in general Markovian models are not restricted to categorical data. One of the main differences between sequence analysis and Markovian modelling is that the former relies on deterministic data mining, whereas the latter uses probabilistic models [3]. Moreover, sequence analysis takes a more holistic approach by analysing sequences as a whole, whereas Markovian modelling focuses on the transitions between states, their probability, and the reasons (covariates) which explain why these transitions happen.\nWe provide an introduction and practical guide to the topic of Markovian models for the analysis of sequence data. While we try to avoid advanced mathematical notations, to allow the reader to continue to other, more advanced sources when necessary, we do introduce the basic mathematical concepts of Markovian models. When doing so, we use the same notation as in the R package seqHMM [4], which we also use in the examples. In particular, we illustrate first-order Markov models, hidden Markov models, mixture Markov models, and mixture hidden Markov models with applications to synthetic data on students’ collaboration roles throughout a complete study program.\nThe chapter proceeds to describe the theoretical underpinnings on each method in turn, then showcases each method with code, before presenting some conclusions and further readings. In addition to the aforementioned applications to collaboration roles and achievement sequences, we also provide a demonstration of the utility of Markovian models in another context, namely process mining. In the process mining application, we leverage Markov models and mixture Markov models to explore learning management system logs. Finally, we conclude with a brief discussion of Markovian models in general and provide some recommendations for further reading of advanced topics in this area as a whole."
  },
  {
    "objectID": "chapters/ch12-markov/ch12-markov.html#methodological-background",
    "href": "chapters/ch12-markov/ch12-markov.html#methodological-background",
    "title": "12  A Modern Approach to Transition Analysis and Process Mining with Markov Models in Education",
    "section": "2 Methodological Background",
    "text": "2 Methodological Background\n\n2.1 Markov model\nThe simple first-order Markov chain or Markov model (MM) can be used to model transitions between successive states. In the first-order MM, given the current observation, the next observation in the sequence is independent of the past —this is called the Markov property (the order of MM determines on how many previous observations the next observation depends on). For example, when predicting a student’s school success in the fourth year under a first-order model, we only need to consider their success in the third year, while their success in the first and second year give no additional information for the prediction (see Figure 12.1 for an illustration). As such, the model is said to be memoryless.\n\n\n\n\n\nFigure 1. Illustration of the Markov Model. The nodes \\(Y_1\\) to \\(Y_4\\) refer to states at time points 1 to 4. The arrows indicate dependencies between states.\n\n\n\n\nAs an example, consider the data described in Table 12.1 which includes four sequences of length ten. The alphabet — that is, the list of all possible states appearing in the data — consists of two types of observed state; low achievement success (\\(L\\)) and high achievement (\\(H\\)). Here, the individuals are assumed to be independent from one another:\n\n\nTable 1. Four example sequences of school achievement with individuals A–D across the rows and years 1–10 across the columns.\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nA\nL\nL\nL\nH\nL\nH\nL\nH\nH\nH\n\n\nB\nL\nH\nH\nL\nH\nL\nH\nL\nL\nH\n\n\nC\nH\nH\nL\nH\nL\nL\nH\nL\nH\nH\n\n\nD\nH\nH\nL\nL\nL\nH\nL\nL\nL\nH\n\n\n\n\nSay \\(t\\) describes the position in the sequence, or in this example, the year (in other words, here \\(t\\) runs from 1 to 10). If we assume that the probability of observing \\(L\\) or \\(H\\) at any given point \\(t\\) depends on the current observation only, we can estimate the transition probabilities \\(a_{LL}\\) (from state \\(L\\) to state \\(L\\)), \\(a_{LH}\\) (\\(L\\) to \\(H\\)), \\(a_{HL}\\) (\\(H\\) to \\(L\\)), and \\(a_{HH}\\) (\\(H\\) to \\(H\\)) by calculating the number of observed transitions from each state to all states and scaling these with the total number of transitions from that state. Mathematically, we can write the transition probability \\(a_{rs}\\) from state \\(r\\) to state \\(s\\) as\n\\[a_{rs}=P(z_t = s\\,|\\,z_{t-1} = r), \\ s,r \\in \\{L,H\\},\\]\nwhich simply states that the observed state \\(z_t\\) in year \\(t\\) being \\(L\\) or \\(H\\) depends on which of the two states were observed in the previous year \\(t-1\\). For example, to compute \\(a_{LH}=P(z_t=H\\,|\\,z_{t-1}=L)\\), the probability of transitioning from the origin state \\(L\\) to the destination state \\(H\\), we divide the eight observed transitions to state \\(H\\) from state \\(L\\) by 20, which is the total number of transitions from \\(L\\) to any state.\nThe basic MM assumes that the transition probabilities remain constant in time (this property is called time-homogeneity). This means, for example, that the probabilities of transitioning from the low-achievement state to the high-achievement state is the same in the ninth year as it was in the second year. We can collect the transition probabilities in a transition matrix (which we call \\(A\\)) which shows all of the possible transition probabilities between each pair of origin and destination states, as illustrated in Table 12.2. For example, when a student has low achievement in year \\(t\\), they have a 40 percent probability to have low achievement in year \\(t+1\\) and a higher 60 percent probability to transition to high achievement instead, regardless of the year \\(t\\). Notice that the probabilities in each row must add up to 1 (or 100%).\n\n\nTable 2. Transition matrix showing the probabilities of transitioning from one state to another (low or high achievement). The rows and columns describe the origin state and the destination state, respectively.\n\n\n\n\\(\\to\\) Low\n\\(\\to\\) High\n\n\n\n\nLow \\(\\to\\)\n8/20 = 0.4\n12/20 = 0.6\n\n\nHigh \\(\\to\\)\n10/16 = 0.625\n6/16 = 0.375\n\n\n\n\nLastly, we need to define probabilities for the starting states of the sequences, i.e., the initial probabilities \\[\\pi_s=P(z_1 = s), \\ s \\in \\{L,H\\}.\\]\nIn the example, half of the students have low achievement and the other half have high achievement in the first year, so \\(\\pi_L=\\pi_H = 0.5\\). This basic MM is very simple and is often not realistic in the context of educational sciences. We can, however, extend the basic MM in several ways.\nFirst of all, we can include covariates to explain the transition and/or initial probabilities. For example, if we think that transitioning from low to high achievement becomes more challenging as the students get older we may add time as an explanatory variable to the model, allowing the probability of transitioning from low to high achievement to decrease in time. We could also increase the order of the Markov chain, accounting for longer histories. This may be more realistic, but at the same time increasing the order makes the model considerably more complex, the more so the longer the history considered.\nSecondly, one of the most useful extensions is the inclusion of hidden (or latent) states that cannot be observed directly but can be estimated from the sequence of observed states. An MM with time-constant hidden states is typically called the mixture Markov model (MMM). It can be used to find latent subpopulations, or in other words, to cluster sequence data. A model with time-varying hidden states is called the hidden Markov model (HMM), which allows the individuals to transition between the hidden states. Allowing for both time-constant and time-varying hidden states leads to a mixture hidden Markov model (MHMM). Unless otherwise specified, from now on when talking about hidden states we refer always to time-varying hidden states, while time-constant hidden states are referred to as clusters.\n\n\n2.2 Mixture Markov model\nConsider a common case in sequence analysis where individual sequences are assumed to be clustered into subpopulations such as those with typically high and low achievement. In the introductory sequence analysis chapter, the clustering of sequences was performed based on a matrix of pairwise dissimilarities between sequences. Alternatively, we can use the MMM to group the sequences based on their initial and transition probabilities, for example, into those who tend to stay in and transition to high achievement states and those that tend to stay in and transition to low achievement states, as illustrated in Table 12.3.\n\n\nTable 3. Two transition matrices showing the probabilities of transitioning from one state of achievement to another in two clusters of Low achievement and High achievement. The rows and columns describe the origin state and the destination state, respectively.\n\n\n\n\n(a) Low achievement\n\n\nCluster: Low achievement\n\\(\\to\\) Low\n\\(\\to\\) High\n\n\n\n\nLow \\(\\to\\)\n0.8\n0.2\n\n\nHigh \\(\\to\\)\n0.4\n0.6\n\n\n\n\n\n\n(b) High achievement\n\n\nCluster: High achievement\n\\(\\to\\) Low\n\\(\\to\\) High\n\n\n\n\nLow \\(\\to\\)\n0.6\n0.4\n\n\nHigh \\(\\to\\)\n0.1\n0.9\n\n\n\n\n\n\nIn MMMs, we have a separate transition matrix \\(A^k\\) for each cluster \\(k\\) (for \\(k=1,\\ldots,K\\) clusters/subpopulations), and the initial state distribution defines the probabilities to start (and stay) in the hidden states corresponding to a particular cluster. This probabilistic clustering provides group membership probabilities for each sequence; these define how likely it is that each individual is a member of each cluster. We can easily add (time-constant) covariates to the model to explain the probabilities of belonging to each cluster. By incorporating covariates in this way we could, for example, find that being in a high-achievement cluster is predicted by gender or family background. However, we note that this is distinct from the aforementioned potential inclusion of covariates to explain the transition and/or initial probabilities.\nAn advantage of this kind of probabilistic modelling approach is that we can use traditional model selection methods such as likelihood information criteria or cross-validation for choosing the best model. For example, if the number of subpopulations is not known in advance —as is typically the case— we can compare models with different clustering solutions (e.g., those obtained with different numbers of clusters, different subsets of covariates, or different sets of initial probabilities, for example) and choose the best-fitting model with, for example, the Bayesian information criterion (BIC) [5].\n\n\n2.3 Hidden Markov model\nThe HMM can be useful in a number of cases when the state of interest cannot be directly measured or when there is measurement error in the observations. In HMMs, the Markov chain operates at the level of hidden states, which subsequently generate or emit observed states with different probabilities. For example, think about a progression of a student’s ability as a hidden state and school success as the observed state. We cannot measure true ability directly, but we can estimate the student’s progress by their test scores that are emissions of their ability. There is, however, some uncertainty in how well the test scores represent students’ true ability. For example, observing low test scores at some point in time does not necessarily mean the student has low ability; they might have scored lower than expected in the test due to other reasons such as being sick at that particular time. Such uncertainty can be reflected in the emission probabilities; for example, in the high-ability state students get high test scores eight times out of ten and low test scores with a 20 percent probability, while in the low-ability state the students get low test scores nine times out of ten and high test scores with a 10 percent probability. These probabilities are collected in an emission matrix as illustrated in Table 12.4.\n\n\nTable 4. Emission matrix showing the probabilities of each hidden state (low or high ability) emitting each observed state (low or high test scores).\n\n\n\nLow scores\nHigh scores\n\n\n\n\nLow ability\n0.9\n0.1\n\n\nHigh ability\n0.2\n0.8\n\n\n\n\nAgain, the full HMM is defined by a set of parameters: the initial state probabilities \\(\\pi_s\\), the hidden state transition probabilities \\(a_{rs}\\), and the emission probabilities of observed states \\(b_s(m)\\). What is different to the MM is that in the HMM, the initial state probabilities \\(\\pi_s\\) define the probabilities of starting from each hidden state. Similarly, the transition probabilities \\(a_{rs}\\) define the probabilities of transitioning from one hidden state to another hidden state. The emission probabilities \\(b_s(m)\\) (collected in an emission matrix \\(B\\)) define the probability of observing a particular state \\(m\\) (e.g., low or high test scores) given the current hidden state \\(s\\) (e.g., low or high ability).\nWhen being in a certain hidden state, observed states occur randomly, following the emission probabilities. Mathematically speaking, instead of assuming the Markov property directly on our observations, we assume that the observations are conditionally independent given the underlying hidden state. We can visualise the HMM as a directed acyclic graph (DAG) illustrated in Figure 12.2. Here \\(Z\\) are the unobserved states (such as ability) which affect the distribution of the observed states \\(Y\\) (test scores). At each time point \\(t\\), the state \\(z_t\\) can obtain one of \\(S\\) possible values (there are two hidden states in the example of low and high ability, so \\(S=2\\)), which in turn defines how \\(Y_t\\) is distributed.\n\n\n\nFigure 2. Illustration of the HMM. The nodes \\(Z_1\\) to \\(Z_4\\) refer to hidden states at time points 1 to 4, while the nodes \\(Y_1\\) to \\(Y_4\\) refer to observed states. The arrows indicate dependencies between hidden and/or observed states.\n\n\n\n\n2.4 Mixture hidden Markov models\nCombining the ideas of both time-constant clusters and time-varying hidden states leads to the concept of mixture hidden Markov model (MHMM). Here we assume that the population of interest consists of a finite number of subpopulations, each with their own HMM with varying transition and emission probabilities. For example, we could expect to find underlying groups which behave differently when estimating the progression of ability through the sequence of test scores, such as those that consistently stay on a low-ability or high-ability track (stayers) and those that move between low and high ability (movers). In this case, we need two transition matrices: the stayers’ transition matrix allows for no transitions while the movers’ transition matrix allows for transitioning between low and high ability, as illustrated in Table 12.5.\n\n\nTable 5. Two transition matrices showing the probabilities of transitioning from one state of ability to another in two clusters, the Stayers and the Movers. The rows and columns describe the origin state and the destination state, respectively.\n\n\n\n\n(a) Stayers\n\n\nCluster: Stayers\n\\(\\to\\) Low\n\\(\\to\\) High\n\n\n\n\nLow \\(\\to\\)\n1\n0\n\n\nHigh \\(\\to\\)\n0\n1\n\n\n\n\n\n\n(b) Movers\n\n\nCluster: Movers\n\\(\\to\\) Low\n\\(\\to\\) High\n\n\n\n\nLow \\(\\to\\)\n0.6\n0.4\n\n\nHigh \\(\\to\\)\n0.3\n0.7\n\n\n\n\n\n\nSimilarly, we need two emission matrices that describe how the observed states are related to hidden states, as illustrated in Table 12.6. In this example, there is a closer match between low/high ability and low/high test scores in the Stayers cluster in comparison to the Movers cluster.\n\n\nTable 6. Two emission matrices showing the probabilities of each hidden state (low or high ability) emitting each observed state (low or high test scores).\n\n\n\n\n(a) Stayers\n\n\nCluster: Stayers\nLow scores\nHigh scores\n\n\n\n\nLow ability\n0.9\n0.1\n\n\nHigh ability\n0.1\n0.9\n\n\n\n\n\n\n(b) Movers\n\n\nCluster: Movers\nLow scores\nHigh scores\n\n\n\n\nLow ability\n0.7\n0.3\n\n\nHigh ability\n0.2\n0.8\n\n\n\n\n\n\nMathematically, when estimating a MHMM we first fix the number of clusters \\(K\\), and create a joint HMM consisting of \\(K\\) submodels (HMMs). The number of hidden states does not have to be fixed but can vary by submodel, so that the HMMs have more hidden states for some clusters and fewer for others (in our example, because the transition matrix is of the Stayers cluster is diagonal, we could also split the cluster into two single state clusters, one corresponding to low and other to high ability). This can increase the burden of model selection, so often a common number of hidden states is assumed for each cluster for simplicity. In any case, the initial state probabilities of this joint model define how sequences are assigned to different clusters. We estimate this joint model using the whole data and calculate cluster membership probabilities for each individual. The idea of using mixtures of HMMs has appeared in literature under various names with slight variations, e.g., [6], [7], and [4]. Notably, MHMMs inherit from MMMs the ability to incorporate covariates to predict cluster memberships.\n\n\n2.5 Multi-channel sequences\nThere are two options to analyse multi-channel (or multi-domain or multi-dimensional) sequence data with Markovian models. The first option is to combine observed states in different channels into one set of single-channel sequences with an expanded alphabet. This option is simple, and works for MMs, HMMs, MMMs, and MHMMs, but can easily lead to complex models as the number of states and channels increases considerably. The second option, which can only be used when working with HMMs and MHMMs, is to treat the observed states in each channel independently given the current hidden state. This can be easily performed by defining multiple emission probability matrices, one for each channel. The assumption of conditional independence simplifies the model, but is sometimes unrealistic, in which case it is better to resort to the first option and convert the data into single-channel sequences. Both options are discussed further in Chapter 13 [8], a dedicated chapter on multi-channel sequences, where applications of distance-based and Markovian clustering approaches are presented. In this chapter, we henceforth focus on single-channel sequences.\n\n\n2.6 Estimating model parameters\nThe model parameters, i.e. the elements of the initial probability vectors \\(\\pi\\), transition probability matrices \\(A\\), and emission probability matrices \\(B\\), can be estimated from data using various methods. Typical choices are the Baum-Welch algorithm (an instance of the expectation-maximisation, i.e., the EM algorithm) and direct (numerical) maximum likelihood estimation. It is possible to restrict models, for example, by setting some parameters to fixed values (typically zeros), for example, to make certain starting states, transitions, or emissions impossible.\nAfter the parameter estimation, in addition to studying the estimated model parameters upon convergence, we can, for example, compute cluster-membership probabilities for each individual and find the most probable paths of hidden state sequences using the Viterbi algorithm ([9]). These can be further analysed and visualised for interpretation."
  },
  {
    "objectID": "chapters/ch12-markov/ch12-markov.html#review-of-the-literature",
    "href": "chapters/ch12-markov/ch12-markov.html#review-of-the-literature",
    "title": "12  A Modern Approach to Transition Analysis and Process Mining with Markov Models in Education",
    "section": "3 Review of the literature",
    "text": "3 Review of the literature\nMarkovian methods have been used across several domains in education and have gained renewed interest with the surge in learning analytics and educational data mining. Furthermore, the introduction of specialised R packages (e.g., seqHMM [10]) and software applications (e.g., Mplus [11, 12]) have made it easier to implement Markov models. One of the most common applications of Markovian methods is the clustering of sequence data [13–15]. Markov models offer a credible alternative to existing distance-based methods (e.g. optimal matching) and can be used with different sequence types (e.g. multi-channel sequences). Furthermore, Markovian methods offer some advantages in clustering sequential data such as the inclusion of covariates that can explain why a sequence emerged (e.g., [16]). More importantly, Markovian models are relatively scalable and can be used to cluster large sequences [17]. As Saqr et al. [17] noted, large sequences are hard to cluster using standard methods such as hierarchical clustering, which is memory inefficient, and hard to parallelise or scale [18, 19]. Furthermore, distance-based clustering methods are limited by the theoretical maximum dimension of a matrix in R which is 2,147,483,647 (i.e., a maximum of 46,430 sequences). In such a case, Markovian methods may be the solution.\nExamples of Markovian methods in clustering sequences are plentiful. For example, HMMs have been used to cluster students’ sequences of learning management system (LMS) trace data to detect their patterns of activities or what the authors referred to as learning tactics and strategies [15]. Another close example was that of López-Pernas and Saqr [20], who used HMMs to cluster multi-channel data of students’ learning strategies of two different tools (an LMS and an automated assessment tool). Other examples include using HMM in clustering sequences of students’ engagement states [21], sequences of students’ collaborative roles [16], or sequences of self-regulation [13, 14].\nMarkovian methods are also popular in studying transitions and have therefore been used across several applications and with different types of data. One of the most common usages is what is known as stochastic processes mining which typically uses first-order Markov models to map students’ transitions between learning activities. For example, Matcha et al. [22] used first-order Markov models to study students’ processes of transitions between different learning tactics. Other uses include studying the transitions between tactics of academic writing [23], between self-regulated learning events [24], or within collaborative learning settings [25]. Yet, most of such work has been performed by the pMiner R package [26], which was recently removed from The Comprehensive R Archive Network (CRAN) due to slow updates and incompatibility with existing guidelines. This chapter offers a modern alternative that uses modern and flexible methods for fitting, plotting, and clustering stochastic process mining models as well as the possibility to add covariates to understand “why” different transitions pattern emerged.\nIndeed, transition analysis in general has been a popular usage for Markovian models and has been used across several studies. For instance, for the analysis of temporal patterns of students’ activities in online learning (e.g., [27]), or transitions between latent states [28], or transitions between assignment submission patterns [29]."
  },
  {
    "objectID": "chapters/ch12-markov/ch12-markov.html#examples",
    "href": "chapters/ch12-markov/ch12-markov.html#examples",
    "title": "12  A Modern Approach to Transition Analysis and Process Mining with Markov Models in Education",
    "section": "4 Examples",
    "text": "4 Examples\nAs a first step, we will import all the packages required for our analyses. We have used most of them throughout the book. Below is a brief summary:\n\nqgraph: A package for visualising networks, which can be used to plot transition probabilities [30]. This is used only for the process mining application in Section 12.4.3.\nrio: A package for reading and saving data files with different extensions [31].\nseqHMM: A package designed for fitting hidden (latent) Markov models and mixture hidden Markov models for social sequence data and other categorical time series [32].\ntidyverse: A package that encompasses several basic packages for data manipulation and wrangling [33].\nTraMineR: As seen in the introductory sequence analysis chapter, this package helps us construct, analyze, and visualise sequences from time-ordered states or events [34].\n\n\nlibrary(qgraph)\nlibrary(rio)\nlibrary(seqHMM)\nlibrary(tidyverse)\nlibrary(TraMineR)\n\nHenceforth, we divide our examples into two parts: the first largely focuses on traditional uses of the seqHMM package to fit Markovian models of varying complexity to sequence data; the latter presents a demonstration of Markovian models from the perspective of process mining. We outline the steps involved in using seqHMM in general in Section 12.4.1, demonstrate the application of MMs, HMMs, MMMs, and MHMMs in Section 12.4.2, and explore process mining using Markovian models in Section 12.4.3, leveraging much of the steps and code from the previous two sections. We note that different datasets are used in Section 12.4.2 and Section 12.4.3; we begin by importing the data required for Section 12.4.2 and defer the importing of the data used in the process mining application to the later section.\nWith this in mind, we start by using the ìmport() function from the rio package to import our sequence data. Based on the description of the MHMM in [35], we used the seqHMM package to simulate a synthetic dataset (simulated_data) consisting of students’ collaboration roles (obtained from [36]) on different courses across a whole program. As the original data, the simulation was based on the two-channel model (collaboration and achievement), but we only use the collaboration sequences in the following examples, and leave the multi-channel sequence analysis to Chapter 13 [8]. While not available in the original study, we also simulated students’ high school grade point average (GPA, for simplicity categorised into three levels) for each student, which will be used to predict cluster memberships. Using this data, we show how the seqHMM package can be used to analyse such sequences. We start with the simple MM, and then transition to HMMs and their mixtures. To be able to use the seqHMM functions, we need to convert the imported data to a sequence using the function seqdef() from the TraMineR package (see Chapter 10 [1] for more information about creating stslist objects). We subsequently assign a colour palette to each state in the alphabet for later visualisations using the function cpal(). Finally, we can also extract the covariate information separately (cov_data).\n\nURL <- \"https://github.com/sonsoleslp/labook-data/raw/main/\"\nsimulated_data <- import(paste0(URL, \"12_longitudinalRoles/simulated_roles.csv\"))\n\nroles_seq <- seqdef(\n  simulated_data, \n  var = 3:22, \n  alphabet = c(\"Isolate\", \"Mediator\", \"Leader\"),\n  cnames = 1:20\n)\n\n [>] 3 distinct states appear in the data: \n\n\n     1 = Isolate\n\n\n     2 = Leader\n\n\n     3 = Mediator\n\n\n [>] state coding:\n\n\n       [alphabet]  [label]  [long label] \n\n\n     1  Isolate     Isolate  Isolate\n\n\n     2  Mediator    Mediator Mediator\n\n\n     3  Leader      Leader   Leader\n\n\n [>] 200 sequences in the data set\n\n\n [>] min/max sequence length: 20/20\n\ncpal(roles_seq) <- c(\"#FBCE4B\", \"#F67067\", \"#5C2262\")\n\ncov_data <- simulated_data |>\n  select(ID, GPA) |>\n  mutate(GPA = factor(GPA, levels = c(\"Low\", \"Middle\", \"High\")))\n\n\n4.1 Steps of estimation\nWe will first briefly introduce the steps of the analysis with the seqHMM package and then show examples of estimating MMs, HMMs, MMMs, and MHMMs.\n\n4.1.1 Defining the model structure\nFirst, we need to create the model object which defines the structure of the model. This can be done by using one of the model building functions of seqHMM. The build functions include build_mm() for constructing the simple MM, build_hmm() for the HMM, build_mmm() for the MMM, and build_mhmm() for the MHMM. The user needs to give the build function the sequence data and the number of hidden states and/or clusters (when relevant). The user can also set restrictions on the models, for example, to forbid some transitions by setting the corresponding transition probabilities to zero. To facilitate the estimation of the parameters of more complex models, the user may also set informative starting values for model parameters.\n\n\n4.1.2 Estimating the model parameters\nAfter defining the model structure, model parameters need to be estimated. The fit_model() function estimates model parameters using maximum likelihood estimation. The function has several arguments for configuring the estimation algorithms. For simple models the default arguments tend to work well enough, but for more complex models the user should adjust the algorithms. This is because the more parameters the algorithm needs to estimate, the higher the risk of not finding the model with the optimal parameter values (the one which maximises the likelihood).\nIn order to reduce the risk of being trapped in a local optimum of the likelihood surface (instead of a global optimum), we advise to estimate the model numerous times using different starting values for the parameters. The seqHMM package strives to automate this. One option is to run the EM algorithm multiple times with random starting values for any or all of initial, transition, and emission probabilities. These are specified in the control_em argument. Although not done by default, this method seems to perform very well as the EM algorithm is relatively fast. Another option is to use a global direct numerical estimation method such as the multilevel single-linkage method. See [4] for more detailed information on model estimation.\n\n\n4.1.3 Examining the results\nThe output of the fit_model contains the estimated model (stored in fit_hmm$model) as well as information about the estimation of the model, such as the log-likelihood of the final model (fit_hmm$logLik). The print method provides information about the estimated model in a written format, while the plot() function visualises the model parameters as a graph. For HMMs and MHMMs, we can calculate the most probable sequence of hidden states for each individual with the hidden_paths() function. Sequences of observed and hidden state sequences can be plotted with the ssplot() function for MMs and HMMs and with the mssplot() function for the MMMs and the MHMMs. For MMMs and MHMMs, the summary() method automatically computes some features of the models such as standard errors for covariates and prior and posterior cluster membership probabilities for the subjects.\n\n\n\n4.2 Markov models\nWe now follow the steps outlined above for each model in turn, starting from the most basic Markov model, proceeding through a hidden Markov model and a mixture Markov model, and finally concluding with a mixture hidden Markov model.\n\n4.2.1 Markov model\nWe focus on the sequences of collaboration roles, collected in the roles_seq object. The build_mm() function only takes one argument, observations, which should be an stslist object created with the seqdef() function from the TraMineR package as mentioned before. We can build a MM as follows:\n\nmarkov_model <- build_mm(roles_seq)\n\nFor the MM, the build_mm() function estimates the initial probabilities and the transition matrix. Note that the build_mm() function is the only build function that automatically estimates the parameters of the model. This is possible because for the MM the estimation is a simple calculation while for the other types of models the estimation process is more complex. The user can access the estimated parameters by calling markov_model$initial_probs and markov_model$transition_probs or view them by using the print method of the model:\n\nprint(markov_model)\n\nInitial probabilities :\n Isolate Mediator   Leader \n   0.375    0.355    0.270 \n\nTransition probabilities :\n          to\nfrom       Isolate Mediator Leader\n  Isolate   0.4231    0.478 0.0987\n  Mediator  0.1900    0.563 0.2467\n  Leader    0.0469    0.428 0.5252\n\n\nWe can see that the initial state probabilities are relatively uniform, with a slightly lower probability for starting in the Leader state. In terms of the transition probabilities, the most distinct feature is that that it is rare to transition directly from the Leader state to Isolate and vice versa (estimated probabilities are about 5% and 10%, respectively). It is also more common to drop from Leader to Mediator (43%) than to increase collaboration from Mediator to Leader (25%). Similarly, the probability of moving from Mediator to Isolate is only 19 percent, but there is a 48 percent chance of transitioning from Isolate to Mediator.\nWe can also draw a graph of the estimated model using the plot method which by default shows the states as pie graphs (for the MM, the pie graphs only consist of one state), transition probabilities as directed arrows, and initial probabilities below each state (see Figure 12.3).\n\nplot(markov_model, \n  legend.prop = 0.2, ncol.legend = 3, \n  edge.label.color = \"black\", vertex.label.color = \"black\")\n\n\n\n\nFigure 3. Estimated Markov model as a pie charts with the transition probabilities shown as labelled edges.\n\n\n\n\n\n\n4.2.2 Hidden Markov models\nThe structure of an HMM is set with the build_hmm() function. In contrast to build_mm(), other build_*() functions such as build_hmm() do not directly estimate the model parameters. For build_hmm(), in addition to observations (an stslist), we need to provide the n_states argument which tells the model how many hidden states to construct. Using again the collaboration roles sequences, if we want to estimate an HMM with two hidden states, we can write:\n\nset.seed(1)\nhidden_markov_model <- build_hmm(observations = roles_seq, n_states = 2)\n\nThe set.seed call ensures that we will always end up with the same exact initial model with hidden states in the same exact order even though we use random values for the initial parameters of the model (which is practical for reproducibility). We are now ready to estimate the model with the fit_model() function. The HMM we want to estimate is simple, so we rely on the default values and again use the print method to provide information about the estimated model:\n\nfit_hmm <- fit_model(hidden_markov_model)\nfit_hmm$model\n\nInitial probabilities :\nState 1 State 2 \n  0.657   0.343 \n\nTransition probabilities :\n         to\nfrom      State 1 State 2\n  State 1  0.9089  0.0911\n  State 2  0.0391  0.9609\n\nEmission probabilities :\n           symbol_names\nstate_names Isolate Mediator Leader\n    State 1  0.4418    0.525 0.0336\n    State 2  0.0242    0.478 0.4980\n\n\nThe estimated initial state probabilities show that it is more probable to start from hidden state 1 than from hidden state 2 (66% vs. 34%). The high transition probabilities on the diagonal of the transition matrix indicate that the students typically tend to stay in the hidden state they currently are in. Transition probabilities between the hidden states are relatively low and also asymmetric: it is more likely that students move from state 1 to state 2 than from state 2 to state 1. Looking at the emission matrices, we see that the role of the students in state 2 is mostly Leader or Mediator (emission probabilities are 50% and 48%). On the other hand, state 1 captures more of those occasions where students are isolated or exhibit at most a moderate level of participation (mediators). We can also visualise this with the plot() method of seqHMM (see Figure 12.4):\n\nplot(fit_hmm$model, \n  ncol.legend = 4, legend.prop = 0.2, \n  edge.label.color = \"black\", vertex.label.color = \"black\"\n)\n\n\n\n\nFigure 4. HMM with two hidden states (pie charts), with transitions between hidden states shown as labelled edges.\n\n\n\n\nThe plot values mainly shows the same information. By default, to simplify the graph, the plotting method combines all states with less than 5% emission probabilities into one category. This threshold can be changed with the combine.slices argument (setting combine.slices = 0 plots all states).\nFor simple models, using n_states is sufficient. It automatically draws random starting values that are then used for the estimation of model parameters. However, as parameter estimation of HMMs and mixture models can be sensitive to starting values of parameters, it may be beneficial to provide starting values manually using the initial_probs, transition_probs, and emission_probs arguments. This is also necessary in case we want to define structural zeros for some of these components, e.g., if we want to restrict the initial probabilities so that each sequence starts from the same hidden state, or if we want to set an upper diagonal transition matrix, which means that the model does not allow transitioning back to previous states (this is called a left-to-right model) [4]. It is also possible to mix random and user-defined starting values by using simulate_*() functions (e.g. simulate_transition_probs()) for some of the model components and user-defined values for others.\nIn the following example we demonstrate estimating a three-state HMM with user-defined starting values for the initial state probabilities and the transition matrix but simulate starting values for the emission matrices. For simulating starting values with simulate_emission_probs, we need to define the number of hidden states, and the number of observed symbols, i.e., the length of the alphabet of the sequences.\n\n## Set seed for randomisation\nset.seed(1)\n\n## Initial state probability vector, must sum to one\ninit_probs <- c(0.3, 0.4, 0.3)\n\n## a 3x3 transition matrix, each row should sum to one\ntrans_probs <- rbind(c(0.8, 0.15, 0.05), c(0.2, 0.6, 0.2), c(0.05, 0.15, 0.8))\n\n## Simulate emission probabilities\nemission_probs <- simulate_emission_probs(\n  n_states = 3, n_symbols = length(alphabet(roles_seq))\n)\n\n## Build the HMM\nhidden_markov_model_2 <- build_hmm(\n  roles_seq, initial_probs = init_probs, transition_probs = trans_probs,\n  emission_probs = emission_probs\n)\n\nOur initial probabilities suggest that it is slightly more likely to start from the second hidden state than the first and the third. Furthermore, the starting values for the transition matrices suggest that staying in hidden states 1 and 3 is more likely than staying in hidden state 2. All non-zero probabilities are, however, mere suggestions and will be estimated with the fit_model() function. We now estimate this model 50 times with the EM algorithm using randomised starting values:\n\nset.seed(1)\nfit_hmm_2 <- fit_model(hidden_markov_model_2, \n  control_em = list(restart = list(times = 50))\n)\n\nWe can get the information on the EM estimation as follows:\n\nfit_hmm_2$em_results\n\n$logLik\n[1] -3546.155\n\n$iterations\n[1] 488\n\n$change\n[1] 9.947132e-11\n\n$best_opt_restart\n [1] -3546.155 -3546.155 -3546.155 -3546.155 -3546.155 -3546.155 -3546.155\n [8] -3546.155 -3546.155 -3546.155 -3546.155 -3546.155 -3546.155 -3546.155\n[15] -3546.155 -3546.155 -3546.155 -3546.155 -3546.155 -3546.155 -3546.155\n[22] -3546.155 -3546.155 -3546.155 -3546.155\n\n\nThe loglik element gives the log-likelihood of the final model. This value has no meaning on its own, but it can be used to compare HMMs with the same data and model structure (e.g., when estimating the same model from different starting values). The iterations and change arguments give information on the last EM estimation round: how many iterations were used until the (local) optimum was found and what was the change in the log-likelihood at the final step.\nThe most interesting element is the last one: best_opt_restart shows the likelihood for 25 (by default) of the best estimation rounds. We advise to always check these to make sure that the best model was found several times from different starting values: this way we can be fairly certain that we have found the actual maximum likelihood estimates of the model parameters (global optimum). In this case all of the 25 log-likelihood values are identical, meaning that it is likely that we have found the best possible model among all HMMs with three hidden states.\n\nplot(fit_hmm_2$model, \n  legend.prop = 0.15, ncol.legend = 3,\n  edge.label.color = \"black\", vertex.label.color = \"black\",\n  combine.slices = 0, trim = 0.0001\n)\n\n\n\n\nFigure 5. HMM with three hidden states (pie charts), with transitions between hidden states shown as labelled edges.\n\n\n\n\nInterpreting the results in Figure 12.5 we see that the first hidden state represents about equal amounts of isolate and mediator roles, the second hidden state represents mainly Leaders and some Mediator roles, and the third hidden state represents mainly Mediator roles and partly Leader roles. Interestingly, none of the students start as Mediator/Leader, while of the other two the Isolate/Mediator state is more typical (two thirds). There are no transitions from the first to the second state nor vice versa, and transition probabilities to the second state are considerably higher than away from it. In other words, it seems that the model has two different origin states and one destination state.\nWe can visualise the observed and/or hidden state sequences with the ssplot() function. The ssplot() function can take an stslist object or a model object of class mm or hmm (see Figure 12.6). Here we want to plot full sequence index plots (type = \"I\") of both observed and hidden states (plots = \"both\") and sort the sequences using multidimensional scaling of hidden states (sortv = \"mds.hidden\"). See the seqHMM manual and visualisation vignette for more information on the different plotting options.\n\nssplot(fit_hmm_2$model, \n  # Plot sequence index plot (full sequences)\n  type = \"I\", \n  # Plot observed and hidden state sequences\n  plots = \"both\", \n  # Sort sequences by the scores of multidimensional scaling\n  sortv = \"mds.hidden\",\n  # X axis tick labels\n  xtlab = 1:20\n)\n\n\n\n\nFigure 6. Observed and hidden state sequences from the HMM with three hidden states.\n\n\n\n\nBy looking at the sequences, we can see that even though none of the students start in hidden state 3, the majority of them transition there. In the end, most students end up alternating between mediating and leadership roles.\nIs the three-state model better than the two-state model? As already mentioned, we can use model selection criteria to test that. To make sure that the three-state model is the best, we also estimate a HMM with four hidden states and then use the Bayesian information criterion for comparing between the three models. Because the four-state model is more complex, we increase the number of re-estimation rounds for the EM algorithm to 100.\n\n## Set seed for randomisation\nset.seed(1)\n\n## Build and estimate a HMM with four states\nhidden_markov_model_3 <- build_hmm(roles_seq, n_states = 4)\n\nfit_hmm_3 <- fit_model(hidden_markov_model_3, \n  control_em = list(restart = list(times = 100))\n)\n\nfit_hmm_3$em_results$best_opt_restart\n\n [1] -3534.304 -3534.304 -3534.304 -3534.304 -3534.304 -3534.304 -3534.304\n [8] -3534.304 -3534.304 -3534.304 -3534.304 -3534.304 -3534.304 -3534.305\n[15] -3534.305 -3534.306 -3534.308 -3534.310 -3534.332 -3534.335 -3534.335\n[22] -3534.335 -3534.336 -3534.337 -3534.337\n\n\nThe best model was found only 13 times out of 101 estimation rounds from randomised starting values. A cautious researcher might be wise to opt for a higher number of estimation rounds for increased certainty, but here we will proceed to calculating the BIC values.\n\nBIC(fit_hmm$model)\n\n[1] 7430.028\n\nBIC(fit_hmm_2$model)\n\n[1] 7208.427\n\nBIC(fit_hmm_3$model)\n\n[1] 7259.37\n\n\nGenerally speaking, the lower the BIC, the better the model. We can see that the three-state model (fit_hmm_2) has the lowest BIC value, so three clusters is the best choice (at least among HMMs with 2–4 hidden states).\n\n\n4.2.3 Mixture Markov models\nThe MMM can be defined with the build_mmm() function. Similarly to HMMs, we need to either give the number of clusters with the n_clusters argument, which generates random starting values for the parameter estimates, or give starting values manually as initial_probs and transition_probs. Here we use random starting values:\n\n## Set seed for randomisation\nset.seed(123)\n## Define model structure (3 clusters)\nmmm <- build_mmm(roles_seq, n_clusters = 3)\n\nAgain, the model is estimated with the fit_model() function:\n\nfit_mmm <- fit_model(mmm)\n\nThe results for each cluster can be plotted one at a time (interactively, the default), or in one joint figure. Here we opt for the latter (see Figure 12.7). At the same time we also illustrate some other plotting options:\n\nplot(fit_mmm$model, \n  # Plot all clusters at the same time\n  interactive = FALSE, \n  # Set the number of rows and columns for cluster plots (one row, three columns)\n  nrow = 1, ncol = 3,\n  # Omit legends\n  with.legend = FALSE, \n  # Choose another layout for the vertices (see plot.igraph)\n  layout = layout_in_circle,\n  # Omit pie graphs from vertices\n  pie = FALSE,\n  # Set state colours\n  vertex.label.color = c(\"black\", \"black\", \"white\"),\n  # Set state label colours\n  vertex.color = cpal(roles_seq),\n  # Increase the size of the circle\n  vertex.size = 80,\n  # Plot state labels instead of initial probabilities\n  vertex.label = \"names\", \n  # Choose font colour for state labels\n  vertex.label.color = \"black\", \n  # Set state label in the centre of the circle\n  vertex.label.dist = 0,\n  # Omit labels for transition probabilities\n  edge.label = NA\n)\n\n\n\n\nFigure 7. MMM with three clusters.\n\n\n\n\nThe following code plots the sequence distribution plot of each cluster (Figure 12.8). In Cluster 1, we see low probabilities to downward mobility and high probabilities for upward mobility, so this cluster describes leadership trajectories. In Cluster 2, we can see that the thickest arrows lead to mediator and isolates roles, so this cluster describes trajectories with less central roles in collaboration. In Cluster 3, we see the highest transition probabilities for entering the mediator role but also some transitions from mediator to leader, so this cluster describes trajectories with more moderate levels of participation in comparison to cluster 1. This behavior is easier to see when visualising the sequences in their most probable clusters. The plot is interactive, so we need to hit ‘Enter’ on the console to generate each plot. Alternatively, we can specify which cluster we want to plot using the which.plots argument.\n\ncl1 <- mssplot(fit_mmm$model, \n  # Plot Y axis\n  yaxis = TRUE, \n  # Legend position\n  with.legend   = \"bottom\",\n  # Legend columns\n  ncol.legend = 3,\n  # Label for Y axis\n  ylab = \"Proportion\"\n)\n\n\n\n\n\n\n\n(a) Cluster 1.\n\n\n\n\n\n\n\n(b) Cluster 2.\n\n\n\n\n\n\n\n\n\n(c) Cluster 3.\n\n\n\n\nFigure 8. State distribution plots by most probable clusters estimated with the mixture Markov model.\n\n\n\nWe can add covariates to the model to explain cluster membership probabilities. For this, we need to provide a data frame (argument data) and the corresponding formula (argument formula). In the example data we use the data frame called cov_data that we created at the beginning of the tutorial with columns ID and GPA, where the order of the ID variable matches to that of the sequence data roles_seq (note that the ID variable is not used in the model building, so the user needs to make sure that both matrices are sorted by ID). We can now use the information about students’ GPA level as a predictor of the cluster memberships.\nNumerical estimation of complex models from random starting values may lead to convergence issues and other problems in the estimation (you may, for example, get warnings about the EM algorithm failing). To avoid such issues, giving informative starting values is often helpful. This model is more complex than the model without covariates and estimation from random starting values leads to convergence issues (not shown here). To facilitate model estimation, we use the results from the previous MMM as informative starting values. Here we also remove the common intercept by adding 0 to the formula, which simplifies the interpretation of the covariate effects later (instead of comparing to a reference category, we get separate coefficients for each of the three GPA categories).\n\nset.seed(98765)\nmmm_2 <- build_mmm(\n  roles_seq, \n  # Starting values for initial probabilities\n  initial_probs = fit_mmm$model$initial_probs,\n  # Starting values for transition probabilities\n  transition_probs = fit_mmm$model$transition_probs,\n  # Data frame for covariates\n  data = cov_data, \n  # Formula for covariates (one-sided)\n  formula = ~ 0 + GPA\n)\n\nAgain, the model is estimated with the fit_model() function. Here we use the EM algorithm with 50 restarts from random starting values:\n\nset.seed(12345)\nfit_mmm_2 <- fit_model(\n  mmm_2, \n  # EM with randomised restarts\n  control_em = list(\n    restart = list(\n      # 50 restarts\n      times = 50, \n      # Store loglik values from all 50 + 1 estimation rounds\n      n_optimum = 51\n    )\n  )\n)\n\nWarning in fit_model(mmm_2, control_em = list(restart = list(times = 50, : EM\nalgorithm failed: Estimation of gamma coefficients failed due to singular\nHessian.\n\n\nThe model was estimated 50 + 1 times (first from the starting values we provided and then from 50 randomised values). We get one warning about the EM algorithm failing. However, 50 estimation rounds were successful. We can check that the best model was found several times from different starting values (37 times, to be precise):\n\nfit_mmm_2$em_results$best_opt_restart\n\n [1] -3614.627 -3614.627 -3614.627 -3614.627 -3614.627 -3614.627 -3614.627\n [8] -3614.627 -3614.627 -3614.627 -3614.627 -3614.627 -3614.627 -3614.627\n[15] -3614.627 -3614.627 -3614.627 -3614.627 -3614.627 -3614.627 -3614.627\n[22] -3614.627 -3614.627 -3614.627 -3614.627 -3614.627 -3614.627 -3614.627\n[29] -3614.627 -3614.627 -3614.627 -3614.627 -3614.627 -3614.627 -3614.627\n[36] -3614.627 -3614.627 -3619.695 -3624.547 -3624.547 -3624.547 -3624.547\n[43] -3624.547 -3624.547 -3624.547 -3624.547 -3624.547 -3624.547 -3631.328\n[50] -3637.344      -Inf\n\n\nWe can now be fairly certain that the optimal model has been found, and can proceed to interpreting the results. The clusters are very similar to what we found before. We can give the clusters more informative labels and then show state distribution plots in each cluster in Figure 12.9:\n\ncluster_names(fit_mmm_2$model) <- c(\n  \"Mainly leader\", \"Isolate/mediator\", \"Mediator/leader\"\n)\nmssplot(fit_mmm_2$model, with.legend = \"bottom\", ncol.legend = 3)\n\n\n\n\n\n\n\n(a) Mainly leader.\n\n\n\n\n\n\n\n(b) Isolate/mediator.\n\n\n\n\n\n\n\n\n\n(c) Mediator/leader.\n\n\n\n\nFigure 9. State distribution plots by most probable clusters estimated with the mixture Markov model with covariates.\n\n\n\nThe model summary shows information about parameter estimates of covariates and prior and posterior cluster membership probabilities (these refer to cluster membership probabilities before or after conditioning on the observed sequences, respectively):\n\nsummary_mmm_2 <- summary(fit_mmm_2$model)\nsummary_mmm_2\n\nCovariate effects :\nMainly leader is the reference.\n\nIsolate/mediator :\n           Estimate  Std. error\nGPALow       1.9221       0.478\nGPAMiddle    0.3901       0.314\nGPAHigh     -0.0451       0.277\n\nMediator/leader :\n           Estimate  Std. error\nGPALow        1.670       0.487\nGPAMiddle     0.411       0.312\nGPAHigh      -0.667       0.332\n\nLog-likelihood: -3614.627   BIC: 7461.487 \n\nMeans of prior cluster probabilities :\n   Mainly leader Isolate/mediator  Mediator/leader \n           0.244            0.425            0.331 \n\nMost probable clusters :\n            Mainly leader  Isolate/mediator  Mediator/leader\ncount                  49                87               64\nproportion          0.245             0.435             0.32\n\nClassification table :\nMean cluster probabilities (in columns) by the most probable cluster (rows)\n\n                 Mainly leader Isolate/mediator Mediator/leader\nMainly leader          0.91758          0.00136          0.0811\nIsolate/mediator       0.00081          0.89841          0.1008\nMediator/leader        0.05902          0.10676          0.8342\n\n\nWe will first interpret the information on prior and posterior cluster membership probabilities and then proceed to interpreting covariate effects. Firstly, the means of prior cluster probabilities give information on how likely each cluster is in the whole population of students (33% in Mediator, 24% in Leader, and 43% in Isolate). Secondly, Most probable clusters shows group sizes and proportions if each student would be classified into the cluster for which they have the highest cluster membership probability.\nThirdly, the Classification table shows mean cluster probabilities (in columns) by the most probable cluster (in rows). We can see that the clusters are fairly crisp (the certainty of cluster memberships are fairly high) because the membership probabilities are large in the diagonal of the table. The uncertainty of the classification is the highest for the Mediator/leader cluster (among those that had the highest membership probability in that cluster, average cluster memberships were 84% for the Mediator/leader cluster, 6% for the Mainly leader cluster, and 10% for the Isolate/mediator cluster) and the highest in the Mainly leader cluster (92% for the Mainly leader cluster, 8% for the Mediator/leader cluster, and 0.1% for the Isolate/mediator cluster).\nThe part titled Covariate effects shows the parameter estimates for the covariates. Interpretation of the values is similar to that of multinomial logistic regression, meaning that we can interpret the direction and uncertainty of the effect –relative to the reference cluster Mainly leader– but we cannot directly interpret the magnitude of the effects (the magnitudes are on log-odds scale). We can see that individuals with low GPA more often end up in the Isolate/mediator cluster and the Mediator/leader cluster in comparison to the Mainly leader cluster (i.e., the standard errors are small in comparison to the parameter estimates), while individuals with high GPA levels end up in the Mediator/leader cluster less often but are not more or less likely to end up in the Isolate/mediator cluster. For categorical covariates such as our GPA variable, we can also easily compute the prior cluster membership probabilities from the estimates with the following call:\n\nexp(fit_mmm_2$model$coefficients)/rowSums(exp(fit_mmm_2$model$coefficients))\n\n          Mainly leader Isolate/mediator Mediator/leader\nGPALow       0.07605453        0.5198587       0.4040868\nGPAMiddle    0.25090105        0.3705958       0.3785031\nGPAHigh      0.40497185        0.3870997       0.2079285\n\n\nThe matrix shows the levels of the covariates in the rows and the clusters in the columns. Among the high-GPA students, 41 percent are classified as Mainly leaders, 39 percent as Isolate/mediators, and 21 percent as Mediator/leaders. Among middle-GPA students classification is relatively uniform (25% as Mainly leaders, 37% as Isolate/mediators and 38 Mediator/leaders) whereas most of the low-GPA students are classified as Isolate/mediators or Mediator/leaders (52% and 40%, respectively).\nThe summary object also calculates prior and posterior cluster memberships for each student. We omit them here, for brevity, but demonstrate that they can be obtained as follows:\n\nprior_prob <- summary_mmm_2$prior_cluster_probabilities\nposterior_prob <- summary_mmm_2$posterior_cluster_probabilities\n\n\n\n4.2.4 Mixture hidden Markov models\nFinally, we will proceed to the most complex of the models, the MHMM.\nFor defining a MHMM, we use the build_mhmm() function. Again, we can use the argument n_states which is now a vector showing the number of hidden states in each cluster (the length of the vector defines the number of clusters). We will begin by estimating a MHMM with three clusters, each with two hidden states:\n\n\n\n\nset.seed(123)\nmhmm <- build_mhmm(\n  roles_seq, \n  n_states = c(2, 2, 2),\n  data = cov_data, \n  formula = ~ 0 + GPA\n)\nfit_mhmm <- fit_model(mhmm)\n\nError in fit_model(mhmm): EM algorithm failed: Estimation of gamma coefficients\nfailed due to singular Hessian.\n\n\nIn this case, we get an error message about the EM algorithm failing. This means that the algorithm was not able to find parameter estimates from the random starting values the build_mhmm() function generated and we need to adjust our code.\nStarting values for the parameters of the MHMM can be given with the arguments initial_probs, transition_probs, and emission_probs. For the MHMM, these are lists of vectors and matrices, one for each cluster. We use the same number of hidden states (two) for each cluster. We define the initial values for the transition and emission probabilities as well as regression coefficients ourselves. We also restrict the initial state probabilities so that in each cluster every student is forced to start from the same (first) hidden state.\n\nset.seed(1)\n\n## Set initial probabilities\ninit <- list(c(1, 0), c(1, 0), c(1, 0))\n\n## Define own transition probabilities\ntrans <- matrix(c(\n  0.9, 0.1,\n  0.1, 0.9\n), nrow = 2, byrow = TRUE)\n\ntranslist <- list(trans, trans, trans)\n\n## Simulate emission probabilities\nemiss <- simulate_emission_probs(\n  n_states = c(2, 2, 2), \n  n_symbols = 3, \n  n_clusters = 3\n)\n\nemiss <- replicate(3, matrix(1/3, 2, 3), simplify = FALSE)\n\n## Define initial values for coefficients\n## Here we start from a case where low GPA correlates with Cluster 1, \n## whereas middle and high GPA has no effect\nbeta <- cbind(0, c(-2, 0, 0), c(-2, 0, 0))\n\n## Define model structure\nmhmm_2 <- build_mhmm(\n  roles_seq, \n  initial_probs = init, transition_probs = translist, \n  emission_probs = emiss, data = cov_data, \n  formula = ~ 0 + GPA, beta = beta\n)\n\nNow that we have built the MHMM, we can estimate its parameters:\n\nset.seed(1)\nsuppressWarnings(fit_mhmm_2 <- fit_model(\n  mhmm_2,\n  control_em = list(restart = list(times = 100, n_optimum = 101)))\n)\n\nWe can now check how many times the log-likelihood values occurred in the 101 estimations:\n\ntable(round(fit_mhmm_2$em_results$best_opt_restart, 2))\n\n\n    -Inf -3672.25 -3595.82 -3588.58 -3584.14 -3526.42 -3525.06 -3519.53 \n      56        2        1        3        1        4        1        2 \n -3519.5 -3519.24 \n      15       16 \n\n\nThe best model was found 16 times out of 101 times, although the second beset model with log-likelihood of -3519.5 is likely almost indistinguishable from the optimal model (-3519.24) as their log-likelihoods are so close to each other.\nWe will start to interpret the model by looking at the sequence plots in each cluster (see Figure 12.10). The function call is interactive. As before, if you only want to plot one cluster you can use the which.plots argument:\n\nmssplot(fit_mhmm_2$model, \n        plots = \"both\", type = \"I\", sortv = \"mds.hidden\", \n        with.legend = \"bottom.combined\", legend.prop = .15)\n\n\n\n\n\n\n\n(a) Cluster 1\n\n\n\n\n\n\n\n(b) Cluster 2\n\n\n\n\n\n\n\n(c) Cluster 3\n\n\n\n\nFigure 10. MHMM estimated sequence distribution plot with hidden states.\n\n\n\nWe can also visualise the model parameters in each cluster (see Figure 12.11):\n\nplot(fit_mhmm_2$model, \n  vertex.size = 60,\n  label.color = \"black\",\n  vertex.label.color = \"black\",\n  edge.color = \"lightgray\",\n  edge.label.color = \"black\",\n  legend.prop = 0.4,\n  ncol.legend = 1, \n  ncol = 3,\n  interactive = FALSE,\n  combine.slices = 0\n)\n\n\n\n\nFigure 11. Transitions between states for each trajectory.\n\n\n\n\nBased on the two plots, we can determine that Cluster 1 describes students who start as leaders but then transition to alternating between mediator and leader. Cluster 2 describes students who start by alternating between isolate and mediator roles and then mainly transition to alternating between mediator and leader roles. Cluster 3 describes students who start as alternating between isolate and mediator roles, after which they transition between isolate/mediator and mediator/leader.\n\ncluster_names(fit_mhmm_2$model) <- c(\n  \"Downward transition\", \"Upward transition\", \"Alternating\"\n)\n\nWith summary(fit_mhmm_2$model) we get the parameter estimates and standard errors for the covariates and information about clustering:\n\nsummary(fit_mhmm_2$model)\n\nCovariate effects :\nDownward transition is the reference.\n\nUpward transition :\n           Estimate  Std. error\nGPALow       -0.455       0.464\nGPAMiddle     0.440       0.310\nGPAHigh      -2.743       0.727\n\nAlternating :\n           Estimate  Std. error\nGPALow       1.3560       0.324\nGPAMiddle    0.3461       0.316\nGPAHigh      0.0468       0.250\n\nLog-likelihood: -3519.243   BIC: 7237.543 \n\nMeans of prior cluster probabilities :\nDownward transition   Upward transition         Alternating \n              0.302               0.181               0.517 \n\nMost probable clusters :\n            Downward transition  Upward transition  Alternating\ncount                        61                 30          109\nproportion                0.305               0.15        0.545\n\nClassification table :\nMean cluster probabilities (in columns) by the most probable cluster (rows)\n\n                    Downward transition Upward transition Alternating\nDownward transition             0.95727            0.0267      0.0161\nUpward transition               0.03007            0.8037      0.1662\nAlternating                     0.00975            0.0962      0.8940\n\n\nWe can see, that the prior probabilities of belonging to each cluster are very different: half of the students can be described as alternating, while of the rest, a downward transition is more typical (31%). Based on the classification table, the Downward transition cluster is rather crisp, while the other two are partly overlapping (see the MMM example for more information on interpreting the classification table).\nThe Covariate effects tables show that, in comparison to Alternating cluster, students with low GPA are less likely to end up in the Upward or Downward transition clusters and students with high GPA are less likely to end up in Upward transition cluster. Again, we can calculate the probabilities of belonging to each cluster by GPA levels:\n\nexp(fit_mhmm_2$model$coefficients)/rowSums(exp(fit_mhmm_2$model$coefficients))\n\n          Downward transition Upward transition Alternating\nGPALow              0.1813217        0.11502283   0.7036555\nGPAMiddle           0.2521406        0.39144399   0.3564154\nGPAHigh             0.4734128        0.03048189   0.4961054\n\n\nThe table shows that students with low GPA typically belong to the Alternating cluster (70 % probability) while students with high GPA mainly end up in the Downward transition cluster (47%) or the Alternating cluster (50%). Most students with middle GPA end up in the Upward transition cluster (39%), but the probabilities are almost as high for the Alternating cluster (36%) and also fairly high for the Downward transition cluster (25%).\nIn light of this, it is worth noting that the covariates do not merely explain the uncovered clusters; as part of the model, they drive the formation of the clusters. In other words, an otherwise identical model without the dependence on the GPA covariate may uncover different groupings with different probabilities.\nIf we are not sure how many clusters or hidden states we expect, or if we wish to investigate different combinations of covariates, we can estimate several models and compare the results with information criteria or cross-validation. Estimating a large number of complex models is, however, very time-consuming. Using prior information for restricting the pool of potential models is useful, and sequence analysis can also be used as a helpful first step [10, 37].\n\n\n\n4.3 Stochastic process mining with Markovian models\nProcess miming is a relatively recent method for the analysis of event-log data (time-stamped logs) which aims to understand the flow and dynamics of the process under study. In education, process mining has been used extensively to analyse learners’ online logs collected from Learning Management Systems (LMS), to understand how they utilize learning resources and transitions between learning activities to mention a few [38, 39]. In this book, we have devoted a full chapter for process mining where we explained how process mining can be performed in R [40]. Yet, in this chapter we will present a novel method that we propose to perform stochastic process mining using MMs. While process mining can be performed using different software, techniques and algorithms, MMs offer a powerful framework for process mining with several advantages over the commonly used methods. First, it is more theoretically aligned with the idea of a transition from an action to an action and that actions are temporally dependent on each other. Second, MMs allow for data to be clustered into similar transition patterns, a possibility not offered by other process mining methods (see the process mining chapter of this book [40]). Third, contrary to other process mining methods, MMs do not require researchers to arbitrarily exclude —or trim— a large part of the data to “simplify” the model. For instance, most of the process mining analyses require an arbitrary cutoff to trim the data so that the process model is readable. This trimming signficaintly affect the resulting model and makes it hard to replicate. Most importantly, MMs have several fit statistics that we can use to compare and judge the model fit as we have seen before.\nSeveral R packages can perform stochastic process mining; in this tutorial we will rely on the same package we discussed earlier and combine it with a powerful visualization that allows us to effectively visualize complex processes. In the next example, we will analyse data extracted from the learning management system logs and offer a detailed guide to process mining. We will also use MMMs to cluster the data into latent patterns of transitions. Given that the traditional plotting function in seqHMM works well with a relatively short alphabet, we will use a new R package called qgraph for plotting. The package qgraph offers powerful visualizations which makes plotting easier, and more interpretable especially for larger models. Furthermore, qgraph allows researchers to use a fixed layout for all the plotted networks so the nodes can be compared to each other more easily.\nLet us now go through the analysis. The next chunk of code imports the prepared sequence data from the sequence analysis chapter. The data belong to a learning analytics course and the events are coded trace logs of students’ actions such as Course view, Instructions, Practicals, Social, etc. Then, we build a sequence object using the function seqdef() from TraMineR.\n\nseq_data <- import(paste0(URL, \"1_moodleLAcourse/LMS_data_wide.xlsx\"))\nseq_data_all <- seqdef(seq_data, var = 7:54 )\n\nBefore proceeding further, it is advisable to visualise the sequences. Figure 12.12 shows the sequence index plot, sorted according to the first states. The data are much larger than the collaboration roles and achievement sequences analysed previously; there are 9478 observations with an alphabet of 12 states. Unlike in the previous example, the sequence lengths vary considerably. Due to this, shorter sequences contain missing values to fill the empty cells in the data frame. However, there are no internal gaps. When creating the sequence object with the seqdef function, TraMineR allows for distinguishing between real missing values (NA, where the true state is unknown) and technical missing values (void) used to pad the sequences to equal lengths. The seqHMM package is able to account for both types of missing values and treats them slightly differently, for example when calculating the most probable paths of hidden states.\n\nseqplot(seq_data_all, \n  type = \"I\", ncol = 4, sortv = \"from.start\",\n  legend.prop = 0.2, cex.legend = 0.7, border = NA,\n  ylab = \"Sequence (sorted)\", xlab = \"Time\"\n)\n\n\n\n\nFigure 12. Sequence index plot for the learning management system logs.\n\n\n\n\nA simple transition analysis can be performed by estimating and plotting the transition probabilities. This can be performed using the TraMineR package. Yet, this simple approach has drawbacks and it is advisable to estimate the MM and use their full power. The next code estimates the transition probabilities of the full dataset and visualize them using the function seqtrate() from TraMineR package.\n\noveralltransitions <- seqtrate(seq_data_all)\n\n\n\n\n\n\n\nTable 7.  Transition probabilities \n  \n    \n    \n      From\\To\n      Applications\n      Assignment\n      Course_view\n      Ethics\n      Feedback\n      General\n      Group_work\n      Instructions\n      La_types\n      Practicals\n      Social\n      Theory\n    \n  \n  \n    Applications\n0.46\n0.07\n0.13\n0.01\n0.01\n0.19\n0.05\n0.01\n0.01\n0.05\n0.00\n0.00\n    Assignment\n0.00\n0.70\n0.19\n0.00\n0.01\n0.02\n0.03\n0.02\n0.02\n0.02\n0.00\n0.00\n    Course_view\n0.01\n0.07\n0.35\n0.01\n0.03\n0.03\n0.28\n0.10\n0.02\n0.08\n0.02\n0.01\n    Ethics\n0.01\n0.00\n0.12\n0.61\n0.01\n0.04\n0.10\n0.01\n0.03\n0.04\n0.01\n0.02\n    Feedback\n0.00\n0.02\n0.23\n0.00\n0.56\n0.00\n0.11\n0.04\n0.01\n0.02\n0.00\n0.00\n    General\n0.04\n0.05\n0.18\n0.01\n0.00\n0.49\n0.06\n0.06\n0.05\n0.03\n0.01\n0.02\n    Group_work\n0.00\n0.01\n0.19\n0.00\n0.01\n0.01\n0.73\n0.02\n0.00\n0.01\n0.01\n0.00\n    Instructions\n0.00\n0.02\n0.33\n0.00\n0.03\n0.04\n0.12\n0.37\n0.02\n0.03\n0.04\n0.00\n    La_types\n0.01\n0.06\n0.24\n0.01\n0.00\n0.10\n0.07\n0.05\n0.38\n0.03\n0.01\n0.03\n    Practicals\n0.00\n0.02\n0.17\n0.00\n0.01\n0.01\n0.03\n0.02\n0.01\n0.73\n0.00\n0.01\n    Social\n0.00\n0.01\n0.25\n0.00\n0.00\n0.01\n0.12\n0.11\n0.01\n0.02\n0.48\n0.00\n    Theory\n0.00\n0.02\n0.15\n0.03\n0.00\n0.02\n0.06\n0.01\n0.05\n0.05\n0.00\n0.60\n  \n  \n  \n\n\n\n\n\nAs we mentioned earlier, we will use a novel plotting technique that is more suitable for large process models. Below, we plot the transition probabilities with the qgraph() function from the qgraph package (Figure 12.13). We use some arguments to improve the process model visualization. First, we use the argument cut = 0.15 to show the edges with probabilities below 0.15 in lower thickness and colour intensity. This cut makes the graph easier to read and less crowded, and gives emphasis to the edges which matter. The argument minimum = 0.05 hides small edges below the probability threshold of 0.05. We use edge.labels = TRUE to show the transition probabilities as edge labels. The argument color gets the colour palette from the sequence with the function cpal() and the argument curveAll = TRUE ensures the graph shows curved edges. The \"colorblind\" theme makes sure that the colours can be seen by everyone regardless of colour vision abilities. Lastly, the mar argument sets the margin of the figure to make all graphical aspects fit within the figure area.\n\nLabelx <- alphabet(seq_data_all) # get the labels to use them as nodes names.\ntransitionsplot <- qgraph(\n  overalltransitions, cut = 0.15, minimum = 0.05, \n  labels = Labelx, edge.labels = TRUE, edge.label.cex = 0.65, \n  color = cpal(seq_data_all), curveAll = TRUE, \n  theme = \"colorblind\", mar = c(4, 3, 4, 3)\n)\n\n\n\n\nFigure 13. Process map for the overall process.\n\n\n\n\nThe seqtrate() function only computes the transition probabilities but does not compute the initial probabilities. While it is not difficult to calculate the proportions of starting in each state, we can also estimate a simple Markov model which does the same with a short command. We do so using the build_mm() function as per Section 12.4.2, recalling that the build_mm() function is distinct from build_hmm(), build_mmm(), and build_mhmm() in that it is the only build function that automatically estimates the parameters of the model.\nThe plotting now includes an extra option called pie = overallmodel$initial_probs which tells qgraph to use the initial probabilities from the fitted MM as the sizes of the pie charts in the borders of the nodes in Figure 12.14. For instance, the pie around Course view is around half of the circle corresponding to 0.48 initial probability to start from Course view. Please also note that the graph is otherwise equal to the one generated via seqtrate() apart from these initial probabilities.\n\noverallmodel <- build_mm(seq_data_all)\n\noverallplot <- qgraph(\n  overalltransitions, \n  cut = 0.15, \n  minimum = 0.05, \n  labels = Labelx, \n  mar = c(4, 3, 4, 3), \n  edge.labels = TRUE, \n  edge.label.cex = 0.65, \n  color = cpal(seq_data_all), \n  curveAll = TRUE, \n  theme = \"colorblind\", \n  pie = overallmodel$initial_probs\n)\n\n\n\n\nFigure 14. Process map for the overall process with initial probabilities.\n\n\n\n\nHaving plotted the transitions of the full dataset, we can now look for transition patterns, that is typical transition patterns (i.e., clusters) that are repeated within the data. The procedure is the same as before. In the next example, we use the function build_mmm() to build the model with four clusters as a demonstration. Ideally, researchers need to estimate several models and choose the best model based on model selection criteria (such as BIC) values as well as interpretability.\nThe steps involved in fitting the model are as before; we make use of the function fit_model() to estimate the model. The results of the running the code will be an MM for each cluster (with distinct initial and transition probabilities). Given the number of sequences in the dataset, their length, and the number of states, the computational burden is larger than for previous applications in this chapter. For illustrative purposes, instead of repeated EM runs with random starting values, we use single EM run followed by global optimisation, using the argument global_step = TRUE. One benefit of this global (and local) step in fit_model over the EM algorithm is the flexibility to define a maximum runtime (in seconds) for the optimization process (argument maxtime in control_global). This can be valuable for larger problems with predefined runtime (e.g., in a shared computer cluster). Note, however, that relying on the runtime can lead to non-reproducible results even with fixed seed if the optimisation terminates due to the time limit. Finally, we run additional local optimisation step using the results of the global optimisation, for more accurate results. The last argument threads = 16 instructs to use parallel computing to enable faster fitting (please, customise according to the number of cores in your computer). As for the starting values, we use the transition probabilities computed from the full data for all clusters, and random values for the initial probabilities.\nWhile in theory many of global optimisation algorithms should eventually find the global optimum, in practice there are no guarantees that it is found in limited time. Thus, as earlier, in practice it is advisable to try different global/local optimisation algorithms and/or EM algorithm with different initial values to make it more likely that the global optimum is found (see [4] for further discussion).\n\nset.seed(1)\ntrans_probs <- simulate_transition_probs(12, 4, diag_c = 5)\ninit_probs <- as.numeric(prop.table(table(seq_data_all[,1])[1:12]))\ninit_probs <- replicate(4, init_probs, simplify = FALSE)\n\nbuiltseqLMS <- build_mmm(\n  seq_data_all,\n  transition_probs = trans_probs,\n  initial_probs = init_probs\n)\n\nfitLMS <- fit_model(\n  builtseqLMS, \n  global_step = TRUE,\n  control_global = list(\n    maxtime = 3600, \n    maxeval = 1e5,\n    algorithm = \"NLOPT_GD_STOGO_RAND\"),\n  local_step = TRUE,\n  threads = 16\n)\n\nfitLMS$global_results$message\nfitLMS$logLik\n\n\n\n\n\n\n[1] \"NLOPT_SUCCESS: Generic success return value.\"\n\n\n[1] -114491.2\n\n\nBefore plotting the clusters, let us do some cleanups. First, we get the transition probabilities of each cluster and assign them to a variable. In that way, it is easier to manipulate and work with. In the same way, we can extract the initial probabilities for each cluster.\n\n##extract transition probabilities of each cluster\nClustertp1 <- fitLMS$model$transition_probs$`Cluster 1`\nClustertp2 <- fitLMS$model$transition_probs$`Cluster 2`\nClustertp3 <- fitLMS$model$transition_probs$`Cluster 3`\nClustertp4 <- fitLMS$model$transition_probs$`Cluster 4`\n\n##extract initial probabilities of each cluster\nClusterinitp1 <- fitLMS$model$initial_probs$`Cluster 1`\nClusterinitp2 <- fitLMS$model$initial_probs$`Cluster 2`\nClusterinitp3 <- fitLMS$model$initial_probs$`Cluster 3`\nClusterinitp4 <- fitLMS$model$initial_probs$`Cluster 4`\n\nPlotting the process maps can be performed in the same way we did before. However, if we need to compare clusters, it is best if we use a unified layout. An average layout can be computed with the function averageLayout() which takes the transition probabilities of the four clusters as input and creates — as the name implies— an averaged layout. Another option is to use the same layout of the overallplot in the previous example. This can be obtained from the plot object overallplot$layout. This can be helpful if you would like to plot the four plots corresponding to each cluster with the same layout as the overall plot (see Figure 12.15).\n\nLabelx <- colnames(Clustertp1) # we need to get the labels\n\nAveragelayout <- averageLayout(\n  list(Clustertp1, Clustertp2, Clustertp3, Clustertp4)\n)\n## You can also try with this layout from the previous plot\nOveralllayout <- overallplot$layout \n\nqgraph(\n  Clustertp1, cut = 0.15, minimum = 0.05 , labels = Labelx,\n  edge.labels = TRUE, edge.label.cex = 0.65, color = cpal(seq_data_all), \n  layout = Averagelayout, pie = Clusterinitp1, curveAll = TRUE, \n  theme = \"colorblind\", title = \"Diverse\"\n)\n\nqgraph(\n  Clustertp2, cut = 0.15, minimum = 0.05, labels = Labelx,\n  edge.labels = TRUE, edge.label.cex = 0.65, color = cpal(seq_data_all),  \n  layout = Averagelayout, pie = Clusterinitp2, curveAll = TRUE, \n  theme = \"colorblind\", title = \"Assignment-oriented\"\n)\n\nqgraph(\n  Clustertp3, cut = 0.15, minimum = 0.05, labels = Labelx,\n  edge.labels = TRUE, edge.label.cex = 0.65, color = cpal(seq_data_all),  \n  layout = Averagelayout, pie = Clusterinitp3, curveAll = TRUE, \n  theme = \"colorblind\", title = \"Practical-oriented\"\n)\n\nqgraph(\n  Clustertp4, cut = 0.15, minimum = 0.05 , labels = Labelx, \n  edge.labels = TRUE, edge.label.cex = 0.65, color = cpal(seq_data_all),  \n  layout = Averagelayout, pie = Clusterinitp4, curveAll = TRUE, \n  theme = \"colorblind\", title = \"Group-centered\"\n)\n\n\n\n\nFigure 15. Process maps for each cluster.\n\n\n\n\nOftentimes, the researcher is interested in comparing two pre-defined fixed groups, e.g., high achievers and low achievers, rather than between the computed clusters. In the next example we will compare high to low achievers based on their achievement levels. First, we have to create a separate sequence object for each group. We do this by filtering but you can do it in other ways. For instance, you can create two sequences from scratch for each group. The next is to build the MMs separately for each group.\n\nseq_high <- seq_data_all[seq_data$Achievementlevel4 <= 2,]\nseq_low <-  seq_data_all[seq_data$Achievementlevel4 > 2,]\n\nhigh_mm <- build_mm(seq_high)\nlow_mm <- build_mm(seq_low)\n\nBefore plotting the groups, let us do some cleaning, like we did before. First, we get the transition and initial probabilities of each group. We also compute an average layout. Please note that you can use the layout from the previous examples if you are comparing the models against each other and you need a unified framework. The plotting is the same as before (see Figure 12.16).\n\npar(mfrow=c(1, 2))\n\n##extract transition probabilities of each cluster\nHighprobs <- high_mm$transition_probs\nLowprobs <- low_mm$transition_probs\n\n##extract initial probabilities of each cluster\nHighinit <- high_mm$initial_probs\nLowinit <- high_mm$initial_probs\n\nAveragelayout <- averageLayout(list(Highprobs, Lowprobs))\n\nHighplot <- qgraph(\n  Highprobs, cut = 0.15, minimum = 0.05, labels = Labelx,\n  edge.labels = TRUE, edge.label.cex = 0.65, \n  color = cpal(seq_data_all), layout = Averagelayout, \n  pie = Highinit, theme = \"colorblind\", title = \"High achievers\"\n)\n\nLowplot <-  qgraph(\n  Lowprobs, cut=0.15, minimum = 0.05, labels = Labelx,\n  edge.labels = TRUE, edge.label.cex = 0.65, \n  color = cpal(seq_data_all), layout = Averagelayout, \n  pie = Lowinit, theme = \"colorblind\", title = \"Low achievers\"\n)\n\n\n\n\nFigure 16. Process maps for high achievers and low achievers using average layout.\n\n\n\n\nWe can also plot the difference plot (see Figure 12.17); that is, what the low achievers do less than high achievers. In this case, red edges are negative (events they do less) and blue edges are positive (events that they do more than high achievers). As you can see, the differences are not that huge. In fact, much of the literature comparing high and low achievers uses higher thresholds e.g., top 25% to bottom 25% or even top 10% to bottom 10%.\n\ndiffplot <- qgraph(\n  Lowprobs - Highprobs, cut = 0.15, minimum = 0.05, labels = Labelx,\n  edge.labels = TRUE, edge.label.cex = 0.65, layout = Averagelayout, \n  color = cpal(seq_data_all), theme = \"colorblind\"\n)\n\n\n\n\nFigure 17. Difference between process maps of high achievers and low achievers using average layout."
  },
  {
    "objectID": "chapters/ch12-markov/ch12-markov.html#conclusions-further-readings",
    "href": "chapters/ch12-markov/ch12-markov.html#conclusions-further-readings",
    "title": "12  A Modern Approach to Transition Analysis and Process Mining with Markov Models in Education",
    "section": "5 Conclusions & further readings",
    "text": "5 Conclusions & further readings\nMarkovian models provide a flexible model-based approach for analysing complex sequence data. MMs and HMMs have proven useful in many application areas such as biology and speech recognition, and can be a valuable tool in analysing data in educational settings as well. Their mixture variants allow for the representation of complex systems by combining multiple MMs or HMMs, each capturing different aspects of the underlying processes, allowing probabilistic clustering, information compression (e.g. visualisation of multicategory data from multiple domains), and detection of latent features of sequence data (e.g, extraction of different learning strategies). The ability to incorporate covariates in the case of MMMs and MHMMs makes those models even more powerful, and generally MMs and MMMs represent useful tools in the field of process mining also.\nThe seqHMM package used in the examples supports time-constant covariates for predicting cluster memberships for each individual. In theory, covariates could be used to define transition or emission probabilities as well, leading to subject-specific and possibly time-varying transition and emission probabilities (in the case of time-varying covariates). However, at the time of writing this chapter, these are not supported in seqHMM (this may change in the future). In R, there are at least two other, potentially useful packages: for MMs, the dynamite [41] package supports covariates on the transition probabilities with potentially time-varying effects, whereas LMest [42] supports MMs and HMMs with covariates, and restricted variants of the MHMM where only the initial and transition matrices vary between clusters. Going beyond the R software, some commercial software also offers tools to analyse Markovian models, including latentGold [43] and Mplus [11].\nThe conditional independence assumption of observations given the latent states in HMMs can sometimes be unrealistic. In these settings, the so-called double chain MMs can be used [44]. There the current observation is allowed to depend on both the current state and the previous observation. Some restricted variants of such models are implemented in the march package in R [45]. Finally, variable-order MMs extend basic MMs by allowing the order of the MM to vary in time. A TraMineR-compatible implementation of variable-order models can be found in the PST package [46].\nWe encourage readers to read more about how to interpret the results in the original study where the data for this chapter was drawn from [36]. We also encourage readers to learn more about Markovian models in the context of multi-channel sequence analysis in Chapter 13 [8]."
  },
  {
    "objectID": "chapters/ch12-markov/ch12-markov.html#acknowledgements",
    "href": "chapters/ch12-markov/ch12-markov.html#acknowledgements",
    "title": "12  A Modern Approach to Transition Analysis and Process Mining with Markov Models in Education",
    "section": "6 Acknowledgements",
    "text": "6 Acknowledgements\nJH and SH were supported by Research Council of Finland (PREDLIFE: Towards well-informed decisions: Predicting long-term effects of policy reforms on life trajectories, decision numbers 331817 and 331816, and Research Flagship INVEST: Inequalities, Interventions and New Welfare State, decision number 345546). SH was also supported by the Strategic Research Council (SRC), FLUX consortium: Family Formation in Flux – Causes, Consequences, and Possible Futures (decision numbers: 345130 and 345130). MS was supported by Research Council of Finland (TOPEILA: Towards precision education: Idiographic learning analytics, decision number 350560)"
  },
  {
    "objectID": "chapters/ch13-multichannel/ch13-multi.html",
    "href": "chapters/ch13-multichannel/ch13-multi.html",
    "title": "13  Multi-channel sequence analysis in educational research: An introduction and tutorial with R",
    "section": "",
    "text": "14 Acknowledgements\nSH was supported by the Academy of Finland (decision number 331816), the Academy of Finland Flagship Programme (decision number 320162), and the Strategic Research Council (SRC), FLUX consortium (decision numbers: 345130 and 345130). MS was supported by the Academy of Finland (TOPEILA: Towards precision education: Idiographic learning analytics, grant 350560)."
  },
  {
    "objectID": "chapters/ch13-multichannel/ch13-multi.html#introduction",
    "href": "chapters/ch13-multichannel/ch13-multi.html#introduction",
    "title": "13  Multi-channel sequence analysis in educational research: An introduction and tutorial with R",
    "section": "13.1 Introduction",
    "text": "13.1 Introduction\nLearning is a dynamic phenomenon which follows the unidirectional forward law of time; that is, learning occurs sequentially in a time-ordered manner [1, 2]. Throughout this book, we have devoted several chapters to sequence analysis of learner-related data, due to its increasingly central role in learning analytics and education research as a whole. First, in Chapter 10 [3], we presented an introduction to sequence analysis and a discussion on the relevance of this method. Subsequently, in Chapter 11 [4], we studied how to build sequences from multiple variables and analyse more complex aspects of sequences. Both Chapter 10 and Chapter 11 also deal with clustering sequences into trajectories which undergo a similar evolution using distance-based methods. Then, in Chapter 12 [5], we learned how to cluster sequential data using Markov models. Though Chapter 12 [5] touched briefly on simultaneous analysis of multi-channel sequences —albeit only from the Markovian point of view— we present the multi-channel perspective in greater detail here.\nIn this new chapter, we cover multi-channel sequence analysis, which deals with the analysis of two or more synchronised sequences, in greater detail. An example in educational research would be the analysis of the simultaneous unfolding of motivation, achievement, and engament sequences. Multi-channel sequence analysis is a rather novel method in social sciences [6–9] and its applications within educational research in general remain scarce. The increasing availability of student multimodal temporal data makes multi-channel sequence analysis a relevant and timely method to tackle the challenges that these new data bring.\nThroughout this chapter, we describe multi-channel sequence analysis in detail, with an emphasis on how to detect patterns within the sequences, i.e., clusters —or trajectories— of multi-channel sequences that share similar temporal evolutions (or similar trajectories). We illustrate the method with a case study in which we examine students’ sequences of online engagement and academic achievement, where we analyse the longitudinal association between both constructs simultaneously. Here, we outline two perspectives on clustering multi-channel sequences and present both in the tutorial. The first approach uses distance-based clustering algorithms, very much in the spirit of the single-channel cluster analysis described in Chapter 10. We describe some limitations of this framework and then principally focus on a more efficient approach to identifying distinct trajectories using mixture hidden Markov models. We also show how covariates can be incorporated to make the Markovian framework even more powerful. This main analysis is inspired by the recently published paper by [10]. In the next section, we provide a description of the multi-channel sequence analysis framework. We follow with a review of the existing literature in learning analytics that has relied on multi-channel sequence analysis. Next, we include a step-by-step tutorial on how to implement multi-channel sequence analysis with R, with particular attention paid to clustering via distance-based algorithms and mixture hidden Markov models. Finally, we conclude with a brief discussion and provide recommendations for further reading."
  },
  {
    "objectID": "chapters/ch13-multichannel/ch13-multi.html#multi-channel-sequence-analysis",
    "href": "chapters/ch13-multichannel/ch13-multi.html#multi-channel-sequence-analysis",
    "title": "13  Multi-channel sequence analysis in educational research: An introduction and tutorial with R",
    "section": "13.2 Multi-channel sequence analysis",
    "text": "13.2 Multi-channel sequence analysis\nMulti-channel sequence analysis refers to the process of analysing sequential data that consists of multiple parallel channels or streams of (categorical) information. Each channel provides a different perspective or type of information. The goal of multi-channel sequence analysis is to jointly explore the dependencies and temporal interactions between the different channels and extract meaningful insights that may not be evident when considering each channel in isolation. The sources of information can be of very varied nature. For example, using video data of students working on a collaborative task, we could code students’ spoken utterances in one channel, and their facial expressions in another channel throughout the whole session (see Figure 13.1). In this way, we could analyse how students’ expressions relate to what they are saying. Multi-channel sequence analysis often follows the following steps:\n\n\n\nFigure 1. An example of two channels of sequence data where the first channel (top) represents students’ utterances in collaborative work and the second channel (bottom) represents their facial expressions.\n\n\n\n13.2.1 Step 1: Building the channel sequences\nJust like in regular sequence analysis (see Chapter 10 [3]), the first step in multi-channel sequence analysis is to construct the sequences which constitute the different channels. A sequence is an ordered list of categorical elements (i.e., events, states or categories). These elements are discrete (as opposed to numerical values such as grades) and are ordered chronologically. In a sequence, time is not a continuum but a succession of discrete timepoints. These timepoints can be generated by sampling (e.g., each 30 seconds constitutes a new timepoint) or can emerge from an existing time scheme (e.g., each new lesson in a course is a new timepoint). In multi-channel sequence analysis, it is very important that all channels (i.e., parallel sequences) follow the same time scheme so they can be synchronised. As well as time points being aligned in this fashion, it is also typically assumed that the sequence length for each observation matches across channels (although some of the methods used can also deal with partially missing information). The elements of each sequence —typically referred to as the alphabet, which is unique for each channel— can be predefined events or categories from the beginning (e.g., utterances or facial expressions) or, in cases where we are dealing with numerical variables (e.g., heart rate or grades), we can convert them into a state or category by dividing the numerical variable into levels (e.g., tertiles, quartiles) or using clustering techniques. This way, we can focus on sharp changes in numerical variables and conceal small, probably insignificant changes. As Winne puts it [11], “reliability can sometimes be improved by tuning grain size of data so it is neither too coarse, masking variance within bins, nor too fine-grained, inviting distinctions that cannot be made reliably”. Once we have our channels represented as sequences of ordered elements that follow the same time scheme, we can use the steps learned in the introductory sequence analysis chapter to construct the sequences.\n\n\n13.2.2 Step 2: Visualising the multi-channel sequence\nWhen we have built the sequences corresponding to all of the channels, we can visualise the data. Visualising multi-channel sequence data is not a straightforward task, and we need to decide whether we want to present each channel separately or extend the alphabet to show a plot of combined states—or sometimes even both. The extended alphabet approach —in which the states are the combination of the states of all the channels— helps us understand the structure of the data and the typical combinations and evolution of the combined states from all of the channels. It usually works well when (1) the extended alphabet is moderate in size, (2) when there are no state combinations that are very rare, and (3) when the states are either fully observed or missing in all channels at the same time. The extended alphabet is likely to be of moderate size when there are at most 2–3 channels with a small alphabet in each, or if some combinations are not present in the data. Rare state combinations are typically impractical both when visualizing and analyzing the data—especially if there are many rare combinations or if the data are very sensitive in nature. Finally, if there are partially missing observations, i.e., observations that at some specific time point are missing from some of the channels but not all, we would need to deal with each combination of missing and observed states separately, which further extends the alphabet. If one or more of the three conditions are not met, it is often preferable to resort to presenting each channel separately or using visualization techniques that summarise information (e.g., sequence distribution plots).\nContinuing with the previous example, the possible states in the extended alphabet would be all the combinations between the alphabet of the utterances channel (Question-Argument-Agreement) and the alphabet of the facial expressions channel (Happy-Neutral-Anxious). In Figure 13.2, we can see that the first student starts by ‘Question/Happy’, then goes to ‘Argument/Anxious’ and so on.\n\n\n\nFigure 2. An example of a multi-channel sequence object obtained by using the extended alphabet approach, i.e., combining the states of the utterances and facial expressions channels of the data shown in Figure 13.1.\n\n\n\n\n13.2.3 Step 3: Finding patterns (clusters or trajectories)\nPatterns may exist in all types of sequences and, in fact, in all types of data. Discovering such patterns, variations, or groups of patterns is a central theme in analytics. In sequence analysis, similar patterns reflect common temporal pathways where the data share a similar temporal course. This could be a group of students, a typical succession of behavior, or a sequence of interactions. As such, the last typical step in the analysis is to find such patterns. Since these patterns are otherwise latent or unobservable, we need a clustering algorithm that unveils such hidden patterns. There are different possibilities to cluster multi-channel sequence data, of which two are described below: extensions to traditional distance-based clustering approaches widely used in single-channel analyses and Markovian models. The two approaches are then further distinguished in Section 13.2.4, with regard to the ability of the Markovian approach to directly accommodate covariates.\n\n13.2.3.1 Traditional sequence analysis extensions\nBroadly speaking, the first paradigm involves extending techniques commonly adopted in single-channel analyses to the multi-channel domain, by suitably summarising the information arising from all constituent channels. Within this first framework, two different approaches have been proposed.\nThe first consists of flattening the multi-channel data by combining the states (such as in the example in Figure 13.2) and treating the sequence as a single channel with an extended alphabet. Typically, one would then compute dissimilarities using this extended alphabet, most commonly using Optimal Matching (OM). However, this approach is not limited to distance-based clustering [12]; any of the methods, distance-based or otherwise, that we have seen in the previous chapters related to sequence analysis can be used to cluster sequences represented in this way (e.g., agglomerative hierarchical clustering using OM, or hidden Markov models). While this seems like an easier choice, the models tend to become too complex —in the sense of requiring a larger number of clusters and/or hidden states in order to adequately characterise the data— when the number of channels or numbers of per-channel states increase. Moreover, for even a moderate number of channels and/or moderate numbers of states per-channel, the size of the combined alphabet can become unwieldy. Indeed, using this approach in conjunction with OM has been criticised because of the difficulty of specifying appropriate substitution costs for such large combined alphabets [13]. As a reminder, the substitution cost refers to the penalty associated with replacing one element of a sequence with another one; these dissimilarites between all pairs of states are then used to calculate all pairwise distances between whole sequences, which are treated as the input to distance-based clustering algorithms (see Chapter 10 [3]).\nThe second, more common approach is explicitly distance-based and relies on an extension of the OM metric itself, by using a set of overall multi-channel costs derived from channel-specific costs, to calculate a pairwise dissimilarity matrix. The method relies on computing substitution costs for each constituent channel, combining the channels into a new multi-channel sequence object, and deriving an overall substitution cost matrix for the multi-channel object by adding (or averaging) the appropriate substitution costs across channels for a particular multi-state change, typically with equal weight given to each channel [14]. For instance, if the cost associated with substituting the states ‘Question’ and ‘Agreement’ in the utterances channel in Figure 13.1 is \\(0.6\\) and the cost of substituting ‘Happy’ and ‘Neutral’ in the facial expressions channel is \\(1.2\\), the cost associated with substitutions of ‘Question/Neutral’ for ‘Question/Happy’ and ‘Question/Neutral’ for ‘Agreement/Happy’ in Figure 13.2 would be \\(1.2\\) and \\(1.8\\), respectively. From such multi-channel costs, a pairwise multi-channel dissimilarity matrix can be obtained and all subsequent steps of the clustering analysis can proceed as per Chapter 10 thereafter.\nThat being said, adopting the extended OM approach rather than the extended alphabet approach does not resolve all limitations of the distance-based clustering paradigm. As Saqr et al. [15] noted, large sequences are hard to cluster using standard methods such as hierarchical clustering, which is memory inefficient and hard to parallelise or scale [16, 17]. Furthermore, distance-based clustering methods are limited by the theoretical maximum dimension of a matrix in R which is 2,147,483,647,  corresponding to a maximum of 46,430 sequences. Using weights can fix the memory issue if the number of unique sequences remains below the threshold [18]. However, the more states (and combinations of states), the more unique the sequences tend to be, so the memory issue is even more typical with multi-channel sequence data. In such a case, Markovian methods may be the solution. Moreover, the particular choice of distance-based clustering algorithm must be chosen with care, and as ever the user must pre-specify the number of clusters, thereby often necessitating the evaluation of several computing solutions with different numbers of clusters, different hierarchical clustering linkage criteria, or different distance-based clustering algorithms entirely.\n\n\n13.2.3.2 Mixture hidden Markov models\nEven so, the extended OM approach is still liable to overestimate the number of clusters. A second, more sophisticated option is to use mixture hidden Markov models (MHMM) [19], which we have already encountered in some detail in Chapter 12. Such models notably support both single- and multi-channel sequences and often lead to more parsimonious representations of the latent group structure than the distance-based paradigm. MHMMs are a thus a far more efficient and powerful method for temporally aligning the multiple channels of data into homogeneous temporal patterns. As we have learned in Chapter 12, when we estimate a mixture hidden Markov model, we need to specify the number of clusters (\\(K\\)) and we create \\(K\\) submodels that are each a hidden Markov model. Each submodel can have a different number of hidden states, which allows for great flexibility in modeling the variability of the data, without unduly inflating the number of clusters as per the distance-based approaches, thereby enabling more concise and interpretable characterisation of the distinct patterns in the data. Accommodating multi-channel sequences in the MHMM framework requires treating the observed states in each channel independently given the current hidden state. This can be easily performed by defining multiple emission probability matrices (one per channel). The assumption of conditional independence simplifies the model but is sometimes unrealistic, particularly if there are strong time-dependencies between observed states even after accounting for the hidden state. In some cases, it would be better to either analyse single-channel sequences (using an extended alphabet) or resort to the distance-based paradigm described above.\nWe also learned in Chapter 12 that an advantage of MHMMs, as a probabilistic modelling approach, is that we can use traditional model selection methods such as likelihood-based information criteria or cross-validation for choosing the best model. For example, if the number of subpopulations is not known in advance —as is typically the case— we can compare models with different clustering solutions (be they those obtained with different numbers of clusters or different sets of initial probabilities, for example) and choose the best-fitting model with, for example, the Bayesian information criterion (BIC) [20]. Conversely, the distance-based paradigm relies on heuristic cluster quality measures for conducting model selection. In addition, an advantage of the distance-based approach is that it does not rely on making any assumptions about the data-generating mechanism. In other words, it does not assume that the data follows a generative probabilistic model, Markovian or otherwise.\nThe extended OM method for calculating multi-channel dissimilarities is implemented in the R package TraMineR and is applied in our case study, while the extended alphabet approach (i.e., combining all channels into a single one where the alphabet contains all possible state combinations) is not pursued any further here. We also cluster via the MHMM framework in our case study, using the R package seqHMM [21], in order to empirically compare and contrast both perspectives. It is worth noting that, of the Markovian methods we learned about in Chapter 12, only hidden Markov models and mixture hidden Markov models explicitly allow for multi-channel sequences by treating the observed states in each channel independently given the current hidden state. The more basic Markov and mixture Markov models can only accommodate multi-channel sequences by treating them as a single channel using the extended alphabet approach described above, but we do not consider this option any further here. To summarise, we do not explore the extended alphabet approach in either the distance-based or Markovian settings; we only demonstrate the multi-channel extension of the OM metric from the distance-based point of view, and only the MHMM approach from the Markovian point of view.\n\n\n\n13.2.4 Step 4: Relating clusters to covariates\nAs we have seen, there are, broadly speaking, two conflicting perspectives on clustering in the sequence analysis community; algorithmic, distance-based clustering on the one hand, and model-based clustering on the other. Distance-based clustering of multi-channel sequences has already been described above, but it is useful to acknowledge that mixture Markov models and mixture hidden Markov models are precisely examples of model-based clustering methods. Moreover, seqHMM and the methods it implements are unique in offering a model-based approach to clustering multi-channel sequences. A key advantage of the model-based clustering framework is that it can easily be extended to directly account for information available in the form of covariates, as part of the underlying probabilistic model. Though the recently introduced mixture of exponential-distance models framework [22] attempts to reconcile the distance-based and model-based cultures in sequence analysis, while allowing for the direct incorporation of covariates, it is not based on Markovian principles, and most importantly can currently only accommodate single-channel sequences.\nOtherwise, however, distance-based approaches typically cannot accommodate covariates, except as part of a post-hoc step whereby, typically, covariates are used as explanatory variables in a post-hoc multinomial regression with the cluster memberships used as the response (or as independent variables in linear or non-linear regression models). This is questionable as substituting a categorical variable indicating cluster membership disregards the heterogeneity within clusters and is clearly only sensible when the clusters are sufficiently homogeneous [23, 24]. It is often more desirable to incorporate covariates directly, i.e. to cluster sequences and relate the clusters to the covariates simultaneously. This represents an important distinction between the two approaches outlined above; this step does not apply to distance-based clustering approaches and is a crucial advantage of the Markovian approach which makes MHMMs even more powerful tools for clustering multi-channel sequences.\nIn theory, MHMMs can include covariates to explain the initial, transition, and/or emission probabilities. A natural use-case would be to allow subject-specific and possibly time-varying transition and emission probabilities (in the case of time-varying covariates). For example, if we think that transitioning from low to high achievement gets harder as the students get older we may add time as an explanatory variable to the model, allowing the probability of transitioning from low to high achievement to diminish in time. However, at the time of writing this chapter, these extensions are not supported in seqHMM (this may change in the future). Instead, the seqHMM package used in the examples supports time-constant covariates only. Specifically, the manner in which covariates are accommodated is similar in spirit to the aforementioned mixture of exponential-distance models framework and latent class regression [25], in that covariates are used for predicting cluster memberships for each observed sequence. Adding covariates to the model thus helps to both explain and influence the probabilities that each individual has of belonging to each cluster. By incorporating covariates in this way, we could, for example, find that being in a high-achievement cluster is predicted by gender, previous grades, or family background\nIn the terminology of mixtures of experts modelling, this approach corresponds to a so-called “gating network mixture of experts model”, whereby the distribution of the sequences depends on the latent cluster membership variable, which in turn depends on the covariates, while the sequences are independent of the covariates, conditional on the latent cluster membership variable (see [26] and [22] for examples). This, rather than having covariates affect the component distributions, is particularly appealing, as the interpretation of state-specific parameters is the same as it would be under a model without covariates. However, it is worth noting that the covariates do not merely explain the uncovered clusters; as part of the model, they drive the formation of the clusters. In other words, an otherwise identical model without dependence on covariates may uncover different groupings with different probabilities. However, this can be easily overcome by treating the selection of the most relevant subset of covariates as an issue of model selection via the BIC or other criteria and taking the number of clusters/states, set of initial probabilities, and set of covariates which jointly optimise the chosen criterion. The incorporation of covariates in MHMMs and related practical issues are also illustrated in the case study on the longitudinal association of engagement and achievement which follows."
  },
  {
    "objectID": "chapters/ch13-multichannel/ch13-multi.html#review-of-the-literature",
    "href": "chapters/ch13-multichannel/ch13-multi.html#review-of-the-literature",
    "title": "13  Multi-channel sequence analysis in educational research: An introduction and tutorial with R",
    "section": "13.3 Review of the literature",
    "text": "13.3 Review of the literature\nThe use of multi-channel sequence analysis in learning analytics has so far been scarce. Most of the few studies that implemented this method used it to combine multiple modalities of data such as coded video data [27, 28], electrodermal activity [27, 28], clickstream/logged data [10, 29, 30], and assessment/achievement data [10, 30, 31]. The data were converted into different sequence channels using a variety of methods. For example, [27] manually coded video interactions as positive/negative/mixed to represent the emotional valence of individual interaction, and used a pre-defined threshold to discern between high and low arousal from electrodermal activity data. In the work by [30], two sequence channels were built from students’ daily tactics using the learning management system and an automated assessment tool for programming assignments respectively. The learning tactics were identified through hierarchical clustering of students’ trace log data in each environment. Similarly [10], created an engagement channel for each course in a study program based on students’ engagement states derived from the learning management system data, and an achievement state based on their grade tertiles. Another study [29] had five separate channels: the first channel represented the interactive dimension built from the social interactions through peer communications and online behaviours; the second channel represented the cognitive dimension constructed from students’ knowledge contributions at the superficial, medium, and deep levels; the third channel represented the regulative dimension which represented students’ regulation of their collaborative processes, including task understanding, goal setting and planning, as well as monitoring and reflection; the fourth channel represented the behavioural dimension which analysed students’ online behaviours, including resource management, concept mapping and observation, and the fifth channel represented the socio-emotional dimension, which included active listening and respect, encouraging participation and inclusion, as well as fostering cohesion.\nA common step in the existing works is the use of clustering techniques to detect unobserved patterns within the multi-channel sequences. Articles have relied on hidden Markov models to identify hidden states in the data. For instance, [30] found three hidden states consisting of learning strategies which combine the use of the learning management system and an automated assessment tool: an instruction-oriented strategy where students consume learning resources to learn how to code; an independent coding strategy where students relied on their knowledge or used the learning resources available to complete the assignments, and a dependent coding strategy where students mostly relied on the help forums to complete their programming assignments. Most studies go one step further and identify distinct trajectories based on such hidden states. For example, [28] found four clusters of socio-emotional interaction episodes (positive, negative, occasional regulation, frequent regulation), which differed in terms of fluctuation of affective states and activated regulation of learning. The authors of [29] found three types of group collaborative patterns: behaviour-oriented, communication-behaviour-synergistic, and communication-oriented. To the knowledge of the authors, no article has relied on the distance-based approach (commonly used in single-channel sequence analysis) to cluster multi-channel sequences. However, this approach has been used in social sciences [9] as well as in other disciplines [e.g., 32]."
  },
  {
    "objectID": "chapters/ch13-multichannel/ch13-multi.html#case-study-the-longitudinal-association-of-engagement-and-achievement",
    "href": "chapters/ch13-multichannel/ch13-multi.html#case-study-the-longitudinal-association-of-engagement-and-achievement",
    "title": "13  Multi-channel sequence analysis in educational research: An introduction and tutorial with R",
    "section": "13.4 Case study: the longitudinal association of engagement and achievement",
    "text": "13.4 Case study: the longitudinal association of engagement and achievement\nTo learn how to implement multi-channel sequence analysis we are going to practice with a real case study: we will investigate the longitudinal association between engagement and achievement across a study program using simulated data based on the study by [10]. We begin by creating a sequence for each of the channels (engagement and achievement) and then explore visualisations thereof. We then focus on clustering these data using the methods described above specifically tailored for multi-channel sequences, in order to present an empirical view of both the distance-based and Markovian perspectives. In doing so, we will first demonstrate how to construct a multi-channel pairwise OM dissimilarity matrix in order to adapt and apply the distance-based clustering approach of Chapter 10 to these data. Secondly, we will largely focus on using a mixture hidden Markov model to detect the longitudinal clusters of students that share a similar trajectory of engagement and achievement. Finally, we will demonstrate how to incorporate covariates under the model-based Markovian framework.\n\n13.4.1 The packages\nTo accomplish our task, we will rely on several R packages We have use most of them throughout the book. Below is a brief summary of the packages required:\n\nrio: A package for reading and saving data files with different extensions [33].\nseqHMM: A package designed for fitting hidden (latent) Markov models and mixture hidden Markov models for social sequence data and other categorical time series [21].\ntidyverse: A package that encompasses several basic packages for data manipulation and wrangling [34].\nTraMineR: As seen in the introductory sequence analysis chapter, this package helps us construct, analyse, and visualise sequences from time-ordered states or events [35].\nWeightedCluster: A package to cluster sequences and computing quality measures [18].\n\nIf you have not done so already, install the packages using the install.packages() command (e.g., install.packages(\"seqHMM\")). We can then import them as follows:\n\nlibrary(rio)\nlibrary(tidyverse)\nlibrary(TraMineR)\nlibrary(seqHMM) \nlibrary(WeightedCluster) \n\n\n\n13.4.2 The data\nThe data that we are going to use in this chapter is a synthetic dataset which contains the engagement states (Active, Average, or Disengaged) and achievement states (Achiever, Intermediate, or Low) for eight successive courses (each course is a timepoint). Each row contains an identifier for the student (UserID), an identifier for the course (CourseId), and the order (Sequence) in which the student took that course (1–8). For each course, a student has both an engagement state (Engagement) and an achievement state (Achievement). For example, a student can be 'Active' and 'Achiever' in the first course (Sequence = 1) they take, and 'Disengaged' and 'Low' achiever in the next. In addition, the dataset contains the final grade (0-100) and three time-constant covariates (they are the same for each student throughout all four courses): the previous grade (Prev_grade, 1-10), Attitude (0-20), and Gender (Male/Female). The dataset is described in more detail in the Data chapter of this book. We can import the data using the ìmport() command from rio:\n\nURL <- \"https://github.com/sonsoleslp/labook-data/raw/main/\"\nfileName <- \"9_longitudinalEngagement/SequenceEngagementAchievement.xlsx\"\ndf <- import(paste0(URL, fileName))\ndf\n\n\n\n  \n\n\n\n\n\n13.4.3 Creating the sequences\nWe are going to first construct and visualise each channel separately (engagement and achievement), and then study them in combination.\n\n13.4.3.1 Engagement channel\nTo build the engagement channel, we need to construct a sequence of students’ engagement states throughout the eight courses. To do so, we need to first arrange our data by student (UserID) and course order (Sequence). Then, we pivot the data to a wide format, where the first column is the UserID and the rest of the columns are the engagement states at each of the courses (ordered from 1 to 8). For more details about this process, refer to the introductory sequence analysis chapter.\n\neng_seq_df <- df |> arrange(UserID, Sequence) |>\n  pivot_wider(id_cols = \"UserID\", names_from = \"Sequence\", values_from = \"Engagement\")\n\nNow we can use TraMineR to construct the sequence object and assign colours to it to represent each of the engagement states:\n\nengagement_levels <- c(\"Active\", \"Average\", \"Disengaged\")\ncolours <- c(\"#28a41f\", \"#FFDF3C\", \"#e01a4f\")\neng_seq <- seqdef(eng_seq_df , 2:9, alphabet = engagement_levels, cpal = colours)\n\nWe can use the sequence distribution plot from TraMineR to visualise the distributions of the states at each time point (Figure 13.3 left), and the sequence index plot to visualise each student’s sequence of engagement states (Figure 13.3, right), here sorted according to the elements of the alphabet at the successive positions starting from the beginning of the sequences:\n\nseqdplot(eng_seq, border = NA,  ncol = 3, legend.prop = 0.2)\nseqIplot(eng_seq, border = NA,  ncol = 3, legend.prop = 0.2, sortv = \"from.start\")\n\n\n\n\n\n\nFigure 3. Sequence distribution plot and index plot of the course engagement states.\n\n\n\n\nPlease refer to Chapter 10 [3] for guidance on how to interpret sequence distribution and index plots.\n\n\n13.4.3.2 Achievement channel\nWe follow a similar process to construct the achievement sequences. First we arrange the data by student and course order and then we convert the data into the wider format using —this time— the achievement states as the values.\n\nach_seq_df <- df |> arrange(UserID,Sequence) |>\n  pivot_wider(id_cols = \"UserID\", names_from = \"Sequence\", values_from = \"Achievement\")\n\nWe use again TraMineR to construct the sequence object for achievement and we again provide a suitable, distinct colour scheme:\n\nachievement_levels <- c(\"Low\", \"Intermediate\", \"Achiever\")\n\ncoloursA <- c(\"#a5e3ff\",\"#309bff\",\"#3659bf\")\nach_seq <- seqdef(ach_seq_df , 2:9, cpal = coloursA, alphabet = achievement_levels)\n\nWe may use the same visualisations to plot our achievement sequences in Figure 13.4:\n\nseqdplot(ach_seq, border = NA,  ncol = 3, legend.prop = 0.2)\nseqIplot(ach_seq, border = NA,  ncol = 3, legend.prop = 0.2, sortv = \"from.start\")\n\n\n\n\n\n\nFigure 4. Sequence distribution plot and index plot of the course achievement states.\n\n\n\n\n\n\n13.4.3.3 Visualising the multi-channel sequence\nNow that we have both channels, we can use the mc_to_sc_data() function from seqHMM to convert the two channels of engagement and achievement sequences to a single channel with an extended alphabet. If we had additional channels, we would just add them to the list, after ach_seq.\n\nmulti_seq <- mc_to_sc_data(list(eng_seq, ach_seq))\n\nIf we check the alphabet of the sequence data, we can see that it contains all combinations of engagement and achievement states.\n\nalphabet(multi_seq)\n\n[1] \"Active/Achiever\"         \"Active/Intermediate\"    \n[3] \"Active/Low\"              \"Average/Achiever\"       \n[5] \"Average/Intermediate\"    \"Average/Low\"            \n[7] \"Disengaged/Achiever\"     \"Disengaged/Intermediate\"\n[9] \"Disengaged/Low\"         \n\n\nWe now need to provide an appropriate colour scale to accommodate the extended alphabet. The colour scale we define below uses green colours to represent active students, blue colours represent averagely engaged students, and red colours represent disengaged students, whereas the intensity of the colour represents the achievement level: darker colours represent higher achievement, and lighter colours represent low achievement.\n\ncoloursM <-  c(\"#115a20\",\"#24b744\",\"#78FA94\",\n               \"#3659bf\",\"#309bff\",\"#93d9ff\",\n               \"#E01A1A\",\"#EB8A8A\",\"#F5BDBD\")\n\ncpal(multi_seq) <- coloursM\n\nWe can finally plot the sequence distribution and index plots for the multi-channel sequence (Figure 13.5). We can already hint that the students’ multi-channel sequences are quite heterogeneous. In the next steps, we will use two distinct clustering techniques to detect distinct combined trajectories of engagement and achievement, beginning with the distance-based paradigm and then the using the more sophisticated MHMM framework.\n\nseqdplot(multi_seq, border = NA,  ncol = 3, legend.prop = 0.2)\nseqIplot(multi_seq, border = NA,  ncol = 3, legend.prop = 0.2, sortv = \"from.start\")\n\n\n\n\n\n\nFigure 5. Sequence distribution plot and index plot of the multi-channel sequence.\n\n\n\n\n\n\n\n13.4.4 Clustering via multi-channel dissimilarities\nWe now describe how to construct a dissimilarity matrix for a combined multi-channel sequence object to use as input for distance-based clustering algorithms, using utilities provided in the TraMineR R package and relying on concepts described in Chapter 10. We begin by creating a list of substitution cost matrices for each of the engagement and achievement channels. For simplicity, we adopt the same method of calculating substitution costs for each channel, though this need not be the case. Here, we use the data-driven \"TRATE\" method, which relies on transition rates, as the method argument in both calls to seqsubm()}.In other scenarios, we might want to use a constant substitution cost (same cost for all substitutions) for one or some of the channels and, for example, manually specify the substitution cost matrix for the others.\n\nsub_mats <- list(engagement = seqsubm(eng_seq, method = \"TRATE\"),\n                 achievement = seqsubm(ach_seq, method = \"TRATE\"))\n\nSubsequently, we invoke the function seqMD() to compute the multi-channel dissimilarities. This requires the constituent channels to be supplied as a list of sequence objects, along with the substitution cost matrices via the argument sm. The argument what = \"diss\" ensures that dissimilarities are returned, according to the OM method, with equal weights for each channel specified via the argument cweight.\n\nchannels <- list(engagement = eng_seq, achievement = ach_seq)\n\nmc_dist <- seqMD(channels, \n                 sm = sub_mats, \n                 what = \"diss\", \n                 method = \"OM\", \n                 cweight = c(1, 1))\n\nThereafter, mc_dist can be used as input to any distance-based clustering algorithm. Though we are not limited to doing so, we follow Chapter 10 in applying hierarchical clustering with Ward’s criterion. However, we note that users of the distance-based paradigm are not limited to Ward’s criterion or hierarchical clustering itself either.\n\nClustered <- hclust(as.dist(mc_dist), method = \"ward.D2\")\n\nWe then obtain the implied clusters by cutting the produced tree using the cutree() function, where the argument k indicates the desired number of clusters. We also create more descriptive labels for each cluster.\n\nCuts <- cutree(Clustered, k = 6)\nGroups <- factor(Cuts, labels = paste(\"Cluster\", 1:6))\n\nIt is worth noting that the Groups vector contains the clustering assignment for each student. This vector is a factor in the same order as the engagement and achievement sequences, and hence the same order as the data frames used to create the sequences. Hence, information about which student belongs to which cluster is readily accessible. Here, we show the assignments of the first 5 students only, for brevity.\n\nhead(Groups, 5)\n\n        1         2         3         4         5 \nCluster 1 Cluster 2 Cluster 3 Cluster 4 Cluster 3 \nLevels: Cluster 1 Cluster 2 Cluster 3 Cluster 4 Cluster 5 Cluster 6\n\n\nGenerally speaking, the resulting clusters for the chosen k value may not represent the best solution. In an ideal analysis, one would resort to one or more of the cluster quality measures given in Table 9 of Chapter 10 to inform the choice of an optimal k value. In doing so, we determined the chosen number of six clusters is optimal according to the average silhouette width (ASW) criterion (again, see Chapter 10). However, we omit repeating the details of these steps here for brevity and leave them as an exercise for the interested reader.\nThe obtained Groups can be used to produce visualisations of the clustering solution. We do so in two ways; first showing the combined multi-channel object with an extended alphabet (as per Figure 13.2) using the seqplot() function from TraMiner in Figure 13.6, and second using a stacked representation of both constituent channels using the ssp() and gridplot() functions from seqHMM in Figure 13.7. State distribution plots per cluster are depicted in each case.\n\nseqplot(multi_seq, type = \"d\", group = Groups, \n         ncol = 3, legend.prop = 0.1, border = NA)\n\n\n\n\nFigure 6. State distribution plots per cluster for the combined multi-channel sequences.\n\n\n\n\nThough there is some evidence of well-defined clusters capturing different engagement and achievement levels over time, a plot such as this can be difficult to interpret, even for a small number of channels and small numbers of per-channel states. A more visually appealing alternative is provided by the stacked state distribution plots below, which shows each individual channel with its own alphabet on the same plot, with one such plot per cluster. This requires supplying a list of the observations ìn each cluster in each channel to the ssp() function to create each plot, with each plot then arranged and displayed appropriately by the function gridplot().\n\nssp_k <- list()\nfor(k in 1:6) {\n  ssp_k[[k]] <- ssp(list(eng_seq[Cuts == k,], \n                         ach_seq[Cuts == k,]),\n                    type = \"d\", border = NA, \n                    with.legend = FALSE, title = levels(Groups)[k])\n}\ngridplot(ssp_k, ncol = 3, byrow = TRUE, \n         row.prop = c(0.4, 0.4, 0.2), cex.legend = 0.8)\n\n\n\n\nFigure 7. Stacked state distribution plots per cluster showing both constituent channels.\n\n\n\n\nFrom this plot, we can see six clusters, relating respectively to 1) mostly averagely engaged high achievers, 2) mostly actively engaged intermediate achievers, 3) mostly disengaged intermediate or high achievement, 4) mostly averagely engaged low achievers, 5) mostly disengaged low achievers, and 6) mostly actively engage high achievers. This clustering approach and this set of plots provides quite a granular view of the group structure and interdependencies across channels, which differs somewhat from the clustering solution obtained by the application of MHMMs which now follows.\n\n\n13.4.5 Building a mixture hidden Markov model\nThe seqHMM package supports several types of (hidden) Markov models for single- or multi-channel sequences of data. The package provides functions for evaluating and comparing models, and methods for the visualisation of multi-channel sequence data and hidden Markov models. The models are estimated through maximum likelihood, using the Expectation Maximisation (EM) algorithm and/or direct numerical maximisation with analytical gradients. For more details on the seqHMM package, refer to the “Markov models” chapter. We are going to construct a mixture hidden Markov model (MHMM). For this purpose, we will rely on the build_mhmm() function of seqHMM. In its most basic form, the build_mhmm() function takes as arguments a list of observations (i.e., a list of the sequences that make up the channels), and a vector n_states whose length indicates the number of clusters to estimate, for which the value of each position specifies the number of hidden states in each cluster. The build_mhmm() function defines that structure of the model (including possible restrictions) and also assigns random or user-defined starting values to model parameters for estimation. Though we are not restricted to assuming all clusters have an equal number of hidden states, we are going to build a model with three clusters and two hidden states per cluster, inspired by the results obtained in the previous section (i.e., six clusters). Therefore, our input for n_states is c(2, 2, 2). If we instead wanted to build a model with four clusters and three hidden states, we would need to provide c(3, 3, 3, 3).\n\ninit_mhmm <- build_mhmm(observations = list(eng_seq, ach_seq), \n                        n_states = c(2, 2, 2))\n\nNow we can fit the model using the fit_model() function. Using the EM algorithm and the times argument, we can provide a number to control the number of times the model should be fit in a hope to find the globally optimal solution. We need to find a number that is not too low (or else we might be settling for a non-optimal solution too early) but also not too high (or the code will take forever to run). If we provide 100, for example, the model will be estimated 101 times: once from the user-defined or random starting values at the build stage and then 100 re-estimations with randomised starting values. The function will give the best model as an output. Generally, the more complex the model, the more re-estimation rounds are needed.\n\nset.seed(2294)\nmhmm_fit <- fit_model(init_mhmm, \n                      control_em = list(restart = list(times = 100)))\n\nOnce the function has finished fitting the model, we need to check if it has likely reached the optimal solution. We can do so by checking the variable mhmm_fit$em_results$best_opt_restart of the fitted model. For the results to be reliable, we would expect to find the best-fitting model a number of times from different starting values. In other words, we want to see the same value repeating at the beginning of the sequence of numbers returned by mhmm_fit$em_results$best_opt_restart for several times if we estimate the model 100 times. In this case, we observe the same number quite many times, so the stability of our results is acceptable. If the same number does not appear several times, we can increase the number provided to the times argument or we can provide more informative starting values (for example, using the parameters of the newly fitted suboptimal model as starting values for the new estimation).\n\nmhmm_fit$em_results$best_opt_restart\n\n [1] -1659.712 -1659.712 -1659.712 -1659.712 -1659.712 -1659.712 -1659.712\n [8] -1659.712 -1659.712 -1659.712 -1659.712 -1659.712 -1659.712 -1660.642\n[15] -1660.642 -1660.642 -1660.642 -1660.642 -1660.642 -1660.791 -1660.791\n[22] -1662.099 -1663.458 -1663.465 -1664.863\n\n\nNow we can plot our results using the mssplot() function. Figure 13.8 shows index plots for each of the three fitted clusters. These plots show the categories at each time point within each cluster, as well as the hidden states. We can see that Cluster 1 corresponds to students who are mostly low achievers, wherein State 1 represents students who are disengaged and State 2 represents students who are averagely engaged. Cluster 2 includes the Active students who are mostly high achievers (State 2), or intermediate achievers (State 1). Cluster 3 represents mostly low achievers, where State 1 mostly represents the disengaged and State 2 the mostly averagely engaged. Therefore, the results are quite interpretable and make sense given our data.\n\nHIDDEN_STATES <- c(\"darkred\", \"pink\", \"darkblue\", \"purple\", \n                   \"lightgoldenrod1\", \"orange\")\n\nmssplot(mhmm_fit$model,  \n        plots = \"both\", \n        type = \"I\", \n        sortv = \"mds.hidden\",\n        yaxis = TRUE, \n        with.legend = \"bottom\",\n        ncol.legend = 1,\n        cex.title = 1.3,\n        hidden.states.colors = HIDDEN_STATES)\n\n\n\n\n\n\n\n(a) Cluster 1.\n\n\n\n\n\n\n\n(b) Cluster 2.\n\n\n\n\n\n\n\n(c) Cluster 3.\n\n\n\n\nFigure 8. Multi-channel sequence index plots.\n\n\n\nWe may also plot the transitions between the hidden states for each cluster using the plot() function (Figure 13.9). Most arguments used here are purely cosmetic in nature, with the exception of combine.slice which corresponds to the highest probability for which emission probabilities are combined into one state. We adopt the default value of 0.05 to aid the legibility of the plots. We see that the probabilities of starting at each hidden state (initial probabilities), as shown at the bottom of each pie chart, are quite balanced: 0.61/0.39, 0.58/0.42, 0.62/0.38. The transition probabilities (i.e., the probabilities of transitioning from one hidden state to another within the same cluster) are in turn quite low (all < 0.07), meaning that in most cases students remain in the same hidden state throughout their whole academic trajectory. Lastly, the emission probabilities (i.e., the probabilities of each state within each hidden state) are quite diverse, where some hidden states are quite homogeneous (e.g, State 2 in Cluster 2 has a 0.84 probability of having Disengaged students) whereas others are more evenly distributed among all states (e.g., State 1 in Cluster 3).\n\nplot(mhmm_fit$model, \n     vertex.size = 60,\n     cpal = coloursM, \n     label.color = \"black\",\n     vertex.label.color = \"black\",\n     edge.color = \"lightgray\",\n     edge.label.color = \"black\",\n     ncol.legend = 1, \n     ncol = 3,\n     rescale = FALSE,\n     interactive = FALSE,\n     combine.slices = 0.05,\n     combined.slice.label = \"States with probability < 0.05\")\n\n\n\n\nFigure 9. Transitions between hidden states for each trajectory.\n\n\n\n\nIn the previous example, we have used build_mhmm() with the default values in which we only need to provide the number of clusters and hidden states. However, the function allows much more flexibility by allowing us to constrain the different probabilities (initial, transition, and emission) that we have just talked about. An interesting case study can be to investigate how students who start in the same hidden state evolve throughout their academic trajectory; whether they remain in the same hidden state or switch to a different one. To do that, we would need to fix the initial probabilities to be 1 for the first hidden state, and 0 for the other. To do this for the three clusters, we need to create a list with the vector c(1, 0) repeated three times. For the initial transition and emission probabilities, we can just simulate random ones using the functions simulate_transition_probs() and simulate_emission_probs() respectively. It is worth noting that in the previous MHMM example, when we fitted the model using the default probabilities, we just needed to provide the number of clusters and numbers of hidden states per cluster, via n_states. However, now that we need to fix the initial probabilities, we also need to simulate the transition and emission probabilities using the corresponding functions, as build_mhmm() requires either n_states to be provided or all three of initial_probs, transition_probs, and emission_probs to be provided.\n\ninitial_probs <- list(c(1, 0), c(1, 0), c(1, 0))\ntransition_probs <- simulate_transition_probs(n_states = 2, \n                                              n_clusters = 3, \n                                              left_right = TRUE)\nemission_probs <- simulate_emission_probs(n_states = 2, \n                                          n_symbols = 3, \n                                          n_clusters = 1)\n\nWe must now provide the probability objects to the build_mhmm() function along with the observations. Since the number of clusters and hidden states can be derived from the probabilities, we no longer need to provide the n_states argument. Specifically, this is because emission_probs is given as a list of length 3 with each element being a list of length 2, corresponding to three clusters each with two hidden states per cluster, as before. This new model is fitted in the same way, by using the fit_model() function.\n\nset.seed(2294)\ninit_emission_probs <- list(list(emission_probs, emission_probs),\n                            list(emission_probs, emission_probs),\n                            list(emission_probs, emission_probs))\ninit_mhmm_i <- build_mhmm(observations = list(eng_seq, ach_seq), \n                          initial_probs = initial_probs, \n                          transition_probs = transition_probs, \n                          emission_probs = init_emission_probs)\n\nmhmm_fit_i <- fit_model(init_mhmm_i, \n                        control_em = list(restart = list(times = 200)))\n\nAgain, we need to check whether the model has converged to a valid solution by checking if the same number is repeated at the beginning of the best_opt_start output:\n\nmhmm_fit_i$em_results$best_opt_restart\n\n [1] -1726.463 -1726.463 -1726.463 -1726.463 -1726.463 -1726.463 -1726.463\n [8] -1726.463 -1726.463 -1726.463 -1726.463 -1726.463 -1726.463 -1726.463\n[15] -1726.463 -1726.463 -1726.463 -1726.463 -1726.463 -1726.463 -1735.624\n[22] -1735.624 -1735.624 -1735.624 -1735.624\n\n\nNow that we have checked that our model is correct, we can give the channels and clusters representative names so that they look better when plotting:\n\nmhmm_fit_i$model$cluster_names <- c(\"Trajectory 1\",\n                                    \"Trajectory 2\", \n                                    \"Trajectory 3\")\nmhmm_fit_i$model$channel_names <- c(\"Engagement\",\n                                    \"Achievement\")\n\nNow we may plot our results, as before, in the form of sequence index plots (see Figure 13.10 (a), Figure 13.10 (b), and Figure 13.10 (c)). We see that —due to the specified initial probabilities— all clusters start with the same hidden state and mostly remain there throughout the whole trajectory, although some students switch to the other hidden state as time advances. In Cluster 1, students are mostly low achievers and start by being mostly averagely engaged (State 1), while some transition to disengaged (State 2). In Cluster 2, students are mostly highly engaged and start being high achievers (State 1), while some students transition to being mostly intermediate achievers (State 2). Lastly, in Cluster 3, students are mostly averagely engaged or disengaged high or intermediate achievers (State 1), while a small fraction of students become more averagely engaged and more highly achieving over time (State 2).\n\nHIDDEN_STATES <- c(\"darkred\", \"pink\", \"darkblue\", \"purple\", \n                   \"lightgoldenrod1\", \"orange\")\n\nmssplot(mhmm_fit_i$model,  \n        plots = \"both\", \n        type = \"I\", \n        sortv = \"mds.hidden\",\n        yaxis = TRUE, \n        with.legend = \"bottom\",\n        ncol.legend = 1,\n        cex.title = 1.3,\n        hidden.states.colors = HIDDEN_STATES)\n\n\n\n\n\n\n\n(a) Trajectory 1.\n\n\n\n\n\n\n\n(b) Trajectory 2.\n\n\n\n\n\n\n\n(c) Trajectory 3.\n\n\n\n\nFigure 10. Multi-channel sequence index plots with fixed start.\n\n\n\nThere are countless combinations of numbers of clusters, hidden states, and initial/transition/emission probabilities that we can use to fit our model. The choice depends on our data, our research question, the interpretability of the results, and the goodness of fit. Regarding the latter, a common way to compare the performance of several models is by using the Bayesian Information Criterion (BIC; [20]). The BIC can help us score the models which better fit our data while penalising us if we overfit it by adding too many parameters (e.g, if we specified 50 clusters instead of 3, we would better capture the variability of our data but our model would hardly generalise to other scenarios). Therefore, as a general rule, the lower the BIC the better the model, although we need to take into account the aforementioned issues (research questions, interpretability, etc.) to make the final choice. Below we show an example of how to calculate the BIC of the models we have estimated. We see that the first model with the fixed initial probabilities has a somewhat higher BIC than the second one with the restricted initial probabilities. However, we might need to try different numbers of clusters and hidden states to make our final choice.\n\nBIC(mhmm_fit$model)\n\n[1] 3565.658\n\nBIC(mhmm_fit_i$model)\n\n[1] 3656.949\n\n\n\n\n13.4.6 Incorporating covariates in MHMMs\nThe BIC can also be used to select the optimal model when different candidate models with different subsets of covariates are fitted. Recall that the clustering and the relation of clusters to covariates is performed simultaneously. Thus, otherwise identical models without dependence on covariates, or using different covariates, may uncover different groupings with different probabilities and hence a different BIC score. Here, we take the optimal model –with three clusters, each with two hidden states, using fixed initial probabilities– and demonstrate how to add and select covariates to make the model more powerful and interpretable. We begin by extracting the covariate information from the original, long format df. For each student, the available time-constant covariates are a numeric variable giving their previous grades from an earlier course, a numeric measure of their attitude towards learning, and their gender (male or female). The order of the rows of cov_df matches the order in the two eng_seq and ach_seq channels.\n\ncov_df <- df |> \n  arrange(UserID, Sequence) |> \n  filter(!duplicated(UserID)) |> \n  select(Prev_grade, Attitude, Gender)\n\nThe code below replicates the code used to fit the previously optimal MHMM model, and augments it by providing a data frame (via the data argument, using the cov_df just created) and the corresponding one-sided formula for specifying the covariates to include (via the formula argument). For simplicity, we keep the same number of clusters and hidden states and fit three models (one for each covariate in cov_df). To mitigate against convergence issues, we use the results of the previously optimal MHMM to provide informative starting values.\n\nset.seed(2294)\ninit_mhmm_1 <- build_mhmm(observations = list(eng_seq, ach_seq), \n                          initial_probs = mhmm_fit_i$model$initial_probs, \n                          transition_probs = mhmm_fit_i$model$transition_probs, \n                          emission_probs = mhmm_fit_i$model$emission_probs,\n                          data = cov_df,\n                          formula = ~ Prev_grade)\ninit_mhmm_2 <- build_mhmm(observations = list(eng_seq, ach_seq), \n                          initial_probs = mhmm_fit_i$model$initial_probs, \n                          transition_probs = mhmm_fit_i$model$transition_probs, \n                          emission_probs = mhmm_fit_i$model$emission_probs,\n                          data = cov_df,\n                          formula = ~ Attitude)\ninit_mhmm_3 <- build_mhmm(observations = list(eng_seq, ach_seq), \n                          initial_probs = mhmm_fit_i$model$initial_probs, \n                          transition_probs = mhmm_fit_i$model$transition_probs, \n                          emission_probs = mhmm_fit_i$model$emission_probs,\n                          data = cov_df,\n                          formula = ~ Gender)\n\nWe estimate each model 50 + 1 times (first from the starting values we provided and then from 50 randomised values).\n\nmhmm_fit_1 <- fit_model(init_mhmm_1, \n                        control_em = list(restart = list(times = 50)))\nmhmm_fit_2 <- fit_model(init_mhmm_2, \n                        control_em = list(restart = list(times = 50)))\nmhmm_fit_3 <- fit_model(init_mhmm_3, \n                        control_em = list(restart = list(times = 50)))\n\nNote that in an ideal analysis, however, one would vary the number of clusters and hidden states along with varying sets of covariates and initial probabilities in order to find the model settings which jointly optimise the BIC. Moreover, one is not limited to including only a single covariate; one could specify for example formula = ~ Attitude + Gender, but we omit this consideration here for simplicity. Furthermore, one should check in an ideal analysis that the optimal model was found in a majority of the 51 runs. We did so and can be satisfied that there are no convergence issues. We select among the three models with three different, single covariates using the BIC, and include the previous model with no covariates in the comparison also.\n\nBIC(mhmm_fit_i$model)\n\n[1] 3656.949\n\nBIC(mhmm_fit_1$model)\n\n[1] 3614.673\n\nBIC(mhmm_fit_2$model)\n\n[1] 3632.004\n\nBIC(mhmm_fit_3$model)\n\n[1] 3649.353\n\n\nRecalling that lower BIC values are indicative of a better model, we can see that each covariate yields a better-fitting model. Though a better model could be found by adding more covariates or modifying the numbers of clusters and hidden states, we now proceed to interrogate, interpret, and visualise the results of the best model only; namely the MHMM with Prev_grade as a predictor of cluster membership.\nWe begin by examining the clusters via sequence index plots using mssplot(), as before, in Figure 13.11.\n\nmssplot(mhmm_fit_1$model,  \n        plots = \"both\", \n        type = \"I\", \n        sortv = \"mds.hidden\",\n        yaxis = TRUE, \n        with.legend = \"bottom\",\n        ncol.legend = 1,\n        cex.title = 1.3,\n        hidden.states.colors = HIDDEN_STATES)\n\n\n\n\n\n\n\n(a) Cluster 1.\n\n\n\n\n\n\n\n(b) Cluster 2.\n\n\n\n\n\n\n\n(c) Cluster 3.\n\n\n\n\nFigure 11. Multi-channel sequence index plots for the optimal covariate-dependent MHMM.\n\n\n\nCompared to Figure 13.10, the results are notably similar. One observation previously assigned to the first trajectory is now assigned to the third, but the clusters retain precisely the same interpretations of Cluster 1 being mostly low achievers who move from averagely engaged to disengaged, Cluster 2 being mostly highly engaged high achievers, some of whom transition to being mostly intermediate achievers, and Cluster 3 being most averagely engaged or disengaged high or intermediate achievers, some of whom become more averagely engaged and highly achieving over time. It is worth noting that it is possible for the clusters and the interpretation thereof to change more drastically than this when covariates are added. As the clusters are very similar to what we found before, we give the clusters more informative labels as a reminder:\n\ncluster_names(mhmm_fit_1$model) <- c(\"LowEngLowAch\", \"HighEngHighAch\", \"LowEngHighAch\")\n\nWe now present a summary of the model which shows information about parameter estimates of covariates and prior and posterior cluster membership probabilities, which refer to cluster membership probabilities before or after conditioning on the observed sequences, respectively. In other words, prior cluster membership probabilities are calculated using each individual’s observed covariate values only, while posterior cluster membership probabilities are calculated using individual’s covariate values their observed sequence. In each case, these represent the probabilities of belonging to a particular HMM component in the MHMM mixture.\n\n\nsummary_mhmm_1 <- summary(mhmm_fit_1$model)\nsummary_mhmm_1\n\nCovariate effects :\nLowEngLowAch is the reference.\n\nHighEngHighAch :\n             Estimate  Std. error\n(Intercept)    -6.253       1.220\nPrev_grade      0.817       0.159\n\nLowEngHighAch :\n             Estimate  Std. error\n(Intercept)    -1.486       0.960\nPrev_grade      0.158       0.139\n\nLog-likelihood: -1708.843   BIC: 3614.673 \n\nMeans of prior cluster probabilities :\n  LowEngLowAch HighEngHighAch  LowEngHighAch \n          0.40           0.34           0.26 \n\nMost probable clusters :\n            LowEngLowAch  HighEngHighAch  LowEngHighAch\ncount                 57              48             37\nproportion         0.401           0.338          0.261\n\nClassification table :\nMean cluster probabilities (in columns) by the most probable cluster (rows)\n\n               LowEngLowAch HighEngHighAch LowEngHighAch\nLowEngLowAch        0.98577        0.00702       0.00721\nHighEngHighAch      0.00316        0.98499       0.01185\nLowEngHighAch       0.01335        0.01552       0.97113\n\n\nWe will first interpret the information on prior and posterior cluster membership probabilities and then proceed to interpreting covariate effects. Firstly, the section named Means of prior cluster probabilities gives information on how likely each cluster is in the whole population of students (40% in “LowEngLowAch”, 34% in “HighEngHighAch”, and 26% in “LowEngHighAch”). Secondly, the section Most probable clusters shows group sizes and proportions considering that each student would be classified into the cluster for which they have the highest cluster membership probability. Thirdly, the Classification table shows mean cluster probabilities (in columns) by the most probable cluster (in rows). We can see that the clusters are very crisp (the certainty of cluster memberships are very high) because the membership probabilities are large in the diagonal of the table.\nThe part titled Covariate effects shows the parameter estimates for the covariates. Interpretation of the values is similar to that of multinomial logistic regression, meaning that we can interpret the direction and uncertainty of the effect –relative to the reference level of “LowEngLowAch” – but we cannot directly interpret the magnitude of the effects. The magnitude of the Prev_grade coefficient is small with respect to its associated standard error, so we can say there is no significant relationship between previous grades and membership of the poorly engaged yet highly achieving “LowEngHighAch”. However, the large positive coefficient in “HighEngHighAch” indicates that students with high previous grades are more likely to belong to the highly engaged high achieving cluster, which makes intuitive sense.\nThe summary object also calculates prior and posterior cluster memberships for each student. We omit them here, for brevity, but demonstrate that they can be obtained as follows:\n\nprior_prob <- summary_mhmm_1$prior_cluster_probabilities\nposterior_prob <- summary_mhmm_1$posterior_cluster_probabilities\n\nSimilarly, to obtain the most likely cluster assignment for each student according to the posterior probabilites, we simply need to access the most_probable_cluster element from the summary object (summary_mhmm_1). As per the Groups vector defined in the earlier distance-based clustering analysis, we show the assignments for the first 5 students only below, for brevity.\n\ncluster_assignments <- summary_mhmm_1$most_probable_cluster\nhead(cluster_assignments, 5)\n\n[1] LowEngHighAch  LowEngLowAch   LowEngLowAch   HighEngHighAch LowEngLowAch  \nLevels: LowEngLowAch HighEngHighAch LowEngHighAch\n\n\nIf we are interested in getting the most probable path of hidden states for each student, we may use the hidden_paths() function from seqHMM and pass the MHMM model as an argument (mhmm_fit_1$model). We then need to convert it to the long format to get a row per student and timepoint and then we extract only the value column to get only the hidden state. We can then assign the resulting vector to a column (e.g., HiddenState) in the original dataframe which follows the same data order.\n\ndf$HiddenState <- hidden_paths(mhmm_fit_1$model) |> \n  pivot_longer(everything()) |> pull(value)\nhead(df$HiddenState)\n\n[1] LowEngHighAch:State 1 LowEngHighAch:State 1 LowEngHighAch:State 2\n[4] LowEngHighAch:State 2 LowEngHighAch:State 2 LowEngHighAch:State 2\n8 Levels: LowEngLowAch:State 1 LowEngLowAch:State 2 ... %\n\n\nIt is good to be aware that the most probable cluster according to posterior cluster probabilities may not always match with the cluster producing the single most probable path of hidden states. This is because the posterior cluster probabilities are calculated based on all of the possible hidden state paths generated from that cluster, while the single most probable path of hidden states may sometimes come from a different cluster.\nLastly, we advise readers to consult Chapter 12 for more examples of interpreting covariate effects in the context of MHMM models."
  },
  {
    "objectID": "chapters/ch13-multichannel/ch13-multi.html#discussion",
    "href": "chapters/ch13-multichannel/ch13-multi.html#discussion",
    "title": "13  Multi-channel sequence analysis in educational research: An introduction and tutorial with R",
    "section": "13.5 Discussion",
    "text": "13.5 Discussion\nThis chapter has provided an introduction to multi-channel sequence analysis, a method that deals with the analysis of two or more synchronised sequences. Throughout this chapter, we have highlighted the growing significance of multi-channel sequence analysis in educational research. While earlier chapters in this book focused on sequence analysis of learner-related data, this method presents a suitable approach to leverage the increasing availability of student multimodal temporal data. Our review of the existing literature that has utilised multi-channel sequence analysis concluded that the applications of this method within educational research are still relatively scarce. However, the potential of this framework is immense, as it opens up new possibilities to address the challenges posed by the complex and dynamic nature of educational data.\nIn the tutorial part of the chapter, we examined a case study on students’ online engagement and academic achievement to demonstrate how multi-channel sequence analysis can reveal valuable insights into the longitudinal associations between these constructs. The identification of distinct trajectories through mixture hidden Markov models allows for a deep understanding of how both constructs evolve together over time and the distance-based clustering approach offers an alternative perspective on the group structure within these data. The step-by-step tutorial on implementing multi-channel sequence analysis with R according to both clustering frameworks provides readers with practical guidance to explore and apply these methods in their own research. Armed with this knowledge, researchers and educators can unlock valuable insights from their data and make informed decisions to support student learning and success.\nMarkovian models are relatively scalable and can be used to cluster large sequence data. See, e.g., [36] for scalability in terms of hidden states and time points in hidden Markov models. In terms of the number of sequences, computations can be easily parallelised; see, e.g., [19]. Then again, unlike data-mining type sequence analysis, probabilistic models require making assumptions about the data-generating mechanisms which may not always be realistic (such as time-homogeneity or the Markov property itself [19]. Furthermore, analysis of complex multi-channel sequence data can be very time consuming [6, 37]. Although we have highlighted the advantages of the model-based Markovian approach, particularly with regard to the ability to incorporate covariates as predictors of cluster membership, it is difficult to say if one approach is definitively better than the other for all potential use-cases. Although information criteria such as the BIC or cross-validation can be used to guide the choice of MHMM, particularly regarding the choice of the number of clusters (and numbers of hidden states within each cluster, and/or initial/transition/emission probabilities), we did not comprehensively do so here. Rather, we opted for three clusters, each with two hidden states, in both applications of MHMM herein, and only considered different initial probability settings and different covariates to arrive at the best model in terms of BIC. In an ideal analysis, one would compare different MHMM fits with different numbers of clusters and hidden states, different restrictions on the probabilities, and different covariates to arrive at an optimal model in terms of BIC or other model selection criteria.\nConversely, the choice of six clusters for the hierarchical clustering solution was arrived at in a different fashion, driven by the average silhouette width cluster quality measure. Indeed, the BIC cannot be used to choose the number of clusters with hierarchical clustering, nor can it be used to compare a hierarchical clustering solution with MHMM. However, it is interesting to note that the six clusters obtained by hierarchical clustering map reasonably well to the six hidden states of the MHMM solutions. As examples, considering the MHMM with fixed initial probabilities, one could say that clusters 4 and 5 of the hierarchical clustering solution, capturing averagely engaged and disengaged low achievers respectively, matches the two hidden states of the first MHMM cluster, while clusters 2 and 6, capturing actively engaged intermediate and high achievers respectively, matches the two hidden states of the second MHMM cluster.\nOverall, we encourage readers to further explore the potential of multi-channel sequence analysis, the merits of both clustering approaches, and broader implications in the field of education by diving into the recommended readings. The next section provides a list of valuable resources for readers to expand their knowledge on the topic."
  },
  {
    "objectID": "chapters/ch13-multichannel/ch13-multi.html#further-readings",
    "href": "chapters/ch13-multichannel/ch13-multi.html#further-readings",
    "title": "13  Multi-channel sequence analysis in educational research: An introduction and tutorial with R",
    "section": "13.6 Further readings",
    "text": "13.6 Further readings\nFor more thorough presentations and comparisons of different options for the distance-based analysis of multi-channel sequences, we recommend the text book by Struffolino and Raab [38] and articles by Emery and Berchtold [8] and Ritschard et al. [13]. For assessing whether sequences in different channels are associated to the extent of needing joint analysis, Piccarreta and Elzinga [39, 40] have proposed solutions for quantifying the extent of association between two or more channels using Cronbach’s \\(\\alpha\\) and principal component analysis.\nFor general presentations of the MHMM for multi-channel sequences, see the chapter by Vermunt et al. [41] and the article by Helske and Helske [19]. For further examples and tips for visualization of multi-channel sequence data and estimation of Markovian models with the seqHMM package, see the seqHMM vignettes [42, 43].\nTo learn how to combine distance-based sequence analysis and hidden Markov models for the analysis of complex multi-channel sequence data with partially missing observations, see the chapter by Helske et al. [6]."
  },
  {
    "objectID": "chapters/ch14-process-mining/ch14-process.html",
    "href": "chapters/ch14-process-mining/ch14-process.html",
    "title": "14  The Why, the How and the When of Educational Process Mining in R",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch14-process-mining/ch14-process.html#introduction",
    "href": "chapters/ch14-process-mining/ch14-process.html#introduction",
    "title": "14  The Why, the How and the When of Educational Process Mining in R",
    "section": "1 Introduction",
    "text": "1 Introduction\nNowadays, almost all learning platforms generate vast amounts of data that include every interaction a student has with the learning environment. Such large amounts of data offer a unique opportunity to analyze and understand the dynamics of the learning process. In the previous chapters of the book, we covered several methods for analyzing the temporal aspects of learning, such as sequence analysis [1, 2], Markovian modeling [3] or temporal network analysis [4]. In this chapter, we present an analytical method that is specifically oriented at analyzing time-stamped event log data: process mining. Process mining is a technique that allows us to discover, visualize, and analyze the underlying process from time-stamped event logs. Through process mining, we may uncover hidden patterns, bottlenecks, and inefficiencies in students’ learning journeys. By tracking students’ actions step-by-step, we can identify which resources are most effective, which topics are more challenging, and even predict possible problems before they may occur.\nProcess mining emerged as a business tool that allows organizations to analyze and improve their operational processes. The field has rapidly expanded with several modelling methods, algorithms, tools and visualization techniques. Further, the method has been met with enthusiasm from several researchers leading to a rapid uptake by other disciplines such as health care management and education. As the field currently stands, it is a blend of process management and data science with less emphasis on statistical methods. The field has found its place in educational research with the recent surge of trace log data generated by students’ activities and the interest that learning analytics and educational data mining have kindled.\nThis tutorial chapter will introduce the reader to the fundamental concepts of process mining technique and its applications in learning analytics and education research at large. We will first describe the method, the main terminology, and the common steps of analysis. Then, we will provide a review of related literature to gain an understanding of how this method has been applied in learning analytics research. We then provide a step-by-step tutorial on process mining using the R programming language and the bupaverse framework. In this tutorial, we analyze a case study of students’ online activities in an online learning management system using the main features of process mining."
  },
  {
    "objectID": "chapters/ch14-process-mining/ch14-process.html#basic-steps-in-process-mining",
    "href": "chapters/ch14-process-mining/ch14-process.html#basic-steps-in-process-mining",
    "title": "14  The Why, the How and the When of Educational Process Mining in R",
    "section": "2 Basic steps in process mining",
    "text": "2 Basic steps in process mining\nThe goal of process mining is to extract process models from event data. The resulting models can then be used to portray students’ pathways in learning, identify common transitions, and find issues of their approach. In doing so, process mining promises to find deviations from the norm, suggest corrective actions, and optimize processes as an ultimate goal [5]. Process mining starts by the extraction of event data. In the case of educational process mining, event data often reflects students’ activities in learning management systems (LMSs), or in other types of digital tools that record time-stamped events of students’ interactions with the digital tools, such as automated assessment tools or online learning games. The said data is used to construct what is known as an event log. An event log has three necessary parts:\n\nCase identifier: A case represents the subject of the process. For example, if we are analyzing students’ enrollment process, each student would represent a different case. If we are analyzing students’ online activities in the LMS, we can also consider each student as a separate case; alternatively, if we want a greater level of granularity, each student’s learning session can be then considered a separate case. All event logs need to have a case identifier that unequivocally identifies each case and that allows to group together all the events that belong to the same case.\nActivity identifier: Activities represent each action or event in the event data. Continuing with the previous examples, an activity would represent each step in the enrollment process (e.g, application, revision, acceptance, payment etc.), or each action in the LMS (e.g., watch video, read instructions, or check calendar).\nTimestamp: The timestamp is a record of the moment each event has taken place. It allows to establish the order of the events. In the case of online activity data, for instance, the timestamp would be the instant in which a student clicks on a learning resource. In some occasions, activities are not instantaneous, but rather have a beginning and an end. For example, if we are analyzing student’s video watching, we might have an event record when they start watching and when they finish. If we want to treat these events as parts of the same activity, we need to provide additional information when constructing an event log. As such, we need to specify an activity instance identifier, which would allow us to unequivocally identify and group together all instances of the same overarching activity (watching a video, in our example). Moreover, we would need to provide a lifecycle identifier (e.g., start and end), to differentiate between all stages of the same activity. A common limitation of LMS data is that only one click is recorded per activity so this type of analysis is often not possible.\n\nOnce we have our event log defined, we can calculate multiple metrics that allow us to understand the data. For example, we can see the most frequent activities and the most frequent transitions. We can also see the case coverage for each activity, e.g., how many cases contain each activity, and the distribution of the length of each case (how many activities they have). We can also calculate performance metrics, such as idle time (i.e., time spent without doing any activities) or throughput time (i.e., overall time taken).\nFrom the event log, we often construct what is known as the Directly-Follows Graph (DFG), in which we graphically represent all the activities in the event log as nodes and all the observed transitions between them as edges [5]. Figure 14.1 shows an example with event log data from students, where each case identifier represents a student’s session. First, we build the sequence of activities for each case. As such, Layla’s path for her first learning session would be: Calendar → Lecture → Video → Assignment. The path for Sophia’s first session would be: Calendar → Instructions → Video → Assignment. We put both paths together and construct a partial DFG that starts from Calendar, then it transitions either to Lecture or Instructions and then it converges back into Video and ends in Assignment. We create the paths for the remaining student sessions. Then, we combine them together through an iterative process until we have the complete graph with all the possible transitions between activities. Our final graph with the four sessions shown in Figure 14.1 would start by Calendar, then transition either to Lecture or Instructions. Then the process could transition from Lecture to Instructions or viceversa, or to Video. In addition, Lecture has a self-loop because it can trigger another lecture (see Layla’s session 2). From Video, the only possible transition is to Assignment.\nIn real-life scenarios, building the DFG for a complete —or large— event log may turn to be overcrowded and hard to visualize or interpret [5]. Therefore, it is common to trim activities or transitions that do not occur often. Other options include splitting the event logs by group (e.g., class sections) to reduce the granularity of the event log to be able to compare processes between groups. We can also zoom into specific parts of the course (e.g, a specific assignment or lecture) to better understand students’ activities at that time. Moreover, we can filter the event log to see cases that contain specific activities or that match certain conditions.\n\n\n\nFigure 1. Step-by-step construction of a DFG\n\n\nThe DFGs are often enhanced with labels that allow us to understand the graph better. These labels are often based on the frequency of activities and transitions. For example, the nodes (representing each activity) can be labeled with the number of times (or proportion) they appear in the event data, or with the case coverage, i.e., how many cases (or what percentage thereof) they appear in. The edges (representing transitions between activities) can be labeled with the frequency of the transitions or the case coverage as well, among others. Another common way of labeling the graph is using performance measures that indicate the mean time (or median, maximum, etc.) taken by each activity and/or transition.\nThe DFG gives us an overarching view of the whole event log. However, in some occasions, we would like to understand the underlying process that is common to most cases of our event log. This step is called process discovery [5] and there are several algorithms to perform it such as the alpha algorithm [6], inductive techniques [7] or region-based approaches [8]. The discovered processes are then represented using specialized notation, such as Business Process Model and Notation (BPMN) [9] or Petri nets [10].\nIn some occasions, there are certain expectations regarding how the process should go. For instance, if we are analyzing students’ activities during a lesson, the teacher might have given a list of instructions that students are expected to follow in order. To make sure that the discovered process (i.e., what students’ data reveal) aligns with the expected process (i.e., what students were told to do in our example), conformance checking is usually performed. Conformance checking deals with comparing the discovered process with an optimal model or theorized process (i.e., an ideal process) [5]. The idea is to find similarities and differences between the model process and the real observed process, identify unwanted behavior, detect outliers, etc. As seen from our example, in educational settings, this can be used, for instance, to detect whether students are following the learning materials in the intended order or whether they implement the different phases of self-regulated learning. However, given that students are rarely asked to access learning materials in a strict sequential way, this feature has been rarely used. In the next section, we present a review of the literature on educational process mining where we discuss more examples."
  },
  {
    "objectID": "chapters/ch14-process-mining/ch14-process.html#review-of-the-literature",
    "href": "chapters/ch14-process-mining/ch14-process.html#review-of-the-literature",
    "title": "14  The Why, the How and the When of Educational Process Mining in R",
    "section": "3 Review of the literature",
    "text": "3 Review of the literature\nA review of the literature by Bogarín et al. [11] mapped the landscape of educational process mining and found a multitude of applications of this technique in a diversity of contexts. The most common application was to investigate the sequence of students’ activities in online environments such as MOOCs and other online or blended courses, as well as in computer-supported collaborative learning contexts [11]. One of the main aims was to detect learning difficulties to be able to provide better support for students. For example, López-Pernas et al. [12] used process mining to explore how students’ transition between a learning management system and an automated assessment tool, and identified how struggling students make use of the resources to solve their problems. Arpasat et al. [13] used process mining to study students’ activities in a MOOC, and compared the behavior of high- and low-achieving students in terms of students’ activities, bottlenecks and time performance. A considerable volume of research has studied processes where the events are instantaneous, such as clicks in online learning management systems (e.g., [14–16]). Fewer are the studies that have used activities with a start time and an end time due to the limitations in data collection in online platforms. However, this limitation has been often overcome by grouping clicks into learning sessions, as is often done in the literature on students’ learning tactics and strategies, or self-regulated learning (e.g., [12, 17–19]).\nRegarding the methods used, much of the existing research is limited to calculating performance metrics and visualizing DFGs, whereby researchers attempt to understand the most commonly performed activities and the common transitions between them. For example, Vartiainen et al. [20] used video coded data of students’ participation in an educational escape room to visualize the transitions between in-game activities using DFGs. Oftentimes, researchers use DFGs to compare (visually) across groups, for example high vs. low achievers, or between clusters obtained through different methods. For instance, Saqr et al. [21] implemented k-means clustering to group students according to their online activity frequency, and used DFGs to understand the strategies adopted by the different types of learners and how they navigate their learning process. Using a different approach, Saqr and López-Pernas [22] clustered students groups according to their sequence of interactions using distance-based clustering, and then compared the transitions between different interactions among the clusters using DFGs.\nGoing one step further, several studies have used process discovery to detect the underlying overall process behind the observed data [11]. A variety of algorithms have been used in the literature for this purpose, such as the alpha algorithm [23], the heuristic algorithm [24], or the fuzzy miner [18]. Less often, research on educational proecss mining has performed conformance checks [11], comparing the observed process with an “ideal” or “designed” one. An example is the work by Pechenizkiy et al. [25], who used conformance checking to verify whether students answered an exam’s questions in the order specified by the teacher.\nWhen it comes to the tools used for process mining, researchers have relied on various point-and-click software tools [11]. For example, Disco [26] has been used for DFG visualization by several articles (e.g., [27]). ProM [28] is the dominant technology when it comes to process discovery (e.g., [19, 29]) and also conformance checking (e.g., [25]). Many articles have used the R programming language to conduct process mining, relying on the bupaverse [30] framework for basic metrics and visualization (the one covered in the present chapter), although not for process discovery (e.g., [12]) since the algorithm support is scarce."
  },
  {
    "objectID": "chapters/ch14-process-mining/ch14-process.html#process-mining-with-r",
    "href": "chapters/ch14-process-mining/ch14-process.html#process-mining-with-r",
    "title": "14  The Why, the How and the When of Educational Process Mining in R",
    "section": "4 Process mining with R",
    "text": "4 Process mining with R\nIn this section, we present a step-by-step tutorial with R on how to conduct process mining of learners’ data. First, we will install and load the necessary libraries. Then, we will present the data that we will use to illustrate the process mining method.\n\n4.1 The libraries\nAs a first step, we need two basic libraries that we have used multiple times throughout the book: rio for importing the data [31], and tidyverse for data manipulation [32]. As for the libraries used for process mining, we will first rely on bupaverse, a meta-package that contains many relevant libraries for this purpose (e.g., bupaR), which will help us with the frequentist approach [30]. We will use processanimateR to see a play-by-play animated representation of our event data. You can install the packages with the following commands:\n\ninstall.packages(\"rio\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"bupaverse\")\ninstall.packages(\"processanimateR\")\n\nYou can then load the packages using the library() function.\n\nlibrary(bupaverse)\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(processanimateR)\n\n\n\n4.2 Importing the data\nThe dataset that we are going to analyze with process mining contains logs of students’ online activities in an LMS during their participation on a course about learning analytics. We will also make use of students’ grades data to compare activities between high and low achievers. More information about the dataset can be found in the data chapter of this book [33]. In the following code chunk, we download students’ event and demographic data and we merge them together into the same dataframe (df). \n\ndf <- import(\"https://github.com/lamethods/data/raw/main/1_moodleLAcourse/Events.xlsx\") \nall <- import(\"https://github.com/lamethods/data/raw/main/1_moodleLAcourse/AllCombined.xlsx\") |> \n  select(User, AchievingGroup)\ndf <- df |> merge(all, by.x = \"user\", by.y = \"User\")\n\nWhen analyzing students’ learning event data, we are often interested in analyzing each learning session separately, rather than considering a longer time span (e.g., a whole course). A learning session is a sequence (or episode) of un-interrupted learning events. To do such grouping, we define a threshold of inactivity, after which, new activities are considered to belong to a new episode of learning or session. In the following code, we group students’ logs into learning sessions considering a threshold of 15 minutes (15 min. × 60 sec./min. = 900 seconds), in a way that each session will have its own session identifier (session_id). For a step-by-step explanation of the sessions, code and rationale, please refer to the sequence analysis chapter [1]. A preview of the resulting dataframe can be seen below. We see that each group of logs that are less than 900 seconds (15 minutes) apart (Time_gap column) are within the same session (new_session = FALSE) and thus have the same session_id. Logs that are more than 900 seconds apart are considered a new session (new_session = TRUE) and get a new session_id.\n\nsessioned_data <- df |>\n  group_by(user) |> \n  arrange(user, timecreated) |>  \n  mutate(Time_gap = timecreated - (lag(timecreated))) |>  \n  mutate(new_session = is.na(Time_gap) | Time_gap > 900) |> \n  mutate(session_nr = cumsum(new_session)) |> \n  mutate(session_id = paste0 (user, \"_\", \"Session_\", session_nr)) |> ungroup()\n\nsessioned_data\n\n\n\n  \n\n\n\n\n4.2.1 Creating an event log\nNow, we need to prepare the data for process mining. This is performed by converting the data into an event log. At a minimum, to construct our event log we need to specify the following:\n\ncase_id: As we have explained before, the case identifier allows to differentiate between all cases (i.e, subjects) of the process. In our dataset, since we are analyzing activities in each learning session, the case_id would represent the identifier of each session (the column session_id of our dataframe that we have just created in the previous step).\nactivity_id: It indicates the type of activity of each event in the event log. In our dataset, we will use the column Action that can take the values: “Course_view”, “Assignment”, “Instructions”, “Group_work”, etc. LMS logs often contain too much granularity so it is useful to recode the events to give them more meaningful names [33].\ntimestamp: It indicates the moment in which the event took place. In our dataset, this is represented by the timecreated column which stores the time where each ‘click’ on the LMS took place.\n\nNow that we have defined all the required parameters, we can create our event log. We will use the simple_eventlog() function from bupaR and supply the arguments to the function (corresponding to the aforementioned process elements) to the respective columns in our dataset.\n\nevent_log <- simple_eventlog(sessioned_data, \n              case_id = \"session_id\",\n              activity_id = \"Action\",\n              timestamp = \"timecreated\")\n\nIn our example, each row in our data represents a single activity. This is often the case in online trace log data, as each activity is a mere instantaneous click with no duration. As explained earlier, in other occasions, activities have a beginning and an end, and we have several rows to represent all instances of an activity. For example, we might have a row to represent that a student has begun to solve a quiz, and another one to represent the quiz’s submission. If the data looks like that, we might need to use the activitylog() function from bupaR to create our activity log, and indicate, using the lifecycle_id argument, which column indicates whether it is the start or end of the activity, or even intermediate states.\n\n\n4.2.2 Inspecting the logs\nNow that we have created our event log, we can use the summary() function to get the descriptive statistics. The results show that we have a total of 95,626 events for 9,560 cases (in our example, student sessions) and 12 distinct activities (the actions in the learning management system). We have 4,076 distinct traces (i.e., unique cases), which in our example means that there are sessions that have the exact same sequence of events. These traces are, on average, 10 events long, and span from September 9th 2019 to October 27th 2019.\n\nevent_summary <- summary(event_log)\n\nNumber of events:  95626\nNumber of cases:  9560\nNumber of traces:  4076\nNumber of distinct activities:  12\nAverage trace length:  10.00272\n\nStart eventlog:  2019-09-09 14:08:01\nEnd eventlog:  2019-10-27 19:27:41\n\n\nWe may now inspect what the most common activities are in our event log (Table 14.1). The absolute_frequency column represents the total count of activities of each type, whereas the relative_frequency represents the percentage (as a fraction of 1) that each activity represents relative to the total count of activities. We see that the working on the group project (Group_work) is the most frequent activity (with 32,748 instances), followed by viewing the course main page (Course_view) with 25,293 instances.\n\nactivities(event_log)\n\n\n\n\n\n\n\nTable 1.  Frequency of each activity \n  \n    \n    \n      Action\n      absolute_frequency\n      relative_frequency\n    \n  \n  \n    Group_work\n32748\n0.34245916\n    Course_view\n25293\n0.26449919\n    Practicals\n10020\n0.10478322\n    Assignment\n7369\n0.07706063\n    Instructions\n6474\n0.06770125\n    General\n3346\n0.03499048\n    Feedback\n3114\n0.03256437\n    Social\n2191\n0.02291218\n    La_types\n1891\n0.01977496\n    Theory\n1377\n0.01439985\n    Ethics\n1028\n0.01075021\n    Applications\n775\n0.00810449\n  \n  \n  \n\n\n\n\n\nWe can also view the most frequent activities in graphical form. We can even visually compare the frequency of activities between different groups (e.g., high achievers vs. low achievers).\n\nevent_log |> activity_frequency(\"activity\") |> plot()\nevent_log |> group_by(AchievingGroup) |> activity_frequency(\"activity\") |> plot()\n\n\n\n\n\n\n\n(a) Overall frequency\n\n\n\n\n\n\n\n(b) Frequency by group\n\n\n\n\nFigure 2. Activity frequency\n\n\n\nWe may be also interested in looking at activity presence, also known as case coverage. This refers to the percentage of cases that contain each of the activities at least once. Most sessions (85.8%) include visiting the main page of the course (Course_view). Slightly over half (54.6%) involve working on the group project (Group_work). Around one quarter include working on the Practicals (25.7%), Instructions (25.2%), and Assignment (23.2%). Other activities are less frequent.\n\nevent_log |> activity_presence() |> plot()\nevent_log |> group_by(AchievingGroup) |> activity_presence() |> plot()\n\n\n\n\n\n\n\n(a) Overall presence\n\n\n\n\n\n\n\n(b) Presence by group\n\n\n\n\nFigure 3. Activity presence\n\n\n\nIf we are interested in the transition between activities, we can plot the antecedent-consequent matrix. This visualization tells us how many transitions there has been from the activities on the left side (antecedent) to the activities on the bottom side (consequent).\n\nevent_log |> process_matrix() |> plot()\n\n\n\n\nFigure 4. Antedecent-consequent matrix\n\n\n\n\nFigure 14.4 shows that there are 6416 transitions from Course_view to Group_work within the same session. The most common transition is from Group_work to more Group_work, with 22060 instances. Note that, on the antecedent side (left), there is an additional row for Start. This allows to represent the activities that occur right at the beginning as a transition from Start, since they do not have any other activity as antecedent. In this case, we see that most sessions start with Course_view (4612). On the consequent side (bottom), we see that there is an additional column on the right side for End, representing the end of a session. This allows to represent the activities that occur at the very end of a session, since they do not have any other activities following. We see that sessions are most likely to end by Group_work (2762), closely followed by Course_view (2669). Another aspect worth noting is that some cells in our matrix are empty, which represents the fact that a transition between the activity on the left, and the activity on the bottom is not present in any of the cases in our dataset. For example, we see that there are no transitions between Applications and Theory. Another —obvious— example is the cell from Start to End, as sessions should have at least one event and therefore they can never go from Start to End without any other event in the middle.\n\n\n4.2.3 Visualizing the process\nNow, it is time to visualize the event log using the DFG. Within the bupaverse framework this graph is referred to as process map. We can use the process_map() function for this purpose (Figure 14.5).\n\nevent_log |> process_map() \n\n\n\n\nFigure 5. Default process map\n\n\n\nBy default, the process map contains every single activity and transition between activities. As the number of activities and cases increase, this can become unreadable, as is the case with our example. Therefore, we might need to trim some less frequent activities to be able to better visualize the process. We will choose to cover 80% of the activities. This means that we will select the most frequent activities in order, up to when we have accounted for 80% of all the activities in the event log. If we go back to Table 14.1, we can see that all activities ranging from the most frequent activity (Group_work) to Instructions account roughly to 80%, so we are left with the top five activities and exclude the rest. There is no standard for choosing a threshold for trimming and it is up to the researcher to define the threshold. As discussed in the introduction, such approach may yield different process maps according to the trimming threshold. A good rule of thumb for choosing this threshold is that the process map should be readable but at the same time no essential information should be removed. In our case, the activities filtered out are very infrequent and specific to certain lessons in the course, so they are not representative to the course as a whole. Another strategy would be to combine these activities into a single one such as Reading, which would allow to simplify our analysis while keeping all the information. For demonstration processes, we proceed by filtering the log and to operate with it in subsequent steps.\n\nevent_log |> filter_activity_frequency(percentage = 0.8) -> event_log_trimmed\n\nIf we now plot the process map of the trimmed event log, we will be able to see the process much more clearly (Figure 14.6). When interpreting a process map it is common to start by commenting on the nodes (the activities) and their frequency. For example, we would say that Group_work is the most frequent activity with 32,748 instances, followed by Group_work, etc., as we have already done in Table 14.1. Next, we would describe the transitions between activities, with emphasis on how sessions start. Most sessions (5,059) —in the trimmed dataset— start by viewing the course main page Course view. We can see this information on the edge (arrow) that goes from Start to Course_view. The second most common first step is Group_work (2,129). Most instances of Group_work are followed by more Group_work (22,429, as can be seen from the self-loop). The most common transitions between activities are between Course_view and Group_work, with 6,000+ instances between one another both ways, as we can infer from the labels of the edges between one another. Viewing the course main page (Course view) is the most central activity in the process map as it has multiple transitions with all the other activities, since students often use the main page to go from one activity to the next.\n\nevent_log_trimmed |> process_map() \n\n\n\n\nFigure 6. Process map trimmed\n\n\n\nAs we have seen, each node of the process contains the activity name along with its overall frequency, that is, the number of instances of that activity in our event log. Similarly, the edges (the arrows that go from activity to activity) are labeled with the number of times each transition has taken place. These are the default process map settings. Another way of labeling our process map is using the frequency(\"absolute-case\") option, which counts the number of cases (i.e., sessions in our event log) that contain each given activity and transition (Figure 14.7). Instead of seeing the count of activities and transitions, we are seeing the count of sessions that contain each activity and transition. For example, we could say that 8,203 sessions contain the activity Course_view.\n\nevent_log_trimmed |> process_map(frequency(\"absolute-case\"))\n\n\n\n\nFigure 7. Absolute case process map\n\n\n\nSometimes we may be interested in the percentage (relative proportion) rather than the overall counts because it may be easier to compare and interpret. In such a case, we can use the frequency(\"relative\") and frequency(\"relative-case\") options. In the former, the labels will be relative to the total number of activities and transitions (Figure 14.8 (a)), whereas in the latter, the labels will be relative to the number of cases (or proportion thereof) (Figure 14.8 (b)). For example, in Figure 14.8 (a), we see that Group_work accounts for 39.98% of the activities, whereas in Figure 14.8 (b) we see that it is present in 55.25% of the cases (i.e., sessions).\n\nevent_log_trimmed |> process_map(frequency(\"relative\"))\nevent_log_trimmed |> process_map(frequency(\"relative-case\"))\n\n\n\n\n\n\n\n(a) Relative activity frequency\n\n\n\n\n\n\n(b) Relative case coverage\n\n\n\nFigure 8. Relative process maps\n\n\n\nWe can combine several types of labels in the same process map, and even add labels related to aspects other than frequency. In the next example, we label the nodes based on their absolute and relative frequency and we label the edges based on their performance, that is, the time taken in the transition (Figure 14.9). We have chosen the mean time to depict the performance of the edges, but we could also choose median, minimum, maximum, etc. We see, for example, that the longest transition seems to be between Assingment and Group_work, in which students take on average 1.59 minutes (as seen on the edge label). The shortest transition is from Practicals to Practicals (self-loop) with 0.22 minutes on average.\n\nevent_log_trimmed |> process_map(type_nodes = frequency(\"absolute\"),\n                                 sec_nodes = frequency(\"relative\"),\n                                 type_edges = performance(mean))\n\n\n\n\nFigure 9. Relative nodes and performance edges process map\n\n\n\nAnother very useful feature is to be able to compare processes side by side. We can do so by using the group_by() function before calling process_map(). We see that the process map does not capture any meaningful difference between the two groups (Figure 14.10).\n\nevent_log_trimmed |> group_by(AchievingGroup) |> process_map(frequency(\"relative\"))\n\n\n\n\n\n\n\n\n(a) High achievers\n\n\n\n\n\n\n(b) Low achievers\n\n\n\nFigure 10. Comparison between two process maps\n\n\nWe can even reproduce the whole event log as a video using the function animate_process from processanimateR.\n\nanimate_process(event_log_trimmed)\n\n\n\n\nFigure 11. Process animation"
  },
  {
    "objectID": "chapters/ch14-process-mining/ch14-process.html#discussion",
    "href": "chapters/ch14-process-mining/ch14-process.html#discussion",
    "title": "14  The Why, the How and the When of Educational Process Mining in R",
    "section": "5 Discussion",
    "text": "5 Discussion\nWe have demonstrated how to analyze event log data with process mining and how the visualization of process models can summarize a large amount of activities and produce relevant insights. Process models are powerful, summarizing, visually appealing and easy to interpret and understand. Therefore, process mining as a method has gained large scale adoption and has been used in a large number of studies to analyze all sorts of data. This chapter has offered an easy to understand overview of the basic concepts including the composition of the event log and the modelling approach that process mining undertakes. We also offered a brief overview of some examples of literature that have used the method in the analysis of students’ data. Then, a step-by-step tutorial has shown different types of visualization in the form of process maps that are based on raw frequency, relative frequency, performance (time). Later, we have shown how process maps can be used to compare between groups and show their differences. The last part of the chapter offered a guide of how to animate the process maps and see how events occur in real-time. Please note that our approach is restricted to , which is the most popular and well-maintained framework for process mining within the R environment.\nWhereas we share the enthusiasm for process mining as a powerful method, we also need to discuss the method from the rigor point of view. In many ways —as we will discuss later— process mining is largely descriptive and often times exploratory. While this is acceptable in the business domain, it needs to be viewed here from the viewpoint of scientific research and what claims we can make or base on a process map. As the tutorial has already shown, several parameters and decisions have to be made by the researcher and such decisions affect the output visualizations in considerable ways (e.g., filtering the event log). The said decisions are often arbitrary with no guidance or standards to base such decisions on. So, caution has to be exercised from over interpreting the process maps that have undergone extensive filtering. In fact, most researchers have used process mining as an auxiliary or exploratory method hand-in-hand with other tools such as sequence analysis or traditional statistics. In most cases, the conclusions and inferences were based on the other rigorous methods that are used, for instance, to statistically compare two groups. One can think of the process mining described in this chapter more of a visualization tool for the most common transitions or a method for visualizing the big picture of an event log in an intuitive way.\nAnother known problem of process mining is that most algorithms are black-boxed (i.e., the data processing is unclear to the researcher), with no fit statistics or randomization techniques to tell if the estimated process model is different from random. The absence of confirmatory tests makes the answer to the question of whether process mining actually recover the underlying process largely speculative. Moreover, all of the tools, algorithms, and visualization techniques have been imported verbatim from the business world and therefore, their validity for educational purposes remains to be verified. In the same vein, comparing across groups is rather done descriptively, without a statistic to tell whether the observed differences are actually significant or just happened by chance. In other methods such as psychological network analysis [34], several methods are available to create randomized models that assess the credibility of each edge, the stability of centrality measures, the differences between edges as well as to compare across networks regarding each of these parameters. Given that the algorithms, and the modelling technique necessitate trimming (discarding a considerable amount of data), it is bold to assume that the remaining process map (after trimming) is a true representation of the underlying process.\nIt is also important to emphasize here that, given that the field name is “process mining”, the method does not translate to efficient analysis of the learning process. In fact, it is unrealistic to assume that process mining as a method —or any other method at large— can capture the full gamut of the complexity of the learning process as it unfolds across various temporal scales in different ways (e.g., transitions, variations, co-occurrence). A possible remedy for the problem lies in the triangulation of process mining models with other methods, e.g., statistics or better use a Markovian process modeling approach [3]. Stochastic process mining models (covered in detail in Chapter 12 [3]) are more advantageous theoretically and methodologically. Stochastic process mining models are theoretically robust, account for time and variable dependencies, offer fit statistics, clustering methods, several other statistics to assess the models and a wealth of possibilities and a growing repertoire of inter-operable methods."
  },
  {
    "objectID": "chapters/ch14-process-mining/ch14-process.html#further-readings",
    "href": "chapters/ch14-process-mining/ch14-process.html#further-readings",
    "title": "14  The Why, the How and the When of Educational Process Mining in R",
    "section": "6 Further readings",
    "text": "6 Further readings\nThere are multiple resources for the reader to advance their knowledge about process mining. To learn more about the method itself, the reader should refer to the book Process Mining: Data Science in Action [35] and the Process Mining Handbook [36]. For more practical resources, the reader can refer to the bupaverse [30] documentation as well as find out more about the existing bupaverse extensions. The tutorial presented in this chapter has dealt with process analysis and visualization but not process discovery or conformance checking as the tools available in R are limited for these purposes. For process discovery in R, the reader can use the heuristicsMineR package [37], and for conformance checking, the pm4py package [38], a wrapper for the Python library of the same name. A practical guide with Python is also available in A Primer on Process Mining: Practical Skills with Python and Graphviz [39]. For specific learning resources on educational process mining, the reader can refer to the chapter “Process Mining from Educational Data” [40] in the Handbook of Educational Data Mining, and “Educational process mining: A tutorial and case study using Moodle data sets” [41] in the book Data Mining and Learning Analytics: Applications in Educational Research. Moreover, the reader is encouraged to read the existing literature reviews on educational process mining (e.g., [11, 42])."
  },
  {
    "objectID": "chapters/ch15-sna/ch15-sna.html",
    "href": "chapters/ch15-sna/ch15-sna.html",
    "title": "15  Social Network Analysis: A primer, a guide and a tutorial in R",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch15-sna/ch15-sna.html#introduction",
    "href": "chapters/ch15-sna/ch15-sna.html#introduction",
    "title": "15  Social Network Analysis: A primer, a guide and a tutorial in R",
    "section": "1 Introduction",
    "text": "1 Introduction\nSocial Network Analysis (SNA) has emerged as a computational method for studying interactions, relationships, and connections among a vast array of different entities that include humans, animals, and cities, just to name a few. Two related and largely overlapping fields are also concerned with the network as a concept: network science and network analysis. Network science is concerned with the study of the structure of networks, finding patterns and universal laws that may explain or underpin such structure in a large variety of phenomena. Network analysis is a very closely related field that is concerned with the analysis of networks that are not necessarily “social”. In this chapter, we will simply use the terms social network analysis and network analysis interchangeably.\n\n1.1 What are networks?\nA quintessential concept in most analytical methods is that observations are —or should be — independent from each other whereas, in network analysis, observations can be related and interdependent and may interact with or influence each other [1]. As such, network analysis offers a more realistic view of the interconnected world around us and allows us to paint an accurate picture of the relationships and interactions that underpin our world [2, 3]. For instance, when we study students’ engagement at a school, we may use a survey to measure each individual student’s engagement and compute statistics such as the correlation between engagement and grades. In doing so, we ignore that students interact with peers, teachers, and the environment around them [4, 5]. We also ignore that students get influenced by an engaged student, get supported by their friends, or face a problematic social environment that may hinder their engagement [1, 6]. Network analysis offers a rich set of methods for modeling and addressing such issues [7].\nA network is simply a group of entities (often called vertices, nodes, or actors) connected through a relationship (often called edges, links, or arcs) [3]. In this chapter, we will use the terms vertices and edges for simplicity. Vertices can be humans (students, teachers, or families), ideas, keywords, behaviors, emotions, feelings, concepts, schools, countries, or any entity that can be hypothesized to have a relationship with other entities. Vertices may be connected through a vast array of relations. For instance, students may be connected to each other by being friends, teammates, classmates, group members, neighbors, sporting club fans, competitors, sharing a desk, or working together on an assignment. There is virtually no limit to how a researcher can hypothesize a network. Nevertheless, the interpretation of network analysis relies heavily on how the network was constructed [3, 8].\nIn learning contexts, the most common type of networks comes from digital data, and in particular online communications, the most common of which are discussion forums, where the conversation occurs between forum participants (vertices). Each reply from one participant to another forms a relationship between the two vertices, creating an edge in the network. In such a situation, we are talking about a directed network, where the interaction has a source (the person who replies) and a target (the replied-to) [9]. Other examples of directed networks are citation networks (where documents cite other documents), or users that follow other users in social media [10]. In turn, an undirected network contains non-directional relationships such as a network of siblings, friends, teammates, husband and wife, or co-workers [3.] Representing interactions as a network equips the researchers with a rich toolset to harness the power of social network analysis methods. Please note that the researcher can choose to model a directed network as undirected in case direction is deemed inappropriate according to theory, connect or research question."
  },
  {
    "objectID": "chapters/ch15-sna/ch15-sna.html#analysis-of-social-networks",
    "href": "chapters/ch15-sna/ch15-sna.html#analysis-of-social-networks",
    "title": "15  Social Network Analysis: A primer, a guide and a tutorial in R",
    "section": "2 Analysis of social networks",
    "text": "2 Analysis of social networks\nTwo types of analysis methods are commonly used: network visualization and mathematical network analysis [3.] Visualization summarizes all the interactions in the network, giving the researcher a bird’s eye view of the structure of relationships, who is interacting with whom, who is leading the conversation, and who is isolated. Visualization is so powerful that it can summarize a whole semester of interactions in one visualization and still give meaningful information that can be used for intervention e.g., [11]. Mathematical network analysis offers quantification of all the network properties and the individual actors. The network properties are commonly known as graph-level measures. The mathematical measures of individual nodes are known node-level measures (often referred to as centrality measures) [3, 8, 12] Visualization is not discussed in details in this chapter, but interested readers are encouraged to see the excellent visualization tutorial maintained by Kateto Ognyanova [13].\n\n2.1 Mathematical analysis\nThree types of mathematical analysis can be obtained with SNA: graph-level measures, node-level measures (centrality measures) and edge-level measures. Graph level measures describe the network as a whole in terms of size, interactivity and components. Centrality measures quantify the connectivity and importance of individual actors such as the degree of involvement in interactions, the distance to others, or the importance of position among other interacting actors [12, 14, 15]. However, each of these “importance” examples can be measured in different ways. For example, students can be considered central if they frequently contribute to discussions, reply to multiple unique threads of discussions, their contributions receive multiple replies, or their contributions trigger several threads of discussions. As such, there are several centralities with diverse methods that allow us to quantify the degree of connectedness. The website centiserver.org counts more than 400 centrality measures to this date and counting. In this book chapter, we will review and learn the most commonly used centralities in education. We rely on the recent meta-analysis 2022 for listing the used centrality measures, their classification, and operationalization in the literature. Edge measures describe the edge strength, edge weights or edges or edge centralities.\n\n2.1.1 Graph-level measures\nGraph-level measures are a type of analysis used in social network analysis that describes the overall structure and characteristics of a network. They can be used to compare different networks or track changes in a network over time.\nSize or vertex count is the number of vertices (individuals or groups) in the network.\nEdge count is the number of edges (interactions) between vertices in the network.\nDensity represents the number of edges that are present in the network divided by the number of all possible connections. High density indicates that many vertices in the network are connected to one another.\nReciprocity is another important graph-level measure that also reflects group cohesion. A reciprocated edge is an edge where two vertices have a reciprocal relationship (e.g., they are simultaneously source and target) [16]. The higher the ratio of the reciprocated edges, the more collaborative the network is, less centralized (dominated by few) and more participatory.\nTransitivity (or global clustering coefficient) measures the tendency of the vertices to cluster together or form tightly knit cliques. There is a large volume of research that associates the ratio of cliques with cohesion in collaborative groups, strong ties and productive knowledge construction [16]\nOther graph-level measures are aggregations of vertex-level or edge-level measures. For example, the mean degree is the mean number of edges of all the vertices in the network. In a collaborative group, the higher the mean degree, the more interactive the group is.\n\n\n2.1.2 Local centrality measures\nLocal centrality measures are centralities that map the direct or immediate connections of an actor. Put another way, the local centralities quantify the neighbors of a certain vertex where each of these neighbors is directly connected to the target vertex without any intermediates (Figure 15.1).\nIn-degree centrality represents the total number of incoming interactions or relationships of a vertex [12, 14]. Several examples exist in the literature with different operationalization and interpretations. In the case of collaborative learning, in-degree centrality has been interpreted as the worthiness of a participant’s contributions to receive replies, popularity, or influence [17–19].\nOut-degree centrality represents the total number of outgoing interactions or links from an actor to other actors in the network [12, 14, 20]. Out-degree has been commonly interpreted as an indicator of participation, effort, and activity [21, 22].\nDegree centrality refers to the total number of connections or interactions (incoming or outgoing) a vertex has. In undirected networks, it is simply the number of all connections of the vertex. Degree centrality can be interpreted in a similar way to the previous similar centralities as an indication of interactivity, communication, and social role in the collaborative process [23–25].\n\n\n\nFigure 1. Representation of a vertex in-degree (left), out-degree (middle), and degree (right)\n\n\n\n\n2.1.3 Measures based on shortest paths\nOther relevant measures in assessing social network graphs are the shortest paths. These are based on the shortest distance between a pair of points (vertex) in the graph and represent how easy it is for a vertex to access others’ resources [12, 14, 26]. Shortest paths can be used to better measure and understand centrality, as it is based on the distribution and distance of different points in a network which can help understand other educational insights [8., 27] The most relevant measures in this sense are represented in Figure 15.2 and described below.\nCloseness centrality is a measure based on the farness between the vertices of a network; more specifically between a specific vertex and the rest of the network [12, 14]. Small closeness values indicate greater proximity to other vertices, whereas larger values indicate greater distances from other vertices. In the field of education, it can be seen as a way to measure: 1) closure in interaction [11, 28–32], 2) ease of interaction [11, 24, 29, 33, 34], 3) time for accessing information [23, 31, 35], 4) dependencies [30], 5) control over resources [25, 28, 33], and 6) awareness of opportunities [28].\nBetweenness centrality is a measure to show the frequency of a vertex lying on the shortest path between two other vertices. In the field of education, it indicates how actors mediate communication among themselves [8, 27]. It can be a way to understand who are the leaders of the interactions, that is, those who can moderate interactions, reach unconnected groups (inter-group connection) , influence the information flow (information brokering) and manage that information to solve problems effectively [28, 29, 36].\nEccentricity can help to see the vertices not involved in the interactions. It is calculated as the distance to the farthest other vertices in the network. It can be used in the educational field to understand which are the students at risk of dropout or those that are not participating in the activities [11, 34, 36].\n\n\n\nFigure 2. Examples of networks where the highlighted vertex has a high value of betweenness (left), closeness (middle), and eccentricity (right)\n\n\n\n\n2.1.4 Eigenvector-Based centralities\nOther useful metrics in social network graphs are those based on eigenvector centralities, i.e., those related to the value of a connection between vertices. It is based on the idea that it is preferable to have fewer connections to strong well-connected vertices than to have many connections to weak isolated vertices. There are several measures based on this principle:\nEigenvector centrality assesses the importance of a vertex based on the centralities of the vertex to which it is connected, that is, how many connections it has to influential vertices. It has been used to understand social capital, ego, and connection strength [21, 23, 34, 37].\nPagerank is based on link analysis and allows for defining the popularity of an individual in a network. It can be used in education to understand the reputation or influence upon a group of stakeholders based on his/her contacts [36, 38].\nHub centrality is based on Kleinberg’s HITS algorithm [39] that measures who interacts more with the most influential vertices in a network. In the educational field, it could help to understand the type of students’ interaction [23, 38, 40].\nAuthority is also based on Kleinberg’s HITS algorithm but it is used to identify which vertices might be considered an authority on a specific topic score. It is calculated taking into account if its incoming links connect the vertex to others that have an important number of outgoing links [23, 38].\n\n\n2.1.5 Other measures\nThere are other possible metrics to be applied in social network graphs:\nClustering coefficient is a measure of how individuals, represented as vertices, are embedded in their neighborhood [36], with interactions with other individuals forming triangles. In the educational field, it shows how an individual works with peers in groups or clusters, which shows network cohesion [11, 25, 34, 38].\nDiffusion Centrality belongs to the diffusion measures that explore the structural properties that facilitate the diffusion and uptake process; that is, the diffusion resulting from the interaction. It tries to measure how well a vertex can diffuse a property given the semantics and structure of a social network and a model of diffusion [41, 42]. In education, it can be used to show the possibility of an interaction to generate replies and these other replies [19].\nCross Clique Centrality assesses the number of cliques or triangles a vertex belongs to. It is related to the degree of embeddedness, connectivity with other vertices, and strength of ties among vertices. In education, it allows understanding, for example, whether a post is going to be replied to and spread throughout the network [19].\nCoreness (k-core or linkage) is similar to h-index metrics for publications, and is based on networks where all vertices have at least a k degree [43]. It is interesting because if an individual may produce promising contributions, he or she is going to attract others with similar contributions and establish strong connections. In the field of education, those students involved in active interactions/discussions will attract other active users, which will enrich the discussion and therefore the interaction, collaboration, the quality of content, etc. [19].\n\n\n\n2.2 Network visualization\nAs we discussed earlier, networks represent the relationships (edges) between actors (vertices). A vertex is commonly represented as a circle —although other shapes are also used— and the edge as an arrow (in the case of a directed network) from the source of interaction to the target of the interaction (Figure 15.3). For example, a phone call from the caller to the call receiver is represented by an arrow from the caller to the receiver. In the case of an undirected network, the edge is represented by a line connecting both vertices, for instance, two siblings, where the relationship is mutual.\n\n\n\nFigure 3. Left: a directed edge where an interaction happens from A->B. Right: an undirected edge where A-B are connected\n\n\n\n\n2.3 Network analysis\nLet us start with a simple example that uses a common analysis scenario. Our example is a fictional discussion among students where we model interactions between students as a network. Building a network from interactions in discussion forums has been commonly performed by building a post-reply network where an edge is constructed from the author (source of the reply) to the replied-to (target of the reply). For instance, in Figure 15.4, B replies to A by saying “I agree, this could slow the progression of the disease but does it prevent the spread completely or terminate the pandemic?”. This can be represented as an edge from B to A. In the same token, C replies to B which represents another edge C to B. We can compile all of these interactions in a list of edges as shown in Table 15.1 which is often referred to as an edge list. An edge list is a simple way —among many others— to represent the network and therefore we will use it in this chapter. Constructing the network can be performed by aggregating all edges as a network as Figure 15.4.\n\n\n\nFigure 4. A conversation between students in a discussion forum\n\n\n\n\nTable 1. Edge list of students’ interactions in the discussion forum\n\n\nSource\nTarget\n\n\n\n\nB\nA\n\n\nC\nB\n\n\nD\nC\n\n\nE\nD\n\n\nF\nE\n\n\nB\nD\n\n\nC\nA\n\n\nA\nC\n\n\nD\nA\n\n\n\n\nThere are other ways to construct the network that depend on the researcher’s conceptualization, context, and research question. For instance, researchers may consider all students present in a discussion (co-present) and therefore linked together, which means that all vertices will have connections to each other [9, 22, 44]. Similarly, several ways exist to aggregate the network. Figure 5 shows three identical networks where we have duplicate ties (i.e., repeated interactions between the same pair of vertices). In Figure 15.5 A, all interactions are represented. We can see, for instance, that there are six edges between A and F vertices. We can also see that there is a “loop” or “self-loop” around C and E. A loop exists when an interaction occurs between the vertex and itself, as is the case when one replies to their own post. This type of network configuration where multiple edges and self-loops are allowed is often referred to as multigraph. Another possible way to aggregate the network is to create a weight for each edge which represents the frequency of interactions between each pair of vertices (Figure 15.5 B). For instance, the edge between A and F will have a weight of 6 which is the number of edges between A and F, whereas the weight of the edge between H and B is 1 since this interaction only happened once. The network in Figure 5B has an edge with a thickness that corresponds to the weight of 6, i.e., six times as thick as the edge between H and B. In other instances, we may disregard these repeated edges when they do not make a difference to our conceptualization of the network (Figure 15.5 C). Such a type of network is often referred to as a simplified network. For a discussion about network configurations and how they influence research results see [22].\n\n\n\nFigure 5. Several ways of constructing a network: A. Multigraph, B. Weighted, C. Simplified."
  },
  {
    "objectID": "chapters/ch15-sna/ch15-sna.html#network-analysis-in-r",
    "href": "chapters/ch15-sna/ch15-sna.html#network-analysis-in-r",
    "title": "15  Social Network Analysis: A primer, a guide and a tutorial in R",
    "section": "3 Network analysis in R",
    "text": "3 Network analysis in R\nThe R language has a large number of libraries for the analysis of networks. The igraph package —the one used in our chapter— seems to be the preferred package by the R community given the number of dependencies, i.e., the number of other packages that rely on igraph or work with the igraph format [45–47]. The igraph package is fast, efficient, and well-respected within the academic community. The igraph package is also well maintained, continuously updated, has a large community, and has been released for other platforms besides R, e.g., Python. Other packages, such as sna and network, have a large user base, especially among those who are interested in statistical network modeling. Any of these packages —sna, network or igraph— can effectively perform the analysis described in this chapter. However, we will use igraph based on its relative ease of use and convenience for the chapter objectives.\nExample 1\nLet us start with a simple example where we analyze the network created for Figure 6. Before doing anything else, we need to import the necessary packages. We will use igraph to construct and represent networks, and we will use rio to download and import the data files that we need to use as an input for igraph.\n\nlibrary(igraph)\nlibrary(rio)\n\nWe can now use the import function from rio to download the data for the example (the data shown in Table 1), and assign it to a variable named SNA_example1.\n\nSNA_example1 <- \n  import(\"https://github.com/lamethods/data/raw/main/8_examples/SNA_example1.xlsx\")\n\nThe function graph_from_data_frame from igraph converts the edge list in Table 15.1 into a network. R expects a dataframe where the first two columns are used as edges (the first column is used as source column, and the second is used as source column). Please also note that the two columns can have any name. Also, all extra columns —if they are there— will be used as edge attributes. We can print it to see if it has been created correctly. The print function is commonly used to test if the graph creation has been successful. In other words, does the created network have the expected number of vertices, edges, and attributes?\n\nNet <- graph_from_data_frame(SNA_example1)\nprint(Net)\n\nIGRAPH 6b3ad19 DN-- 6 9 -- \n+ attr: name (v/c)\n+ edges from 6b3ad19 (vertex names):\n[1] B->A C->B D->C E->D F->E B->D C->A A->C D->A\n\n\nThe output of the print function gives a glimpse of the network properties. First, igraph states that the object is an igraph object (a network). Then, igraph gives a unique seven-letter identifier for the network (not usually needed for analysis). Next, igraph tells us that the network was directed (D) and named (N), i.e., vertices have a name attribute. Then, igraph lists the attributes and the edges of the network. We can also visualize the created network (Figure 15.6) by using the function plot.\n\nplot(Net)\n\n\n\n\n\n\nFigure 6. Example of a simple network plotted with igraph\n\n\n\n\nWe have seen here the most basic functions we can use in a graph with no arguments. As shown, networks work with little effort with R. In the next section, we will take a deeper look into these functions and others using another network from a published paper.\nExample 2\nThe next example is a larger network that comes from the interactions of a group of teachers in a Massive Open Online Course (MOOC). The MOOC included 445 participants from different places in the United States. The dataset has an edges file where the first two columns are the sender (source) and receiver (target). There is also a file for the vertices that contains demographic and other relevant information about each vertex: gender, location, and their role, etc. For more information about this dataset, please refer to the data chapter [48]. To get the data into R, we first need to read the data, store it in a dataframe and then build a network with the appropriate arguments.\nThe first line of the code reads the edges list data with their attributes into a dataframe with the name net_edges. The second line imports the vertex data with their attributes into a dataframe with the name net_nodes.\n\nnet_edges <- \n  import(\"https://github.com/lamethods/data/raw/main/6_snaMOOC/DLT1%20Edgelist.csv\")\nnet_nodes <- \n  import(\"https://github.com/lamethods/data/raw/main/6_snaMOOC/DLT1%20Nodes.csv\")\n\nTo create the network, we again use the function graph_from_data_frame. This time we have to specify the edges dataframe using the argument d=net_edges. The second argument (optional) tells igraph that the network should be directed, if not provided, the network is created directed by default. The third argument which is also optional vertices = net_nodes tells igraph to use the dataframe net_nodes for vertex attributes. If the vertices argument is not provided, igraph will extract vertex names from the edges data. In case there are important vertex attributes for the analysis, providing the vertices data can be useful. Building the network and explicitly setting all arguments —as we did— helps avoid the problems that could happen from the default settings of the function. For instance, the network could be created as directed where we aim at creating an undirected network. Note that igraph generates a multigraph network by default (see Figure 15.7).\n\nDLT1 <- graph_from_data_frame(d=net_edges, directed = TRUE, vertices = net_nodes)\n\nLet us now explore the network and see if it was built correctly using the function print. The print function output shows that the network is an igraph object, directed and named (DN) has 445 vertices, 2529 edges and then igraph lists the attributes of the vertices and the edges. Vertex attributes are listed along with their type. For instance, name (v/c) means the name attribute is a (v)ertex attribute and a (c)haracter. Edge attributes are listed in the same way. For instance, timestamp (e/c) means that it is an (e)dge and a (c)haracter.\n\nprint(DLT1)\n\nIGRAPH 4f68816 DN-- 445 2529 -- \n+ attr: name (v/c), Facilitator (v/n), role1 (v/c), experience (v/n),\n| experience2 (v/c), grades (v/c), location (v/c), region (v/c),\n| country (v/c), group (v/c), gender (v/c), expert (v/c), connect\n| (v/c), Timestamp (e/c), Discussion Title (e/c), Discussion Category\n| (e/c), Parent Category (e/c), Category Text (e/c), Discussion\n| Identifier (e/c), Comment ID (e/n), Discussion ID (e/n)\n+ edges from 4f68816 (vertex names):\n [1] 360->444 356->444 356->444 344->444 392->444 219->444 318->444 4  ->444\n [9] 355->356 355->444 4  ->444 310->444 248->444 150->444 19 ->310 216->19 \n[17] 19 ->444 19 ->4   217->310 385->444 217->444 393->444 217->19  256->219\n+ ... omitted several edges\n\n\nA network can also be plotted with the function plot() (Figure 15.7). However, plotting with R is a vast field and will not be discussed in detail here.\n\nplot(DLT1,  layout = layout.fruchterman.reingold, \n     vertex.size = 5, vertex.label.cex = 2)\n\n\n\n\n\n\nFigure 7. Example of a complex network plotted with igraph\n\n\n\n\n\n3.1 Graph level analysis\nNow that we have seen how to build a network from edge and vertex data, we are ready to understand some of the most commonly performed analyses in learning settings. The first type of analysis will look at the network level, or the whole group of collaborators. Analyzing the network level can tell us how interactive the group is, how cohesive, and how distributed the interactions are. We will go through each of these graph-level measures with a brief explanation of what they actually mean. We will use the data from example 1 (DLT1).\nLet us first start by calculating the basic measures of the network. The number of vertices can be queried using the function vcount, which adds up to 445, and the number of edges can be queried using the function ecount, which is 2,529. We can get the average number of interactions by a participant by dividing the number edges by the number of vertices which is 5.68.\n\nvcount(DLT1) ## 445\necount(DLT1) ## 2529\necount(DLT1) / vcount(DLT1) ## 5.683146\n\nThe density of a graph is an important parameter of a collaborative network that refers to the ratio of existing edges to the maximum possible among all participants. Density is maximum (1) when every vertex has interacted with every other vertex in the network. Graph density can be measured using the function graph.density.\n\ngraph.density(DLT1) ## 0.01279988\n\nHowever, the graph.density function may result in erratic results if the network is multigraph; this is because the igraph algorithm will count the repeated edges and loops. Thus, we need to simplify the network (delete all repeated edges and loops) before computing the density and use the simplified network to compute the graph density. The results of the density of the graph of 0.0097 which is rather a low value.\n\ngraph.density(simplify(DLT1)) ## 0.009798563\n\nReciprocity is another important graph-level measure that also reflects group cohesion. A reciprocated edge is an edge where two vertices have a reciprocal relationship (e.g., they are simultaneously source and target) [16]. The higher the ratio of the reciprocated edges, the more collaborative the network is, less centralized (dominated by few) and more participatory. Reciprocity can be computed using the function reciprocity, which automatically removes the loops (i.e., does not consider when a person replies to oneself). The reciprocity by igraph definition is the fraction of reciprocated edges in a directed graph. The value here is 0.1997544 which means that only 20% of all edges were reciprocated.\n\nreciprocity(DLT1) ## 0.1997544\n\nWe can also compute the dyad.census which returns the number of mutual interactions (reciprocated between a pair of vertices), the number of asymmetric interactions (interactions that are not reciprocated), and the number of non-connected pairs. The number of mutual interactions in our network is 212, which is relatively small given the asymmetric (1512) and non-connected pairs (97066).\n\ndyad.census(DLT1) ## $mut [1] 212     $asym [1] 1512     $null [1] 97066\n\nTransitivity (or global clustering coefficient) measures the tendency of the vertices to cluster together or form tightly knit cliques. In igraph, transitivity is measured as the probability that the neighboring vertices of a vertex are also connected to each other or, more accurately, the ratio of triangles in the network to the total count of triplets (all occurrences of three vertices connected by two edges). There is a large volume of research that associates the ratio of cliques with cohesion in collaborative groups, strong ties and productive knowledge construction Block_2015. There are several methods for the estimation of transitivity. Here, we are going to focus on global transitivity (i.e., at the network-level) using the igraph method. The transitivity can be calculated by the function transitivity; the default function returns the global transitivity measure by default. The transitivity of our network here is 0.08880774.\n\ntransitivity(DLT1) ## 0.08880774\n\nAnother possible way is to use the related function triad_census which reports the numbers of triangles and their different types. The reader may need to refer to the package manual to dig deeper in the results.\n\ntriad_census(DLT1) \n\n [1] 13901588   486626   124805     4227    35745    11186    15929     3668\n [9]      932       81     1857      376      223      334      345       68\n\n\nGroup productivity or intensity of interactivity can be explored using the degree measures and its variants. The average degree of the network measures how much on average each group member has contributed and received interactions. To compute the average degree, we first have to compute the degree for each member and then compute the mean.\nIn directed networks (like the one in this example), we can also compute the average in-degree and out-degree. For the same set of vertices, the network average in-degree should be equal to out-degree and both combined should be equal to the total degree. However, if we, for instance, compute a subset of vertices (only students excluding the teachers), in-degree and out-degree may be different. The code below computes the mean and median of the three measures, using the function degree with the argument mode=\"total\" for the total degree, mode=\"in\" for the in-degree, and mode=“out” for out-degree.\n\nMean_degree <-  mean(degree(DLT1, mode = \"total\")) ## 11.36629\nMean_in_degree <-  mean(degree(DLT1, mode = \"in\")) ## 5.683146\nMean_out_degree <-  mean(degree(DLT1, mode = \"out\")) ## 5.683146 \nMedian_degree <-  median(degree(DLT1, mode = \"total\")) ## 4\nMedian_in_degree <-  median(degree(DLT1, mode = \"in\")) ## 1\nMedian_out_degree <-  median(degree(DLT1, mode = \"out\")) ## 2\n\nThe mean degree is 11.36629 and the mean in-degree and out-degree are 5.683146. The median degree is 4, the median in-degree is 1, and the median out-degree is 2. The median differs significantly from the mean and may be more relevant here in this large network, where participation may not be well-distributed (see next section).\nCollaboration is participatory by design but, oftentimes, some students may dominate and contribute disproportionately more than others. In the same vein, some may prefer to be isolated and thus rarely participate. Several measures allow us to measure the distribution of interactions across the network and how skewed the network contribution patterns are. An obvious method that comes straight from statistics is the standard deviation (SD) of the degree centrality. We can compute the SD like we calculated the mean and median in the previous step. The SD of degree centrality in our case is 34.2, SD for in-degree centrality is 26.7, and SD for out-degree centrality is 9.8. The SD is higher than the mean which suggests that calculation and inspection of the median was justified. We can also see that the SD of the in-degree centrality is much higher than the SD of the out-degree, which means that the variability in receiving replies is higher than that of contributions. This variability is rather common since students are selective about whom they respond to and choose the reply-worthy contributions.\n\nSD_degree <-  sd(degree(DLT1, mode = \"total\")) ## 34.20511\nSD_in_degree <-  sd(degree(DLT1, mode = \"in\")) ## 26.73596\nSD_out_degree <-  sd(degree(DLT1, mode = \"out\")) ## 9.84249\n\nSNA has dedicated indices for measuring dominance in networks, known as centralization indices. Centralization indices are 0 when every vertex contributes equally and reaches the maximum of 1 when a single vertex dominates. A centralization index exists for many centralities —e.g., degree, closeness, and betweenness. Nevertheless, most of the literature has reported degree centralization, which we will demonstrate here.\nThe code below computes the degree, in-degree and out-degree centralization. Please note that we use a simplified network to avoid the loops and repeated edges. The results of degree centralization confirm the previous results. The degree centralization is 0.38, out-degree centralization is 0.16, and in-degree centralization is 0.60. In our network, we see that the in-degree centralization is the highest index (0.60), which means that only a few students received replies.\n\nCentralization_degree <- centralization.degree(simplify(DLT1), \nmode =  \"all\", loops = FALSE)$centralization  ## 0.3826871 \n\nWarning: `centralization.degree()` was deprecated in igraph 2.0.0.\nℹ Please use `centr_degree()` instead.\n\nCentralization_in_degree <-  centralization.degree(simplify(DLT1), \nmode = \"in\", loops = FALSE)$centralization    ## 0.6064291\nCentralization_out_degree <- centralization.degree(simplify(DLT1), \nmode =  \"out\", loops = FALSE)$centralization  ## 0.1572214\n\nAnother way to see how the interactions are distributed is to plot the degree distribution using the hist function, as demonstrated in Figure 15.8.\n\npar(mfrow=c(1,2))\nhist(degree(DLT1, mode = \"in\"), breaks = 100)\nhist(degree(DLT1, mode = \"out\"), breaks = 100)\n\n\n\n\nFigure 8. Distribution of degree. Left: In-degree. Right: Out-degree\n\n\n\n\n\n\n3.2 Network connectivity\nWe can also examine how connected the whole group is; this can be performed using the function is.connected which returns FALSE in our case, meaning that the graph has some disconnected components or subgroups of vertices that are isolated. We can check these subgroups by the function components which tells us that there are four components:\n\nis.connected(DLT1)\n\nWarning: `is.connected()` was deprecated in igraph 2.0.0.\nℹ Please use `is_connected()` instead.\n\n\n[1] FALSE\n\nComponents <- components(DLT1)\nprint(Components)\n\n$membership\n  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n 41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n 61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n 81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n441 442 443 444 445 \n  2   3   4   1   1 \n\n$csize\n[1] 442   1   1   1\n\n$no\n[1] 4\n\n\nUsing the function decompose, we can look at each of the components. The largest component has 442 vertices, and three others have one vertex. These isolated vertices are simply students who did not contribute at all and do not represent a real subgroup.\n\nDecomposed <- decompose(DLT1)\nDecomposed[[1]]\n\nIGRAPH be7756b DN-- 442 2529 -- \n+ attr: name (v/c), Facilitator (v/n), role1 (v/c), experience (v/n),\n| experience2 (v/c), grades (v/c), location (v/c), region (v/c),\n| country (v/c), group (v/c), gender (v/c), expert (v/c), connect\n| (v/c), Timestamp (e/c), Discussion Title (e/c), Discussion Category\n| (e/c), Parent Category (e/c), Category Text (e/c), Discussion\n| Identifier (e/c), Comment ID (e/n), Discussion ID (e/n)\n+ edges from be7756b (vertex names):\n [1] 360->444 356->444 356->444 344->444 392->444 219->444 318->444 4  ->444\n [9] 355->356 355->444 4  ->444 310->444 248->444 150->444 19 ->310 216->19 \n[17] 19 ->444 19 ->4   217->310 385->444 217->444 393->444 217->19  256->219\n+ ... omitted several edges\n\n\nWe can also look at the network diameter or largest number of steps between vertices to see how far distant vertices are (using the distance function). A more representative variable would be the average distance between vertices, which we can obtain from the mean_distance function. The network diameter is 8 and the mean distance is 3. Both numbers are relatively high. Global efficiency is another network-level measure that examines how effective is the network structure as a conduit for information exchange using the distances between vertices. When all vertices are close to each other, reachable with a few number of steps, the network is said to be efficient. The value of efficiency is high in well connected groups, and low otherwise. We can examine the efficiency using the function global_efficiency.\n\ndiameter(DLT1) ## 8\n\n[1] 8\n\nmean_distance(DLT1) ## 3.030694\n\n[1] 3.030694\n\nglobal_efficiency(DLT1) ## 0.1961034\n\n[1] 0.1961034\n\n\n\n\n3.3 Network operations\nThere are many functions and tools to manipulate networks in igraph, which makes a comprehensive discussion of all of them beyond the scope of this introductory chapter. Nonetheless, we will discuss the most important functions. Oftentimes, we need to set an attribute to the vertices —e.g., setting the gender attribute for vertices— to be used in the analysis. Setting an attribute can be performed by using the V function followed by the $ character and the attribute that we want to set. Similarly, setting edge attributes can be done using the function E. In the next example, we define an attribute called weight for the vertices and we do the same for the edges. Using the skills we learnt, we can use them to create a simplified weighted network. We do so by concatenating all repeated edges into a single edge with the weight as the frequency. For that, we start by first assigning weights of 1 to each node and edge. Lastly, we use the function simplify to remove the duplicated edges and aggregate the weights (edge.attr.comb = list(weight = \"sum\", \"ignore\")) while all other edge attributes will be ignored.\n\nV(DLT1)$weight <- 1\nE(DLT1)$weight <- 1\nsimple.DLT1 <- simplify(DLT1, remove.multiple = TRUE, remove.loops = TRUE, \n                        edge.attr.comb = list(weight = \"sum\", \"ignore\"))\n\nThere are two important functions that we may need if we want to divide or create a subnet of the network. The function subgraph.edges allows us to create a subset of a network based on edge characteristics. In the following example, we create a subgraph with the discussions involving Curriculum & Instruction by using the argument E(DLT1)$'Discussion Category'== 'Curriculum & Instruction'. In the same way, the induced_subgraph function allows us to specify a subgraph based on the vertex characteristics. In the next example, we create a network for North Carolina teachers using V(DLT1)$location== \"NC\".\n\nk <- subgraph.edges(DLT1, \n    eids = which(E(DLT1)$'Discussion Category' == 'Curriculum & Instruction'))\nNC_network <- induced_subgraph(DLT1, vids = which(V(DLT1)$location == \"NC\"))\n\n\n\n3.4 Individual vertex measures (centrality measures)\nWe have discussed the centrality measures in the introductory section and how they can be used in an educational context. Centrality measures may serve many functions, —e.g., indicators of performance in collaborative environments [49] or indicators for roles in collaboration [21]. The igraph package allows the calculation of several centrality measures, many of which have been used in educational research and some of which may not be relevant. Other packages, such as centiserve, allow an even larger number of centrality measures [50]. In this section, we will focus on the common centrality measures according to the recent meta-analysis by [49] and other recently used measures, such as diffusion centrality measures.\nDegree centrality measures can be computed using the function degree and the argument mode specifies the type of degree where mode=\"in\" returns in-degree, mode=\"out\" returns out-degree, mode=\"total\" returns total degree centrality. In case of undirected networks, the mode argument is ignored and the function returns only the degree centrality, since there is no direction. We can combine all the centralities that we calculate together in a dataframe using the tibble function from the tibble package.\n\nInDegree  <- degree(DLT1, mode = \"in\")\nOutDegree <- degree(DLT1, mode = \"out\")\nDegree    <- degree(DLT1, mode = \"total\")\nDegree_df <- tibble::tibble(name = V(DLT1)$name, InDegree, OutDegree, Degree)\nprint(Degree_df)\n\n# A tibble: 445 × 4\n   name  InDegree OutDegree Degree\n   <chr>    <dbl>     <dbl>  <dbl>\n 1 1           20        33     53\n 2 2            2         5      7\n 3 3            2         4      6\n 4 4            2        14     16\n 5 5           16        17     33\n 6 6            9        24     33\n 7 7           32        26     58\n 8 8           13        18     31\n 9 9            2        12     14\n10 10           8        12     20\n# ℹ 435 more rows\n\n\nNote that igraph has another function called graph.strength that computes the degree centrality and takes the edge weight attribute into account. In multigraph networks —like ours— degree centrality and graph.strength should return the same result. However, in networks where the edges have a weight attribute both functions (degree and graph.strength) return different results. In such weighted networks —like the simplified network we created in the previous example— the degree function will return the unique connections of every vertex or the size of the vertex direct collaborators —known as the size of the ego network. The graph.strength function will return the number of interactions a vertex has made. See the next example and compare the results. For more information about the different calculation methods of degree centrality of weighted networks, readers are advised to refer to the seminal article by [51].\n\nInStrength  <- graph.strength(DLT1, mode = \"in\")\n\nWarning: `graph.strength()` was deprecated in igraph 2.0.0.\nℹ Please use `strength()` instead.\n\nOutStrength <- graph.strength(DLT1,mode = \"out\")\nStrength    <- graph.strength(DLT1, mode = \"total\")\nStrength_df <- tibble::tibble(name=V(DLT1)$name,InStrength,OutStrength,Strength)\nprint(Strength_df)\n\n# A tibble: 445 × 4\n   name  InStrength OutStrength Strength\n   <chr>      <dbl>       <dbl>    <dbl>\n 1 1             20          33       53\n 2 2              2           5        7\n 3 3              2           4        6\n 4 4              2          14       16\n 5 5             16          17       33\n 6 6              9          24       33\n 7 7             32          26       58\n 8 8             13          18       31\n 9 9              2          12       14\n10 10             8          12       20\n# ℹ 435 more rows\n\n\nCloseness and betweenness centralities are the most commonly used centrality measures according to [49] and both rely on the position of the vertex on the shortest paths between others. Closeness centrality can be calculated using the function closeness, which is directional; this means that we can compute in-closeness, out-closeness and total closeness centrality. Betweenness centrality can be computed using the function betweenness and the function is directional: the argument directed=TRUE computes the directional version, and vice versa. Another commonly used centrality measure is eigenvector centrality, which can be computed using the function eigen_centrality. The eigen_centrality function default is directed=FALSE, since it is less suited for directed networks [14]. Pagerank is another closely related centrality measure that uses a similar algorithm and is more suitable for directed networks. The Pagerank centrality can be calculated using the function pagerank. Please note that to obtain the value of the centrality, you need to use $vector at the end as demonstrated in the code.\nAn important question here is whether to compute these centralities with a simplified network, weighted network or a multigraph network. The answer depends on the context, the network structure and the research question. However, evidence suggests that multigraph configuration may render the most accurate results when centralities are used as indicators for performance [22]. The code below computes the aforementioned centralities, you may need to read the help of each centrality function for more options and arguments for customization:\n\nCloseness_In <- closeness(DLT1, mode = c(\"in\"))\nCloseness_Out <- closeness(DLT1, mode = c(\"out\"))\nCloseness_total <- closeness(DLT1, mode = c(\"total\"))\n\nBetweenness <- betweenness(simple.DLT1, directed = FALSE)\nEigen <- eigen_centrality(simple.DLT1, directed = FALSE)$vector\nPagerank <- page.rank(DLT1, directed = FALSE)$vector\n\nWarning: `page.rank()` was deprecated in igraph 2.0.0.\nℹ Please use `page_rank()` instead.\n\n\nDiffusion centralities have been introduced recently in several studies and seem to offer a more robust estimation of a vertex role in spreading information [19, 52]. Diffusion centrality can be computed in the same way as degree centrality. However, there is no function in the igraph package to calculate this centrality, so we rely on the diffusion.degree function from the centiserve package. The function diffusion.degree accepts the mode argument to compute different variants, i.e., \"in\", \"out\" and \"total\" diffusion degrees.\n\nlibrary(centiserve)\nDiffusion.degree_in  <- diffusion.degree(DLT1, mode = c(\"in\"))\nDiffusion.degree_out  <- diffusion.degree(DLT1, mode = c(\"out\"))\nDiffusion.degree  <- diffusion.degree(DLT1, mode = c(\"all\"))\n\nCoreness and cross-clique connectivity are related centralities that estimate the embeddedness of the vertex in the network can be calculated using the functions coreness and crossclique. Both coreness and crossclique centralities have been shown to better correlate with performance as well as with productive and reply-worthy content [19].\n\nCoreness <- coreness(DLT1)\nCross_clique_connectivity <- crossclique(DLT1)\n\nWarning in cliques(graph): At vendor/cigraph/src/cliques/cliquer_wrapper.c:42 :\nEdge directions are ignored for clique calculations.\n\n\nWe can also combine the rest of the centralities together in a single dataframe:\n\nCentdf  <- tibble::tibble(name=V(DLT1)$name,Closeness_total,Betweenness,\nEigen,Pagerank,Diffusion.degree,Coreness,Cross_clique_connectivity)\nprint(Centdf)\n\n# A tibble: 445 × 8\n   name  Closeness_total Betweenness   Eigen Pagerank Diffusion.degree Coreness\n   <chr>           <dbl>       <dbl>   <dbl>    <dbl>            <dbl>    <dbl>\n 1 1            0.00110       1258.  0.206    0.00840             1865       18\n 2 2            0.000808        26.5 0.0107   0.00140              218        6\n 3 3            0.000799        30.6 0.00862  0.00130              191        6\n 4 4            0.00102         72.5 0.0803   0.00273              965       13\n 5 5            0.00106        309.  0.162    0.00525             1508       18\n 6 6            0.00108        250.  0.155    0.00539             1607       18\n 7 7            0.00111       1935.  0.230    0.00931             2088       21\n 8 8            0.00106        164.  0.136    0.00503             1483       18\n 9 9            0.00104         69.5 0.119    0.00251             1216       13\n10 10           0.00106        716.  0.0875   0.00343             1432       17\n# ℹ 435 more rows\n# ℹ 1 more variable: Cross_clique_connectivity <int>\n\n\nThe calculation of graph level measures and centrality measures are usually a step in the analysis to answer a research question. For instance, density can tell how distributed the interactions between students are and therefore, how it is collaborative [6]. Centrality may be calculated to identify who are the most important students in the discussions or used to infer the roles e.g., who are the leaders who drive the discussion [21, 23]. Several studies have calculated centrality measures to investigate their relationship with performance [18]. All of such types of analysis can be performed using the analysis we have demonstrated. Of course, there are no limits to the potentials of SNA and researchers have a wide range of possibilities and potentials that they can achieve by building on the aforementioned tutorials."
  },
  {
    "objectID": "chapters/ch15-sna/ch15-sna.html#discussion",
    "href": "chapters/ch15-sna/ch15-sna.html#discussion",
    "title": "15  Social Network Analysis: A primer, a guide and a tutorial in R",
    "section": "4 Discussion",
    "text": "4 Discussion\nThe present chapter offered a primer on social network analysis as well as a tutorial on the most common types of SNA analysis. SNA is a vast field with diverse applications that are far beyond a chapter or even a whole book. Readers who are interested in expanding their knowledge about SNA are advised to read the literature cited in this chapter. Furthermore, several systematic reviews have tried to offer a synthesis of the extant literature and can help the readers get an idea about the status of SNA research in education. Two systematic reviews, [27, 53]. – despite being relatively old– they give a useful review on the uses and applications of SNA in learning settings. For instance, the methods used by SNA researchers have been addressed in a dedicated systematic review by [8], where the authors offered a detailed review of methodological approaches used in SNA research. Centrality measures were the topic of a recent systematic review and meta-analysis that synthesized the literature and offered evidence of the association of centrality measures with academic achievement [49].\nA more recent scientometric study by [1] offers a comprehensive review of all research on network analysis and network science across the past five decades. The study also offers a review of authors, countries, research themes and research foundations. Whereas not a traditional systematic review, the recent paper by [9] offers a review of the seminal papers of SNA with a methodological approach. The paper also offers recommendations for a reporting scheme for research using SNA. It is also important to mention that our chapter covered only static networks. Readers who are interested in the more advanced time varying networks, the temporal network chapter offers a great starting point [54]. Also several guides and empirical papers demonstrate examples of temporal network analysis [20, 55, 56]. Readers who want to go deeper in analysis of learning communities, the community detection chapter can be a good place [57]. Also, for readers interested in the novel methods of psychological networks, they are encouraged to read the psychological network chapter [58]."
  },
  {
    "objectID": "chapters/ch15-sna/ch15-sna.html#more-reading-resources",
    "href": "chapters/ch15-sna/ch15-sna.html#more-reading-resources",
    "title": "15  Social Network Analysis: A primer, a guide and a tutorial in R",
    "section": "5 More reading resources",
    "text": "5 More reading resources\nBooks related to SNA that the readers can consult are:\n\nKolaczyk, E. D., & Csárdi, G. (2014). Statistical analysis of network data with R (Vol. 65). New York: Springer.\nLuke, D. A. (2015). A user’s guide to network analysis in R (Vol. 72, No. 10.1007, pp. 978-3). New York: Springer.\nNewman, Mark. Networks. Oxford university press, 2018.\nHanneman, R. A., & Riddle, M. (2005). Introduction to social network methods. (Link)\nCarolan, B. V. (2013). Social network analysis and education: Theory, methods & applications. Sage Publications.\nNetwork Science by Albert-László Barabási (Link)"
  },
  {
    "objectID": "chapters/ch15-sna/ch15-sna.html#acknowledgements",
    "href": "chapters/ch15-sna/ch15-sna.html#acknowledgements",
    "title": "15  Social Network Analysis: A primer, a guide and a tutorial in R",
    "section": "6 Acknowledgements",
    "text": "6 Acknowledgements\nThis paper is co-funded by the Academy of Finland the project Towards precision education: Idiographic learning analytics (TOPEILA), Decision Number 350560"
  },
  {
    "objectID": "chapters/ch16-community/ch16-comm.html",
    "href": "chapters/ch16-community/ch16-comm.html",
    "title": "16  Community Detection in Learning Networks Using R",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch16-community/ch16-comm.html#introduction-to-community-detection-in-social-networks",
    "href": "chapters/ch16-community/ch16-comm.html#introduction-to-community-detection-in-social-networks",
    "title": "16  Community Detection in Learning Networks Using R",
    "section": "1 Introduction to Community Detection in Social Networks",
    "text": "1 Introduction to Community Detection in Social Networks\nThe world is essentially social. Interactions, relationships and connections form the social fabric of the structure that makes it “social” [1]. Nevertheless, the world is far from uniform or random. The social actors (e.g., humans) tend to aggregate, coalesce, or work together in teams, groups, and communities [2, 3]. In humans, communities form neighborhoods, villages, cities, and even counties. On a smaller scale, communities can be a group of friends, teams, or work colleagues. These are —more or less— organized communities. However, communities can also emerge spontaneously among, for instance, people who happen to work together more than they work with others. Other examples include groups of universities that collaborate with each other more than they collaborate with other universities. In learning situations, communities can be groups of students within a whole cohort who collaborate with each other to a larger extent than with other students in a discussion forum. In the broader context of network analysis, community structure, or simply 'community', denotes those nodes within the network that can be categorized into distinct sets, ensuring that nodes within each set exhibit a high degree of internal connectivity.\nFinding these communities is integral to understanding the interaction process, the structure of the formed groups and how they contribute to the overall discussion. Community detection is not limited to discussion boards but extends to all types of networks where we would like to understand their structure. For instance, communities of key terms in a semantic network, where each concept is represented as a node, would allow us to understand the main themes of the conversation [4]. Communities of behaviors or strategies would allow us to understand the structure of human behavior and discern a better classification. The power of community detection has made it one of the most commonly used network approaches in education, social sciences and in fact, in scientific method at large [3].\nAs such, the main aim of community detection is to identify different groups or clusters of nodes within the network that show strong interconnectivity. Generally, most community detection algorithms are based on the idea that some nodes are more densely connected to each other than to nodes outside of the group. As a result, the application of community detection algorithms partitions the network into different groups, clusters or communities, based on the strength of their connections. An alternate branch of community detection, blockmodeling, adopts the perspective that people with similar patterns of ties are likely to act in similar ways within the network. This approach seeks to identify those similarly connected actors to abstract the network into different positions or roles [5, 6.] A special type of communities are the rich club communities [7, 8]. A rich club is a subset of nodes in the network who are working together —and who are therefore strongly connected— far more than with others in the network. The rich club entails a few doing most of the interaction and a majority who are far less involved and less favorable in collaborative networks. This chapter explores the first and more common approach, that of identifying densely connected subgraphs.\nAs mentioned above, a basic approach understands communities in social networks as subgraphs where the number of internal edges is larger than the number of external edges, and therefore they usually group nodes that have a higher probability of being connected to each other than to members of other groups [9]. The goal of identifying communities in social networks depends on the research question and context and, same as in the case of network analysis in general [10], the communities found in the analysis depend on how the network was constructed, and therefore their meaning must be explained by the researcher in each different context of application. For instance, a researcher might want to find communities to understand a social shift in some larger group [11, 12], to map political polarization [13], to interrupt disease transmission or to spread health-related behaviors [14], to study how students in a course self-segregate and stabilize into groups [15], or for many other reasons.\nMany partitioning algorithms use a quantitative measure called modularity index, which is, up to a multiplicative constant, the number of edges falling within groups minus the expected number in an equivalent network with edges placed at random [16]. Modularity, which ranges between -0.5 and 11 [18] and of which positive values indicate the possible presence of community structure [16] is not itself a clustering method, but a measure for comparing different partitionings to judge the best. There are no established values/thresholds for optimum modularity, but in practice it is found that a value above about 0.3 is a good indicator of significant community structure in a network [19].\nIn the previous chapter on Social Network Analysis [10], we learned how to build a network from a set of vertices (or nodes) and edges, how to visualize it, and how to calculate centrality measures that help us understand and describe the structure of the network and the position of the nodes with respect to others. In this new chapter, we focus on detecting communities (groups of highly connected nodes) within a wider network, and how to visualize them using R."
  },
  {
    "objectID": "chapters/ch16-community/ch16-comm.html#community-detection-in-social-networks-based-on-educational-data",
    "href": "chapters/ch16-community/ch16-comm.html#community-detection-in-social-networks-based-on-educational-data",
    "title": "16  Community Detection in Learning Networks Using R",
    "section": "2 Community detection in social networks based on educational data",
    "text": "2 Community detection in social networks based on educational data\nThe application of community detection techniques in social networks analysis of educational data (for further information about educational data and sources of educational data, we refer the reader to Chapter 2 in this book [20]) dates back to the emergence of learning analytics as a discipline. For example, in a 2012 study, Rabbany et al. [21] implemented community mining in their implementation of Meerkat-ED to monitor how student interactions and collaborations occurred and changed over time.\nCommunity detection can serve multiple purposes in learning analytics. For example, Pham et al. [22] explored the community structure of the eTwinning project collaboration network and how the quality of the contributors’ work relates to their level of participation. Similarly, Suthers and Chu [23] unveiled communities emerging within the Tapped In network of educational professionals from the associations between members of this network distributed across media (chat rooms, discussion forums and file sharing). Orduña et al. [24] applied modularity analysis to identify collaborative behaviors in a mobile remote laboratory. Skrypnyk et al. [25] and Gruzd et al. [26] identified emerging communities from Twitter-based interactions in a cMOOC. Joksimovic et al. [27] extracted clusters of concepts into topics (concept clusters) exchanged through social media also in the scope of a cMOOC. Hernández-García et al. [28] used connected components to analyze group cohesion. Adraoui et al. [29] identified student groups through their social interactions in Facebook groups. Nistor et al.[30] used cohesion network analysis to predict newcomer integration in online learning communities. López Flores et al. [31] performed community detection analysis to identify the learning behavior profiles of undergraduate computer science students. Abal Abas et al. [32] determined study groups during peer learning interaction. Li et al. [33]\nexamined whether students formed communities dominated by a specific gender or race, and Nguyen [34] explored youth's perspectives to frame climate change education by using community detection to find hashtag groups and identify different discourse themes in TikTok. The reader may find more examples of community detection in online learning environments in the recent review by Yassine et al. [4]. Another important aspect of community detection relates to the algorithms used to perform the detection, which will be discussed in the next section."
  },
  {
    "objectID": "chapters/ch16-community/ch16-comm.html#algorithms-for-community-detection",
    "href": "chapters/ch16-community/ch16-comm.html#algorithms-for-community-detection",
    "title": "16  Community Detection in Learning Networks Using R",
    "section": "3 Algorithms for community detection",
    "text": "3 Algorithms for community detection\nCommunity detection in social network analysis involves the application of specific algorithms (the number of community detection algorithms is rather high and will not be addressed in this chapter; for further information on the nature and performance of community detection algorithms, we suggest reading Lancichinetti and Fortunato [35], Fortunato [36], Fortunato and Hric [9] and Chunaev [37]), the most popular being Louvain [38], the one implemented in the software applications Gephi, Pajek and visone by default, and Girvan-Newman’s edge betweenness [39] and clique percolation [40], used in CFinder.\nIn this chapter, we will focus on how to perform community detection with the R programming language. In R, community detection algorithms are implemented as functions of the igraph library [41], which was already introduced in the Social Network Analysis chapter [10]. The library includes functions to apply the following methods:\n\nLouvain (cluster_louvain): A multi-level modularity optimization algorithm that aims to discover community structure [42].\nGirvan-Newman (cluster_edge_betweenness): The concept behind this method is that edges connecting distinct communities tend to have a high edge betweenness and all the shortest paths from one community to another must pass through these edges [43].\nFast greedy optimization (cluster_fast_greedy): A fast greedy modularity optimization method to identify communities within a network [19].\nFluid communities (cluster_fluid_communities): This method is based on the concept of multiple fluids interacting in a non-uniform environment (represented by the graph topology). The method identifies communities by observing their expansion and contraction patterns, driven by their interactions and density [44].\nRandom walk-based:\nInfomap (cluster_infomap): This method discovers the community structure that minimizes the expected description length of a random walker's trajectory [45].\nWalktrap (cluster_walktrap): This method is based on the underlying concept is that short random walks have a tendency to remain within the same community [46].\nLabel propagation (cluster_label_propagation): This method for detecting community structure in networks is efficient, with a nearly linear time complexity [47]. It operates by assigning unique labels to the vertices and subsequently updating these labels through majority voting within the vertex's neighborhood.\nLeading eigenvector (cluster_leading_eigen): This method identifies densely connected subgraphs within a graph by computing the principal non-negative eigenvector of the modularity matrix associated with the graph [48].\nLeiden (cluster_leiden): The Leiden algorithm is similar to the Louvain algorithm but offers superior speed and delivers higher-quality solutions. It has the capability to optimize both modularity and the Constant Potts Model, which overcomes the resolution limit challenge [49].\nOptimal modularity clustering (cluster_optimal): This method computes the optimal community structure of a graph by maximizing the modularity measure across all potential partitions [18].\nSimulated annealing (cluster_spinglass): This method leverages the spin-glass model and simulated annealing techniques to explore the graph's structure and identify cohesive communities [50].\n\nOther available libraries to perform community detection include nett, which allows for spectral clustering (function spec_clust()) and implements methods for hypothesis testing.\nWhen to choose one algorithm over another usually depends on the characteristics of the network data, whether or not directed edges are allowed and the goals of the analysis. For example, if the network is very large, Louvain or Label Propagation algorithms may offer satisfactory results with a small computational effort, whereas Girvan-Newman might be more appropriate if the network has a hierarchical structure or when networks are large. If the network has a flow of information, Infomap may be better suited, and spectral clustering might be preferrable when community structures are complex. Guidance about which algorithms to use is not robust, and issues such as computational efficiency are more often discussed than the match with research questions [9, 51]. Ideally, the choice should be made to align with the processes believed to drive community formation and the research purpose in seeking this structure [51]. In any case, we would recommend trying different algorithms and comparing their results before choosing one.\nIt is important to note the limitations of some community detection algorithms in igraph. For example, fast greedy optimization, leading eigenvector, Leiden, Louvain and simulated annealing only work with undirected graphs —even though some of these algorithms, such as Leiden, work in directed networks, their implementation in igraph does not allow for community detection in directed networks—, whereas fluid communities or simulated annealing only work with simple and connected graphs.\nFinally, it is worth mentioning that a limitation of some methods is that they are non-overlapping community detection algorithms; that is, they consider that a node belongs only to one group, partition or community. However, that is not often the case, which is why some overlapping community detection algorithms have been proposed (for example, random walk-based algorithms may handle overlapping), and the reader might want to check whether the chosen algorithm finds overlapping or non-overlapping communities when choosing what algorithm to use. Fortunato and Hric [9] offer additional guidance if overlapping communities are required. Now that we have covered the igraph library along with its features and limitations, in the next section we will present an example of how to use this library."
  },
  {
    "objectID": "chapters/ch16-community/ch16-comm.html#community-detection-in-r-an-annotated-example-using-igraph",
    "href": "chapters/ch16-community/ch16-comm.html#community-detection-in-r-an-annotated-example-using-igraph",
    "title": "16  Community Detection in Learning Networks Using R",
    "section": "4 Community detection in R: An annotated example using igraph",
    "text": "4 Community detection in R: An annotated example using igraph\nTo illustrate the use of igraph to perform community detection algorithms in R, we will use data from one of the courses reported in an article by Hernández-García and Suárez-Navas [52]. More specifically, this example focuses on the data from the “Programming Basics” undergraduate course. The data set includes forum activity of 110 students from the BSc Degree on Biotechnology, which follows the Comprehensive Training Model of the Teamwork Competence (CTMTC) methodology [53]. In this course, students work in groups of between five and seven members each. The activity in the forum includes Q&As, technical and academic support in general forums, and message exchanges between group members in group-exclusive forums.\nThe original data set including all forum activity was divided into three different subsets, with the help of GraphFES [54]: views (how many times user a read a message posted by user b), replies (which user replies to a different user, and how many times) and messages (which message is a reply to another message).\nIn this example, we will focus on the Replies data set, a directed graph. The node list of the Replies data set includes a total of 124 nodes with three attributes: initPosts (number of first posts of a discussion sent by a user), replyPosts (number of posts replying to a previous post in the discussion) and totalPosts (the sum of initPosts and replyPosts). Weights in the edge list (attribute w) represent the number of times that user Target replied to user Source. The data set includes a total of 662 weighed edges. First off, we load the required libraries for data loading (rio) and analysis (igraph). Install them first if you have not done so before.\n\nlibrary(igraph) \nlibrary(rio)\n\nThen, we import the node and edges data, and we build the graph object, indicating that the network is directed (with the argument directed = TRUE in the call to graph_from_data_frame()):\n\nrepo <- \"https://github.com/lamethods/data/raw/main/10_snaProgramming/\"\nds.nodes <- import(paste0(repo,\"hg_data_nodes.xlsx\"))\nds.edges <- import(paste0(repo,\"hg_data_edges.xlsx\"))\nds <- graph_from_data_frame(d = ds.edges, directed = TRUE, vertices = ds.nodes)\n\nWe can now observe the structure of the graph:\n\nstr(ds)\n\nClass 'igraph'  hidden list of 10\n $ : num 124\n $ : logi TRUE\n $ : num [1:662] 101 73 48 51 84 58 48 101 62 27 ...\n $ : num [1:662] 73 1 51 1 1 1 101 62 27 51 ...\n $ : NULL\n $ : NULL\n $ : NULL\n $ : NULL\n $ :List of 4\n  ..$ : num [1:3] 1 0 1\n  ..$ : Named list()\n  ..$ :List of 5\n  .. ..$ name      : chr [1:124] \"54\" \"55\" \"56\" \"57\" ...\n  .. ..$ User      : chr [1:124] \"user_54\" \"user_55\" \"user_56\" \"user_57\" ...\n  .. ..$ initPosts : num [1:124] 0 23 0 0 0 0 1 0 3 3 ...\n  .. ..$ replyPosts: num [1:124] 0 2 0 0 0 0 3 0 19 53 ...\n  .. ..$ totalPosts: num [1:124] 0 25 0 0 0 0 4 0 22 56 ...\n  ..$ :List of 1\n  .. ..$ weight: num [1:662] 1 2 1 2 5 4 1 1 1 1 ...\n $ :<environment: 0x55f1523dba60> \n\n\nWe can also inspect the main attributes of the graph, which is shown as a directed, named and weighted network with 124 nodes and 662 edges:\n\nprint(ds)\n\nIGRAPH 895fd99 DNW- 124 662 -- \n+ attr: name (v/c), User (v/c), initPosts (v/n), replyPosts (v/n),\n| totalPosts (v/n), weight (e/n)\n+ edges from 895fd99 (vertex names):\n [1] 192->164 164->55  139->142 142->55  175->55  149->55  139->192 192->153\n [9] 153->118 118->142 160->160 158->55  163->175 152->55  182->161 161->55 \n[17] 210->55  149->138 138->55  117->178 178->55  127->55  160->55  197->55 \n[25] 155->55  122->55  189->55  145->55  135->55  207->55  203->55  140->55 \n[33] 159->55  126->55  123->55  139->55  133->55  153->55  201->55  139->139\n[41] 139->180 192->134 134->206 206->192 192->205 205->205 205->192 192->113\n[49] 113->192 134->192 192->206 113->205 205->206 192->192 113->134 206->205\n+ ... omitted several edges\n\n\nIn addition, we may use the plot function to visualize the network, A first glance at the graph does not offer much information about the underlying community structure of the graph:\n\nplot(ds)\n\n\n\n\nFigure 1. Graph of the Replies from the data set of [52] using the plot function.\n\n\n\n\nAt any moment, we can apply a community detection algorithm to the graph. In Figure 16.1, we observe that the graph is not connected and is directed. Therefore, we can only apply a subset of community finding algorithms, such as Girvan-Newman, Infomap, Label propagation, or Walktrap (note that R will trigger a warning for Girvan-Newman, because edge weights have different meaning in modularity calculation and edge betweenness community detection). For the purpose of this example, we will apply the Infomap algorithm —because it is a random walk-based algorithm, we provide a random seed for reproducibility.\n\nset.seed(1234)\ncomm.ds <- cluster_infomap(ds)\n\nThe basic call to the cluster_infomap() function takes the graph as an argument (other arguments include edge weights, node weights, number of attempts to partition the network and whether modularity needs to be calculated; in this case, the function takes the ‘weight’ edge attribute as default edge weight) and returns a community object. We can observe its structure:\n\nstr(comm.ds)\n\nClass 'communities'  hidden list of 6\n $ membership: num [1:124] 1 2 3 4 5 6 7 8 9 10 ...\n $ codelength: num 3.95\n $ names     : chr [1:124] \"54\" \"55\" \"56\" \"57\" ...\n $ vcount    : num 124\n $ algorithm : chr \"infomap\"\n $ modularity: num 0.927\n\n\nThe attributes of the community object include the group each node belongs to, the code length or average length of the code describing a step of the random walker (this parameter only adopts a value in random walk algorithms), node names, number of nodes and algorithm used to partition the network in communities. It is also possible to access values of the community object using different functions such as length(), sizes() or membership(), which return the number of communities, sizes of each community and membership of each node, respectively. At this point, it is possible to plot the graph and the communities (Figure 16.2):\n\nplot(comm.ds, ds)\n\n\n\n\nFigure 2. Communities emerging from the Replies data set using the Infomap community finding algorithm.\n\n\n\n\nSee help(plot.communities) for more details on this method, which takes a communities object as its first argument and an igraph object as its second argument. By default, it colors the nodes and their surrounding “bubbles” with a different color for each community, and marks community-bridging edges in red. In many networks this produces a very overlapping and indistinct picture2.\nBecause Figure 16.2 does not provide useful information (yet), we have to proceed to clean the graph. First, we simplify the graph by removing multiple edges between nodes and turning them into a single weight (this is not strictly necessary in this case because the original edge data already included calculated weights) and self-edges. Additionally, and because isolated nodes do not belong to any community (they are their own community), we can remove them from the graph. We then re-calculate the Infomap clustering for the simplified graph and plot it again (Figure 16.3). We see that the self-loops and repeated edges have disappeared.\n\nsimple.ds <- simplify(ds, remove.multiple = TRUE, remove.loops = TRUE,\n                      edge.attr.comb = list(weight = \"sum\", \"ignore\"))\nsimple.ds <- delete.vertices(simple.ds, which(degree(simple.ds) == 0))\n\nWarning: `delete.vertices()` was deprecated in igraph 2.0.0.\nℹ Please use `delete_vertices()` instead.\n\ncomm.simple.ds <- cluster_infomap(simple.ds)\nplot(comm.simple.ds, simple.ds)\n\n\n\n\nFigure 3. Visualization of the communities emerging from the simplified Replies data set using the Infomap community finding algorithm.\n\n\n\n\nWe can now further refine the graph visualization to better highlight the communities:\n\nlo <- layout_with_fr(simple.ds, niter = 50000, \n                     weights = E(simple.ds)$weight * 0.05)\nplot(comm.simple.ds,\n     simple.ds,\n     layout = lo,\n     vertex.size = V(simple.ds)$totalPosts * 0.025,\n     vertex.label = NA,\n     edge.arrow.size = 0.1\n)\n\n\n\n\nFigure 4. Fine-tuned visualization of the simplified graph.\n\n\n\n\nFrom Figure 16.4, we can clearly observe 19 communities, of which 18 correspond to the student groups and the remaining one (in the center of the graph) corresponds to the course instructor. Even in a network such as this with a relatively clear modular structure, different community detection algorithms can return different results. For example, cluster_spinglass() (which allows for adjustment of the importance of present vs. absent edges through a gamma parameter) returns a different partitioning:\n\nset.seed(4321)\ncomm.simple2.ds <- cluster_spinglass(simple.ds, gamma = 1.0)\nplot(comm.simple2.ds,\n     simple.ds,\n     layout = lo,\n     vertex.size = V(simple.ds)$totalPosts * 0.025,\n     vertex.color = membership(comm.simple2.ds),\n     vertex.label = NA,\n     edge.arrow.size = 0.1\n)\n\n\n\n\nFigure 5. Visualization of simplified graph with spinglass communities.\n\n\n\n\nFigure 16.5 shows that the instructor is now included in one of the student communities, and a weakly connected member of another student group has split into their own community. Two groups with no direct bridging edges are also clustered together. This behavior is more common in blockmodeling, which looks for similarity of ties rather than direct links to identify groups; however, it can occur in standard community detection methods as well.\nAdditionally, it is possible to further simplify the network graph, by plotting only the communities and their inter-relationships. To do so, we build a condensed graph using the Infomap clustering where each node summarizes the information from all the members of the community (Figure 16.6).\n\ncomms <- simplify(contract(simple.ds, membership(comm.simple.ds)))\nplot(comms,\n     vertex.size = 2.5 * sizes(comm.simple.ds),\n     vertex.label = 1:length(comm.simple.ds),\n     vertex.cex = 0.8,\n     edge.arrow.size = 0.1\n)\n\n\n\n\nFigure 6. Network graph of the links between Infomap communities using the simplified version of the Replies data set.\n\n\n\n\nIt is also worth noting that igraph incorporates a function, compare(), that takes different community objects from different partitioning methods, and allows for their comparison, based on different methods, such as variation of information, normalized mutual information, split-join distance or Rand and adjusted Rand indices. We have not included an example here because additional knowledge of the comparison metrics is needed to interpret the results, but see help(igraph::compare) for references.\n\n4.1 Interactive visualization of communities in R\nIn the first sections of this chapter, we have highlighted the uses and applications of community finding using educational data, as well as the main principles and methods, complemented with an example in igraph. However, the last section also highlights the limitations of the igraph library to provide advanced graphic features, such as interactive plotting. To overcome these limitations, we will further explore interactive visualization of communities using two different libraries: visNetwork and networkD3.\n\n4.1.1 visNetwork\nvisNetwork is an R package for network visualization that uses the vis.js javascript library. It is based on htmlwidgets, and therefore it is compatible with Shiny, R Markdown documents, and RStudio viewer. To access its functions, we must first load the visNetwork package\n\nlibrary(visNetwork)\n\nThen, we need to build a data set that visNetwork can read. To do so, we need to create a data frame with all the original data (in this example, the data set corresponding to the simplified graph), to which we add the group that each node belongs to.\nThe first step to create this data frame is to build a data frame with the community of the node (in the column group) and a column id with a list of all nodes. The last line resets the row columns to a sequence starting in 1. In this step, it is critical to rename the column that represents the group assignment to “group”, as this field is internally interpreted by visNetwork as the different communities of the network.\n\nmemberships <- as.data.frame(as.matrix(membership(comm.simple.ds)))\ncolnames(memberships)[1]<- \"group\"\nmemberships$id <- rownames(memberships)\nrownames(memberships) <- 1:nrow(memberships)\n\nThe memberships data frame now has a column of group (community) numbers and a column of the original node id numbers:\n\nhead(memberships)\n\n\n\n  \n\n\n\nNext, we retrieve the original node and edge list as data sets, using the as_data_frame function. While we could extract both data sets in a single step using the argument what = \"both\", in this example we extract them separately for clarity of the manipulations required for each data set.\n\nsimple.ds.nodes <- as_data_frame(simple.ds, what = \"vertices\")\nsimple.ds.edges <- as_data_frame(simple.ds, what = \"edges\")\n\nIn the node list, and while it is not absolutely necessary, we reset the row columns to a sequence starting in 1. After that, we need to rename the original ‘name’ and ‘User’ columns to ‘id’ and ‘title’. The former manipulation will allow us to add the group number to the dataset with the information included in the memberships object, while the latter is used by visNetwork to identify the different nodes.\n\nrownames(simple.ds.nodes) <- 1:nrow(simple.ds.nodes)\ncolnames(simple.ds.nodes)[1] <- \"id\"\ncolnames(simple.ds.nodes)[2] <- \"title\"\n\nFinally, we combine the node data set with the membership data set.\n\nvis.comm <- merge(simple.ds.nodes, y = memberships, by = \"id\", all.x = TRUE)\n\nFor visualization purposes, we add a column with the size of the nodes in the visualization.\n\nvis.comm$size <- vis.comm$totalPosts * 0.2\n\nFinally, we proceed to visualize the graph. To do so, we call the visNetwork function, and we pipe different visualization options (|>3), which we apply to the visualization object: (1) the manual random seed ensures reproducibility; (2) the legend displays the different communities in the right part of the viewer; (3) we highlight connected nodes on selection, and allow for selection in a dropdown menu by ‘id’ and ‘group’; and (4) we allow drag/zoom on the network, with navigation buttons that are displayed on the upper part of the viewer.\n\nvisNetwork(vis.comm, simple.ds.edges, width = \"100%\", height = \"800px\", \n           main = \"Interactive Communities\") |>    \n  visLayout(randomSeed = 1234) |> \n  visLegend(position = \"right\", main = \"group\") |> \n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE, selectedBy = \"group\") |>  \n  visInteraction(hideEdgesOnDrag = TRUE, dragNodes = TRUE, dragView = TRUE, \n                 zoomView = TRUE, navigationButtons = TRUE)\n\n\n\n\n\n\n\n4.1.2 networkD3\nThe networkD3 is an advanced library for the interactive visualization of networks. It creates D3 network graphs and it is also based on the htmlwidgets framework, therefore simplifying the package’s syntax for exporting the graphs and allowing integration with RStudio, RMarkdown and Shiny web apps. To access its functions, we first load the library.\n\nlibrary(networkD3)\n\nAn advantage of the networkD3 library is that it provides a function, igraph_to_networkD3(), that allows direct loading of an igraph network as a networkD3 compatible graph. In this case, analogously to the previous example in visNetwork, we will visualize the simplified network. We provide two arguments to the igraph_to_networkD3 function: the original igraph object and the node membership list, also obtained before with the help of igraph.\n\ngraph.d3 <- igraph_to_networkD3(simple.ds, group = membership(comm.simple.ds))\n\nAnalogously to the example in visNetwork, we add node sizes for improved visualization.\n\ngraph.d3$nodes$size <- simple.ds.nodes$totalPosts * 0.2\n\nAnd finally, we use the forceNetwork function to display the graph. In this case, we will store the interactive visualization in an object to enhance the behavior of the legend later by using the htmlwidgets framework. From the example below, no additional manipulation of the original data sets was required4.\nThe forceNetwork() function requires the edge (Links) and node (Nodes) data frames, as well as the name of the source and target columns in the edge data frame, the node id and group columns in the node data frame. In the following code, it is also worth noting that the argument provided to display the size of the nodes refers to the column number —and not the column name—, and that we can provide different repulsion values using the charge parameter —the strength of node repulsion (negative values) or attraction (positive values).\n\nd3.comm <- forceNetwork(Links = graph.d3$links, Nodes = graph.d3$nodes, \n                        Source = 'source', Target = 'target', \n                        NodeID = 'name', Group = 'group',\n                        linkColour = \"#afafaf\", fontSize = 12, zoom = T, \n                        legend = T, Nodesize = 3, opacity = 0.8, \n                        charge = -25,  width = 800, height = 800)\nd3.comm\n\n\n\n\n\nBy default, the legend moves when we zoom in or out, or drag the graph. A possible workaround to fix the legend is to use the htmlwidgets library [55].\n\nlibrary(htmlwidgets)\n\n\nhtmlwidgets::onRender(d3.comm, jsCode = '\n  function (el, x) {\n    d3.select(\"svg\").append(\"g\").attr(\"id\", \"legend-layer\");\n    var legend_layer = d3.select(\"#legend-layer\");\n    d3.selectAll(\".legend\")\n      .each(function() { legend_layer.append(() => this); });\n  }\n')"
  },
  {
    "objectID": "chapters/ch16-community/ch16-comm.html#concluding-notes",
    "href": "chapters/ch16-community/ch16-comm.html#concluding-notes",
    "title": "16  Community Detection in Learning Networks Using R",
    "section": "5 Concluding notes",
    "text": "5 Concluding notes\nIn this chapter we have introduced the literature on community detection in social network analysis, highlighted its uses in learning analytics, and worked through an example of finding and visualizing communities in R. The process begins outside of R, by identifying possible mechanisms of community formation in this network and why they are of interest. In educational settings, researchers may be concerned with information flow, dissemination of norms or attitudes, or other social forces. It is also important to consider whether the directionality and frequency (or other strength measure) of the interactions is important and that these considerations align with the theory and contextual peculiarity [56, 57]. Once these factors are thought out, there will still probably be a few options for community detection algorithms. It is worth trying more than one algorithm and comparing their groupings, as well as reading up on the method to see if it has tunable parameters such as the tightness of a random walk or the relative importance of missing links. Moreover, once the communities have been detected they can be explored in several ways. For example, one can investigate the demographic differences between the communities and determine whether they are formed based on shared characteristics between the nodes (e.g., gender, race, and nationality). Other aspects to look into are the content of the interactions, the difference in performance (e.g., final grade) between communities, and their temporal evolution to understand how they formed. Furthermore, each community can be visualized and analyzed as a network of its own using the methods explained in the Social Network Analysis chapter of this book.\nThough we began with the visualization tools available in the igraph package, in many cases researchers will want to go further. In these cases, the results of clustering algorithms can be used with libraries like visNetwork or networkD3. In the end, the goal of the visualization is to explore or present insights about network subgroups that speak to the original research questions, and it is helpful to be familiar with a range of tools for this purpose.\nThis chapter can be considered an introduction to the topic of community detection. However, interested users can resort to our cited papers and, for further readings, the selected papers and books provided in the following section can be a good start."
  },
  {
    "objectID": "chapters/ch16-community/ch16-comm.html#further-readings",
    "href": "chapters/ch16-community/ch16-comm.html#further-readings",
    "title": "16  Community Detection in Learning Networks Using R",
    "section": "6 Further readings",
    "text": "6 Further readings\nInterested readers can refer to the following resources about community detection in general:\n\nFortunato, S., & Hric, D. (2016). Community detection in networks: A user guide. Physics Reports, 659, 1-44.\nTraag, V. A., Waltman, L., & Van Eck, N. J. (2019). From Louvain to Leiden: Guaranteeing well-connected communities. Scientific Reports, 9(1), 5233.\nXie, J., Kelley, S., & Szymanski, B. K. (2013). Overlapping community detection in networks: The state-of-the-art and comparative study. ACM Computing Surveys, 45(4), 1-35.\n\nFor specific resources using R, the reader can consult the following:\n\nBorgatti, S. P., Everett, M. G., Johnson, J. C., & Agneessens, F. (2022). Analyzing social networks using R. SAGE.\nKolaczyk, E. D., & Csárdi, G. (2014). Statistical analysis of network data with R (Vol. 65). New York: Springer.\nLuke, D. A. (2015). A user's guide to network analysis in R (Vol. 72, No. 10.1007, pp. 978-3). Cham: Springer."
  },
  {
    "objectID": "chapters/ch17-temporal-networks/ch17-tna.html",
    "href": "chapters/ch17-temporal-networks/ch17-tna.html",
    "title": "17  Temporal network analysis: Introduction, methods and analysis with R",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch17-temporal-networks/ch17-tna.html#introduction",
    "href": "chapters/ch17-temporal-networks/ch17-tna.html#introduction",
    "title": "17  Temporal network analysis: Introduction, methods and analysis with R",
    "section": "1 Introduction",
    "text": "1 Introduction\nLearning is social and therefore, involves relations, interactions and connections between learners, teachers and the world at large. Such interactions are essentially temporal and unfold in time [1]; that is, facilitated, curtailed or influenced at different temporal scales [2, 3]. Therefore, time has become a quintessential aspect in several learning theories, frameworks and methodological approaches to learning [3–5]. Modeling learning as a temporal and relational process is, nevertheless, both natural, timely and more tethered to reality [4, 6]. Traditionally, relations have been modeled with Social Network Analysis (SNA) and temporal events have been modeled with sequence analysis or process mining [3, 7].Yet, researchers have rarely combined the two aspects (the temporal and relational aspects) in an analytics framework [1]. Considering how important the timing and order of the learning process are, it is all-important that our analysis lens is not time-blind [8, 9]. Using time-blind methods flattens an essentially temporal process where the important details of progression are lost or distorted [10, 11]. In doing so, we miss the rhythm, the evolution and devolution of the process, we overlook the regularity and we may fail to capture the events that matter [9–11].\nTemporal networks\nRecent advances in network analysis have resulted in the emergence of the new field of temporal network analysis which combines both the relational and temporal dimensions into a single analytical framework: temporal networks, also referred to as time-varying networks, dynamic networks or evolving networks [10]. Today, temporal networks are increasingly adopted in several fields to model dynamic phenomena, e.g., information exchange, the spread of infections, or the reach of viral videos on social media [12]. Whereas temporal networks are concerned with the modeling of relationships similar to traditional social networks (i.e., static or aggregate networks), they are conceptually fundamentally different [10, 11, 13]. Additionally, temporal networks are not a simple extension of social networks, nor are they time-augmented social networks or time-weighted networks. In that, temporal networks are based on different representations of data, have a different mathematical underpinning, and use distinct visualization methods. In temporal networks, edges emerge (get activated or born) and dissolve (get deactivated or die) compared to always present edges in static social networks. Also, in temporal networks, an edge represents temporary interaction, contact, co-presence, or concurrency between two nodes interacting at a specific time. The fact that static networks represent nodes as being connected together all the time exaggerates connectivity [14, 15]. For instance, in Figure 17.1, we have five network visualizations, each network belonging to a weekday. We see that Monday, Tuesday, and Wednesday networks are relatively connected, whereas Thursday and Friday networks are disconnected. The corresponding aggregated or static network on the right is densely connected. The example in Figure 17.1 shows how a static network both conflates connectivity and obfuscates dynamics, you can read more about this example in [15]. Similarly, network measures calculated in static networks are inflated and biased -skewed towards higher values - because they ignore the temporal direction of edges allowing the edges to run back in time. Another characteristic of temporal networks is that edges have a starting time point and ending time point, the end of each edge is understandably later than the start, i.e., follows the forward-moving direction of time. Therefore, the paths in the temporal network are unidirectional or time-restricted [10, 11]. The next section discusses the temporal networks in detail.\n\n\n\nFigure 1. Interactions are aggregated per day showing that some days show an active group e.g., Monday, and other days have an inactive group e.g., Friday. Meanwhile, the static network on the right appears dense [15]"
  },
  {
    "objectID": "chapters/ch17-temporal-networks/ch17-tna.html#the-building-blocks-of-a-temporal-network",
    "href": "chapters/ch17-temporal-networks/ch17-tna.html#the-building-blocks-of-a-temporal-network",
    "title": "17  Temporal network analysis: Introduction, methods and analysis with R",
    "section": "2 The building blocks of a temporal network",
    "text": "2 The building blocks of a temporal network\n\n2.1 Edges\nIn temporal networks, edges are commonly referred to as events, links, or dynamic edges. Two types of temporal networks are commonly described based on their edge type [12].\n\nContact temporal networks: In contact temporal networks, edge duration is very brief, undefined, or negligible. For example, instant messages have no obvious duration but have a clear source (sender), target (receiver), and timestamp. Figure 17.2 shows a contact temporal network where the edges are represented as sequences of contacts between nodes with no duration.\n\n\n\n\nFigure 2. Example of a contact temporal network. Edges form momentarily and have no obvious duration.\n\n\n\nInterval temporal networks: In interval temporal networks, each interaction has a duration. An example of such a network would be a conversation where each of the conversants talks for a certain length of time. In the interval temporal network, the duration of interactions matters and the modeling thereof helps understand the process. In Figure 17.3, we see an interval temporal network where each edge has a clear start and clear end. For example, an edge forms between node A and node B at time point 1 and dissolves at time point 3, i.e., lasts for two time points.\n\n\n\n\nFigure 3. An example of an interval network. On the top, we see an example of an edge. In the bottom arrows pointing to two edges B-D and A-D, we can see that the two edges do not overlap and therefore, although they share a connection (A), we can’t assume that B is connected to A through D.\n\n\n\n\n2.2 Paths, concurrency, and reachability\nPaths represent the pathways that connect edges, the identification of which can help solve essential problems like the shortest path between two places in a route planning application, e.g., Google maps. In a dynamic process, the paths represent a time-respecting sequence of edges i.e., where the timing of each edge follows one another according to time passage, that is, the timestamps are incrementally increasing [10, 11]. For instance, let’s assume we have a group of students interacting about a problem, starting by defining the problem, argumenting, debating, and finding a solution. The temporal path that would represent the sequence of interactions among students in this process will be a defining->argumenting->debating->solving. We expect that the timestamp of defining precedes argumenting and argumenting precedes debating and so on. In that way, the path is unidirectional, follows a time-ordered sequence, and requires that each node is temporally connected, i.e., the two nodes coexist or interact with each other at the same time [11]. Such temporal co-presence is known as concurrent. Concurrency defines the duration of the nodes where they were co-present together and therefore can be a measure of the magnitude of contact between the two nodes. This is particularly important when we are modeling processes where the path length matters e.g., social influence. A student is more likely to be influenced by an idea when the student discusses the idea with another for a longer period of time. Similarly, self-regulation could be more meaningful when phases are more concurrent rather than disconnected [3]. Reachability is the proportion of nodes that can be reached from a node using time-respecting paths. A node is more influential or central, if it can reach a larger number of nodes [12].\n\n\n2.3 Nodes\nNodes in temporal networks are similar to static networks at large. Such nodes can be humans, objects, semantics, historical events or chemical reactions to mention a few. Perhaps, the possible difference —if it at all exists— is that temporal network tend to be studied in fields where temporal order is consequential e.g., epidemics, linguistics and spread of ideas."
  },
  {
    "objectID": "chapters/ch17-temporal-networks/ch17-tna.html#previous-work-and-examples-of-temporal-network-analysis",
    "href": "chapters/ch17-temporal-networks/ch17-tna.html#previous-work-and-examples-of-temporal-network-analysis",
    "title": "17  Temporal network analysis: Introduction, methods and analysis with R",
    "section": "3 Previous work and examples of temporal network analysis",
    "text": "3 Previous work and examples of temporal network analysis\nFew studies have addressed temporal network analysis. Yet, some examples exist that may shed light on the novel framework and how it can be harnessed in education. In a study by Saqr and Nouri [15], the authors investigated how students interact in a problem-based learning environment using temporal networks. The study estimated temporal centrality measures, used temporal network visualization, and examined the predictive power of temporal centrality measures. The study reported rhythmic changes in centrality measures, network properties as well as the way students mix online. The study also found that temporal centrality measures were predictive of students” performance from as early as the second day of the course. Models that included temporal centrality measures have performed consistently better and from as early as the first week of the course. Another study by [9] analyzed students’ interactions in an online collaborative environment where students interacted in Facebook groups. The authors compared centrality measures from traditional social networks to temporal centrality measures and found that temporal centralities are more predictive of performance. Another study from the same group has used chat messages to study how students interact online and how temporal networks can shed light on different dynamics of students interacting using Discord instant messaging platform compared to students interacting using the forums in Moodle. Temporal networks were more informative in capturing the differences in dynamics and how such dynamics affected students” way of communicating [16]."
  },
  {
    "objectID": "chapters/ch17-temporal-networks/ch17-tna.html#tutorial-building-a-temporal-network",
    "href": "chapters/ch17-temporal-networks/ch17-tna.html#tutorial-building-a-temporal-network",
    "title": "17  Temporal network analysis: Introduction, methods and analysis with R",
    "section": "4 Tutorial: Building a temporal network",
    "text": "4 Tutorial: Building a temporal network\nTemporal network is a relatively new field with an emerging repertoire of methods that are continuously expanding. As we currently stand, a coherent tutorial that combines all possible steps of the analysis does not exist, and that is what this chapter aims to fill. The tutorial will introduce the R packages, visualization and mathematical analysis e.g., graph and node level centrality measures.\nThe first step is to load the needed packages. Unlike the SNA chapter [17] where we relied on the igraph framework, we will rely on the statnet framework that has a rich repertoire of temporal network packages. We will use three main packages, namely tsna (Temporal Social Network Analysis) which provides most functions for dealing with temporal networks as an extension for the popular sna package. The package networkDynamic offers several complementary functions for the network manipulation, whereas the package ndtv (Network Dynamic Temporal Visualization) offers several functions for visualizing temporal networks. To learn more about these packages, please visit their help pages. The next code chunk loads these packages as well as tidyverse packages to process the network dataframe [20]. We also need tidyverse for manipulating the file and preparing the data.\n\nlibrary(tsna)\nlibrary(ndtv)\nlibrary(networkDynamic)\nlibrary(tidyverse)\nlibrary(rio)\n\nTo create a temporal network, we need a timestamped file with interactions. The essential fields are the source, target and time, and perhaps also some information about the interactions or the nodes (but these constitute extra information that is good to have). A temporal network is created by combining a base static network (that has the network base information) and a dynamic network with time information. As such, we need to prepare the Massive Open Online Course (MOOC) dataset described in detail here [21] and prepare it for creating a static network that will serve as a base network.\nThe next code chunk loads the dataset files (edges and nodes data) from the MOOCs dataset. Some cleaning of the data is necessary. \n\nnet_edges <- import(\"https://raw.githubusercontent.com/lamethods/data/main/6_snaMOOC/DLT1%20Edgelist.csv\")\nnet_nodes <- import(\"https://raw.githubusercontent.com/lamethods/data/main/6_snaMOOC/DLT1%20Nodes.csv\")\n\nFirst, we have to clean the column names from extra spaces using the function clean_names from the janitor package. Next, we have to remove loops, or instances where the source and target of the interaction are the same since it makes little sense that a person responds to oneself in a temporal network (this is not essential). Third, we need to create a dataframe where we replace duplicate edges with a weight equal to the frequency of repeated interactions, we will need this file for the creation of the base network (see later). Fourth, we recode the expertise level in the nodes file to meaningful codes (from its original numerical coding as 1,2,3) so that we can use them later in the analysis. The fifth step is to convert the timestamp to sequential days starting from the first day of the course; this makes sense for easy interpretation. Also, time works better in networkDynamic when it is numeric. The final step is to remove discussions where there are no replies. This cleaning is necessary since we have a dataset that was not essentially prepared for temporal networks.\n\nnet_edges <- net_edges |> janitor::clean_names() #1 cleaning column names\nnet_edges_NL <- net_edges |> filter(sender != receiver) #2 removing loops\n\n## Removing duplicates and replacing them with weight\nnet_edges_NLW <- net_edges_NL |> group_by(sender, receiver) |> tally(name = \"weight\") #3\n\n## Recoding expertise\nnet_nodes <- net_nodes |> \n  mutate(expert_level = case_match(experience, #4\n        1 ~\"Expert\",\n        2 ~ \"Student\",\n        3 ~ \"Teacher\"))\n\n## A function to create serial days\ndayizer = function(my_date) {\n  numeric_date = lubridate::parse_date_time(my_date, \"mdy HM\")\n  Min_time = min(numeric_date)\n  my_date = (numeric_date - Min_time) / (24*60*60)\n  my_date = round(my_date,2)\n  return(as.numeric(my_date))\n}\n\nnet_edges_NL$new_date = dayizer(net_edges_NL$timestamp) #5\n\n## Remove dicussions with no interactions\nnet_edges_NL <- net_edges_NL |> group_by (discussion_title) |> filter(n() > 1)\n\nAs mentioned before, the first step in creating a temporal network is creating a static base network (base network) which carries all the information about the network, e.g., the nodes, edges as well as their attributes. The base network is typically a static weighted network. Here we define the base network file (the weighted edge file we created before), we use directed = TRUE to create our network as directed and we tell the network function that the vertices attributes are in the net_nodes file.\n\nNetworkD <- network(net_edges_NLW, directed = TRUE, matrix.type = \"edgelist\", \n                    loops = FALSE, multiple = FALSE, vertices = net_nodes)\n\nFor creating a temporal network, we need more than the source and the target commonly needed for the static network. In particular, the following variables are required to be defined.\n\ntail: the source of the interaction\nhead: the target of the interaction\nonset: The starting time of the interaction\nterminus: the end time of the interaction\nduration: the duration of the interaction\n\nOur dataset —which comes from forum MOOC interactions, see net_edges below— has an obvious starting time (which is the timestamp of each interaction) but has no clear end time. There is no straightforward answer to this question. Nonetheless, a possible way to consider the duration of every post is the duration the post was active in the discussion or continued to be discussed. That is the time from the post in a discussion thread to the last post in the same threads of replies. Such a method —while far from perfect— offers a rough method for estimating the time during which this interaction has been “active” in the discussion [15, 22]. For an illustration, see Figure 17.4 which shows the duration for the first and second posts.\n\nnet_edges\n\n\n\n  \n\n\n\n\n\n\nFigure 4. A sample discussion demonstrating a way to compute the duration of the post. The first post lasts active in the discussion from starting time to the last reply it stimulated in the same thread (D1)\n\n\nThe next code chunk creates a variable for the starting time of each interaction, computes the ending time where this post was part of an active discussion, and then computes the duration.\n\n# Create the required variables (start, end, and duration) defined by\n\nnet_edges_NL <- net_edges_NL |> group_by(discussion_title) |>\nmutate(start = min(new_date), end = max(new_date), duration = end - start)\n\nIn the same way, the second duration is computed in the same way (D2). The next step of the analysis creates a dataframe where all the needed information for the network is specified and in the next step we simply use the networkDynamic with two arguments, the base network and the dataframe with all the temporal network information created in the previous step. Nonetheless, dealing with around 450 nodes in a network is hard, it becomes impossible to visualize or get insights from a large number of crowded nodes. So, for the sake of simplicity of demonstration in this tutorial, we will create a smaller subset of the network of people who had a reasonable number of interactions (degree more than 20) using get.inducedSubgraph argument. The resulting network is then called Active_Network which we will analyze.\n\n## Creating a dataframe with needed variables\n\nedge_spells <- data.frame(\"onset\" = net_edges_NL$start ,\n                         \"terminus\" = net_edges_NL$end, \n                         \"tail\" = net_edges_NL$sender, \n                         \"head\" = net_edges_NL$receiver, \n                         \"onset.censored\" = FALSE,\n                         \"terminus.censored\" = FALSE, \n                         \"duration\" = net_edges_NL$duration)\n\n## Creating the dynamic network network\nDynamic_network <- networkDynamic(NetworkD, edge.spells = edge_spells)\n\nEdge activity in base.net was ignored\nCreated net.obs.period to describe network\n Network observation period info:\n  Number of observation spells: 1 \n  Maximal time range observed: 0 until 72.01 \n  Temporal mode: continuous \n  Time unit: unknown \n  Suggested time increment: NA \n\nActive_Network <- get.inducedSubgraph(Dynamic_network,\n                                v = which(degree(Dynamic_network) > 20))\n\nWe can then confirm that the network has been created correctly using the print function. As the output shows, we have 521 distinct time changes, 72 days, 445 vertices and 1936 edges. We can also use the function plot to see how the network looks. The argument pad helps us remove the additional whitespace around the network. Plotting a temporal network helps summarize all the interactions in the network. As we can see in Figure 17.5, the network is dense with several edges between interacting students.\n\nprint(Dynamic_network)\n\nNetworkDynamic properties:\n  distinct change times: 495 \n  maximal time range: 0 until  72.01 \n\nIncludes optional net.obs.period attribute:\n Network observation period info:\n  Number of observation spells: 1 \n  Maximal time range observed: 0 until 72.01 \n  Temporal mode: continuous \n  Time unit: unknown \n  Suggested time increment: NA \n\n Network attributes:\n  vertices = 445 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = FALSE \n  bipartite = FALSE \n  net.obs.period: (not shown)\n  total edges= 1936 \n    missing edges= 0 \n    non-missing edges= 1936 \n\n Vertex attribute names: \n    connect country experience experience2 expert expert_level Facilitator gender grades group location region role1 vertex.names \n\n Edge attribute names not shown \n\nplot.network(Active_Network, pad = -0.5)\n\n\n\n\nFigure 5. A plot of the full network as plotted by the plot function.\n\n\n\n\n\n4.1 Visualization of temporal networks\nTo take advantage of the temporal network, we can use a function to extract the network at certain times to explore the activity. In the next example in Figure 17.6, we chose the first four weeks one by one and plotted them alongside each other. The function filmstrip can create a similar output with a snapshot of the network at several time intervals.\n\nplot.network(network.extract(Active_Network, onset = 1, terminus = 7))\nplot.network(network.extract(Active_Network, onset = 8, terminus = 14))\nplot.network(network.extract(Active_Network, onset = 15, terminus = 21))\nplot.network(network.extract(Active_Network, onset = 22, terminus = 28))\n\n\n\n\nFigure 6. A plot of the network at weeks 1, 2, 3, and 4\n\n\n\n\nA similar result, yet with three-dimensional placing, can also be obtained with the timePrism function, as shown in Figure 17.7. You may need to consult the package manual to get more information about the arguments and options for the plots.\n\ncompute.animation(Active_Network)\n\nslice parameters:\n  start:0\n  end:72.01\n  interval:1\n  aggregate.dur:1\n  rule:latest\n\ntimePrism(Active_Network, at = c(1, 7, 14, 21),\n  spline.lwd = 1,\n  box = TRUE,\n  angle = 60,\n  axis = TRUE,\n  planes = TRUE,\n  plane.col = \"#FFFFFF99\",\n  scale.y = 1,\n  orientation = c(\"z\", \"x\", \"y\"))\n\n\n\n\nFigure 7. A three-dimensional visualization of the network as a sequence of snapshots in space and time prism\n\n\n\n\nHowever, a better way is to take advantage of the capabilities of the package ndtv and the network temporal information by rendering a full animated movie of the network and explore each and every event as it happens.\n\nrender.d3movie(Active_Network)\n\n\n\n\n\n\n\nAs we mentioned in the introduction section, in temporal networks, edges or relationships form “get activated” and dissolve “get deactivated”. We can plot such the dynamic edge formation and dissolution process using the functions tEdgeFormation which as the name implies plots the edges forming at the given time point. The function tEdgeDissolution returns the edges terminating and can be plotted in the same way as seen in Figure 17.8. Obviously, at the beginning of the MOOC, we see new relationships form and, at the end, most relationships dissolve.\n\nplot(tEdgeFormation(Active_Network, time.interval = 0.01), ylim = c(0,50))\nplot(tEdgeDissolution(Active_Network, time.interval = 0.01), ylim = c(0, 50))\n\n\n\n\n\n\n\n(a) Edge formation\n\n\n\n\n\n\n\n(b) Edge dissolution\n\n\n\n\nFigure 8. Edge formation and dissolution. We see that edge formation occurs towards the start and increases till almost t=50 and edge dissolution starts scanty at the beginning and increases with time, peaking at t=70\n\n\n\nAnother way to visualize a temporal network is to use the proximity timeline, the proximity.timeline function tries to draw the temporal network in two dimensions, that is, it draws nodes at each time point taking into account how closely connected they are and renders them accordingly, nodes that are interacting are rendered close to each other, and nodes that not interacting are rendered apart. Technically as described in the function manual: “The passed network dynamic object is sliced up into a series of networks. It loops over the networks, converting each to a distance matrix based on geodesic path distance with layout.distance. The distances are fed into an MDS algorithm (specified by mode) that lays them out in one dimension: essentially trying to position them along a vertical line. The sequence of 1D layouts are arranged along a timeline, and a spline is drawn for each vertex connecting its positions at each time point. The idea is that closely-linked clusters form bands of lines that move together through the plot” [20]. The result is a timeline of temporal proximity. The next code draws the proximity timeline but also adds some colors, and a start and end for the plot, see the result in Figure 17.9.\n\nproximity.timeline(Active_Network, default.dist = 1, mode = \"sammon\",\n                   labels.at = 1, vertex.col = grDevices::colors(),\n                   start = 1, end = 30, label.cex = 0.5)\n\n\n\n\nFigure 9. A proximity timeline shows connected interacting nodes closer to each other.\n\n\n\n\n\n\n4.2 Statistical analysis of temporal networks\n\n4.2.1 Graph level measures\nGraph properties in temporal networks are dynamic and vary by time. When graph measures are computed we get a time series of the computed measures. Such fine-grained measures allow us to understand how the networks and their structure evolve or unfold in time and thus, such information can help us understand collaboration or interaction dynamics as they occur [9, 15].\nThe function tSnaStats from the package tsna has a large number of measures that can be computed by specifying the argument snafun. In the next example, we compute the graph level density with the argument snafun=gden. The function allows the choice of a range of time, for instance, from the end of the second week to the end of second month by supplying the arguments start = 14 to end = 60. We can also use the time.interval to specify the granularity of the calculation. The argument aggregate.dur specifies the period of the aggregation, for instance, aggregate.dur = 7 will compute the density for every seven days. We can also plot the density time series by simply using the function plot.\n\nDensity <- tSnaStats(\n  nd = Active_Network,\n  snafun = \"gden\",\n  start = 14,\n  end = 60,\n  time.interval = 1,\n  aggregate.dur = 7)\nplot(Density)\n\n\n\n\nFigure 10. A plot of temporal density from t=14 to t=60\n\n\n\n\nYou can see, in the resulting graph in Figure 17.10, that the density increases until day 50 and then starts to drop. Of note, another type of density can be computed, known as temporal density, which computes the observed total duration of all edges and divides it by the maximum duration possible. Temporal density can be computed using the command tEdgeDensity.\n\ntEdgeDensity(Active_Network) \n\n[1] 0.3901841\n\ngden(Active_Network) \n\n[1] 0.2186869\n\n\nSimilar to density, we can compute reciprocity, i.e., the ratio of reciprocated edges to asymmetric edges. Note, that we calculate reciprocity here from day 1 to day 73 on a daily basis (this is just for demonstration of different periods). Since the function default is to calculate the reciprocated dyads, we specify the argument measure = “edgewise” to calculate the proportion of reciprocated edges. As the graph in Figure 17.11 (a) shows, reciprocity increases steadily for the first 50 days pointing to a build up of trust between collaborators. The dyad.census function offers a more granular view of the dyads and their reciprocity as shown in Figure 17.11 (b). Similarly, mutuality is a very similar function and returns the number of complete dyads (reciprocated dyads), plotted in Figure 17.11 (c). All of the aforementioned functions deal with reciprocity, for differences and usages, readers are encouraged to read the functions’ help to explore the differences, arguments as well as the equation for each function.\n\nReciprocity <- tSnaStats(\n    nd=Dynamic_network,\n    snafun = \"grecip\" ,\n    start = 1,\n    end = 73,\n    measure = \"edgewise\",\n    time.interval = 1,\n    aggregate.dur = 1)\nplot(Reciprocity)\n\nDyad.census <- tSnaStats(Active_Network, \n                         snafun = \"dyad.census\")\nplot(Dyad.census)\n\ndynamicmutuality <- tSnaStats(\n  Active_Network,\n  snafun = \"mutuality\",\n  start = 1,\n  end = 73,\n  time.interval = 1,\n  aggregate.dur = 1\n  )\nplot(dynamicmutuality)\n\n\n\n\n\n\n\n(a) Reciprocity over time\n\n\n\n\n\n\n\n(b) Dyad census over time\n\n\n\n\n\n\n\n(c) Mutuality over time\n\n\n\n\nFigure 11. Network descriptive statistics over time\n\n\n\nCentralization measures the dominance of members within the network and can be traced temporally using the function snafun = “centralization”. Please note that we can choose the period, the interval and the aggregation periods using function arguments as mentioned before. The next example computes the degree centralization as demonstrated in Figure 17.12. Also note that we can also compute other centralization measures such as centralization indegree, centralization outdegree, centralization betweenness, centralization closeness and eigenvector.\n\nDegree_centralization <- tSnaStats(\n  Active_Network,\n  snafun = \"centralization\",\n  start = 1,\n  end = 73,\n  time.interval = 1,\n  aggregate.dur = 1,\n  FUN = \"degree\")\n\nplot(Degree_centralization)\n\n\n\n\nFigure 12. A plot of degree centralization over time\n\n\n\n\nSeveral other graph-level measure can be computed in the same way using the following arguments passed to the snafun: - components: count of Components within the graph over time - triad.census: the triad census and types of triads over time - connectedness: the connectedness score of the network efficiency network efficiency over time - gtrans: network transitivity over time - hierarchy: network Hierarchy over time - lubness: network LUBness over time - efficiency: network efficiency over time - hierarchy: network hierarchy over time\n\n\n4.2.2 Node-level measures (Temopral centrality measures)\nCentrality measures has been used to identify important actors, as a proxy indicator for academic achievement or to identify students’ collaborative roles [23–26]. In temporal networks, centrality measures are fine-grained estimates of students” real-time centralities or importance, i.e., shows who were central and when considering the temporarily of their interaction. In doing so, we can see exactly when and for how long, at what pace, and with which rhythm a behavior happens (compare this to traditional social network analysis where centralities are computed as a single number). There are several possible uses of temporal centrality measures. For instance [15] have used them to create predictive models of students’ performance. In another study by the same authors, they demonstrated that temporal centrality measures were more predictive of performance compared to traditional static centrality measures [9]. Since the temporal centralities are computed as time series, their temporal characteristics can also be used to compute other time series properties e.g., stability, variabilities, and pace, you can see for example [15]. There is a growing list of temporal centrality measures e.g., [13]. We will study here the most commonly used ones according to the latest review [25], but readers are encouraged to explore the tsna manual for more centrality measures.\nTemporal degree centrality measures can be computed in the same way as we computed the graph level properties shown before. The next code defines the function snafun = \"degree\", the start, the end date, and aggregation (which you can modify). An important argument here is the cmode argument which defines the type of centrality: “freeman”, “indegree” or “outdegree” for the calculation of total in or out degree centralities. The result is a time series with the 73 values for each day from start to end, each day having a unique value for the degree centrality for each node. The rest of the code is intended to organize the results. We convert the time series to a dataframe, create a variable for the day number to make it easier to identify the day and create a variable to define the type of centrality, and then combine all centrality measures into a single data frame as below.\n\nDegree_Centrality <- tSnaStats(\n  Active_Network,\n  snafun = \"degree\",\n  start = 1,\n  end = 73,\n  time.interval = 1,\n  aggregate.dur = 1,\n  cmode = \"freeman\")\n\ninDegree_Centrality <- tSnaStats(\n  Active_Network,\n  snafun = \"degree\",\n  start = 1,\n  end = 73,\n  time.interval = 1,\n  aggregate.dur = 1,\n  cmode = \"indegree\")\n\nOutDegree_Centrality <- tSnaStats(\n  Active_Network,\n  snafun = \"degree\",\n  start = 1,\n  end = 73,\n  time.interval = 1,\n  aggregate.dur = 1,\n  cmode = \"outdegree\")\n\nDegree_Centrality_DF <- Degree_Centrality |> as.data.frame() |> \n  mutate(Day = 1:73, centrality = \"Degree_Centrality\", .before = 1L)\n\n\ninDegree_Centrality_DF <- inDegree_Centrality |> as.data.frame() |> \n  mutate(Day = 1:73,centrality = \"inDegree_Centrality\", .before = 1L)\n\n\nOutDegree_Centrality_DF <- OutDegree_Centrality |> as.data.frame() |> \n  mutate(Day = 1:73,centrality = \"OutDegree_Centrality\", .before = 1L)\n\nrbind(Degree_Centrality_DF, inDegree_Centrality_DF, OutDegree_Centrality_DF) \n\n\n\n  \n\n\n\nIn the same way, closeness, betweenness and eigenvector centralities can be computed. Please note that we have a new argument here gmode=\"graph\" which tells snafun that we would like to compute these centralities considering the network as undirected. You can also use the gmode=\"digraph\" to compute the measures on a directed basis. You may need to use the previous code to convert each of the resulting time series into a data frame and add the day of the centralities. The snafun can compute other centrality measures in the same way e.g., information centrality, Bonacich Power Centrality, Harary Graph Centrality, Bonacich Power Centrality among others.\n\nCloseness_Centrality <- tSnaStats(\n  Active_Network,\n  snafun = \"closeness\",\n  start = 1,\n  end = 73,\n  time.interval = 1,\n  aggregate.dur = 1,\n  gmode = \"graph\")\n\nBetweenness_Centrality <- tSnaStats(\n  Active_Network,\n  snafun = \"betweenness\",\n  start = 1,\n  end = 73,\n  time.interval = 1,\n  aggregate.dur = 1,\n  gmode = \"graph\")\n\nEigen_Centrality <- tSnaStats(\n  Active_Network,\n  snafun = \"evcent\",\n  start = 1,\n  end = 73,\n  time.interval = 1,\n  aggregate.dur = 1,\n  gmode = \"graph\")\n\nReachability is another important measure that temporal networks offer. You can calculate who, when, and after how many steps an interaction reaches from a certain vertex to another or to every other node in the network (if it at all does). In the next code, we compute the forward pathway (earliest reachable node) from node 44 (one of the course facilitators) using the function tPath. The output is a time series with all the time distance values and the number of steps.\n\nFwdPathway <- tPath(\n  Active_Network,\n  v = 44,\n  start = 0,\n  graph.step.time = 7,\n  end = 30,\n  direction = \"fwd\")\n\nFwdPathway \n\n$tdist\n [1]  7.09 32.92 34.76 14.09 16.07 30.92 28.09   Inf  7.28 24.25 23.32 28.07\n[13] 23.92 14.09 27.11 24.17 17.35 33.20 28.29 26.08 22.14 30.07 21.09 25.34\n[25] 24.25 28.09   Inf   Inf 28.89 25.74 16.32   Inf 23.07 32.68 20.42 17.35\n[37] 24.26 14.09 14.28 15.32  9.25  7.28   Inf  0.00 21.09\n\n$previous\n [1] 44 45 31  1 44 13 23  0 44  9 31 44 44  1 14 14  1 14 14  1 44 33 14 44  9\n[26] 45  0  0 31 14  1  0  5 25 44  9 44  1  9  9 44 44  0  0  4\n\n$gsteps\n [1]   1   4   3   2   1   2   4 Inf   1   2   3   1   1   2   3   3   2   3   3\n[20]   2   1   3   3   1   2   4 Inf Inf   3   3   2 Inf   2   3   1   2   1   2\n[39]   2   2   1   1 Inf   0   3\n\n$start\n[1] 0\n\n$end\n[1] 30\n\n$direction\n[1] \"fwd\"\n\n$type\n[1] \"earliest.arrive\"\n\nattr(,\"class\")\n[1] \"tPath\" \"list\" \n\n\nMore importantly, we can plot the hierarchical path from the given node, by simply using plot function, see Figure 17.13.\n\nplot(FwdPathway)\n\n\n\n\nFigure 13. A forward pathway (earliest reachable node) pathway of interactions with node 44\n\n\n\n\nWe may also use Graphviz to make the plot look better and hierarchical (Figure 17.14).\n\nplot(FwdPathway, edge.lwd = 0.1, vertex.col= \"blue\", pad = -4,\n     coord=network.layout.animate.Graphviz(as.network(FwdPathway), \n                   layout.par = list(gv.engine='dot',\n                              gv.args = '-Granksep=2')))\n\n\n\n\nFigure 14. An improved forward pathway with Graphviz\n\n\n\n\nAnother option is to plot the network diffusion or transmission hierarchical tree with generation time vs. clock/model time using transmissionTimeline function as in Figure 17.15. For more on these functions, readers are invited to read the function manuals and for usage, these papers offer a starting point [3, 9].\n\ntransmissionTimeline(FwdPathway, jitter = F, displaylabels = TRUE, \n                    main = \"Earliest forward path\" )\n\n\n\n\nFigure 15. Transmission hierarchical tree showing the earliest forward interaction with node 44\n\n\n\n\nAnother useful package that offers a wealth of graph level measures is tErgmStats, you may need to consult the help files which are available by using the command ?tErgmStats. One of the important functions that we can try here is the nodemix. We can use nodemix to examine who and when different actors interact with each other, see here for an example where the authors examined how low and high achievers mix with each other and with the teachers [15]. The next code demonstrates how to compute the mixing patterns between expertise levels. We then convert the time series as a dataframe, clean the names and then plot the results as demonstrated in Figure 17.16.\n\nMix_experience <- tErgmStats(Active_Network,\n  \"nodemix('expert_level')\",\n  start = 1,\n  end = 73,\n  time.interval = 1,\n  aggregate.dur = 1)\n\nMixing <- as.data.frame(Mix_experience)\n\ncolnames(Mixing) <- gsub(\"mix.expert_level.\", \"\", colnames(Mixing))\n\nMixing$Day <- 1:73\n\nMixing_long= pivot_longer(Mixing, contains(\".\"))\n\nnames(Mixing_long)= c(\"Day\", \"Mixing\", \"Frequency\")\n\nggplot(Mixing_long, aes(Day, Frequency, group = Mixing, color = Mixing)) + \n  geom_line(alpha = .95) + theme_bw()\n\n\n\n\nFigure 16. Mixing patterns between expertise levels and the teachers."
  },
  {
    "objectID": "chapters/ch17-temporal-networks/ch17-tna.html#discussion",
    "href": "chapters/ch17-temporal-networks/ch17-tna.html#discussion",
    "title": "17  Temporal network analysis: Introduction, methods and analysis with R",
    "section": "5 Discussion",
    "text": "5 Discussion\nLearning can be viewed as relational, interdependent, and temporal and therefore, methods that account for such multifaceted dynamic processes are required [1, 3]. We have shown the main advantages of temporal networks and the potentials it offers for modeling dynamic learning processes. These potentials or features can facilitate the modeling of the complex natural processes–including the emergence, evolution, diffusion or disappearance of learners’ activities, communities or social processes that unfold over time. Such features can augment the existing analytics method and help shed light on many learning phenomena [16]. Taking advantage of time dynamics allows us temporal evolution of co-construction of knowledge, the flow of information and the building of social relationships, to name a few examples. What is more, temporal networks allow the longitudinal modeling and analysis of interactions across longer periods of time e.g., full duration of a course, project or meeting using time-respecting paths [9, 15].\nThere are several methods that can harness the temporal dimensions of a learning process, e.g., process and sequence mining, time series methods and epistemic network analysis [27, 28]. While such methods have given a wealth of information and insights about learning processes, they fall short when it comes to the relational aspects [9]. We review here and in short the main differences between such methods of temporal networks. Process mining is a method for the discovery and modeling of a temporal process [1, 29].Yet, the relational aspect is completely ignored. The case is similar for sequence mining where the time-ordered sequences are modeled regardless of the theri interactions [27, 30, 31]. Epistemic network analysis is another method that allows the study of co-temporal interactions. However, the “temporal aspect” is limited to combining data within a temporal window and later modeling the interactions as a static network. Put another way, Epistemic network analysis is a special type of static networks where edges are defined based on co-occurrence [32]. For a comparison between the various methods see [3]."
  },
  {
    "objectID": "chapters/ch17-temporal-networks/ch17-tna.html#learning-resources",
    "href": "chapters/ch17-temporal-networks/ch17-tna.html#learning-resources",
    "title": "17  Temporal network analysis: Introduction, methods and analysis with R",
    "section": "6 Learning resources",
    "text": "6 Learning resources\nA good place to start is to get acquainted with the cited research in the literature review section. Other good references could be the methodological paper that gives a detailed overview of temporal networks which some parts of this chapter has been built around it [33]. There are few, yet very informative tutorials that we can suggest, most notable are the tutorials by [34] and [18]. The packages used in this chapter have very informative manuals: TSNA[19], NDTV[20] and networkDynamic[35]. Some seminal papers can be recommended here, especially the following papers and books.\n\nHolme, P. (2015). Modern temporal network theory: a colloquium. European Physical Journal B, 88(9).\nHolme, P., & Saramäki, J. (2019). A Map of Approaches to Temporal Networks (pp. 1–24).\nHolme, P., & Saramäki, J. (Eds.). (2019). Temporal network theory (Vol. 2). New York: Springer."
  },
  {
    "objectID": "chapters/ch18-ena-ona/ch18-ena.html#introduction",
    "href": "chapters/ch18-ena-ona/ch18-ena.html#introduction",
    "title": "18  Epistemic Network Analysis and Ordered Network Analysis in Learning Analytics",
    "section": "1 Introduction",
    "text": "1 Introduction\nThis chapter provides a tutorial on conducting epistemic network analysis (ENA) and ordered network analysis (ONA) using R. We introduce these two techniques together because they share similar theoretical foundations, but each addresses a different challenge for analyzing large-scale qualitative data on learning processes.\nENA and ONA are methods for quantifying, visualizing, and interpreting network data. Taking coded data as input, ENA and ONA represent associations between codes in undirected or directed weighted network models, respectively. Both techniques measure the strength of association among codes and illustrate the structure of connections in network graphs, and they quantify changes in the composition and strength of those connections over time. Importantly, ENA and ONA enable comparison of networks both visually and via summary statistics, so they can be used to explore a wide range of research questions in contexts where patterns of association in coded data are hypothesized to be meaningful and where comparing those patterns across individuals or groups is important.\nIn the following sections, we will (1) briefly review literature relevant to the application of ENA and ONA, (2) provide a step-by-step guide to implementing ENA and ONA in R, and (3) suggest additional resources and examples for further exploration. By the end of this chapter, readers will be able to apply these techniques in their own research."
  },
  {
    "objectID": "chapters/ch18-ena-ona/ch18-ena.html#literature-review",
    "href": "chapters/ch18-ena-ona/ch18-ena.html#literature-review",
    "title": "18  Epistemic Network Analysis and Ordered Network Analysis in Learning Analytics",
    "section": "2 Literature review",
    "text": "2 Literature review\n\n2.1 Epistemic network analysis (ENA)\nENA is a method for identifying and quantifying connections in coded data and representing them in undirected weighted network models (Shaffer et al., 2016). There are two key features that differentiate ENA from other networks analysis tools or multivariate analyses: (1) ENA produces summary statistics that can be used to compare the differences in the content of networks rather than just their structure; and (2) ENA network visualizations provide information that is mathematically consistent with those summary statistics, which facilitates meaningful interpretation of statistical differences (Bowman et al., 2021). These features enable researchers to analyze a wide range of phenomena in learning analytics, including complex thinking and knowledge construction (Csanadi et al, 2018; Oshima et al., 2018), collaborative problem solving (Bressler et al., 2019; Swiecki et al., 2020), socio-emotional aspects of learning (Prieto et al., 2021), mentoring (Zhang et al., 2022), and teacher professional development (Bauer et al., 2019; Fernandez-Nieto et al., 2021; Phillips et al., 2023)\nOne key feature that makes ENA an effective method in modeling collaborative interaction is that ENA can model individuals’ unique contributions to collaborative discourse while accounting for group context, and thus both individuals and groups can be analyzed in the same model. This feature is particularly valuable in collaborative learning environments, where the interactions and contributions of each individual are related and should not be treated as a series of isolated events. For example, Swiecki et al., (2020) analyzed the communications of air defense warfare teams in training exercises and found that ENA was not only able to reveal differences in individual performance identified in a qualitative analysis of the collaborative discourse, but also to test those differences statistically.\n\n\n2.2 Ordered network analysis (ONA)\nOrdered Network Analysis (ONA) extends the theoretical and analytical advantages of ENA to account for the order of events by producing directed weighted networks rather than undirected models (Tan et al., 2022). Like ENA, ONA takes coded data as input, identifies and measures connections among coded items, and visualizes the structure of connections in a metric space that enables both statistical and visual comparison of networks. However, ONA models the order in which codes appear in the data, enabling analysis of phenomena in which the order of events is hypothesized to be important.\nFor example, Tan et al. (2022) used ONA to model the performance of military teams learning to identify, assess, and respond to potential threats detected by radar. The findings demonstrate that ONA could detect qualitative differences between teams in different training conditions that were not detected with unordered models and show that they are statistically significant. In their work, Tan et al. (2022) argued that ONA possesses an advantage over methods such as Sequential Pattern Mining (SPM), which is widely used to identify frequent sequential patterns. In contrast to SPM, which prioritizes the specific micro-sequential order of events, ONA models processes by accounting for the co-temporal order of interactions between the units of analysis in response and what they are responding to. Consequently, ONA is a more appropriate methodological choice when modeling processes in ill-formed problem-solving scenarios, where collaborative interactions do not follow a prescribed sequence of steps but where the order of activities is still important.\nONA has also been used to analyze log data from online courses. For example, Fan et al. (2021) analyzed self-regulated learning tactics employed by learners in Massive Open Online Courses (MOOC) using ONA and process mining. The authors found that ONA provided more nuanced interpretations of learning tactics compared to process mining because ONA models learning tactics across four dimensions: frequency, continuity, order, and the role of specific learning actions within broader tactics.\nLike ENA, ONA produces summary statistics for network comparison and mathematically consistent network visualizations that enable interpretation of statistical measures. Unlike ENA, ONA models the order in which codes appear in data, enabling researchers to investigate whether and to what extent the order of events is meaningful in a given context.\nIn the following sections, we provide a step-by-step guide to conducting ENA and ONA analyses in R."
  },
  {
    "objectID": "chapters/ch18-ena-ona/ch18-ena.html#epistemic-network-analysis-in-r",
    "href": "chapters/ch18-ena-ona/ch18-ena.html#epistemic-network-analysis-in-r",
    "title": "18  Epistemic Network Analysis and Ordered Network Analysis in Learning Analytics",
    "section": "3 Epistemic network analysis in R",
    "text": "3 Epistemic network analysis in R\nIn this section, we demonstrate how to conduct an ENA analysis using the rENA package. If you are not familiar with ENA as an analytic technique, we recommend that you first read Shaffer & Ruis (2017) and Bowman et al. (2022) to familiarize yourself with the theoretical and methodological foundations of ENA.\n\n3.1 Install the rENApackage and load the library\nBefore installing the rENA package, be sure that you are using R version 4.1 or newer. To check your R version, type R.version in your console. To update your R version (if needed), download and install R from the official R website: https://cran.r-project.org/\nFirst, install the rENA package and then load the rENA library after installation is complete.\n\n# install.packages(\"rENA\", repos = c(\"https://cran.qe-libs.org\", \"https://cran.rstudio.org\"))\n\n\nlibrary(rENA)\n\nWe also install the other package that is required for accessing the view() function (section 3.7.3) in rENA.\n\n# install.packages(\"tma\", repos = c(\"https://cran.qe-libs.org\", \"https://cran.rstudio.org\"))\n\n\nlibrary(tma)\n\n\n\n3.2 Dataset\nThe dataset we will use as an example, RS.data, is included in the rENA package. Note that the RS.data file in the package is only a subset of the full dataset, and is thus intended for demonstration purposes only.\nTo start, pass RS.data from the rENApackage to a data frame named data.\n\ndata = rENA::RS.data\n\nYou can preview the input data frame to familiarize yourself with the data structure.\n\ndata\n\n\n\n  \n\n\n\nRS.data consists of discourse from RescuShell, an online learning simulation where students work as interns at a fictitious company to solve a realistic engineering design problem in a simulated work environment. Throughout the internship, students communicate with their project teams and mentors via online chat, and these chats are recorded in the “text” column. A set of qualitative codes were applied to the data in the “text” column, where a value of 0 indicates the absence of the code and a value of 1 indicates the presence of the code in a given line.\nFurther details about the RS.data dataset can be found in Shaffer & Arastoopour (2014). Analyses of data from RescuShell and other engineering virtual internships can be found in Arastoopour et al. (2016) and Chesler et al. (2015).\n\n\n3.3 Construct an ENA model\nTo construct an ENA model, there is a function called ena which enables researchers to set the parameters for their model. This function wraps two other functions—ena.accumulate.data and ena.make.set—which can be used together to achieve the same result.\nIn the following sections, we will demonstrate how to set each parameter and explain how different choices affect the resulting ENA model.\n\n3.3.1 Specify units\nIn ENA, units can be individuals, ideas, organizations, or any other entity whose structure of connections you want to model. To set the units parameter, specify which column(s) in the data contain the variables that identify unique units.\nFor this example, choose the “Condition” column and the “UserName” column to define the units. The “Condition” column has two unique values: FirstGame, and SecondGame, representing novice users and relative expert users, respectively, as some students participated in RescuShell after having already completed a different engineering virtual internship. The “UserName” column includes unique user names for all students (n=48). This way of defining the units means that ENA will construct a network for each student in each condition.\n\nunitCols = c(\"Condition\", \"UserName\")\n\nTo verify that the units are correctly specified, subset and preview the unique values in the units columns. There are 48 units from two conditions, which means that the ENA model will produce 48 individual-level networks for each of the units, and each unit is uniquely associated with either the novice group (FirstGame) or the relative expert group (SecondGame).\n\nunique(data[, unitCols])\n\n\n\n  \n\n\n\n\n\n3.3.2 Specify codes\nNext, specify the columns that contain the codes. Codes are concepts whose pattern of association you want to model for each unit. ENA represent codes as nodes in the networks and co-occurrences of codes as edges. Most researchers use binary coding in ENA analyses, where the values in the code columns are either 0 (indicating that the code is not present in that line) or 1 (indicating that the code is present in that line). RS.data contains six code columns, all of which will be used here.\nTo specify the code columns, enter the code column names in a vector.\n\ncodeCols = c('Data', 'Technical.Constraints', 'Performance.Parameters', 'Client.and.Consultant.Requests', 'Design.Reasoning', 'Collaboration')\n\nTo verify that the codes are correctly specified, preview the code columns selected.\n\ndata[,codeCols]\n\n\n\n  \n\n\n\n\n\n3.3.3 Specify conversations\nThe conversation parameter determines which lines in the data can be connected. Codes in lines that are not in the same conversation cannot be connected. For example, you may want to model connections within different time segments, such as days, or different steps in a process, such as activities.\nIn our example, choose the “Condition”, “GroupName”, and “ActivityNumber” columns to define the conversations. These choices indicate that connections can only happen between students who were in the same condition (FirstGame or SecondGame) and on the same project team (group), and within the same activity. This definition of conversation reflects what actually happened in the simulation: in a given condition, students only interacted with those who were in the same group, and each activity occurred on a different day.\nTo specify the conversation parameter, enter the column names in a vector.\n\nconversationCols = c(\"Condition\", \"GroupName\", \"ActivityNumber\")\n\nTo verify that the conversations are correctly specified, subset and preview the unique values in the conversation columns.\n\nunique(data[, conversationCols])\n\n\n\n  \n\n\n\n\n\n3.3.4 Specify the window\nOnce the conversation parameter is specified, a window method needs to be specified. Whereas the conversation parameter specifies which lines can be related, the window parameter determines which lines within the same conversation are related. The most common window method used in ENA is called a moving stanza window, which is what will be used here.\nBriefly, a moving stanza window is a sliding window of fixed length that moves through a conversation to detect and accumulate code co-occurrences in recent temporal context. The lines within a designated stanza window are considered related to each other. For instance, if the moving stanza window is 7, then each line in the conversation is linked to the six preceding lines. See Siebert-Evenstone et al. (2017) and Ruis et al. (2019) for more detailed explanations of windows in ENA models.\nHere, set the window.size.back parameter equal to 7. User can specify a different moving stanza window size by passing a different numerical value to the window.size.back parameter.\n\nwindow.size.back = 7\n\nThe ENA package also enables use of an infinite stanza window, which assumes that lines in any part of a conversation are related. The infinite stanza window works the same way as a moving stanza window, but there is no limit on the number of previous lines that are included in the window besides the conversation itself. The infinite stanza window is less commonly used in ENA, but is specified as follows:\n\nwindow.size.back = \"INF\"\n\n\n\n3.3.5 Specify groups and rotation method\nWhen specifying the units, we chose a column that indicates two conditions: FirstGame (novice group) and SecondGame (relative expert group). To enable comparison of students in these two conditions, three additional parameters need to be specified: groupVar, groups, and mean.\n\ngroupVar = \"Condition\" # \"Condition\" is the column used as our grouping variable \ngroups = c(\"FirstGame\", \"SecondGame\") # \"FirstGame\" and \"SecondGame\" are the two unique values of the \"Condition\" column\nmean = TRUE\n\nThese three parameters indicate that when building the ENA model, the first dimension will maximize the difference between the two conditions: FirstGame and SecondGame. This difference maximization is achieved through mean = TRUE, which specifies that a means rotation will be performed at the dimensional reduction stage. If the means rotation is set to FALSE or there aren’t two distinct groups in your data, ENA will by default use singular value decomposition (SVD) to perform the dimensional reduction. Bowman et al. (2022) provide a mathematical explanation of the methods used in ENA to perform dimensional reductions.\n\n\n3.3.6 Specify metadata\nThe last parameter to be specified is metadata. Metadata columns are not required to construct an ENA model, but they provide information that can be used to subset units in the resulting model.\nSpecify the metadata columns shown below to include data on student outcomes related to reported self-confidence before and after participating in engineering virtual internships. We will use this data to demonstrate a simple linear regression analysis that can be done using ENA outputs as predictors.\n\nmetaCols = c(\"CONFIDENCE.Change\",\"CONFIDENCE.Pre\",\"CONFIDENCE.Post\",\"C.Change\") # optional\n\n\n\n3.3.7 Construct a model\nNow that all the essential parameters have been specified, the ENA model can be constructed.\nTo build an ENA model, we need two functions ena.accumulate.data and ena.make.set, and we recommend that you store the output in an object (in this case, set.ena).\n\naccum.ena = \n  ena.accumulate.data(\n    text_data = RS.data[, 'text'],\n    units = data[,unitCols],\n    conversation = data[,conversationCols],\n    metadata = data[,metaCols], # optional\n    codes = data[,codeCols],\n    window.size.back = 7\n)\n\nset.ena = \n  ena.make.set(\n    enadata = accum.ena, # the accumulation run above\n    rotation.by = ena.rotate.by.mean, # equivalent of mean=TRUE in the ena function\n    rotation.params = list(\n      accum.ena$meta.data$Condition==\"FirstGame\", # equivalent of groups in the ena function\n      accum.ena$meta.data$Condition==\"SecondGame\" # equivalent of groups in the ena function\n  )\n)\n\n\n\n\n3.4 Summary of key model outputs\nUsers can explore what is stored in the object set by typing set$ and select items from the drop down list. Here, we briefly describe the top-level items in set that are often of interest.\n\n3.4.1 Connection counts\nConnection counts are the frequencies of unique connections a unit made. For each unit, ENA creates a cumulative adjacency vector that contains the sums of all unique code co-occurrences for that unit across all stanza windows. Here, there are 48 units in the ENA model, so there are 48 adjacency vectors. Each term in an ENA adjacency vector represents a unique co-occurrence of codes. Thus with six codes, each vector has 15 terms (n choose two). This is because ENA models are undirected and do not model co-occurrences of the same code.\nTo access ENA adjacency vectors, use set.ena$connection.counts.\n\nset.ena$connection.counts\n\n\n\n  \n\n\n\n\n\n3.4.2 Line weights\nTo compare networks in terms of their relative patterns of association, researchers can spherically normalize the cumulative adjacency vectors by diving each one by its length. The resulting normalized vectors represent each unit’s relative frequencies of code co-occurrence. In other words, the sphere normalization controls for the fact that different units might have different amounts of interaction or different numbers of activities.\nNotice that in set.ena$connection.counts, the value for each unique code co-occurrence is an integer equal or greater than 0, because they represent the raw connection counts between each unique pair of codes. In set.``ena``$line.weights, those raw counts are normalized, and therefore the values are rational numbers between 0 and 1.\nTo access the normalized adjacency vectors, use set.ena$line.weights.\n\nset.ena$line.weights\n\n\n\n  \n\n\n\n\n\n3.4.3 ENA points\nAs the product of a dimensional reduction, for each unit, ENA produces an ENA point in a two-dimensional space. Since there are 48 units, ENA produces 48 ENA points.\nBy default, rENAvisualizes ENA points on an x-y coordinate plane defined by the first two dimensions of the dimensional reduction: for a means rotation, MR1 and SVD2, and for an SVD, SVD1 and SVD2.\nTo access these points, use set.ena$points .\n\nset.ena$points\n\n\n\n  \n\n\n\nENA points are thus summary statistics that researchers can use to conduct statistical tests, and they can also be used in subsequent analyses. For example, statistical differences between groups in the data can be tested using ENA dimension scores, and those scores can also be used in regression analyses to predict outcome variables, which we will demonstrate later.\n\n\n3.4.4 Rotation matrix\nThe rotation matrix used during the dimensional reduction can be accessed through set.ena$rotation. This is mostly useful when you want to construct an ENA metric space using one dataset and then project ENA points from different data into that space, as in section 5.1.\n\nset.ena$rotation.matrix\n\n\n\n  \n\n\n\n\n\n3.4.5 Metadata\nset$meta.data returns a data frame that includes all the columns of the ENA set except for the columns representing code co-occurrences.\n\nset.ena$meta.data\n\n\n\n  \n\n\n\n\n\n\n3.5 ENA visualization\nOnce an ENA set is constructed, it can be visualized, which facilitates interpretation of the model. Here, we will look at the two conditions, FirstGame (novices) and SecondGame (relative experts), by plotting their mean networks.\n\n3.5.1 Plot a mean network\nTo plot a network, use the ena.plot.network function. This function requires the network parameter (a character vector of line weights), and the line weights come from set$line.weights.\nFirst, subset line weights for each of the two groups.\n\n# Subset lineweights for `FirstGame`\nfirst.game.lineweights = as.matrix(set.ena$line.weights$Condition$FirstGame)\n\n# Subset lineweights for `SecondGame`\nsecond.game.lineweights = as.matrix(set.ena$line.weights$Condition$SecondGame)\n\nNext, calculate the mean networks for the two groups, and store the line weights as vectors.\n\nfirst.game.mean = as.vector(colMeans(first.game.lineweights))\nsecond.game.mean = as.vector(colMeans(second.game.lineweights))\n\nDuring plotting, use a pipe |> to send the output of one function into the first parameter of the subsequent function. To distinguish the two mean networks, set the color of the FirstGame mean network to red.\n\n\n\n\nena.plot(set.ena, title = \"FirstGame mean plot\")  |>\n  ena.plot.network(network = first.game.mean, colors = c(\"red\"))\n\n\n\n\n\nand the color of the SecondGame mean network to blue.\n\nena.plot(set.ena, title = \"SecondGame mean plot\")  |>\n  ena.plot.network(network = second.game.mean, colors = c(\"blue\"))\n\n\n\n\n\nAs you can see from the two network visualizations above, their node positions are exactly same. All ENA networks from the same model have the same node positions, which are determined by an optimization routine that attempts to place the nodes such that the centroid of each unit’s network and the location of the ENA point in the reduced space are co-located.\nBecause of the fixed node positions, ENA can construct a subtracted network, which enables the identification of the most salient differences between two networks. To do this, ENA subtracts the weight of each connection in one network from the corresponding weighted connection in another network, then visualizes the differences in connection strengths. Each edge is color-coded to indicate which of the two networks contains the stronger connection, and the thickness and saturation of the edges corresponds to the magnitude of the difference.\nTo plot a subtracted network, first calculate the subtracted network line weights by subtracting one group’s line weights from the other. (Because ENA computes the absolute values of the differences in edge weights, the order of the two networks in the subtraction doesn’t matter.)\n\nsubtracted.mean = first.game.mean - second.game.mean\n\nThen, use the ena.plot function to plot the subtracted network. If the differences are relatively small, a multiplier can be applied to rescale the line weights, improving legibility.\n\nena.plot(set.ena, title = \"Subtracted: `FirstGame` (red) - `SecondGame` (blue)\")  |>\n  ena.plot.network(network = subtracted.mean * 5, # Optional rescaling of the line weights\n                   colors = c(\"red\", \"blue\"))\n\n\n\n\n\nHere, the subtracted network shows that on average, students in the FirstGame condition (red) made more connections with Technical.Constraints and Collaboration than students in the SecondGame condition (blue), while students in the SecondGame condition made more connections with Design.Reasoning and Performance.Parameters than students in the FirstGame condition. This is because students with more experience of engineering design practices did not need to spend as much time and effort managing the collaborative process and learning about the basic technical elements of the problem space, and instead spent relatively more time focusing on more complex analysis and design reasoning tasks.\nNote that this subtracted network shows no connection between Technical.Constraints and Design.Reasoning, simply because the strength of this connection was similar in both conditions. Thus, subtraction networks should always be visualized along with with the two networks being subtracted.\n\n\n3.5.2 Plot a mean network and its points\nThe ENA point or points associated with a network or mean network can also be visualized.\nTo visualize the points associated with each of the mean networks plotted above, use set$points to subset the rows that are in each condition and plot each condition as a different color.\n\n# Subset rotated points for the first condition\nfirst.game.points = as.matrix(set.ena$points$Condition$FirstGame)\n\n# Subset rotated points for the second condition\nsecond.game.points = as.matrix(set.ena$points$Condition$SecondGame)\n\nThen, plot the FirstGame mean network the same as above using ena.plot.network, use |> to pipe in the FirstGame points that we want to include, and plot them using ena.plot.points.\nEach point in the space is the ENA point for a given unit. The red and blue squares on the x-axis are the means of the ENA points for each condition, along with the 95% confidence interval on each dimension (you might need to zoom in for better readability)\nSince we used a means rotation to construct the ENA model, the resulting space highlights the differences between FirstGame and SecondGame by constructing a rotation that places the means of each condition as close as possible to the x-axis of the space and maximizes the differences between them.\n\nena.plot(set.ena, title = \" points (dots), mean point (square), and confidence interval (box)\") |> \n          ena.plot.points(points = first.game.points, colors = c(\"red\")) |> \n          ena.plot.group(point = first.game.points, colors =c(\"red\"), \n                         confidence.interval = \"box\")\n\n\n\n\n\n\nena.plot(set.ena, title = \"FirstGame mean network and its points\") |> \n          ena.plot.network(network = first.game.mean, colors = c(\"red\")) |>\n          ena.plot.points(points = first.game.points, colors = c(\"red\")) |> \n          ena.plot.group(point = first.game.points, colors =c(\"red\"), \n                         confidence.interval = \"box\") \n\n\n\n\n\nThen, do the same for the SecondGame condition.\n\nena.plot(set.ena, title = \" points (dots), mean point (square), and confidence interval (box)\") |> \n          ena.plot.points(points = second.game.points, colors = c(\"blue\")) |> \n          ena.plot.group(point = second.game.points, colors =c(\"blue\"), \n                         confidence.interval = \"box\")\n\n\n\n\n\n\nena.plot(set.ena, title = \"SecondGame mean network and its points\") |> \n          ena.plot.network(network = second.game.mean, colors = c(\"blue\")) |>\n          ena.plot.points(points = second.game.points, colors = c(\"blue\")) |> \n          ena.plot.group(point = second.game.points, colors =c(\"blue\"), \n                         confidence.interval = \"box\")\n\n\n\n\n\nLastly, do the same for subtraction as well.\n\nena.plot(set.ena, title = \"Subtracted mean network: `FirstGame` (red) - `SecondGame` (blue)\")  |>\n          ena.plot.network(network = subtracted.mean * 5,\n          colors = c(\"red\", \"blue\")) |>\n          ena.plot.points(points = first.game.points, colors = c(\"red\")) |> \n          ena.plot.group(point = first.game.points, colors =c(\"red\"), \n                         confidence.interval = \"box\") |>\n          ena.plot.points(points = second.game.points, colors = c(\"blue\")) |> \n          ena.plot.group(point = second.game.points, colors =c(\"blue\"), \n                         confidence.interval = \"box\")\n\n\n\n\n\nNote that the majority of the red points (FirstGame) are located on the left side of the space, and the blue points (SecondGame) are mostly located on the right side of the space. This is consistent with the line weights distribution in the mean network: the FirstGame units make relatively more connections with nodes on the left side of the space, while the SecondGame units make relatively more connections with nodes on the right side of the space. The positions of the nodes enable interpretation of the dimensions, and thus interpretation of the locations of the ENA points.\n\n\n3.5.3 Plot an individual unit network and its point\nPlotting the network and ENA point for a single unit uses the same approach. First, subset the line weights and point for a given unit.\n\nunit.A.line.weights = as.matrix(set.ena$line.weights$ENA_UNIT$`FirstGame.steven z`) # subset line weights\nunit.A.point = as.matrix(set.ena$points$ENA_UNIT$`FirstGame.steven z`) # subset ENA point\n\nThen, plot the network and point for that unit.\n\nena.plot(set.ena, title = \"Individual network: `FirstGame`.steven z\") |> \n          ena.plot.network(network = unit.A.line.weights, colors = c(\"red\")) |>\n          ena.plot.points(points = unit.A.point, colors = c(\"red\"))\n\n\n\n\n\nFollowing the exact same procedure, we can, for example, choose a unit from the other condition to plot and also construct a subtracted plot for those two units.\n\nunit.B.line.weights = as.matrix(set.ena$line.weights$ENA_UNIT$`SecondGame.samuel o`) # subset line weights\nunit.B.point = as.matrix(set.ena$points$ENA_UNIT$`SecondGame.samuel o`) # subset ENA point\n\n\nena.plot(set.ena, title = \"Individual network: `SecondGame`.samuel o\") |> \n          ena.plot.network(network = unit.B.line.weights, colors = c(\"blue\")) |>\n          ena.plot.points(points = unit.B.point, colors = c(\"blue\"))\n\n\n\n\n\nTo visually analyze the differences between the two individual networks, plot their subtracted network.\n\nena.plot(set.ena, title = \"Subtracted network: `FirstGame`.steven z (red) - `SecondGame`.samuel o (blue)\")  |>\n          ena.plot.network(network = (unit.A.line.weights - unit.B.line.weights) * 5,\n          colors = c(\"red\", \"blue\")) |>\n          ena.plot.points(points = unit.A.point, colors = c(\"red\")) |> \n          ena.plot.points(points = unit.B.point, colors = c(\"blue\"))\n\n\n\n\n\nIn this unit-level subtracted network, Unit A (red) made relatively more connections with codes such as Technical.Constraints, Data, and Collaboration, while Unit B (blue) made relatively more connections with Design.Reasoning and Performance.Parameters.\n\n\n3.5.4 Plot everything, everywhere, all at once\nThe helper function ena.plotter enables users to plot points, means, and networks for each condition at the same time. This gives the same results as above more parsimoniously. However, this approach does not enable customization of edge and point colors.\n#with helper function\np<-ena.plotter(set.ena,\n            points = T,\n            mean = T, \n            network = T,\n            print.plots = T,\n            groupVar = \"Condition\",\n            groups = c(\"SecondGame\",\"FirstGame\"),\n            subtractionMultiplier = 5)\n$SecondGame\n$FirstGame\n$SecondGame-FirstGame\n  class(p) <- c(\"plotly\", \"enaplot\", \"html-fill-item-overflow-hidden\", \"html-fill-item\", class(p))\n\n\n\n3.6 Compare groups statistically\nIn addition to visual comparison of networks, ENA points can be analyzed statistically. For example, here we might test whether the patterns of association in one condition are significantly different from those in the other condition.\nTo demonstrate both parametric and non-parametric approaches to this question, the examples below use a Student’s t test and a Mann-Whitney U test to test for differences between the FirstGame and SecondGame condition. For more on differences between parametric and non-parametric tests, see Kaur & Kumar (2015).\nFirst, install the lsr package to enable calculation of effect size (Cohen’s d) for the t test.\n\n# install.packages('lsr')\nlibrary(lsr)\n\nThen, subset the points to test for differences between the two conditions.\n\nena_first_points_d1 = as.matrix(set.ena$points$Condition$FirstGame)[,1]\nena_second_points_d1 = as.matrix(set.ena$points$Condition$SecondGame)[,1]\n\nena_first_points_d2 = as.matrix(set.ena$points$Condition$FirstGame)[,2]\nena_second_points_d2 = as.matrix(set.ena$points$Condition$SecondGame)[,2]\n\nConduct the t test on the first and second dimensions.\n\n# parametric tests\nt_test_d1 = t.test(ena_first_points_d1, ena_second_points_d1)\nt_test_d1\n\n\n    Welch Two Sample t-test\n\ndata:  ena_first_points_d1 and ena_second_points_d1\nt = -6.5183, df = 45.309, p-value = 5.144e-08\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.2687818 -0.1419056\nsample estimates:\n  mean of x   mean of y \n-0.09411588  0.11122786 \n\nt_test_d2 = t.test(ena_first_points_d2, ena_second_points_d2)\nt_test_d2\n\n\n    Welch Two Sample t-test\n\ndata:  ena_first_points_d2 and ena_second_points_d2\nt = 1.9334e-16, df = 43.175, p-value = 1\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.07768526  0.07768526\nsample estimates:\n    mean of x     mean of y \n 1.935145e-19 -7.254914e-18 \n\n\nCompute any other statistics that may be of interest. A few examples are given below.\n\nmean(ena_first_points_d1)\n\n[1] -0.09411588\n\nmean(ena_second_points_d1)\n\n[1] 0.1112279\n\nmean(ena_first_points_d2)\n\n[1] 1.935145e-19\n\nmean(ena_second_points_d2)\n\n[1] -7.254914e-18\n\nsd(ena_first_points_d1)\n\n[1] 0.1115173\n\nsd(ena_second_points_d1)\n\n[1] 0.1063515\n\nsd(ena_first_points_d2)\n\n[1] 0.1267104\n\nsd(ena_second_points_d2)\n\n[1] 0.1380851\n\nlength(ena_first_points_d1)\n\n[1] 26\n\nlength(ena_second_points_d1)\n\n[1] 22\n\nlength(ena_first_points_d2)\n\n[1] 26\n\nlength(ena_second_points_d2)\n\n[1] 22\n\ncohensD(ena_first_points_d1, ena_second_points_d1)\n\n[1] 1.880622\n\ncohensD(ena_first_points_d2, ena_second_points_d2)\n\n[1] 5.641688e-17\n\n\nHere, along the x axis (MR1), a two-sample t test assuming unequal variance shows that the FirstGame (mean=-0.09, SD=0.11, N=26) condition is statistically significantly different for alpha=0.05 from the SecondGame condition (mean=0.11, SD=0.10, N=22; t(45.31)=-6.52, p=0.00, Cohen’s d=1.88). Along the y axis (SVD2), a two-sample t test assuming unequal variance shows that the FirstGame condition (mean=0.11, SD=0.13, N=26) is not statistically significantly different for alpha=0.05 from the SecondGame condition (mean=0.00, SD=1.3, N=22; t(43.17)=0, p=1.00).\nThe Mann-Whitney U test is a non-parametric alternative to the independent two-sample t test.\nFirst, install the rcompanion package to calculate the effect size (r) for a Mann-Whitney U test.\n\n# install.packages('rcompanion')\nlibrary(rcompanion)\n\nThen, conduct a Mann-Whitney U test on the first and second dimensions.\n\n# non parametric tests\nw_test_d1 = wilcox.test(ena_first_points_d1, ena_second_points_d1)\nw_test_d2 = wilcox.test(ena_first_points_d2, ena_second_points_d2)\n\nw_test_d1\n\n\n    Wilcoxon rank sum exact test\n\ndata:  ena_first_points_d1 and ena_second_points_d1\nW = 50, p-value = 8.788e-08\nalternative hypothesis: true location shift is not equal to 0\n\nw_test_d2\n\n\n    Wilcoxon rank sum exact test\n\ndata:  ena_first_points_d2 and ena_second_points_d2\nW = 287, p-value = 0.9918\nalternative hypothesis: true location shift is not equal to 0\n\n\nCompute any other statistics that may be of interest. A few examples are given below.\n\nmedian(ena_first_points_d1)\n\n[1] -0.08464154\n\nmedian(ena_second_points_d1)\n\n[1] 0.1300029\n\nmedian(ena_first_points_d2)\n\n[1] -0.007252397\n\nmedian(ena_second_points_d2)\n\n[1] 0.0003031848\n\nlength(ena_first_points_d1)\n\n[1] 26\n\nlength(ena_second_points_d1)\n\n[1] 22\n\nlength(ena_first_points_d2)\n\n[1] 26\n\nlength(ena_second_points_d2)\n\n[1] 22\n\nabs(wilcoxonR(ena_first_points_d1, ena_second_points_d1))\n\n    r \n0.863 \n\nabs(wilcoxonR(ena_first_points_d2, ena_second_points_d2))\n\n    r \n0.863 \n\n\nHere, along the x axis (MR1), a Mann-Whitney U test shows that the FirstGame condition (Mdn=-0.08, N=26) was statistically significantly different for alpha=0.05 from the SecondGame condition (Mdn=-0.007, N=22; U=50, p=0.00, r=0.86). Along the y axis (SVD2), a Mann-Whitney U test shows that the FirstGame condition (Mdn=0.13, N=26) is not statistically significantly different for alpha=0.05 from the SecondGame condition (Mdn=0.00, N=22; U=287, p=0.99). The absolute value of r value in Mann-Whitney U test varies from 0 to close to 1. The interpretation values for r commonly in published literature is: 0.10 - < 0.3 (small effect), 0.30 - < 0.5 (moderate effect) and >= 0.5 (large effect).\n\n\n3.7 Model evaluation\nIn this section, we introduce three ways users can evaluate the quality of their ENA models.\n\n3.7.1 Variance explained\nBriefly, variance explained (also called explained variation) refers to the proportion of the total variance in a dataset that is accounted for by a statistical model or set of predictors.\nIn ENA, to represent high-dimensional vectors in a two-dimensional space, ENA uses either singular value decomposition or means rotation combined with SVD. For each of the reduced dimensions, the variance in patterns of association among units explained by that dimension can be computed.\n\nset.ena$model$variance\n\n        MR1        SVD2        SVD3        SVD4        SVD5        SVD6 \n0.320460221 0.244500582 0.152894192 0.093518444 0.060221209 0.034883009 \n       SVD7        SVD8        SVD9       SVD10       SVD11       SVD12 \n0.027680609 0.017549851 0.013516258 0.009812554 0.008089549 0.005764568 \n      SVD13       SVD14       SVD15 \n0.004938552 0.003474931 0.002695469 \n\n\nHere, the first dimension is MR1 and the second dimension is SVD2. The MR1 dimension has the highest variance explained at 32%.\nAs with any statstical model, greater explained variance does not necessarily indicate a better model, as it may be due to overfitting, but it provides one indicator of model quality.\n\n\n3.7.2 Goodness of fit\nBriefly, a model’s goodness of fit refers to how well a model fits or represents the data. A model with a high goodness of fit indicates that it accurately represents the data and can make reliable predictions.\nIn ENA, a good fit means that the positions of the nodes in the space—and thus the network visualizations—are consistent with the mathematical properties of the model. In other words, we can confidently rely on the network visualizations to interpret the ENA model. The process that ENA uses to achieve high goodness of fit is called co-registration. The mathematical details of co-registration are beyond the scope of this chapter and can be found in Bowman et al., (2022).\nTo test a model’s goodness of fit, use ena.correlations. The closer the value is to 1, the higher the model’s goodness of fit is. Most ENA models have a goodness of fit that is well above 0.90.\n\nena.correlations(set.ena)\n\n\n\n  \n\n\n\n\n\n3.7.3 Close the interpretative loop\nAnother approach to evaluate an ENA model is to confirm the alignment between quantitative model (in our case, our ENA model) and the original qualitative data. In other words, we can return to the original data to confirm that quantitative findings give a fair representation of the data. This approach is an example of what’s called as closing the interpretative loop in Quantitative Ethnography field (Shaffer, 2017).\nFor example, based on our visual analysis of the network of SecondGame.samuel o in previous section, we are interested in what the lines are in the original data that contributed to the connection between Design.Reasoning and Performance.Parameters.\nLet’s first review what SecondGame.samuel o ENA network looks like.\n\nena.plot(set.ena, title = \"Individual network: `SecondGame`.samuel o\") |> \n          ena.plot.network(network = as.matrix(set.ena$line.weights$ENA_UNIT$`SecondGame.samuel o`), colors = c(\"blue\")) |>\n          ena.plot.points(points = as.matrix(set.ena$points$ENA_UNIT$`SecondGame.samuel o`), colors = c(\"blue\"))\n\n\n\n\n\nTo do so, we use view() function and specify required parameters as below.\nThis is going to activate a window shows up in your Viewer panel. If it is too small to read, you can click on the “Show in new window” button to view it in your browser for better readability. (Note: the html page produced by the view() function will show separately from the html file knitted from RMD file)\n\nview(accum.ena, \n     id_col = \"ENA_UNIT\", # do not need to change this\n     wh = c(\"SecondGame.samuel o\"), # the unit we are interested in\n     units.by = c(\"Condition\", \"UserName\"), # consistent with in 3.3.1 \n     conversation.by = c(\"Condition\", \"GroupName\", \"ActivityNumber\"), # consistent with in 4.3.3\n     codes = c(\"Performance.Parameters\", \"Design.Reasoning\"), # codes of choice\n     window = 7) # consistent with in 3.3.4\n\n\n\nIn the Viewer panel, hover over your cursor on any of the lines that are in bold, a size of 7 lines rectangle shows up, representing that in a moving stanza window of size 7, the referent line (the line in bold) and its preceding 6 lines. The 1 and 0 in Technical.Constraints column and Design.Reasoning column shows where the connections happened.\nFor example, line 2477 Samuel shared his Design.Reasoning about “mindful of (the) how one device scores relative to other ones”, to reference back to what Casey said in line 2476 about Performance.Parameters “not one source/censor can be the best in every area so we had to sacrifice certain attributes”, as well as what Jackson said in line 2475 about safety as one of the Performance.Parameters “when it came to the different attributes, i think that all were important in their own way but i think safety is one of the most important”.\nThis is a qualitative example of a connection made between Performance.Parameters and Design.Reasoning.\n\n\n\n3.8 Using ENA model outputs in other analyses\nIt is often useful to use the outputs of ENA models in subsequent analyses. The most commonly used outputs are the ENA points, i.e., set$points. For example, we can use a linear regression analysis to test whether ENA points on the first two dimensions are predictive of an outcome variable, in this case, change in confidence in engineering skills.\n\nregression_data = set.ena$points\nregression_data$CONFIDENCE.Change = as.numeric(regression_data$CONFIDENCE.Change)\n\nWarning: NAs introduced by coercion\n\ncondition_regression = lm(CONFIDENCE.Change ~ MR1 + SVD2 + Condition, \n                          data = regression_data, \n                          na.action = na.omit)\nsummary(condition_regression)\n\n\nCall:\nlm(formula = CONFIDENCE.Change ~ MR1 + SVD2 + Condition, data = regression_data, \n    na.action = na.omit)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.18092 -0.24324 -0.08171  0.30716  1.88404 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           1.1111     0.1490   7.457 2.82e-09 ***\nMR1                  -0.4540     0.8616  -0.527    0.601    \nSVD2                  0.3268     0.7154   0.457    0.650    \nConditionSecondGame  -0.3484     0.2566  -1.358    0.182    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6374 on 43 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.1228,    Adjusted R-squared:  0.0616 \nF-statistic: 2.007 on 3 and 43 DF,  p-value: 0.1273\n\n\nThe results of this regression analysis show that ENA points are not a significant predictor of the students’ pre-post change in confidence (MR1: t=-0.53, p=0.60; SVD2: t=0.46, p=0.65; Condition: t=-1.36, p=0.18). The overall model was also not significant (F(3, 43)=2.01, p=0.13) with an adjusted r-squared value of 0.06.\nRecall that the dataset we are using is a small subset of the full RS.data, and thus results that are significant for the whole dataset may not be for this sample."
  },
  {
    "objectID": "chapters/ch18-ena-ona/ch18-ena.html#ordered-network-analysis-with-r",
    "href": "chapters/ch18-ena-ona/ch18-ena.html#ordered-network-analysis-with-r",
    "title": "18  Epistemic Network Analysis and Ordered Network Analysis in Learning Analytics",
    "section": "4 Ordered Network Analysis with R",
    "text": "4 Ordered Network Analysis with R\nThis section demonstrates how to conduct an ONA analysis using the ona R package. If you are new to ONA as an analytic technique, Tan et al. (2022) provides a more detailed explication of its theoretical and methodological foundations.\nBecause ONA shares some conceptual and procedural similarities with ENA, you may also want to read the recommended papers from the ENA section (Shaffer et al., 2016; Shaffer & Ruis, 2017; Bowman et al., 2022).\n\n4.1 Install the ONA package and load the library\nInstall the ona package and load the ona library after installing.\n\n# install.packages(\"ona\", repos = c(\"https://cran.qe-libs.org\", \"https://cran.rstudio.org\"))\n\n\nlibrary(ona)\n\nThen, install the other package that is required for ONA analysis.\n\n# install.packages(\"tma\", repos = c(\"https://cran.qe-libs.org\", \"https://cran.rstudio.org\"))\n\n\nlibrary(tma)\n\n\n\n4.2 Dataset\n(Refer to section 3.2 for a detailed description of the dataset used here.)\nLoad the RS.data dataset.\n\ndata = ona::RS.data\n\n\n\n4.3 Construct an ONA model\nTo construct an ONA model, identify which columns in the data to use for the parameters required by the ONA modeling function. The parameters are defined identically in both ENA and ONA; see Section 3.3 for detailed explanations.\n\n4.3.1 Specify units\nSelect the units as in Section 3.3.1.\n\nmy_units <- c(\"Condition\", \"UserName\") \n\n\n\n4.3.2 Specify codes\nSelect the codes as in Section 3.3.2.\n\nmy_codes = c(\n          'Data',\n          'Technical.Constraints',\n          'Performance.Parameters',\n          'Client.and.Consultant.Requests',\n          'Design.Reasoning',\n          'Collaboration')\n\n\n\n4.3.3 Specify conversations\nThe parameter to specify conversations in rENAis called conversation; in ONA, the equivalent is called my_hoo_rules, where hoo is an abbreviation of horizon of observation.\nChoose the combination of Condition column, GroupName column, and ActivityNumber column to define the conversation parameter.\nThe syntax to specify conversations using my_hoo_rules in ONA is slightly different from the syntax to specify conversation in ENA, but the conceptual definition is the same.\n\nmy_hoo_rules <- conversation_rules(\n                  (Condition %in% UNIT$Condition & \n                   GroupName %in% UNIT$GroupName & \n                   ActivityNumber %in% UNIT$ActivityNumber))\n\n\n\n4.3.4 Specify the window\nSpecify a moving stanza window size by passing a numerical value to the window_size parameter.\n\nwindow_size = 7\n\nTo specify an infinite stanza window in ONA, set the size of the moving window equal to or larger than the number of lines in the longest conversation. For example, set window_size = 4000, which is greater than the total number of rows in our dataset (nrows=3,824).\n\n\n4.3.5 Specify metadata\nAs in ENA, metadata columns can be included if desired. Metadata columns are not required to construct an ONA model, but they provide information that can be used to subset units in the resulting model.\n\nmetaCols = c(\"CONFIDENCE.Change\",\"CONFIDENCE.Pre\",\"CONFIDENCE.Post\",\"C.Change\")\n\n\n\n4.3.6 Accumulate connections\nNow that all the parameters are specified, connections can be accumulated. For each unit, the ONA algorithm uses a moving stanza window to identify connections formed from a current line of data (e.g., a turn of talk), or response, to the preceding lines within the window (the ground).\nUnlike in ENA, where connections among codes are recorded in a symmetric adjacency matrix, ONA accounts for the order in which the connections occur by constructing an asymmetric adjacency matrix for each unit; that is, the number of connections from code A to code B may be different than the number of connections from B to A.\nTo accumulate connections, pass the parameters specified to the contexts and accumulate_contexts functions, and store the output in an object (in this case, accum.ona).\n\naccum.ona <-\n  contexts(data, \n           units_by = my_units, \n           hoo_rules = my_hoo_rules) |>\n  accumulate_contexts(codes = my_codes, \n                      decay.function = decay(simple_window, window_size = 7),\n                      meta.data = metaCols,\n                      return.ena.set = FALSE) # keep this as FALSE to get an ONA model, otherwise it will return an undirected model)\n\n\n\n4.3.7 Construct an ONA model\nAfter accumulation, call the model function to construct an ONA model. ONA currently implements singular value decomposition (SVD) and means rotation (MR) to perform dimensional reduction.\nTo create an ONA model using SVD, pass the accum.ona object to the model function.\n\nset.ona <- \n  model(accum.ona)\n\nWhen there are two discrete groups to compare, a means rotation can be used, as described in Section 3.3.5.\nA means rotation is specified using rotate.using =\"mean\" in the model function. Additionally, the model function expects rotation.params to be a list with two named elements, each containing a logical vector representing the rows of units to be included in each group.\nHere, construct the ONA model as shown below.\n\nset.ona <-\n  model(accum.ona,                             # the previously run accumulation above\n        rotate.using =\"mean\",                  # means rotation method\n        rotation.params =                      # two groups for means rotation in a list\n              list(FirstGame=accum.ona$meta.data$Condition==\"FirstGame\",\n                   `SecondGame`=accum.ona$meta.data$Condition==\"SecondGame\")   \n        )\n\n\n\n\n4.4 Summary of key model outputs\nInformation about an ONA model is now stored in the R object set.ona.\nAs in rENA, users can explore the data stored in the object by typing set.ona$ and select items from the drop down list. Here, we briefly explain the top-level items in set.ona$.\n\n4.4.1 Connection counts\nBecause ONA accounts for the order in which the connections occur by constructing an asymmetric adjacency matrix for each unit, connection counts from code A to code B and from B to A, as well as self-connections for each code (from A to A) are recorded. Thus, because six codes were included in the model, the cumulative adjacency vector for each unit contains 36 terms (n^2).\n\nset.ona$connection.counts\n\n\n\n  \n\n\n\n\n\n4.4.2 Line weights\nIn set.ona$connection.counts, the value for each unique co-occurrence of codes is an integer equal or greater than 0, because they represent the directional connection counts between each pair of codes. In set.ona$line.weights, the connection counts are sphere normalized, and so the values are between 0 and 1. See section 3.4.2 for more information about line weights.\n\nset.ona$line.weights\n\n\n\n  \n\n\n\n\n\n4.4.3 ONA points\nFor each unit, ONA produces an ONA point in a two-dimensional space formed by the first two dimensions of the dimensional reduction.\nHere, the MR1 column represents the x-axis coordinate for each unit, and the SVD2 column represents the y-axis coordinate for each unit.\n\nset.ona$points\n\n\n\n  \n\n\n\n\n\n4.4.4 Rotation matrix\nThe rotation matrix used during the dimensional reduction can be accessed through set.ona$rotation. This is mostly useful when you want to construct an ONA metric space using one dataset and then project ONA points from different data into that space, as in section 5.2.\n\nset.ona$rotation.matrix\n\n\n\n  \n\n\n\n\n\n4.4.5 Metadata\nset.ona$meta.data gives a data frame that includes all the columns except for the code connection columns.\n\nset.ona$meta.data\n\n\n\n  \n\n\n\n\n\n\n4.5 ONA visualization\nOnce an ONA model is constructed, ONA networks can be visualize. The plotting function in ONA is called plot, and it works similarly to the same function in ENA.\nBefore plotting, you can set up several global parameters to ensure consistency across plots. These parameters will be clearer in subsequent sections.\n\nnode_size_multiplier = 0.4 # scale up or down node sizes\nnode_position_multiplier = 1 # zoom in or out node positions\npoint_position_multiplier =1.5 # zoom in or out the point positions\nedge_arrow_saturation_multiplier = 1.5 # adjust the chevron color lighter or darker\nedge_size_multiplier = 1 # scale up or down edge sizes\n\n\n4.5.1 Plot a mean network\nMean ONA networks can be plotted for each of the conditions along with their subtracted network.\nFirst, plot the mean network for the FirstGame condition. Use a pipe |> to connect the edges function and the nodes function. Users are only required to specify the weights parameter, as the remaining parameters have default values unless specified otherwise.\n\nplot.ena.ordered.set(set.ona, title = \"FirstGame (red) mean network\") |>\n  edges(\n    weights =set.ona$line.weights$Condition$FirstGame,\n    edge_size_multiplier = edge_size_multiplier,\n    edge_arrow_saturation_multiplier = edge_arrow_saturation_multiplier,\n    node_position_multiplier = node_position_multiplier,\n    edge_color = c(\"red\")) |>\n  nodes(\n    node_size_multiplier = node_size_multiplier,\n    node_position_multiplier = node_position_multiplier,\n    self_connection_color = c(\"red\"))\n\n\n\n\n\nSince this is the first ONA network visualization in this chapter, we briefly explain how to read an ONA network.\nNode size: In ONA, the node size is proportional to the number of occurrences of that code as a response to other codes in the data, with larger nodes indicating more responses. For example, in this plot, students in the FirstGame condition responded most frequently with discourse about Technical.Constraints.\nSelf-connections: The color and saturation of the circle within each node is proportional to the number of self-connections for that code: that is, when a code is both what students responded to and what they responded with. Colored circles that are larger and more saturated reflect codes with more frequent self-connections.\nEdges: Note that unlike most directed network visualizations, which use arrows or spearheads to indicate direction, ONA uses a “broadcast” model, where the source of a connection (what students responded to) is placed at the apex side of the triangle and the destination of a connection (what students responded with) is placed at its base.\nChevrons on edges: The chevrons point in the direction of the connection. Between any pair of nodes, if there is a bidirectional connection, the chevron only appears on the side with the stronger connection. This helps viewers differentiate heavier edges in cases such as between Technical.Constraints and Data, where the connection strengths from both directions are similar. When the connection strengths are identical between two codes, the chevron will appear on both edges.\nNow, plot the mean network for SecondGame.\n\nplot.ena.ordered.set(set.ona, title = \"SecondGame (blue) mean network\") |>\n  edges(\n    weights = set.ona$line.weights$Condition$SecondGame,\n    edge_size_multiplier = edge_size_multiplier,\n    edge_arrow_saturation_multiplier = edge_arrow_saturation_multiplier,\n    node_position_multiplier = node_position_multiplier,\n    edge_color = c(\"blue\")) |>\n  nodes(\n    node_size_multiplier = node_size_multiplier,\n    node_position_multiplier = node_position_multiplier,\n    self_connection_color = c(\"blue\"))\n\n\n\n\n\nThen, plot the subtracted network to show the differences between the mean networks of the FirstGame and SecondGame conditions.\n\nplot.ena.ordered.set(set.ona, title = \"Subtracted mean network: `FirstGame` (red) vs `SecondGame` (blue)\") |>\n  edges(\n    weights = (colMeans(set.ona$line.weights$Condition$FirstGame) - colMeans(set.ona$line.weights$Condition$SecondGame))*4, # optional weights multiplier to adjust readability\n    edge_size_multiplier = edge_size_multiplier,\n    edge_arrow_saturation_multiplier = edge_arrow_saturation_multiplier,\n    node_position_multiplier = node_position_multiplier,\n    edge_color = c(\"red\",\"blue\")) |>\n  nodes(\n    node_size_multiplier = node_size_multiplier,\n    node_position_multiplier = node_position_multiplier,\n    self_connection_color = c(\"red\",\"blue\"))\n\n\n\n\n\n\n\n4.5.2 Plot a mean network and its points\nBesides plotting the mean network for each condition and the subtracted network, we can also plot the individual units within each condition.\nUse set.ona$points to subset the rows that are in each condition and plot the units in each condition as a different color.\nThe points are specified in the units function. The edges and nodes functions remain the same as above.\n\nplot.ena.ordered.set(set.ona, title = \"points (dots), mean point (square), and confidence interval\") |>\n  units(\n    points=set.ona$points$Condition$FirstGame, \n    points_color = c(\"red\"),\n    show_mean = TRUE, show_points = TRUE, with_ci = TRUE)\n\n\n\n\n\n\nplot.ena.ordered.set(set.ona, title = \"FirstGame (red) mean network\") |>\n  units(\n    points=set.ona$points$Condition$FirstGame, \n    points_color = c(\"red\"),\n    show_mean = TRUE, show_points = TRUE, with_ci = TRUE) |>\n  edges(\n    weights =set.ona$line.weights$Condition$FirstGame,\n    edge_size_multiplier = edge_size_multiplier,\n    edge_arrow_saturation_multiplier = edge_arrow_saturation_multiplier,\n    node_position_multiplier = node_position_multiplier,\n    edge_color = c(\"red\")) |>\n  nodes(\n    node_size_multiplier = node_size_multiplier,\n    node_position_multiplier = node_position_multiplier,\n    self_connection_color = c(\"red\"))\n\n\n\n\n\n\nplot.ena.ordered.set(set.ona, title = \"points (dots), mean point (square), and confidence interval\") |>\n  units(\n    points=set.ona$points$Condition$SecondGame, \n    points_color = c(\"blue\"),\n    show_mean = TRUE, show_points = TRUE, with_ci = TRUE)\n\n\n\n\n\n\nplot.ena.ordered.set(set.ona, title = \"SecondGame (blue) mean network\") |>\n  units(\n    points=set.ona$points$Condition$SecondGame, \n    points_color = \"blue\", \n    show_mean = TRUE, show_points = TRUE, with_ci = TRUE) |>\n  edges(\n    weights = set.ona$line.weights$Condition$SecondGame,\n    edge_size_multiplier = edge_size_multiplier,\n    edge_arrow_saturation_multiplier = edge_arrow_saturation_multiplier,\n    node_position_multiplier = node_position_multiplier,\n    edge_color = c(\"blue\")) |>\n  nodes(\n    node_size_multiplier = node_size_multiplier,\n    node_position_multiplier = node_position_multiplier,\n    self_connection_color = c(\"blue\"))\n\n\n\n\n\nPlot the subtracted network as follows.\n\n# `FirstGame` and `SecondGame` subtracted plot\nplot.ena.ordered.set(set.ona, title = \"Difference: `FirstGame` (red) vs `SecondGame` (blue)\") |>\n  units(\n    points = set.ona$points$Condition$FirstGame, \n    points_color = \"red\",\n    show_mean = TRUE, show_points = TRUE, with_ci = TRUE) |>\n  units(\n    points = set.ona$points$Condition$SecondGame, \n    points_color = \"blue\",\n    show_mean = TRUE, show_points = TRUE, with_ci = TRUE) |>\n  edges(\n    weights = (colMeans(set.ona$line.weights$Condition$FirstGame) - colMeans(set.ona$line.weights$Condition$SecondGame))*4, # optional multiplier to adjust for readability\n    edge_size_multiplier = edge_size_multiplier,\n    edge_arrow_saturation_multiplier = edge_arrow_saturation_multiplier,\n    node_position_multiplier = node_position_multiplier,\n    edge_color = c(\"red\",\"blue\")) |>\n  nodes(\n    node_size_multiplier = node_size_multiplier,\n    node_position_multiplier = node_position_multiplier,\n    self_connection_color = c(\"red\",\"blue\"))\n\n\n\n\n\n\n\n4.5.3 Plot an individual network and its points\nTo plot an individual student’s network and ONA point, use set.ona$points.\nHere, we choose the same two units we compared in the ENA analysis (Section 3.5.3).\n\n# first game\nplot.ena.ordered.set(set.ona, title = \"FirstGame::steven z\") |>\n  units(\n    points=set.ona$points$ENA_UNIT$`FirstGame::steven z`, \n    points_color = \"red\", \n    show_mean = FALSE, show_points = TRUE, with_ci = FALSE) |>\n  edges(\n    weights = set.ona$line.weights$ENA_UNIT$`FirstGame::steven z`,\n    edge_size_multiplier = edge_size_multiplier,\n    edge_arrow_saturation_multiplier = edge_arrow_saturation_multiplier,\n    node_position_multiplier = node_position_multiplier,\n    edge_color = c(\"red\")) |>\n  nodes(\n    node_size_multiplier = node_size_multiplier,\n    node_position_multiplier = node_position_multiplier,\n    self_connection_color = c(\"red\"))\n\n\n\n\n\n\n# second game\nplot.ena.ordered.set(set.ona, title = \"SecondGame::samuel o\") |>\n  units(\n    points=set.ona$points$ENA_UNIT$`SecondGame::samuel o`, \n    points_color = \"blue\", \n    show_mean = FALSE, show_points = TRUE, with_ci = FALSE) |>\n  edges(\n    weights = set.ona$line.weights$ENA_UNIT$`SecondGame::samuel o`,\n    edge_size_multiplier = edge_size_multiplier,\n    edge_arrow_saturation_multiplier = edge_arrow_saturation_multiplier,\n    node_position_multiplier = node_position_multiplier,\n    edge_color = c(\"blue\")) |>\n  nodes(\n    node_size_multiplier = node_size_multiplier,\n    node_position_multiplier = node_position_multiplier,\n    self_connection_color = c(\"blue\"))\n\n\n\n\n\nIn this case, both units make relatively strong connections between Design.Reasoning and Data. However, for Unit A (red), the connection is relatively more from Design.Reasoning to Data than the other way around. This indicates that more often this unit responded with Data. In contrast, Unit B (blue) responded more frequently to Data with Design.Reasoning.\nA subtracted network can make such differences more salient.\n\n# units difference\nmean1 = as.vector(as.matrix(set.ona$line.weights$ENA_UNIT$`FirstGame::steven z`))\nmean2 = as.vector(as.matrix(set.ona$line.weights$ENA_UNIT$`SecondGame::samuel o`))\n\nsubtracted.mean = mean1 - mean2\n\n\nplot.ena.ordered.set(set.ona, title = \"subtracted network of steven z (red) and Samuel (blue)\") |> \n  units(\n    points = set.ona$points$ENA_UNIT$`FirstGame::steven z`, points_color = \"red\",\n    point_position_multiplier = point_position_multiplier,\n    show_mean = FALSE, show_points = TRUE, with_ci = FALSE) |>\n  units(\n    points = set.ona$points$ENA_UNIT$`SecondGame::samuel o`, points_color = \"blue\",\n    point_position_multiplier = point_position_multiplier,\n    show_mean = FALSE, show_points = TRUE, with_ci = FALSE) |>\n  edges(\n    weights = subtracted.mean*2,\n    edge_size_multiplier = edge_size_multiplier,\n    edge_arrow_saturation_multiplier = edge_arrow_saturation_multiplier,\n    node_position_multiplier = node_position_multiplier,\n    edge_color = c(\"red\", \"blue\")) |>\n  nodes(\n    node_size_multiplier = node_size_multiplier,\n    node_position_multiplier = node_position_multiplier,\n    self_connection_color = c(\"red\", \"blue\")) \n\n\n\n\n\nThe connection between Design.Reasoning and Data consists of two triangles, one in blue pointing from Data to Design.Reasoning, the other in red pointing from Design.Reasoning to Data. This indicates that although both units made strong connections between these two codes, the relative directed frequencies are different. Recall that in the ENA subtracted network for the same two units, the connections between Data and Design.Reasoning were basically the same. ONA, by accounting for the order of events, shows that while the undirected relative frequencies were similar, there was a difference in the order in which the two students made the connection.\n\n\n\n4.6 Compare groups statistically\nIn addition to visual comparison of networks, ENA points can be analyzed statistically. For example, here we might test whether the patterns of association in one condition are significantly different from those in the other condition.\nTo demonstrate both parametric and non-parametric approaches to this question, the examples below use a Student’s t test and a Mann-Whitney U test to test for differences between the FirstGame and SecondGame condition. For more instructions on when to choose parametric and non-parametric tests, please refer to Kaur & Kumar (2015).\nFirst, install the lsr package to enable calculation of effect size (Cohen’s d) for the t test.\n\n# install.packages('lsr')\nlibrary(lsr)\n\nThen, subset the points to test for differences between the two conditions.\n\nona_first_points_d1 = as.matrix(set.ona$points$Condition$FirstGame)[,1]\nona_second_points_d1 = as.matrix(set.ona$points$Condition$SecondGame)[,1]\n\nona_first_points_d2 = as.matrix(set.ona$points$Condition$FirstGame)[,2]\nona_second_points_d2 = as.matrix(set.ona$points$Condition$SecondGame)[,2]\n\nConduct the t test on the first and second dimensions.\n\n# parametric tests\nt_test_d1 = t.test(ona_first_points_d1, ona_second_points_d1)\nt_test_d1\n\n\n    Welch Two Sample t-test\n\ndata:  ona_first_points_d1 and ona_second_points_d1\nt = -3.7729, df = 41.001, p-value = 0.0005111\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.18227713 -0.05517572\nsample estimates:\n  mean of x   mean of y \n-0.05441628  0.06431015 \n\nt_test_d2 = t.test(ona_first_points_d2, ona_second_points_d2)\nt_test_d2\n\n\n    Welch Two Sample t-test\n\ndata:  ona_first_points_d2 and ona_second_points_d2\nt = -6.9301e-16, df = 45.45, p-value = 1\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.1008208  0.1008208\nsample estimates:\n    mean of x     mean of y \n-1.727628e-17  1.742362e-17 \n\n\nCompute any other statistics that may be of interest. A few examples are given below.\n\nmean(ona_first_points_d1)\n\n[1] -0.05441628\n\nmean(ona_second_points_d1)\n\n[1] 0.06431015\n\nmean(ona_first_points_d2)\n\n[1] -1.727628e-17\n\nmean(ona_second_points_d2)\n\n[1] 1.742362e-17\n\nsd(ona_first_points_d1)\n\n[1] 0.09754142\n\nsd(ona_second_points_d1)\n\n[1] 0.1171941\n\nsd(ona_first_points_d2)\n\n[1] 0.1784777\n\nsd(ona_second_points_d2)\n\n[1] 0.1679372\n\nlength(ona_first_points_d1)\n\n[1] 26\n\nlength(ona_second_points_d1)\n\n[1] 22\n\nlength(ona_first_points_d2)\n\n[1] 26\n\nlength(ona_second_points_d2)\n\n[1] 22\n\ncohensD(ona_first_points_d1, ona_second_points_d1)\n\n[1] 1.109985\n\ncohensD(ona_first_points_d2, ona_second_points_d2)\n\n[1] 1.997173e-16\n\n\nHere, along the x axis (MR1), a two-sample t test assuming unequal variance shows that the FirstGame (mean=-0.05, SD=0.09, N=26) condition is statistically significantly different for alpha=0.05 from the SecondGame condition (mean=0.06, SD=0.12, N=22; t(41.001)= -3.77, p=0.00, Cohen’s d=1.1). Along the y axis (SVD2), a two-sample t test assuming unequal variance shows that the FirstGame condition (mean=-1.73, SD=0.17, N=26) is not statistically significantly different for alpha=0.05 from the SecondGame condition (mean=1,74, SD=0.17, N=22; t(45.45)= 0, p=1.00, Cohen’s d=0.00).\nThe Mann-Whitney U test is a non-parametric alternative to the independent two-sample t test.\nFirst, install the rcompanion package to calculate the effect size (r) for a Mann-Whitney U test.\n\n# install.packages('rcompanion')\nlibrary(rcompanion)\n\nThen, conduct a Mann-Whitney U test on the first and second dimensions.\n\n# non parametric tests\nw_test_d1 = wilcox.test(ona_first_points_d1, ona_second_points_d1)\nw_test_d2 = wilcox.test(ona_first_points_d2, ona_second_points_d2)\n\nw_test_d1\n\n\n    Wilcoxon rank sum exact test\n\ndata:  ona_first_points_d1 and ona_second_points_d1\nW = 130, p-value = 0.0009533\nalternative hypothesis: true location shift is not equal to 0\n\nw_test_d2\n\n\n    Wilcoxon rank sum exact test\n\ndata:  ona_first_points_d2 and ona_second_points_d2\nW = 264, p-value = 0.6593\nalternative hypothesis: true location shift is not equal to 0\n\n\nCompute any other statistics that may be of interest. A few examples are given below.\n\nmedian(ona_first_points_d1)\n\n[1] -0.04307778\n\nmedian(ona_second_points_d1)\n\n[1] 0.09596238\n\nmedian(ona_first_points_d2)\n\n[1] 0.001753116\n\nmedian(ona_second_points_d2)\n\n[1] 0.05862436\n\nlength(ona_first_points_d1)\n\n[1] 26\n\nlength(ona_second_points_d1)\n\n[1] 22\n\nlength(ona_first_points_d2)\n\n[1] 26\n\nlength(ona_second_points_d2)\n\n[1] 22\n\nabs(wilcoxonR(ona_first_points_d1, ona_second_points_d1))\n\nr \n0 \n\nabs(wilcoxonR(ona_first_points_d2, ona_second_points_d2))\n\n    r \n0.707 \n\n\nHere, along the x axis (MR1), a Mann-Whitney U test shows that the FirstGame condition (Mdn=-0.04, N=26) was statistically significantly different for alpha=0.05 from the SecondGame condition (Mdn=0.10, N=22 U=130, p=0.001, r=0.00). Along the y axis (SVD2), a Mann-Whitney U test shows that the FirstGame condition (Mdn=0.001, N=26) is not statistically significantly different for alpha=0.05 from the SecondGame condition (Mdn=0.00, N=22, U=264, p=0.66, r=0.71). The absolute value of r value in Mann-Whitney U test varies from 0 to close to 1. The interpretation values for r commonly in published literature is: 0.10 - < 0.3 (small effect), 0.30 - < 0.5 (moderate effect) and >= 0.5 (large effect).\n\n\n4.7 Model evaluation\n\n4.7.1 Variance explained\nBriefly, variance explained (also called explained variation) refers to the proportion of the total variance in a dataset that is accounted for by a statistical model or set of predictors.\nIn ONA, to represent high-dimensional vectors in a two-dimensional space, ONA uses either singular value decomposition or means rotation combined with SVD. For each of the reduced dimensions, the variance in patterns of association among units explained by that dimension can be computed.\n\nset.ona$model$variance\n\n         MR1         SVD2         SVD3         SVD4         SVD5         SVD6 \n1.367940e-01 2.736079e-01 1.751045e-01 1.037211e-01 5.575839e-02 4.824400e-02 \n        SVD7         SVD8         SVD9        SVD10        SVD11        SVD12 \n3.788019e-02 2.985421e-02 2.455490e-02 1.986132e-02 1.547312e-02 1.296796e-02 \n       SVD13        SVD14        SVD15        SVD16        SVD17        SVD18 \n1.190085e-02 8.781167e-03 8.412761e-03 6.174962e-03 4.901254e-03 4.393250e-03 \n       SVD19        SVD20        SVD21        SVD22        SVD23        SVD24 \n3.834715e-03 3.353035e-03 2.662176e-03 2.031686e-03 1.782493e-03 1.557342e-03 \n       SVD25        SVD26        SVD27        SVD28        SVD29        SVD30 \n1.220686e-03 1.053932e-03 9.264935e-04 7.927517e-04 7.377140e-04 5.640286e-04 \n       SVD31        SVD32        SVD33        SVD34        SVD35        SVD36 \n3.789101e-04 1.971669e-04 1.817454e-04 1.602525e-04 1.146862e-04 6.435798e-05 \n\n\nIn our example above, since we used means rotation method, the first dimension is labeled as MR1 and the second dimension is labeled as SVD2.The two dimensions in combination explained more than 40% of the variance.\nHere, the first dimension is MR1 and the second dimension is SVD2. The two dimensions in combination explained more than 40% of the variance.\nAs with any statistical model, greater explained variance does not necessarily indicate a better model, as it may be due to overfitting, but it provides one indicator of model quality.\n\n\n4.7.2 Goodness of fit\nBriefly, a model’s goodness of fit refers to how well a model fits or represents the data. A model with a high goodness of fit indicates that it accurately represents the data and can make reliable predictions.\nIn ONA, a good fit means that the positions of the nodes in the space—and thus the network visualizations—are consistent with the mathematical properties of the model. In other words, we can confidently rely on the network visualizations to interpret the ONA model. The process that ONA uses to achieve high goodness of fit is called co-registration, the same as the one used in ENA. The mathematical details of co-registration are beyond the scope of this chapter and can be found in Bowman et al., (2022).\nTo test a model’s goodness of fit, use ona::correlations. The closer the value is to 1, the higher the model’s goodness of fit is. Most ENA models have a goodness of fit that is well above 0.90.\n\nona::correlations(set.ona)\n\n\n\n  \n\n\n\n\n\n4.7.3 Close the interpretative loop\nAnother approach to evaluate an ONA model is to confirm the alignment between quantitative model (in our case, our ONA model) and the original qualitative data. In other words, we can return to the original data to confirm that quantitative findings give a fair representation of the data. This approach is an example of what’s called as closing the interpretative loop in Quantitative Ethnography field (Shaffer, 2017).\nFor example, based on our visual analysis of the network of “SecondGame::samuel o” in previous section, we are interested in what the lines are in the original data that contributed to the connection from Performance.Parameters to Design.Reasoning.\nLet’s first review what SecondGame::samuel o ONA network looks like. Based on the connection direction and strength from Technical.Constraints to Performance.Parameters, we would expect to see more examples of Samuel responded with Design.Reasoning to Performance.Parameters, than the other way around.\n\nplot.ena.ordered.set(set.ona, title = \"SecondGame::samuel o\") |>\n  units(\n    points=set.ona$points$ENA_UNIT$`SecondGame::samuel o`, \n    points_color = \"blue\", \n    show_mean = FALSE, show_points = TRUE, with_ci = FALSE) |>\n  edges(\n    weights = set.ona$line.weights$ENA_UNIT$`SecondGame::samuel o`,\n    edge_size_multiplier = edge_size_multiplier,\n    edge_arrow_saturation_multiplier = edge_arrow_saturation_multiplier,\n    node_position_multiplier = node_position_multiplier,\n    edge_color = c(\"blue\")) |>\n  nodes(\n    node_size_multiplier = node_size_multiplier,\n    node_position_multiplier = node_position_multiplier,\n    self_connection_color = c(\"blue\"))\n\n\n\n\n\nTo do so, we use view() function and specify required parameters as below.\nThis is going to activate a window shows up in your Viewer panel. If it is too small to read, you can click on the “Show in new window” button to view it in your browser for better readability. (Note: the html page produced by the view() function will show separately from the html file knitted from RMD file)\n\nview(accum.ona, # the object stored our connection accumulation results in 4.3.6\n     wh = c(\"SecondGame::samuel o\"), # the unit we are interested in\n     units.by = c(\"Condition\", \"UserName\"), # consistent with in 4.3.1 \n     conversation.by = c(\"Condition\", \"GroupName\", \"ActivityNumber\"), # consistent with in 4.3.3\n     codes = c(\"Performance.Parameters\", \"Design.Reasoning\"), # codes of choice\n     window = 7) # consistent with in 4.3.4\n\n\n\nIn the Viewer panel, hover over your cursor on any of the lines that are in bold, a size of 7 lines rectangle shows up, representing that in a moving stanza window of size 7, the referent line (the line in bold) and its preceding 6 lines. The 1 and 0 in Technical.Constraints column and Design.Reasoning column shows where the connections happened.\nNotice that here we are viewing the same qualitative example as in section 3.7.3 in ENA. In line 2477 Samuel shared his Design.Reasoning about “mindful of (the) how one device scores relative to other ones”, as a response to what Casey said in line 2476 about Performance.Parameters “not one source/censor can be the best in every area so we had to sacrifice certain attributes”, as well as what Jackson said in line 2475 about safety as one of the Performance.Parameters “when it came to the different attributes, i think that all were important in their own way but i think safety is one of the most important”.\nHere, ONA was able to not only capture the occurrence between code Design.Reasoning and Performance.Parameters as ENA did, but also represent the connection direction from Design.Reasoning to Performance.Parameters-\n\n\n\n4.8 Using ONA outputs in other analysis\nAs with ENA, the outputs of ONA models can be used as inputs in other statistical models. See Section 3.8 for an example using ENA points."
  },
  {
    "objectID": "chapters/ch18-ena-ona/ch18-ena.html#projection",
    "href": "chapters/ch18-ena-ona/ch18-ena.html#projection",
    "title": "18  Epistemic Network Analysis and Ordered Network Analysis in Learning Analytics",
    "section": "5 Projection",
    "text": "5 Projection\nIn the sections above, we demonstrated how to do an ENA analysis and an ONA analysis. In this section, we show how to project new data into a space constructed with different data. This can be done as long as the same codes are used in both sets.\n\n5.1 Projections in ENA\nTo project the ENA points from one model into a space constructed with different data, replace the rotation.set parameter of ena.make.set. In the example below, an “expert” model is developed using the SecondGame units and the FirstGame (novice) units are projected into that space.\n\ndata = rENA::RS.data\n\n#expert data\nexp.data = subset(data, Condition == \"SecondGame\")\n\n#novice data\nnov.data = subset(data, Condition == \"FirstGame\")\n\n#expert model\nunits_exp = exp.data[,c(\"Condition\",\"UserName\")]\nconversation_exp = exp.data[,c(\"Condition\",\"GroupName\",\"ActivityNumber\")]\ncodes_exp = exp.data[,codeCols]\nmeta_exp = exp.data[,c(\"CONFIDENCE.Change\",\n                  \"CONFIDENCE.Pre\",\"CONFIDENCE.Post\",\"C.Change\")]\n\nset_exp = \n  ena.accumulate.data(\n  text_data = exp.data[, 'text'],\n  units = units_exp,\n  conversation = conversation_exp,\n  codes = codes_exp,\n  metadata = meta_exp,\n  window.size.back = 7,\n) |>\n  ena.make.set()\n\nset_exp$rotation$rotation.matrix\n\n\n\n  \n\n\nset_exp$model$points.for.projection\n\n\n\n  \n\n\n#novice model\nunits_nov = nov.data[,c(\"Condition\",\"UserName\")]\nconversation_nov = nov.data[,c(\"Condition\",\"GroupName\",\"ActivityNumber\")]\ncodes_nov = nov.data[,codeCols]\nmeta_nov = nov.data[,c(\"CONFIDENCE.Change\",\n                  \"CONFIDENCE.Pre\",\"CONFIDENCE.Post\",\"C.Change\")]\n\nset_nov = \n  ena.accumulate.data(\n  text_data = nov.data[, 'text'],\n  units = units_nov,\n  conversation = conversation_nov,\n  codes = codes_nov,\n  metadata = meta_nov,\n  window.size.back = 7,\n) |>\n  ena.make.set(rotation.set = set_exp$rotation)\n\n\n# plot expert model (what we projected into) Using plotting wrapper to save time\nplot_exp = ena.plotter(set_exp,\n                        points = T,\n                        mean = T, \n                        network = T,\n                        print.plots = F\n                       )\n\n# plot test model (points from test model in training model space)\nplot_nov = ena.plotter(set_nov,\n                        points = T,\n                        mean = T, \n                        network = T,\n                        print.plots = F)\n\nplot_exp$plot\n\n[[1]]\n\nplot_nov$plot\n\n[[1]]\n\n\n\n\n5.2 Projections in ONA\nProjection works similarly in ONA.\n\ndata = ona::RS.data\n\n#expert data\nexp.data = subset(data, Condition == \"SecondGame\")\n\n#novice data\nnov.data = subset(data, Condition == \"FirstGame\")\n\n#shared unit cols  \nunits = c(\"UserName\",\"Condition\",\"GroupName\") \n\n#shared code cols\ncodes = c(\n          'Data',\n          'Technical.Constraints',\n          'Performance.Parameters',\n          'Client.and.Consultant.Requests',\n          'Design.Reasoning',\n          'Collaboration')\n\n#shared hoo\nhoo = conversation_rules(\n  (Condition %in% UNIT$Condition & GroupName %in% UNIT$GroupName))\n\n\n#expert accum\naccum.exp = contexts(exp.data, units_by = units, hoo_rules = hoo) |>\n  accumulate_contexts(codes = codes, \n                      decay.function = decay(simple_window, window_size = 7),\n                      return.ena.set = FALSE, norm.by = NULL)\n#expert model\nset.exp = model(accum.exp)\n\n#novice accum\naccum.nov = contexts(nov.data, units_by = units, hoo_rules = hoo) |>\n  accumulate_contexts(codes = codes, \n                      decay.function = decay(simple_window, window_size = 7),\n                      return.ena.set = FALSE, norm.by = NULL)\n#novice model\nset.nov = model(accum.nov)\n\n# projecting novice data into expert space\nset = model(accum.nov, rotation.set = set.exp$rotation)\n\nplot.ena.ordered.set(set, title = \"novice data into expert space\") |> \n  units(\n    points = set$points, \n    show_mean = TRUE, show_points = TRUE, with_ci = TRUE) |>\n  edges(\n    weights = set$line.weights) |>\n  nodes(\n    self_connection_color = \"red\",\n    node_size_multiplier = 0.6)"
  },
  {
    "objectID": "chapters/ch18-ena-ona/ch18-ena.html#discussion",
    "href": "chapters/ch18-ena-ona/ch18-ena.html#discussion",
    "title": "18  Epistemic Network Analysis and Ordered Network Analysis in Learning Analytics",
    "section": "6 Discussion",
    "text": "6 Discussion\nIn this chapter, we introduced two techniques, ENA and ONA, for quantifying, visualizing, and interpreting networks using coded data. Through the use of a demonstration dataset that documents collaborative discourse among students collaborating to solve an engineering design problem, we provided step-by-step instructions on how to model complex, collaborative thinking using ENA and ONA in R. The chapter combines theoretical explanations with tutorials, intended to be of aid to researchers with varying degrees of familiarity with network analysis techniques and R. This chapter mainly showcased the standard and most common use of these two tools. The ENA and ONA R packages, akin to other R packages, offer flexibility to researchers to tailor their analyses to their specific needs. For example, users with advanced R knowledge can supply their own adjacency matrices and use ENA or ONA solely as a visualization tool rather than an integrated modeling and visualization tool.\nDue to the technical and practical focus of this chapter, we omitted detailed explanations of the theoretical, methodological, and mathematical foundations of ENA and ONA that are crucial for informed, theory-based learning analytics research using these techniques. Consult the Further Reading section for papers that explain these aspects of ENA and ONA in greater detail."
  },
  {
    "objectID": "chapters/ch18-ena-ona/ch18-ena.html#further-reading",
    "href": "chapters/ch18-ena-ona/ch18-ena.html#further-reading",
    "title": "18  Epistemic Network Analysis and Ordered Network Analysis in Learning Analytics",
    "section": "7 Further reading",
    "text": "7 Further reading\n\nArastoopour Irgens, G., & Eagan, B. (2022, October). The Foundations and Fundamentals of Quantitative Ethnography. In International Conference on Quantitative Ethnography (pp. 3-16). Cham: Springer Nature Switzerland.\nBowman, D., Swiecki, Z., Cai, Z., Wang, Y., Eagan, B., Linderoth, J., & Shaffer, D. W. (2021). The mathematical foundations of epistemic network analysis. In Advances in Quantitative Ethnography: Second International Conference, ICQE 2020, Malibu, CA, USA, February 1-3, 2021, Proceedings 2 (pp. 91-105). Springer International Publishing.\nBrohinsky J., Marquart C., Wang J., Ruis A.R., & Shaffer D.W. (2021). Trajectories in epistemic network analysis. In Ruis, A.R. & Lee, S.B. (Eds.), Advances in Quantitative Ethnography: Second International Conference, ICQE 2020, Malibu, CA, USA, February 1-3, 2021, Proceedings (pp. 106-121). Springer.\nGasevic, D., Greiff, S., & Shaffer, D. W. (2022). Towards strengthening links between learning analytics and assessment: Challenges and potentials of a promising new bond. Computers in Human Behavior, 1–7.\nRuis, A. R. , Tan, Y., Brohinsky, J., Yang, Y., Cai, Z., and Shaffer, D. W. (2023). Thick Description of Thin Data: Modeling Socio-Environmental Problem-Solving Trajectories in Localized Land-Use Simulations (in press)\nShaffer, D. W. (2017). Quantitative ethnography. Cathcart Press.\nShaffer, D. W. (2018). Epistemic network analysis: Understanding learning by using big data for thick description. Fischer, F., Hmelo-Silver, C. E., Goldman, S. R., & Reimann, P. (Eds.) International Handbook of the Learning Sciences (pp. 520-531). New York: Routledge.\nShaffer, D. W. (2018). Big data for thick description of deep learning. In K. Millis, D. Long, J. Magliano, and K. Weimer (Eds.), Deep learning: Multi-disciplinary approaches (pp. 262-275). NY, NY: Routledge.\nShaffer, D. W., Eagan, B., Knowles, M., Porter, C., & Cai, Z. (2022). Zero re-centered projection: An alternative proposal for modeling empty networks in ENA. In B. Wasson & S. Zörgő (Eds.), Advances in Quantitative Ethnography: Third International Conference, ICQE 2021, Virtual Event, November 6–11, 2021, Proceedings (pp. 66–79). Springer.\nShaffer, D. W., & Ruis, A. R. (2022, October). Is QE Just ENA?. In International Conference on Quantitative Ethnography (pp. 71-86). Cham: Springer Nature Switzerland.\nSwiecki, Z., Lian, Z., Ruis, A. R., & Shaffer, D. W. (2019). Does order matter? Investigating sequential and cotemporal models of collaboration. In Lund, K. Niccolai, G., Lavoué, E., Hmelo-Silver, C., Gwon, G. & Baker, M. (Eds.) A Wide Lens: Combining Embodied, Enactive, Extended, and Embedded Learning in Collaborative Settings: 13th International Conference on Computer Supported Collaborative Learning (CSCL), I (pp.112-119).\n\n\n\nTan, Y., Hinojosa, C., Marquart, C., Ruis, A., & Shaffer, D. W. (2022). Epistemic network analysis visualization. In B. Wasson & S. Zörgő (Eds.), Advances in Quantitative Ethnography Third International Conference, ICQE 2021, Virtual Event, November 6–11, 2021, Proceedings (pp. 129–143). Springer.\nTan, Y., Ruis, A.R., Marquart C., Cai, Z., Knowles, M., & Shaffer, D.W. (2022). Ordered network analysis. In 2022 International Conference on Quantitative Ethnography.\n\n\n\nWang Y., Swiecki Z., Ruis A.R., & Shaffer D. W. (2021). Simplification of epistemic networks using parsimonious removal with interpretive alignment. In Ruis, A.R. & Lee, S.B. (Eds.), Advances in Quantitative Ethnography: Second International Conference, ICQE 2020, Malibu, CA, USA, February 1-3, 2021, Proceedings (pp. 137-151). Springer.\nWang, Y., Ruis, A.R., Shaffer, D.W. (2023). Modeling Collaborative Discourse with ENA Using a Probabilistic Function. In: Damşa, C., Barany, A. (eds) Advances in Quantitative Ethnography. ICQE 2022. Communications in Computer and Information Science, vol 1785. Springer, Cham. https://doi-org.ezproxy.library.wisc.edu/10.1007/978-3-031-31726-2_10"
  },
  {
    "objectID": "chapters/ch18-ena-ona/ch18-ena.html#references",
    "href": "chapters/ch18-ena-ona/ch18-ena.html#references",
    "title": "18  Epistemic Network Analysis and Ordered Network Analysis in Learning Analytics",
    "section": "8 References",
    "text": "8 References\n\nAndrist, S., Ruis, A. R., & Shaffer, D. W. (2018). A network analytic approach to gaze coordination during a collaborative task. Computers in Human Behavior, 89, 339-348.\nArastoopour, G., Chesler, N. C., & Shaffer, D. W. (2014). Epistemic persistence: A simulation-based approach to increasing participation of women in engineering. Journal of Women and Minorities in Science and Engineering, 20(3): 211-234.\nArastoopour, G., & Shaffer, D. W. (2015). Epistemography and professional CSCL environment design. Paper presented at the International Conference on Computer Supported Collaborative Learning. Gothenberg, Sweden. \nArastoopour, G., Shaffer, D. W., Swiecki, Z., Ruis, A. R., & Chesler, N. C. (2016). Teaching and assessing engineering design thinking with virtual internships and epistemic network analysis. International Journal of Engineering Education, 32(3B), 1492–1501. \nBaker, R. S., Gašević, D., & Karumbaiah, S. (2021). Four paradigms in learning analytics: Why paradigm convergence matters. Computers and Education: Artificial Intelligence, 2, 100021.\nBauer, E., Sailer, M., Kiesewetter, J., Shaffer, D. W., Schulz, C., Pfeiffer, J., Gurevych, I., Fischer, M. R., & Fischer, F. (2020). Pre-Service teachers’ diagnostic argumentation: What is the role of conceptual knowledge and epistemic activities?. Proceedings of the Fifteenth International Conference of the Learning Sciences, 2399-2400.\nBowman, D., Swiecki, Z., Cai, Z., Wang, Y., Eagan, B., Linderoth, J., & Shaffer, D. W. (2021). The mathematical foundations of epistemic network analysis. In Advances in Quantitative Ethnography: Second International Conference, ICQE 2020, Malibu, CA, USA, February 1-3, 2021, Proceedings 2 (pp. 91-105).\nBressler, D. M., Bodzin, A. M., Eagan, B., & Tabatabai, S. (2019). Using epistemic network analysis to examine discourse and scientific practice during a collaborative game. Journal of Science Education and Technology, 28, 553-566.\nChesler, N., Arastoopour, G., D’Angelo, C., Bagley, E., & Shaffer, D. W. (2013). Design of a professional practice simulator for educating and motivating first-year engineering students. Advances in Engineering Education 3(3): 1-29. \nChesler, N. C., Ruis, A. R., Collier, W., Swiecki, Z., Arastoopour, G., & Shaffer, D. W. (2015). A novel paradigm for engineering education: Virtual internships with individualized mentoring and assessment of engineering thinking. Journal of Biomechanical Engineering, 137(2). \nCsanadi, A., Eagan, B., Shaffer, D. W., Kollar, I., & Fischer, F. (2018). When coding-and-counting is not enough: Using epistemic network analysis (ENA) to analyze verbal data in CSCL research. International Journal of Computer-Supported Collaborative Learning, 13(4), 419-438.\nFan, Y., Tan, Y., Raković, M., Wang, Y., Cai, Z., Shaffer, D. W., & Gašević, D. (2022). Dissecting learning tactics in MOOC using ordered network analysis. Journal of Computer Assisted Learning. 1– 13.\nFernandez-Nieto, G. M., Martinez-Maldonado, R., Kitto, K., & Buckingham Shum, S. (2021, April). Modelling spatial behaviours in clinical team simulations using epistemic network analysis: methodology and teacher evaluation. In LAK21: 11th International Learning Analytics and Knowledge Conference (pp. 386-396).\nKaur, A., & Kumar, R. (2015). Comparative analysis of parametric and non-parametric tests. Journal of computer and mathematical sciences, 6(6), 336-342.\nOshima, J., Oshima, R., & Fujita, W. (2018). A Mixed-Methods Approach to Analyze Shared Epistemic Agency in Jigsaw Instruction at Multiple Scales of Temporality. Journal of Learning Analytics, 5(1), 10–24. https://doi.org/10.18608/jla.2018.51.2\nPhillips, M., Trevisan, O., Carli, M., Mannix, T., Gargiso, R., Gabelli, L., & Lippiello, S. (2023, March). Uncovering patterns and (dis) similarities of pre-service teachers through Epistemic Network Analysis. In Society for Information Technology & Teacher Education International Conference (pp. 1021-1030). Association for the Advancement of Computing in Education (AACE).\nPrieto, L. P., Rodríguez-Triana, M. J., Ley, T., & Eagan, B. (2021). The value of epistemic network analysis in single-case learning analytics: A case study in lifelong learning. In Advances in Quantitative Ethnography: Second International Conference, ICQE 2020, Malibu, CA, USA, February 1-3, 2021, Proceedings 2 (pp. 202-217). Springer International Publishing.\nRuis, A. R., Rosser, A. A., Quandt-Walle, C., Nathwani, J. N., Shaffer, D. W., & Pugh, C. M. (2018). The hands and head of a surgeon: modeling operative competency with multimodal epistemic network analysis. American Journal of Surgery, 216(5), 835-840.\nRuis, A. R., Siebert-Evenstone, A. L., Pozen, R.,  Eagan, B., & Shaffer, D. W. (2019). Finding common ground: A method for measuring recent temporal context in analyses of complex, collaborative thinking. In Lund, K. Niccolai, G., Lavoué, E., Hmelo-Silver, C., Gwon, G. & Baker, M. (Eds.) A Wide Lens: Combining Embodied, Enactive, Extended, and Embedded Learning in Collaborative Settings: 13th International Conference on Computer Supported Collaborative Learning (CSCL), I (pp.136-143).  \nSiebert-Evenstone, A. L., Arastoopour Irgens, G., Collier, W., Swiecki, Z., Ruis, A. R., & Shaffer, D. W. (2017). In search of conversational grain size: Modelling semantic structure using moving stanza windows. Journal of Learning Analytics, 4(3), 123–139.\nSiebert-Evenstone, A. L., & Shaffer, D. W. (2019). Cause and because: Using epistemic network analysis to model causality in the next generation science standards. In Eagan, B., Misfeldt, M., & Siebert-Evenstone, A. L., (Eds.) Advances in Quantitative Ethnography: ICQE 2019. (pp.245-255)\nTan, Y., Hinojosa, C., Marquart, C., Ruis, A., & Shaffer, D. W. (2022). Epistemic network analysis visualization. In B. Wasson & S. Zörgő (Eds.), Advances in Quantitative Ethnography Third International Conference, ICQE 2021, Virtual Event, November 6–11, 2021, Proceedings (pp. 129–143). Springer.\nTan, Y., Ruis, A.R., Marquart C., Cai, Z., Knowles, M., & Shaffer, D.W. (2022). Ordered network analysis. In 2022 International Conference on Quantitative Ethnography.\nWooldridge, A.R, Carayon, P., Shaffer, D. W., & Eagan, B. (2018). Quantifying the qualitative with epistemic network analysis: A human factors case study of task-allocation communication in a primary care team. IISE Transactions on Healthcare Systems Engineering, 8(1) (pp. 72–82).\nZhang, S., Gao, Q., Sun, M., Cai, Z., Li, H., Tang, Y., & Liu, Q. (2022). Understanding student teachers’ collaborative problem solving: Insights from an epistemic network analysis (ENA). Computers & Education, 183, 104485."
  },
  {
    "objectID": "chapters/ch19-psychological-networks/ch19-psych.html",
    "href": "chapters/ch19-psychological-networks/ch19-psych.html",
    "title": "19  Psychological Networks: A Modern Approach to Analysis of Learning and Complex Learning Processes",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch19-psychological-networks/ch19-psych.html#introduction",
    "href": "chapters/ch19-psychological-networks/ch19-psych.html#introduction",
    "title": "19  Psychological Networks: A Modern Approach to Analysis of Learning and Complex Learning Processes",
    "section": "1 Introduction",
    "text": "1 Introduction\nLearning has long been described as a complex system that involves the interactions of several elements across time, persons and contexts [1–3]. Such descriptions include learning theories, constructs, classroom, learners, education as a whole and educational policies. For instance, Zimmerman describes self-regulation as a “complex system of interdependent processes” [4]. Winne describes the SRL process as “complex, dynamically changeable and contextually sensitive” [5]. Similar descriptions and operationalizations exist for engagement [6], motivation [7, 8], metacognition [9], agency [10] and achievement goals [11], to mention a few. Similarly, the students [12], the classroom [13], collaborative groups [14] have all been conceptualized from the perspective of complex systems. Nevertheless, despite the long history and the solid theoretical grounds, methodological approaches that capture the complexities of learning and learners have been lagging behind both in adoption or applications [1, 15]. This chapter introduces the complex systems and offers tutorial on one of the most important and promising methods of analyzing complex systems with a real-life dataset.\n\n1.1 Complex systems\nA complex system is an ensemble of interdependent elements that interact with one another, evolve, adapt or self-organize in a nonlinear way leading to the emergence of new phenomenon or behavior [16, 17]. Let’s for example take engagement as an example. Engagement is understood as a multidimensional construct with three dimensions: behavioral, cognitive and emotional dimensions [18, 19]. Research has shown that each of the engagement dimensions influence (or interact with) each other; for example, enjoying school (emotions) stimulates school attendance (behavior) and investment in learning (cognitive) [18, 20]. Such interactions may lead to the emergence of resilience (new behavior) [21]. We also know that such interactions are nonlinear —that is, we can not combine engagement dimensions together in a single score (e.g., behavioral + cognitive + emotional ≠ engagement). In other words, the sum of the parts does not equal the whole. The same can be said about self-regulated learning, the Zimmerman cyclical phases model describes three phases: forethought (task analysis and goal-setting), performance phase (task enactment and strategies) and self-reflection (evaluation and adaptation). Each of these phases are explicitly modeled as influencing each other in a cyclical way; they interact in a non-linear way and such interactions lead to the emergence of learning strategies [22].\nThe study of these complexities of learning requires methods that account for such interactions, interdependencies and heterogeneity [1, 15]. Linear methods such as correlations, regression or comparison of means are essentially reductionist, that is, disregard the inter-relationships between the variables. For instance, in regression analysis, all variables are included as predictors of one predicted variable (dependable variable). By design, such variables are assumed to be independent and not correlated. While this view can offer understanding through simplification, it does so by compromising the understanding of the big picture. In contrast to the reductionist view of human behavior, embracing the view that learning is complex is more tethered to reality and promises for renewal of our knowledge [1, 15, 23].\n\n\n1.2 Network analysis\nNetwork analysis as a framework affords researchers a powerful tool set to chart relations, map connections, and discover clusters or communities between interacting components and therefore, has become one of the most important methods for understanding complex systems [24, 25]. In education, social network analysis has been used for decades [25], and we covered it in detail in previous chapters of the book [[26]; [27]; [28]}. Yet, to go beyond social interactions, one needs a different type of networks: probabilistic networks [29, 30]. In probabilistic networks, the variables (often indicators of constructs or scale scores) are the nodes or the vertices of the networks, the relationships or magnitude of interactions (i.e. probabilistic associations) between such variables form the edges. In particular, we will focus in this chapter on psychological networks: Gaussian graphical models (GGM) [30]. In psychological networks, the variables may be constructs, behaviors, attitudes, etc., and the interactions are partial correlations among the nodes [3]. A partial correlation measures the relationship (or the conditions dependence) between two variables after removing (controlling or adjusting) for all other variables in the network or what is known as ceteris paribus [30, 31]. For instance, if we are modeling a network where motivation, achievement, engagement, self-regulation and well-being are nodes, and we observed a relationship between well-being and achievement, such relationships means that well-being is associated with achievement beyond what can be explained by their associations with motivation, engagement, self-regulation (all other variables in the network). The absence of a relationship between two variables signifies a conditional independence between the two variables after controlling for all other variables in the network [31]. As such, the presence or lack of interactions are both interpretable and carry a meaning. Psychological networks offer rigorous methods for the assessment of the accuracy of estimated networks, edge weights and centrality measures through bootstrapping, assessment of sampling variability through simulation of “expected replicability” of the studied networks) [29]."
  },
  {
    "objectID": "chapters/ch19-psychological-networks/ch19-psych.html#related-work",
    "href": "chapters/ch19-psychological-networks/ch19-psych.html#related-work",
    "title": "19  Psychological Networks: A Modern Approach to Analysis of Learning and Complex Learning Processes",
    "section": "2 Related work",
    "text": "2 Related work\nAs the term psychological networks suggests, most studies operationalizing this method are rooted in psychological research. Consequently, its connection to educational research is primarily established through educational psychology. For example, a study by Liu et al. [32] sought to identify a link between students’ personality and their experience of psychological distress during and after the COVID-19 lockdown. Their results suggest that neuroticism was linked to heightened psychological distress both during and after the lockdown, whereas extraversion and conscientiousness were associated with the alleviation of psychological distress. Another study [33] investigated the relationship among various aspects influencing nursing students’ psychological well-being, and found that perceived social support and one’s professional self-concept were the most central predictors.\nThe cited studies have relied on self-reported data. However, a recent trend is to take advantage of the intensive trace log data collected from digital educational tools. An example is the work by Saqr et al. [34], who used psychological networks to discover the interplay between self-regulated learning tactics among foreign language students. Their findings reveal a strong correlation between writing text and social bonding, as well as between acknowledging others and social bonding. López-Pernas et al. [35] clustered students into player profiles according to their performance in an educational game and used psychological networks to map the relationships between game performance indicators for each profile. They found that certain indicators (e.g., the use of hints for help) are decisive for the success of certain player profiles and superfluous for others.\nA recent trend in the use of psychological networks in education is to understand and visualize within-person (i.e., idiographic) phenomena, in which networks are constructed for data from a single individual at a time [36, 37]. For instance, Malmberg et al. [38] examined the association between monitoring events and phases of regulation in collaborative learning. Their results showed that cyclical phases of regulation do not occur simultaneously but rather monitoring motivation predicts the monitoring of task definition, ultimately leading to task enactment. A recent study by Saqr [39] constructed a network of engagement indicators using between-person data and another using within-person data. The results revealed that group-level inferences hardly generalize to individuals. For example, regularity in study and frequency of accessing resources were positively correlated at the group level but negatively so at the individual level. Such findings highlighted the need for person-specific insights to leverage personalized interventions to increase learner engagement."
  },
  {
    "objectID": "chapters/ch19-psychological-networks/ch19-psych.html#tutorial-with-r",
    "href": "chapters/ch19-psychological-networks/ch19-psych.html#tutorial-with-r",
    "title": "19  Psychological Networks: A Modern Approach to Analysis of Learning and Complex Learning Processes",
    "section": "3 Tutorial with R",
    "text": "3 Tutorial with R\nIn this section, we present a step-by-step tutorial on how to use psychological networks in cross-sectional survey data. The dataset that we are using contains the results of 6,071 students’ responses to a survey investigating students’ psychological characteristics related to their well-being during the COVID-19 pandemic in Finland and Austria. The survey questions cover students’ basic psychological needs (relatedness, autonomy, and experienced competence), self-regulated learning, positive emotion and intrinsic learning motivation. Moreover, the dataset contains demographic variables, such as country, gender, and age and is well described in the dataset chapter [40]. In the tutorial, we will construct and visualize a network that represents the relationship between the different psychological characteristics, we will interpret and evaluate these relationships, and we will compare how the networks differ among demographic groups.\n\n3.1 The libraries\nWe start by importing the necessary packages We know rio [41] and tidyverse [42] from previous chapters for importing and manipulating data respectively. The bootnet [43] package provides methods to estimate and assess accuracy and stability of estimated network structures and centrality indices. The package networktools [44] includes a selection of tools for network analysis and plotting. NetworkToolbox [45] implements network analysis and graph theory measures used in neuroscience, cognitive science, and psychology. NetworkComparisonTest [46] allows to assess the difference between two networks based on several invariance measures. The package qgraph [47] offers network visualization and analysis, as well as Gaussian graphical model computation. The package mgm [48] provides estimation of k-Order time-varying Mixed Graphical Models and mixed VAR(p) models via elastic-net regularized neighborhood regression. Lastly, matrixcalc [49] offers a collection of functions to support matrix calculations for probability, econometric and numerical analysis.\n\nlibrary(rio)\nlibrary(tidyverse)\nlibrary(bootnet)\nlibrary(networktools)\nlibrary(NetworkToolbox)\nlibrary(NetworkComparisonTest)\nlibrary(qgraph)\nlibrary(mgm)\nlibrary(matrixcalc)\n\n\n\n3.2 Importing and preparing the data\nThe first step is importing the data and doing the ncessary preparation, that is, removing the missing and incomplete responses.\n\ndf <- \n  import(\"https://github.com/sonsoleslp/labook-data/raw/main/11_universityCovid/data.sav\") |>\n  drop_na()\n\nTo represent each of the constructs that the survey aimed at capturing, we combine all the columns representing items from the same construct into one by averaging the responses. The next code calculates the mean for each construct by averaging all the related items:\n\naggregated <- df |> rowwise() |> mutate(\n       Competence = rowMeans(cbind(comp1.rec , comp2.rec, comp3.rec), na.rm = T),\n       Autonomy = rowMeans(cbind(auto1.rec , auto2.rec, auto3.rec), na.rm = T),\n       Motivation = rowMeans(cbind(lm1.rec , lm2.rec, lm3.rec), na.rm = T),\n       Emotion = rowMeans(cbind(pa1.rec , pa2.rec, pa3.rec), na.rm = T),\n       Relatedness = rowMeans(cbind(sr1.rec , sr2.rec, sr3.rec), na.rm = T),\n       SRL = rowMeans(cbind(gp1.rec , gp2.rec, gp3.rec), na.rm = T)) \n\nWe can now keep only the newly created columns. We also create subsets of the data based on gender (a dataset for males and another for females) and country (a data set for Austria and another for Finland). We will use these datasets later for comparison acorss genders and countries.\n\ncols <- c(\"Relatedness\", \"Competence\", \"Autonomy\", \"Emotion\", \"Motivation\", \"SRL\")\n\nfilter(aggregated, country == 1) |> select(all_of(cols)) -> finlandData \nfilter(aggregated, country == 0) |> select(all_of(cols)) -> austriaData \nfilter(aggregated, gender  == 1) |> select(all_of(cols)) -> femaleData \nfilter(aggregated, gender  == 2) |> select(all_of(cols)) -> maleData\nselect(aggregated, all_of(cols)) -> allData \n\nThe allData dataframe should look as follows:\n\nallData\n\n\n\n  \n\n\n\n\n\n3.3 Assumption checks\nAs a first step of the analysis, we need to check some assumptions to make sure that the dataset and the estimated network are appropriate. First, we need to ensure that the correlation matrix is positive definite i.e., the included variables are not a linear combination of each other and therefore, so similar they do not offer new information. This is performed by using the function is.positive.definite() from the package matrixcalc. Please note that in cases where the correlation matrix is non-positive definite, we can use option cor_auto to search for possible positive definite matrices (see later). In our case, the matrix is already positive definite. Note that we also set the use argument to \"pairwise.complete.obs\", or pairwise complete observations to include maximal observations.\n\ncorrelationMatrix <- cor(x = allData, use = c(\"pairwise.complete.obs\"))\nis.positive.definite(correlationMatrix)\n\n[1] TRUE\n\n\nThe second assumption that we need to check is whether some variables are highly correlated and therefore redundant. In doing so we make sure that each variable is sufficiently distinct from all other variables and captures a unique construct. The goldbricker algorithm compares the variables’ correlation patterns with all other variables in the dataset. Below, we search for items which are highly inter-correlated using the default values: (r > 0.50) with 0.25 as the significant fraction of variables and p-value of 0.05. The results show that the variables are significantly distinct from each other.\n\ngoldbricker(allData, p = 0.05, method = \"hittner2003\",\n            threshold = 0.25, corMin = 0.5, progressbar = FALSE)\n\n  Suggested reductions: Less than 25 % of correlations are significantly \n  different for the following pairs: \n  [1] \"No suggested reductions\"\n\n\n3.4 Network estimation\nGiven that we made sure that the data satisfy the necessary assumptions, we can now build or estimate the network. The estimation means that we quantify the associations between the different variables. Several types of associations can be estimated. The most common in psychological networks are dependency measures, such as correlation and regression. In these networks, we are interested in how the levels or values of the variables in the network vary together in a similar way (e.g., if and to what extent higher levels of motivation are associated with higher levels of engagement). Such patterns can be estimated using covariance, simple correlation, partial correlation or relative importance (regression). In this tutorial, we focus on the the most commonly used estimation method, which is regularized partial correlation.\nRegularized partial correlations have been shown to (1) retrieve the true structure of the network in most situations, and (2) offer an interpretable sparse network that shows conditional association between variables. A partial correlation means that the association between each two variables is above and beyond all other variables in the network, or conditioning on all other variables in the network or holding all other variables constant often referred to as ceteris paribus. This allows us to estimate, for example, the association between motivation and engagement beyond other variables that are included in the network, e.g., achievement, anxiety or enjoyment. Regularization is the process of applying an extra penalty for the complexity of network model. A growing body of research recommends the procedure for several reasons. Regularization helps eliminate spurious edges that results from influence of other nodes, and shrinks trivial edges to zero and thus help eliminates Type 1 error or “false positive” edges. In doing so, the resulting network model is less complex, sparser, simpler to interpret with only the strong meaningful correlations. In doing so, regularization helps retrieve a true —and conservative— structure of the network. The penalty is commonly applied using the least absolute shrinkage and selection operator “LASSO”.\nNetwork estimation can be performed using several packages. We will use the package bootnet and the function estimateNetwork(). To estimate the network we need to pass the data as an argument, and select the option default = \"EBICglasso\" to estimate a regularized network. By default, the correlation type is set to cor_auto which can detect the distribution of the variables and set the appropriate correlation coefficient: polychoric, polyserial or Pearson correlation. Other options can be \"cor\" which will compute a correlation network and \"npn\" which will apply non-paranormal transformation —to normalize the data— and then compute correlations. By default, estimateNetwork() computes 100 models with various degrees of sparsity. The best model is selected based on the lowest Extended Bayesian Information Criterion value (EBIC) given a hyperparameter gamma (\\(\\gamma\\)) which balances a trade off between false positive edges and suppressing of the true edges [30]. Gamma ranges from 0 (favors models with more edges [i.e. uses no regularization]) to 0.5 (favors model with fewer edges). The default for the hyper-parameter gamma (γ) is set to 0.5 to ensure that edges included in the model are true and part of the network. The next code estimates the network and assigns the estimated network to an R object allNetwork. The estimated network can be accessed from allNetwork$graph. We can also see the details about the network using the function summary\n\nallNetwork <- estimateNetwork(allData, default = \"EBICglasso\", \n                              corMethod = \"cor_auto\", tuning = 0.5)\nsummary(allNetwork)\n\n=== Estimated network ===\nNumber of nodes: 6 \nNumber of non-zero edges: 15 / 15 \nMean weight: 0.1397203 \nNetwork stored in object$graph \n \nDefault set used: EBICglasso \n\n\n3.5 Plotting the network\nThe network can be plotted by using the function plot(). The resulting plot uses a color blind theme by default where blue edges are positive correlations and red edges are negative correlations. The thickness of the edges is proportional to the magnitude of the regularized partial correlation. As the network shows, we see a very strong correlation between motivation, autonomy and competence. We also observe that emotion is strongly correlated with competence. As we mentioned before, each of the correlations in the network are conditioned —or above and beyond— all other nodes in the network same as regression.\nPlease also note that we assign the plot to an R object under the name allDataPlot. This facilitates further plotting as well as retrieving other data, e.g., the allDataPlot object contains the correlation matrix, the plotting parameters, the call arguments etc. For instance, allDataPlot$layout returns the network layout which we can get and use in further plotting so that we have a fixed layout for all network to enable us to compare them easily against each other. Plotting in bootnet is mostly handled by the qgraph package and most options work with few differences. As such, the plot object works with all functions of the qgraph package as we will see later. The next code plots the network using the default settings (Figure 19.1). The second line retrieves the layout and stores it in an object so we can later reuse it to plot our networks.\n\nallDataPlot <- plot(allNetwork)\nLX <- allDataPlot$layout\n\n\n\n\nFigure 1. Network with default settings\n\n\n\n\nHowever, in most situations, we will need to customize the plot. First, we may need to add a title using the argument title = \"Both countries combined\". Second, we define the node size using the argument vsize=9 (you may need to adjust according to your preference). Third, we choose to show the edge weight (i.e., the magnitude of the partial correlation) by setting edge.labels = TRUE; this is very important as it shows exactly how strong the correlation is. Fourth, we can choose cut = 0.10, which emphasizes edges higher than a threshold of 0.10. Edges below that threshold are shown with less color intensity. Whereas the cut argument is optional and relies on the network, it can be used to emphasize the important strong correlations and downsize the “clutter”. Another cosmetic option is to hide (but not remove) edges below 0.05 with the argument minimum = 0.05. Both cut and minimum make the network easy to read, interpret and less crowded. For more options, you need to consult the plot function manual bootnet.plot() or qgraph(). The final result can be seen in Figure 19.2.\n\nallDataPlot <- plot(\n  allNetwork, \n  title = \"Both countries combined\", \n  vsize = 9,                   \n  edge.labels = TRUE, \n  cut = 0.10, \n  minimum = 0.05, \n  layout = \"spring\") \n\n\n\n\nFigure 2. Network with customized settings\n\n\n\n\n\n\n3.6 Explaining network relationships\nIt may be useful —and all the more advisable— to compute the predictability of all nodes which simply means to calculate to which extent each node is explained by its relationships or the proportion of explained variance when regressing all nodes over the said node. In other words, the function computes regression for each node (where all other nodes are the predictors) and returns the R^2. This process is repeated for each node. When the predictability (R2) is zero, the node is not explained at all by it connections and one should think whether the node belongs to this network or whether the measurement was accurate. When predictability is 1, then 100% of the variance is explained by the relationships of the node. This of course is unlikely and warrants serious checks. Predictability has also been linked to controlability, or the extent to which one can control all other nodes if acted on this node e.g. intervention targeting this node.\nFor the calculation of predictability, we will need the function mgm() from the package mgm to estimate the model. The only required parameter for the function is the type of each variable which we specify as type = rep ('g', 6) or c(\"g\" ,\"g\", \"g\", \"g\" ,\"g\" ,\"g\"), which means all variables are Gaussian.\n\nfitAllData <- mgm(as.matrix(allData), type = c(\"g\", \"g\", \"g\", \"g\", \"g\", \"g\"))\n\nThe predictability of each variable can be obtained by querying the resulting object as the code shows below. We can also obtain the mean predictability to see how far our network nodes are explained by their relationships on average. Using predictability in plots can enhance the readability of the network and give more information about the extent the node is explained by the current network. We do so by assigning the pie the value of R^2: pie= predictAll$errors$R2. We can also see the the RMSE (Root Mean Square Error) which measures the difference between predicted values and the actual values using predictAll$errors$RMSE.\n\n## Compute the predictability\npredictAll <- predict(fitAllData, na.omit(allData))\npredictAll$errors$R2         # Check  predictability for all variables\n\n[1] 0.139 0.518 0.442 0.315 0.458 0.126\n\nmean(predictAll$errors$R2)   # Mean predictability\n\n[1] 0.333\n\nmean(predictAll$errors$RMSE) # Mean RMSE\n\n[1] 0.8113333\n\n\nWe can also create a data frame of the predictability.\n\ndata.frame(\n  var = predictAll$errors$Variable, \n  R2 = predictAll$errors$R2, \n  RMSE = predictAll$errors$RMSE\n  ) \n\n\n\n  \n\n\n\nAs the network plot (Figure 19.3) and the data frame show, the highest predictability belongs to the competence, motivation and autonomy. We also see that SRL and relatedness have low predictability and therefore, are less explained by their connections.\n\nallDataPlot <- plot(\n  allNetwork, \n  title = \"Both countries combined\", \n  vsize = 9, \n  edge.labels = TRUE, \n  cut = 0.10, \n  minimum = 0.05, \n  pie = predictAll$errors$R2\n  ) \n\n\n\n\nFigure 3. Network with predicatability as pie charts in the nodes\n\n\n\n\n\n\n3.7 Network inference\nSimilar to traditional networks, centrality measures can be computed for psychological networks. Here —as in any network— the interpretation depends on the network variables, the estimation method, the weights of edges and of course, the theoretical underpinning that underpins the network structure. In the general sense, centrality measures estimate important, influential or central nodes. Early research has shown that centrality measures can be potential targets for intervention among others. Whereas many centrality measures exist, two centralities have gained traction: degree (or strength) centrality and expected influence. Others, e.g., betweenness, closeness and eigenvector centralities can be calculated but have not so far being “well understood” to be routinely recommended for analysis.\nDegree centrality is the tally of connections a node has regardless of weight. Strength centrality is the sum of absolute weights of all connections (the sum of correlations weights positive or negative). Expected influence is similar, however, expected influence sums the raw values. For instance, if a node has connections of 0.3, -0.1 and 0.5, the degree centrality is 3, the strength centrality is 0.3 + 0.1 + 0.5 = 0.9 and the expected influence centrality is 0.3 - 0.1 + 0.5 = 0.7. Given that the network we have does not have any negative edges, strength and expected influence centralities are equivalent. To compute the centrality measures, we use the function centralityPlot() and provide the network as the main argument. We also need to provide the centralities that we wish to compute. The function plots the centralities by default (Figure 19.4).\n\ncentralityPlot(allNetwork, include = c(\"ExpectedInfluence\", \"Strength\"), scale = \"z-scores\" )  \n\nNote: z-scores are shown on x-axis rather than raw centrality indices.\n\n\n\n\n\nFigure 4. Centrality plot\n\n\n\n\nIf we wanted only the values, we could obtain the centralities by using the function centralityTable() .\n\ncentralityTable(allNetwork)\n\nAnother possible way to compute several types of centralities is to use the NetworkToolbox package which offers a large collection of centralities, e.g., degree, strength, closeness, eigenvector or leverage centralities. NetworkToolbox has even more centrality measures that the reader can try. Please refer to the manual of this package for more information about the functions and usage. Nevertheless, as mentioned above, the interpretations of these centralities are not clearly understood or straightforward as strength or expected influence.\n\nDegree <- degree(allNetwork$graph)\nStrength <- strength(allNetwork$graph)\nBetweenness <- betweenness(allNetwork$graph)\nCloseness <- closeness(allNetwork$graph)\nEigenvector <- eigenvector(allNetwork$graph)\nLeverage <- leverage(allNetwork$graph)\ndata.frame(Var = names(Degree), Degree, Strength, Betweenness, Closeness, Eigenvector, Leverage) \n\n\n\n  \n\n\n\nAs we mentioned above, besides the regularized partial correlation networks, several other estimation options are possible. We will demonstrate some here (Figure 19.5). However, interested readers can refer to the manual pages of estimateNetwork() function. First, we can fit a model that is simply an association (i.e. correlation) network to explore the correlations between different variables. The association network is not recommended except for data exploration and is shown here for comparison. Another very interesting estimation method is the ggmModSelect(). This is recommended in large datasets with a low number of nodes. The ggmModSelect algorithm starts by estimating a regularized network as a starting baseline network, then estimates all possible un-regularized networks and selects the best model based on the lowest EBIC criteria. The last model we show here is the relative importance model relimp which estimates a directed network where edges are magnitude of the relative importance of the predictors in a linear regression model. You may notice that the ggmModSelect network is rather similar to the regularized network we estimated above whereas the correlation network is very dense. The relative importance network is directed and shows how each variable is expected to be influencing the other as per the regression results.\n\nallNetwork_cor <- estimateNetwork(allData, default = \"cor\", verbose = FALSE)\nallNetwork_mgm <- estimateNetwork(allData, default = \"ggmModSelect\", verbose = FALSE)\nallNetwork_relimp <- estimateNetwork(allData, default = \"relimp\", verbose = FALSE)\n\n\nplot(allNetwork_cor, title = \"Correlation\", vsize = 18, edge.labels = TRUE,\n     cut = 0.10, minimum = 0.05, layout = LX) \nplot(allNetwork, title = \"EBICglasso\", vsize = 18, edge.labels = TRUE, \n     cut = 0.10, minimum = 0.05, layout = LX)\nplot(allNetwork_mgm, title = \"ggmModSelect\", vsize = 18, edge.labels = TRUE, \n     cut = 0.10, minimum = 0.05, layout = LX)\nplot(allNetwork_relimp, title = \"Relative importance\", vsize = 18, edge.labels = TRUE, \n     cut = 0.10, minimum = 0.05, layout = LX) \n\n\n\n\nFigure 5. Correlation networks with several estimation options\n\n\n\n\n\n\n3.8 Comparing networks\nHaving shown the basic steps of estimation of a single network, we now proceed with comparing across different networks. Psychological networks offer rigorous methods to compare networks as a whole as well as edge weights and centralities using robust methods. Given that our data has two countries, we can estimate two networks for each country and test how they differ.\nFirst, to do the comparison we need to estimate the networks as we did before. The next section repeats the estimation steps for each country. We start with Finland and then Austria. First, the code performs the basic steps of assumption checking for each network. As the results show that the matrix of each of the networks is not positive definite and the goldbricker algorithm does not suggest that there are highly similar nodes that need to be reduced. Then, the next code chunk estimates two regularized partial correlation networks. As we have done before, we estimate the predictability of both networks. The results show that in general the mean predictability is similar in the two networks.\n\n#### Check the assumptions\n### Finland\n## check for positive definitiveness\ncorrelationMatrix <- cor(x = finlandData, use = c(\"pairwise.complete.obs\"))\nis.positive.definite(correlationMatrix)\n\n## check for redundancy\ngoldbricker(finlandData, p = 0.05, method = \"hittner2003\",\n            threshold = 0.25, corMin = 0.5, progressbar = FALSE)\n\n  Suggested reductions: Less than 25 % of correlations are significantly \n  different for the following pairs: \n  [1] \"No suggested reductions\"\n\n### Austria\n## check for positive definitiveness\ncorrelationMatrix <- cor(x = austriaData, use = c(\"pairwise.complete.obs\"))\nis.positive.definite(correlationMatrix)\n\n## check for redundancy\ngoldbricker(austriaData, p = 0.05, method = \"hittner2003\",\n            threshold = 0.25, corMin = 0.5, progressbar = FALSE)\n\n  Suggested reductions: Less than 25 % of correlations are significantly \n  different for the following pairs: \n  [1] \"No suggested reductions\"\n\n##Estimate the networks \nfinlandNetwork <- estimateNetwork(finlandData, default = \"EBICglasso\", \n                                  corMethod = \"cor_auto\", tuning = 0.5)\naustriaNetwork <- estimateNetwork(austriaData, default = \"EBICglasso\", \n                                  corMethod = \"cor_auto\", tuning = 0.5)\n\n\n## Compute the predictability\nfitFinland <- mgm(\n  as.matrix(finlandData), # data \n  c(\"g\" ,\"g\", \"g\", \"g\" ,\"g\" ,\"g\"),  # distribution for each var\n  verbatim = TRUE, # hide warnings and progress bar\n  signInfo = FALSE # hide message about signs\n  )\npredictFinland <- predict(fitFinland, na.omit(finlandData))\n\nmean(predictFinland$errors$R2) # Mean predictability of Finland: 0.3085\nmean(predictFinland$errors$RMSE) # Mean RMSE of Finland: 0.8283333\n\nfitAustria <- mgm(\n  as.matrix(austriaData), # data\n  c(\"g\" ,\"g\", \"g\", \"g\" ,\"g\" ,\"g\"), # distribution for each var\n  verbatim = TRUE, # hide warnings and progress bar\n  signInfo = FALSE # hide message about signs\n  )\npredictAustria <- predict(fitAustria, na.omit(austriaData))\n\nmean(predictAustria$errors$R2) # Mean predictability of Austria: 0.3436667\nmean(predictAustria$errors$RMSE) # Mean RMSE of Austria: 0.8036667\n\n  [1] 0.3085\n  [1] 0.8283333\n  [1] 0.3436667\n  [1] 0.8036667\nNow as the networks were estimated, we plot the networks side by side in the same way we used before (Figure 19.6). It is always a good idea to have a common layout to facilitate interpretation. We can use the function averageLayout() to generate an average layout from the two networks as the first line of the code below shows. However, we will use the layout of the first network (LX) to maintain comparability. Please also note that we use the function qgraph() which is very similar to plot() with more options and arguments. Yet, two small differences: in qgraph() you need to supply the labels as an argument, otherwise qgraph() will use shortened labels. Second, qgraph() requires either an estimated network or matrix, to plot the difference network we subtract the two matrices finlandNetwork$graph-austriaNetwork$graph (not the networks) to get the matrix. The following code plots the three networks using the same layout side by side as well as computes a simple difference network. It is obvious that the two network differ substantially regarding several interactions. Finland has stronger connection between competence and emotion and between motivation and relatedness. Whereas in Austria, there is a stronger connection between motivation and competence, motivation and emotion as well as competence and autonomy and autonomy and relatedness.\n\nAverageLayout <-  averageLayout(finlandNetwork, austriaNetwork)\n\nplot(finlandNetwork,             # input network\n     title = \"Finland\",          # plot title\n     vsize = 19,                 # size of the nodes\n     edge.labels = TRUE,         # label the edge weights\n     cut = 0.10,                 # saturate edges > .10\n     minimum = 0.05,             # remove edges < .05\n     pie = predictAll$errors$R2, # put R^2 as pie\n     layout = LX)                # set the layout\n\nplot(austriaNetwork,             # input network\n     title = \"Austria\",          # plot title\n     vsize = 19,                 # size of the nodes\n     edge.labels = TRUE,         # label the edge weights\n     cut = 0.10,                 # saturate edges > .10\n     minimum = 0.05,             # remove edges < .05\n     pie = predictAll$errors$R2, # put R^2 as pie\n     layout = LX)                # set the layout\n\nqgraph(finlandNetwork$graph - abs(austriaNetwork$graph), \n       title = \"Difference\",                  # plot title\n       theme = allDataPlot$Arguments$theme,   # borrow the theme\n       vsize = 19,                            # size of the nodes\n       edge.labels = TRUE,                    # label the edge weights\n       labels = allDataPlot$Arguments$labels, # node labels\n       cut = 0.10,                            # saturate edges > .10\n       layout = LX)                           # set the layout\n\n\n\n\nFigure 6. Country networks and comparison. The weight of the edges represents the magnitude of the correlation while the color represents the sign: blue por positive and red for negative.\n\n\n\n\nA visual comparison of centralities can be performed in the same way we did before (Figure 19.7). Here, we supply the networks that we want to compare as a list, and we specify the centralities. As the results show, Motivation has the highest centrality value in the Austria network, meaning that motivation is the factor that is expected to drive the connectivity. In the Finland network, competence is the most central variable that drives the network connectivity.\n\ncentralityPlot(\n  list(Finland = finlandNetwork,\n       Austria = austriaNetwork),\n  include = c(\"ExpectedInfluence\", \"Strength\")) \n\n\n\n\nFigure 7. Comparison of centralities plot between countries\n\n\n\n\nYet, to compare the networks in a rigorous way, we need a statistical test that tells which edges or centrality measures are actually different and not due to chance. For such a comparison, NCT (short for Network Comparison Test) is an effective method that allows for a detailed comparison regarding the network structure, edges, and centralities. To do so, NCT uses permutation to generate a large number of networks (based on the original network) as a references distribution and later compares the original networks against the permuted. To perform the test, we supply the networks and the number of iterations (a large number, at least 1000 iteration is recommended). In addition, it is good practice to specify test.edges = TRUE, edges = 'all' to test all edges and test.centrality = TRUE to test the centralities since they are not tested by default. The code and the output are in the next chunk. To check the results, we can obtain the global strength (sum of all edge weights) by the using Compared$glstrinv.sep which is 2.148451 for Finland and 2.1725 for Austria. The difference between global strength Compared$glstrinv.real is 0.02404855 and is statistically insignificant (Compared$glstrinv.pval = 0.735). The maximum difference in any of the edges between the networks (Compared$nwinv.real) is 0.1713064. Compared$einv.real returns the difference matrix (as the difference network we computed above). The Holm-Bonferroni adjusted p-value (which adjusts for multiple comparisons) for each edge can be obtained using Compared$einv.pvals. The results show that most of the edges differed significantly except e.g., Relatedness-Emotion, Relatedness-SRL. Similarly, the difference in centralities can be obtained using Compared$diffcen.real and the p-values using Compared$diffcen.pval which shows for example, that the difference in expected influence centrality of competence was not statistically significant.\n\nset.seed(1337)\nCompared <- NCT(\n  finlandNetwork,         # network 1\n  austriaNetwork,         # network 2\n  verbose = FALSE,        # hide warnings and progress bar\n  it = 1000,              # number of iterations\n  abs = T,                # test strength or expected influence?\n  binary.data = FALSE,    # set data distribution\n  test.edges = TRUE,      # test edge differences\n  edges = 'all',          # which edges to test\n  test.centrality   = TRUE, # test centrality\n  progressbar = FALSE     # progress bar\n  )\n\nCompared$glstrinv.sep # Separate global strength values of the individual networks\n\n[1] 2.148451 2.172500\n\nCompared$glstrinv.real # Difference in global strength between the networks\n\n[1] 0.02404855\n\nCompared$glstrinv.pval # p-value of strength difference\n\n[1] 0.7352647\n\nCompared$nwinv.real # Maximum difference in any of the edges between networks\n\n[1] 0.1713064\n\nCompared$einv.real # Difference in edge weight of the observed networks\n\n            Relatedness Competence   Autonomy    Emotion Motivation        SRL\nRelatedness  0.00000000 0.06812689 0.09520436 0.04142711 0.13714460 0.04672616\nCompetence   0.06812689 0.00000000 0.13478602 0.17130636 0.16045438 0.12581888\nAutonomy     0.09520436 0.13478602 0.00000000 0.07667907 0.08758119 0.01228288\nEmotion      0.04142711 0.17130636 0.07667907 0.00000000 0.12873747 0.05299023\nMotivation   0.13714460 0.16045438 0.08758119 0.12873747 0.00000000 0.03062823\nSRL          0.04672616 0.12581888 0.01228288 0.05299023 0.03062823 0.00000000\n\nCompared$einv.pvals # Holm-Bonferroni adjusted p-values for each edge\n\n          Var1       Var2     p-value Test statistic E\n7  Relatedness Competence 0.017982018       0.06812689\n13 Relatedness   Autonomy 0.001998002       0.09520436\n14  Competence   Autonomy 0.000999001       0.13478602\n19 Relatedness    Emotion 0.165834166       0.04142711\n20  Competence    Emotion 0.000999001       0.17130636\n21    Autonomy    Emotion 0.009990010       0.07667907\n25 Relatedness Motivation 0.000999001       0.13714460\n26  Competence Motivation 0.000999001       0.16045438\n27    Autonomy Motivation 0.003996004       0.08758119\n28     Emotion Motivation 0.001998002       0.12873747\n31 Relatedness        SRL 0.077922078       0.04672616\n32  Competence        SRL 0.000999001       0.12581888\n33    Autonomy        SRL 0.688311688       0.01228288\n34     Emotion        SRL 0.073926074       0.05299023\n35  Motivation        SRL 0.301698302       0.03062823\n\nCompared$diffcen.real # Difference in centralities\n\n               strength expectedInfluence\nRelatedness  0.10476808        0.10476808\nCompetence   0.07001173        0.07001173\nAutonomy    -0.22860961       -0.22860961\nEmotion      0.12670208        0.21366531\nMotivation  -0.20900022       -0.20900022\nSRL          0.08803084        0.17499406\n\nCompared$diffcen.pval # Holm-Bonferroni adjusted p-values for each centrality\n\n               strength expectedInfluence\nRelatedness 0.006993007       0.006993007\nCompetence  0.127872128       0.127872128\nAutonomy    0.000999001       0.000999001\nEmotion     0.009990010       0.000999001\nMotivation  0.000999001       0.000999001\nSRL         0.132867133       0.000999001\n\n\n\n\n3.9 The variability network\nThe variability network offers a good indication of how the edge weights vary across the networks (Figure 19.8). In other words, the range of variability (i.e. the degree of individual differences) across the included population. Edges with low variability are expected to be similar across the networks and vice versa. The following code creates two matrices and then loops across the two networks to compute the standard deviation.\n\n## Construct a network where edges are standard deviations across edge weights of networks\nedgeMeanJoint <- matrix(0, 6, 6)\nedgeSDJoint <- matrix(0, 6, 6)\nfor(i in 1:6){\n  for(j in 1:6) {\n    vector <- c(getWmat(finlandNetwork)[i, j], getWmat(austriaNetwork)[i, j])\n    edgeMeanJoint[i, j] <- mean(vector) \n    edgeSDJoint[i, j] <- sd(vector) \n  } \n} \n\nWe then plot the networks where the edge weights are the standard deviations of all edges (Figure 19.8).\n\nqgraph(edgeSDJoint, layout = LX, edge.labels = TRUE,\n       labels = allDataPlot$Arguments$labels, vsize = 9, \n       cut = 0.09, minimum = 0.01, theme = \"colorblind\")\n\n\n\n\nFigure 8. Variability plot\n\n\n\n\nIn the same way we compared countries, we can compare across genders, and as we see in the next code chunk, we estimated the male network, the female network and the difference network (Figure 19.9). Nonetheless, the differences are really small or even trivial.\n\nmaleNetwork <- estimateNetwork(maleData, default = \"EBICglasso\")\nfemaleNetwork <- estimateNetwork(femaleData, default = \"EBICglasso\")\n\nplot(maleNetwork, title = \"Male\", vsize = 9, edge.labels = TRUE, \n     cut = 0.10, minimum = 0.05, layout = LX) \n\nplot(femaleNetwork, title = \"Female\", vsize = 9, edge.labels = TRUE, \n     cut = 0.10, minimum = 0.05, layout = LX)\n\nqgraph(femaleNetwork$graph - maleNetwork$graph, title = \"Difference\", cut = 0.1,\n       labels = allDataPlot$Arguments$labels, vsize = 9,minimum = 0.01, \n       edge.labels = TRUE, layout = LX, theme = \"colorblind\")\n\n\n\n\nFigure 9. Gender networks and comparison\n\n\n\n\nBelow we perform the network comparison test and we see that the p-values of differences between all edges is statistically insignificant.\n\nComparedGender <- NCT(\n  maleNetwork, # network 1\n  femaleNetwork, # network 2\n  verbose = FALSE,  # hide warnings and progress bar\n  it = 1000, # number of iterations\n  abs = T, # test strength or expected influence?\n  binary.data = FALSE, # set data distribution\n  test.edges = TRUE, # test edge differences\n  edges = 'all', # which edges to test\n  progressbar = FALSE) # progress bar\n\nComparedGender$einv.pvals # Holm-Bonferroni adjusted p-values for each edge\n\n\n\n  \n\n\n\n\n\n3.10 Evaluation of robustness and accuracy\nThe most common procedure to evaluate the stability and accuracy of the of the estimated networks is bootstrapping, in which a large number (1000 or more) of bootstrapped networks are created based on the original data. The resulting edge weights of the bootstrapped networks are then used to create confidence intervals to assess the accuracy of the edges. Each edge weight in the estimated networks is contrasted to the confidence intervals of the bootstrapped edges. Edges where the upper and lower bounds do not cross zero are considered statistically significant and edges that either the upper or lower bound of the confidence interval crosses the 0 line are considered not significant.\n\nnCores <- parallel::detectCores() - 1\n## Non-parametric bootstrap for stability of edges and of edge differences\n\nallBoot <-  bootnet(\n  allNetwork,                # network input\n  default = \"EBICglasso\",    # method\n  nCores = nCores,           # number of cores for parallelization\n  computeCentrality = FALSE, # estimate centrality?\n  statistics = \"edge\"        # what statistics do we want?\n  )\n\n\nplot(allBoot, plot = \"area\", order = \"sample\", legend = FALSE)\n\n\n\n\nFigure 10. Edge weights\n\n\n\n\nAs Figure 19.10 shows, only the edges of autonomy-emotion and emotion-SRL are crossing the 0 line and therefore, are insignificant. We can also plot the edge difference plot which tests if the edge weights are different from each other (Figure 19.11). As the figure shows, edges where the 95% bootstrapped confidence interval of the difference between any pair of edges crosses the zero line, the square is grey. The square is black if the edge difference does not cross the 0 (significant). For instance, autonomy-emotion and emotion-SRL have a grey square indicating non-significant difference whereas emotion-SRL and relatedness-SRL has a black square indicating that the two nodes are statistically significantly different.\n\nplot(allBoot, plot = \"difference\", order = \"sample\", onlyNonZero = FALSE, labels = TRUE)\n\n\n\n\nFigure 11. Differences between edges\n\n\n\n\nThe accuracy of centrality is assessed by the case dropping test. In the case dropping test, several proportions of cases are dropped from the data and the correlation between the observed centrality measure and those obtained from the subsetted data is calculated. If the correlation dropped markedly after dropping a small subset of the cases, then the centrality measure is unreliable.\n\nset.seed(1)\ncentBoot <- bootnet(\n  allNetwork,                # network input\n  default = \"EBICglasso\",    # method\n  type = \"case\",             # method for testing centrality stability\n  nCores = nCores,           # number of cores\n  computeCentrality = TRUE,  # compute centrality\n  statistics = c(\"strength\", \"expectedInfluence\"),\n  nBoots = 19000,            # number of bootstraps\n  caseMin = .05,             # min cases to drop\n  caseMax = .95              # max cases to drop\n  )\n\nThe correlation stability coefficient is a metric that is used to judge the stability of the centrality measure using the case dropping test and is estimated as the maximum drop to retain retain 0.7 of the sample.\n\ncorStability(centBoot)\n\nIf we plot the results (Figure 19.12), we can see that the correlation stability coefficient is 0.95 which is very high indicating the stability of the edges.\n\nplot(centBoot)\n\n\n\n\nFigure 12. Average correlation stability coefficient\n\n\n\n\n\n\n3.11 Discussion\nThe field of psychological networks is growing fast and methods are refined at a fast speed. In the current chapter, we have tried to show the basic steps of analyzing a psychological network, visualizing the results and comparing different networks. We have also shown how networks can be compared using robust statistical methods. Furthermore, we also showed how to test the accuracy of the estimated networks using the bootstrapping methods.\nAn important question here is how psychological networks compare to other methods that are prevalent in the educational field e.g., Epistemic Network Analysis (ENA). In ENA, there is no way to test if the network edges are different from random, there is no rigorous method for comparison of networks or verifying the edge weights. Also, there are no centrality measures or network measures. In fact, ENA loses all connection to network methods and therefore, the usual methods for verifying, randomization or computing of network measures do not apply in ENA [50]. The same can be said about process mining which produces transition networks. In process mining, there are few confirmatory tests that verify the resulting model or rigorously compare across process models. Perhaps social networks analysis (SNA) is the closest to psychological networks. However, SNA is limited to —or has been commonly used with–– limited types of edges (e.g., co-occurrence, reply or interactions); these edges are almost always unsigned (i.e., are always positive) and have been limited to either social interactions or semantic interactions [25, 51, 52].\nIn sum, psychological networks offer far more wider perspectives into interactions —or interdependencies— among variables with a vast number of possible estimation methods and optimization techniques. Furthermore, psychological networks have a vibrant community who refine and push the boundary of the existing methods [53]. All of such advantages make psychological networks a promising method for modeling complex systems, understanding interactions and structure of psychological constructs as we demonstrated here [3, 54]. Furthermore, psychological networks require no prior theory or strong assumptions about the modeled variables and can serve as powerful analytical methods. As Borsboom et al. states psychological networks “form a natural bridge from data analysis to theory formation based on network science principles” and therefore can be used “to generate causal hypotheses” [29].\nThe book is a comprehensive reference for psychological networks from theory, to methods and estimation techniques. The following papers offer excellent guides and tutorials:\n\nEpskamp, S., Borsboom, D., & Fried, E. I. (2018). Estimating psychological networks and their accuracy: A tutorial paper. Behavior research methods, 50, 195-212.\nEpskamp, S., & Fried, E. I. (2018). A tutorial on regularized partial correlation networks. Psychological methods, 23(4), 617.\nVan Borkulo, C. D., van Bork, R., Boschloo, L., Kossakowski, J. J., Tio, P., Schoevers, R. A., … & Waldorp, L. J. (2022). Comparing network structures on three aspects: A permutation test. Psychological methods.\nBorsboom, D., Deserno, M. K., Rhemtulla, M., Epskamp, S., Fried, E. I., McNally, R. J., … & Waldorp, L. J. (2021). Network analysis of multivariate data in psychological science. Nature Reviews Methods Primers, 1(1), 58.\nBringmann, L. F., Elmer, T., Epskamp, S., Krause, R. W., Schoch, D., Wichers, M., … & Snippe, E. (2019). What do centrality measures measure in psychological networks?. Journal of abnormal psychology, 128(8), 892."
  },
  {
    "objectID": "chapters/ch20-factor-analysis/ch20-factor.html",
    "href": "chapters/ch20-factor-analysis/ch20-factor.html",
    "title": "20  Factor Analysis in Education Research Using R",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch20-factor-analysis/ch20-factor.html#introduction",
    "href": "chapters/ch20-factor-analysis/ch20-factor.html#introduction",
    "title": "20  Factor Analysis in Education Research Using R",
    "section": "1 Introduction",
    "text": "1 Introduction\nEducational scientists are usually after the invisible: sense of belonging, intelligence, math ability, general aptitude, student engagement…the list of crucial, multifaceted, but not directly observable student characteristics goes on. But how do researchers measure and study what they cannot see? One solution is not looking for the invisible cause itself but for its consequences. Just like one cannot see the wind but can tell it is there from the moving leaves, researchers can indirectly infer student engagement by looking at observable aspects of students' online behavior. For example, the more engaged students are, the more effort they invest in learning (i.e., longer online time), and the more regular and frequent they post on fora. These observable variables are the gateway to the underlying engagement construct that is theorized to drive students to be engaged, and factor analysis is the key to opening that gate. If scores on a set of observed variables are all caused by the same underlying construct of interest, the relations between these variables (i.e., the covariances that indicate how increases/decreases in one behavior are related to increases/decreases in another) are an expression of this underlying construct. This is exactly the core tenet of factor analysis. The method examines the covariances between the observed variables and, from this information, extracts the underlying unobserved or latent construct (as well as students' relative scores on it).\nThe inventor of factor analysis is Spearman, who was trying to make sense of the fact that tests of widely varying cognitive abilities all positively correlated with each other. He reasoned that the cause of this was an underlying construct (or factor) called \"general intelligence\" that was causing people's performance on all of those tests [1]. Spearman's work on factor analysis was later extended by Thurstone, who believed that people's performance was influenced by more than just one latent dimension. Thurstone [2], therefore, expanded factor analysis to enable the extraction of multiple underlying constructs based on the covariances between variables. The extension allowed performance on a math test to, for example, be influenced both by math ability (the key ability that should be measured) and reading ability (the ability to accurately understand questions on the math test in order to answer them correctly).\nJöreskog introduced the final major addition to factor analysis. Although Spearman's and Thurstone's versions of factor analysis already allowed for exploring the factor structure for a given dataset, it was not yet possible to confirm if the factor structure fit well to the data and, thus, if the covariances between variables implied by the factor structure match the observed covariances in the dataset. Jöreskog [3] figured out how to estimate factor models in a way that made this possible. An added benefit of this estimation method was that it allowed for factor models in which observed variables (e.g., behaviors, tasks, or questionnaire items) were not influenced by all the assumed underlying dimensions but, for example, only by one, which was not possible in the methods of Spearman and Thurstone. This extension allowed adding theory and/or practical experience to the factor analysis. For instance, one could test the hypothesis that students' math ability consists of the separate sub-dimensions addition, subtraction, division, and multiplication by letting all tasks of a math test that involve addition belong to just one underlying addition factor, all the tasks involving multiplication to just one underlying multiplication factor, et cetera. If this multidimensional model of math ability fits the data well, students can subsequently be evaluated on each of these dimensions separately (instead of on an overall math ability factor). The approach in which researchers confirm if a specific factor model (i.e., for which both the number of underlying dimensions and the pattern of relations between these dimensions and the observed variables) fits to the data is nowadays called confirmatory factor analysis (CFA). In contrast, the more data-driven approach in which the number of underlying constructs is inferred from the data and all underlying constructs are assumed to influence all observed variables is called exploratory factor analysis (EFA).\nNowadays, both EFA and CFA are readily available in modern statistical (open-source) software and applied regularly across the social sciences in general and educational research in particular. For example, Krijnen et al. [4] used EFA to refine a typology of home literacy activities. They cautiously anticipated four hypothetical categories of home literacy activities (oral language exposure, oral language teaching, code skills exposure, and code skills teaching activities). However, since the authors did not have a strong theory or prior factor-analytical results to support this anticipated factor structure, they refrained from testing if that specific factor structure fit to the data with CFA and instead used the more data-driven EFA approach. Their results suggested that there were actually three factors underlying the observed variables in their dataset (oral language teaching, oral language exposure, and general code activities).\nIn contrast, Hofhuis et al. [5] used factor analysis to validate a shortened version of the Multicultural Personality Questionnaire (MPQ; [6]), a measure of intercultural competence, among a sample of students in an international university program. The authors wanted to determine if this short form of the MPQ (MPQ-SF) could be used instead of the longer original and if item scores of both versions of the questionnaire were influenced by the same five theorized aspects of inter-cultural competence (cultural empathy, emotional stability, flexibility, openmindedness, and social initiative). Since previous research on the original MPQ provided insight into both the number of underlying factors and the specific relations between these factors and the set of items retained on the MPQ-SF, the authors used CFA in this study. They found that the factor structure of the MPQ-SF fit well to the data and that the structure was thus comparable to that of the original MPQ.1\nIn the above, it was stressed that CFA is used when researchers have an idea about the factor structure underlying the data that they want to confirm, while EFA is used more exploratively when researchers have less concrete guiding insights for specifying the model. In practice, CFA and EFA are both used in confirmatory as well as exploratory settings and often even in the same study. Even if researchers have a well-substantiated idea about the number of constructs underlying their observations, they can use EFA to see if the number of factors found by this analysis matches their hypothesis. Similarly, researchers with competing theories about the factors underlying their observed behaviors can still use CFA to explicitly compare these competing models in terms of fit. Flora and Flake [8] discuss how neither EFA nor CFA is purely confirmatory or exploratory in more detail, arguing that, in essence, it comes down to one's specific research context. This will not further be discussed in this chapter. Instead, an integrated use of EFA and CFA often encountered in Educational Sciences is presented. The presentation is kept applied and focuses on conducting factor analysis in R. For more (technical) details, see the readings listed at the end of the chapter."
  },
  {
    "objectID": "chapters/ch20-factor-analysis/ch20-factor.html#literature-review",
    "href": "chapters/ch20-factor-analysis/ch20-factor.html#literature-review",
    "title": "20  Factor Analysis in Education Research Using R",
    "section": "2 Literature review",
    "text": "2 Literature review\nSeveral examples of factor analysis exist in learning analytics, which can be grouped broadly under two categories: factor analysis of self-reported data instruments (e.g., surveys) and factor analysis to explore students' online data. Analysis of self-reported instruments seems to be most widely used within the emerging research field of learning analytics. For instance, Whitelock‐Wainwright et al. [9] used EFA and CFA to validate an instrument that measures students' expectations of ethics, privacy, and usage of their learning analytics data. The analysis suggested a two-factor model that represented two categories of variables of ideal and predicted expectations of learning analytics. Similarly, Oster et al. [10] used EFA to validate an instrument that measures learning analytics readiness among institutions. The authors found that a five-factor model best represents the data (with the dimensions data management expertise, data analysis expertise, communication and policy application, and training). Similarly, factor analysis has been used to create an instrument for measuring variables influencing learning analytics dashboard success. For instance, Park and Jo [11] measured learning analytics dashboard success through an instrument based on Kirkpatrick's four evaluation levels (i.e., Reaction, Learning, Behavior, and Results). Through EFA, they found that five dimensions were more appropriate than four, as suggested by the original instrument, which CFA later confirmed. Similarly, Kokoç and Kara [12] used the Evaluation Framework for Learning Analytics to evaluate the impact of a learning analytics dashboard on learner performance. After conducting CFA, they found that the three-factor model of the Evaluation Framework for Learning Analytics for learners provided the best model fit for the collected data, confirming the structure of the original instrument.\nBesides the aforementioned traditional examples, factor analysis has also been used in learning analytics with students' online data logs. For instance, Hernández-García et al. [13] used factor analysis to identify predictors derived from data about students' interactions. The authors found that a three-factor model best represented students' interaction variables (with dimensions groups' team message exchanges, distribution of postings, and reciprocity of interactions). In that case, factor analysis helps find groups of predictors, understand their underlying structure, and reduce dimensionality. Similarly, Fincham et al. [14] used EFA and CFA to build a theorized model of engagement based on three groups of variables derived from online log data, analysis of sentiments, and metrics derived from the discourse of online posts. Others have applied similar approaches to study the structure of log data. For instance, Baikadi et al.[15] applied EFA to learner activities within four online courses to identify emergent behavior factors. The findings suggested a four-factor model including activity, quiz activity, forum participation, and participation in activities. Another example is the work by Wang [16], who used factor analysis for dimensionality reduction of log data, deriving predictors of learning performance from students' fine-grained actions on the learning management systems.\nThe remainder of this chapter first provides a brief description of the factor analysis model, the model that is at the basis of both EFA and CFA. Second, the steps needed to perform factor analysis in R are presented by applying EFA and CFA to education data that is openly available. The application begins with data preparation, requirements, and initial checks. Additionally, it is shown how to set aside a holdout sample from the original dataset (which is necessary for establishing the generalizability of results from an EFA and/or CFA to future samples, as explained below). After these preliminary steps, it is shown how to run an EFA and interpret the outcomes. The part ends with a thorough description of how to do a CFA and assess generalizability. The chapter ends with a discussion and recommendation for further reading."
  },
  {
    "objectID": "chapters/ch20-factor-analysis/ch20-factor.html#recap-of-the-factor-analysis-model",
    "href": "chapters/ch20-factor-analysis/ch20-factor.html#recap-of-the-factor-analysis-model",
    "title": "20  Factor Analysis in Education Research Using R",
    "section": "3 Recap of the Factor Analysis Model",
    "text": "3 Recap of the Factor Analysis Model\nFactor analysis can be seen as a set of (multiple) linear regression models where each observed variable is regressed on one or more latent factors (see Figure 1 in [17]). Like in regression, researchers get regression weights, intercepts, and error terms. They just get a set of these parameters for each observed variable in the analysis. The regression weights are referred to as loadings in factor analysis (the straight arrows in Figure 21.1) and indicate how strongly the observed variables are related to the underlying factors. The intercepts (not explicitly indicated in Figure 21.1) are the expected scores on the observed variables when the factor means are equal to zero. Finally, the error terms (the \\(\\epsilon\\)'s in Figure 21.1) capture the variance unexplained by the factors and, thus, the unique variance of the individual observed variables. The difference between the factor analysis model and a regular regression model is that the values of the factors are unobserved. Therefore, estimating a factor analysis model is more complicated than estimating a regular regression analysis model with observed predictors and requires specialized software.\n\n\n\nFigure 1. A basic 1-factor model. Each observed variable (X1 – X3) is regressed on the factor. Straight arrows indicate factor loadings (the regression weights obtained by regressing the observed variable on the factor). The \\(\\epsilon\\)'s represent errors. Curved single-headed arrows indicated variances. Each observed variable also has an intercept, but these are not explicitly indicated in the figure.\n\n\nAs mentioned in the introduction, there are two types of factor analyses: Exploratory factor analysis (EFA; Figure 20.2) and confirmatory factor analysis (CFA; Figure 20.3). The key difference is that all loadings (i.e., all variable-factor relations) are estimated in EFA. As a result, variables may load on more than one factor (referred to as cross-loadings). In contrast, in CFA, several loadings are set to zero (as thus not estimated) based on researchers' a priori hypotheses about which variables are unrelated to the underlying factors. Therefore, the CFA model is considered a restricted version of the EFA model.\n\n\n\nFigure 2. An EFA Model with two latent factors. Each observed variable (X1 - X6) is influenced by both factors. Straight arrows indicate factor loadings. Loadings for Factor 2 are depicted with dashed lines for visual clarity. The \\(\\epsilon\\)'s represent errors. Curved single-headed arrows indicate variances and curved double-headed arrows indicate covariances.\n\n\nAnother difference is the determination of the number of factors. In EFA, the number of underlying factors is determined using a data-driven approach; that is, the likely number of underlying factors for the sample data is first estimated, for example, using parallel analysis [18]. Researchers then fit a set of factor models, with the number of underlying factors in these models based on the parallel analysis result. For example, if the parallel analysis suggests 5 factors, researchers can fit models with 4, 5, and 6 underlying factors. These three models are then compared in terms of both model fit—using the Bayesian information criterion [19]—and in terms of the interpretability of the models (i.e., “Do the relations between the factors and the observed variables they load on make sense?”). All this will be discussed in the Section “Factor Analysis in R” of this chapter.\nIn contrast, the number of factors in CFA is determined based on strong a priori hypotheses. When researchers have multiple competing hypotheses, each can be translated into a separate CFA model. These models can then again be compared in terms of how well they fit the data. The hypothesis underlying the best-fitting model can be considered the most likely explanation of the data according to the sample at hand.\n\n\n\nFigure 3. A CFA Model with two latent factors. Each observed variable (X1 - X6) is influenced by only one of the underlying factors. Straight arrows indicate factor loadings. Loadings for Factor 2 are depicted with dashed lines for visual clarity. The \\(\\epsilon\\)’s represent errors. Curved single-headed arrows indicate variances and curved double-headed arrows indicate covariances."
  },
  {
    "objectID": "chapters/ch20-factor-analysis/ch20-factor.html#integrated-strategy-for-a-factor-analysis",
    "href": "chapters/ch20-factor-analysis/ch20-factor.html#integrated-strategy-for-a-factor-analysis",
    "title": "20  Factor Analysis in Education Research Using R",
    "section": "4 Integrated Strategy for a Factor Analysis",
    "text": "4 Integrated Strategy for a Factor Analysis\nAs described in the introduction, there is no clear delineation between when to use either EFA or CFA, and both methods often co-occur within the same study. Therefore, this section provides a detailed description of a principled modeling strategy that integrates both EFA and CFA. More specifically, the three steps that researchers should go through whenever latent constructs are part of their research (either because the instrument is the main focus of the study or because the latent construct is a predictor or outcome in an analysis) are discussed: (1) exploring the factor structure, (2) building the factor model and assessing fit, and (3) assessing generalizability. As a starting point, it is assumed that the researcher has already completed the initial instrument development phase for a construct of interest such as inter-cultural competence (i.e., that the researcher is using an instrument with variables/tasks/behaviors from previous research, has adjusted an instrument from previous research (e.g., by shortening, extending, translating, or revising content), or has newly developed an instrument (e.g., based on theory)). Furthermore, it is assumed that the researcher has gathered data from their population of interest (e.g., students).\n\n4.1 Step 1: Exploring the Factor Structure\nOnce the variables/tasks/behaviors have been selected and data on them have been obtained using a sample from the population of interest, you, the researcher, should start with an EFA. If a previously validated instrument is used, or if strong prior hypotheses about the underlying factor structure of the instruments are available, you should investigate whether the number of factors and the way the variables load on the factors are in line with anticipated results. Thus, the key questions are “Do variable scores believed to be caused by the same underlying factor indeed all load on the same factor?” and, if only a single underlying factor is assumed to cause the scores on a set of variables, “Do these variables indeed have strong factor loadings with only a single factor?”\nIf a new instrument is the starting point, you should determine if the factors and the pattern of loadings can be interpreted. The key questions are “Do all variables (primarily) loading on the same factor indeed have something in common substantively?” and “Are variables that load on different factors indeed qualitatively different somehow?”\nReferring back to the example about a math test, you could see that tasks involving addition, subtraction, division, and multiplication loaded on 4 distinct factors (which could then preliminarily be identified as addition-, subtraction-, division-, and multiplication ability), with each set of tasks being primarily influenced by a single factor. At this stage, you may have to make some adjustments, like removing variables without substantial loadings (e.g., loadings smaller than an absolute value of .3) on any dimensions and reestimating the EFA. Note that you should always think about the reasons for low loadings (e.g., because of an ambiguously formulated item) and not just remove variables.\n\n\n4.2 Step 2: Building the Factor Model and Assessing Fit\nAfter choosing a model with EFA, you need to refine this model and use CFA to assess the model fit (and thus how well the covariances between variables implied by the factor structure match the observed covariances in the dataset). In the EFA in the previous step, all variables were allowed to load on all factors, but often, you have theoretical or (prior) empirical information that you want to include in your analyses, and that restricts the number of these cross-loadings. In this model-building step, you could remove all factor-variable relations (i.e., factor loadings) that do not fit your theory or make substantive sense, but attention must be given to the size of the factor loadings. Close-to-zero factor loadings can be removed relatively risk-free, but larger factor loadings require more careful consideration. Even if these factor-variable relations do not make substantive sense (straight away), the data tell you they should be there. So consider them carefully; if, after closer inspection, they can be incorporated into your prior theory or assumptions, you might want to keep them; if not, you can always remove them and see if the model still fits your data.\nAfter selecting which variable-factor relationships should be removed, you can build the corresponding CFA model and fit it to the data to determine if it fits sufficiently well. If the model does not fit well, you can return to your EFA results and see if you might have to allow additional non-zero loadings (or apply other modifications discussed further below). Note that you should only add relationships to the model that make substantive sense. This process may require several rounds of adjustments until your model fits well and is substantively sound.\n\n\n4.3 Step 3: Assessing Generalizability\nAfter the previous step, you have a preliminary model that fits well with both your new or existing theory and the current data. However, since the ultimate goal should be to present instruments that can be used to measure constructs in future studies, it is essential that you also establish that your preliminary model fits new yet unknown data and, thus, that your model does not describe only your initial sample properly but also other samples from your population. Therefore, in the final step, the preliminary model must be fitted to a new dataset from the same population as the data used to build the preliminary model. This final validation step can be referred to as cross-validation.\nTo assess the generalizability, one could collect a second dataset. However, in practice, gathering data more than once for instrument development purposes is often unfeasible or undesirable. A suitable alternative is to randomly split one dataset into two parts: one sample on which you perform Steps 1 (exploring the factor structure using EFA) and 2 (building the factor model and assessing fit with CFA) and one so-called holdout sample, which you set aside for Step 3 (assessing generalizability). If the CFA model fits the holdout sample, you can be more confident that your instrument can be used in future studies and settle on it as your final model. On the other hand, if the preliminary model does not fit the holdout sample well, you have to conclude that you have not found an adequate instrument yet. In that case, the sources of misfit between the CFA and the holdout sample need to be investigated (more on this below when local fit and modification indices are discussed), and findings from this inspection need to be used to update your theory/model. This updated theory/model then needs to be investigated again by going through all the steps discussed above on a new (split) dataset.\nThe above steps present a suitable factor modeling strategy for any study using instruments to measure latent dimensions. The only situation in which you could theoretically fit a CFA model immediately and assess its fit is when using an existing instrument on a sample from a population for which the instrument had already been validated in previous research. However, even in that situation, going through all the above steps is advisable because your sample might differ in important ways from the ones on which the instrument was validated, which could bias your results."
  },
  {
    "objectID": "chapters/ch20-factor-analysis/ch20-factor.html#factor-analysis-in-r",
    "href": "chapters/ch20-factor-analysis/ch20-factor.html#factor-analysis-in-r",
    "title": "20  Factor Analysis in Education Research Using R",
    "section": "5 Factor Analysis in R",
    "text": "5 Factor Analysis in R\nIn the following, you are taken through the essential steps of investigating the factor structure using both EFA and CFA in the open-source software R. In describing the steps, the following is addressed: checking data characteristics for suitability for EFA/CFA, deciding on the number of factors, assessing global and local model fit, and evaluating the generalizability of the final factor model. To this end, a dataset is used to which factor analysis has been applied before [20]. The dataset contains survey data about teacher burnout in Indonesia. In total, 876 respondents have answered questions on five domains: Teacher Self-Concept (TSC, 5 questions), Teacher Efficacy (TE, 5 questions), Emotional Exhaustion (EE, 5 questions), Depersonalization (DP, 3 questions), and Reduced Personal Accomplishment (RPA, 7 questions). Thus, the total number of variables equals 25. The questions were assessed on a 5-point Likert scale (ranging from 1 = “never” to 5 = “always”). For more information on the dataset, see the data chapter of this book [21].\nThe first section shows the necessary preparation, which involves reading in the data, evaluating whether the data are suited for factor analysis, and setting apart the holdout sample needed for assessing the generalizability. The next two sections show how to conduct an EFA to arrive at a preliminary factor model/factor structure (Step 1) and refine this model using CFA (Step 2). The final section shows how to test the generalizability of the refined factor model using cross-validation (Step 3).\n\n5.1 Preparation\nIn order to follow all the steps, you have to install the following packages with install.packages()(you only have to install packages the first time you want to use them; therefore, the commands are hashtagged below) and load them with library() whenever you open the R script.\n\nlibrary(lavaan) # install.packages(\"lavaan\")\nlibrary(psych) # install.packages(\"psych\")\nlibrary(semTools) # install.packages(\"semTools\")\nlibrary(effectsize) # install.packages(\"effectsize\")\nlibrary(rio) # install.packages(\"rio\")\nlibrary(tidyr) # install.packages(\"tidyr\")\nlibrary(ggplot2) # install.packages(\"ggplot2\")\nlibrary(devtools) # install.packages(\"devtools\")\nlibrary(sleasy) # install.packages(\"combinat\"); install_github(\"JoranTiU/sleasy\")\n\n\n5.1.1 Reading in the data\nThe data can be read in, and the variable names can be extracted with the following commands: \n\ndataset <- import(\"https://github.com/lamethods/data/raw/main/4_teachersBurnout/2.%20Response.xlsx\")\nvar_names <- colnames(dataset)\n\nIf not all variables in the dataset should be used for the factor analysis, you should only add the relevant variables to the var_name object. The commands below will then take this into account automatically.\n\n\n5.1.2 Are the data suited for factor analysis?\nSeveral data characteristics are necessary for both EFA and CFA. First, the variables must be continuous. Variables are seldom truly continuous, but they can be treated as such if they were assessed on a scale with at least five response categories [22]. If the variables are not continuous, factor analysis can still be conducted, but a specific type of estimation for categorical data is required. Note that this is beyond the scope of this chapter (interested readers are referred to [23]). Moreover, the scale on which the observed variables are assessed should be the same, which may not hold in certain educational data. If the variables have been measured on different scales, or if the variables are measured on the same scale, but the range of observed scores on the variables differs substantially between variables (e.g., some variables have scores ranging from 1 to 5, while others have scores ranging only from 2 to 4), the variables should be transformed before the factor analysis to make their scales more comparable. The following command can be used to inspect each variables’ range:\n\ndescribe(dataset)\n\nAll variables were assessed on 5-point Likert scales, and from the output, you can see that all variables have very similar observed score ranges. Therefore, you can treat them as continuous, and transformation is not necessary (for information on how to transform data in R, see [24]).\nSecond, the sample size needs to be sufficiently large. There are several rules of thumb in the literature. Some simply state that a sample size of about 200 should be targeted, although smaller samples may be sufficient for simpler models (e.g., models with fewer factors and/or stronger relations between the factors and observed variables), while more complicated models (e.g., models with more factors and/or weaker relations between the factors and observed variables) will require larger samples. Other rules are based on the ratio between sample size and the number of estimated parameters (i.e., factor loadings, intercepts, and error variances). Bentler and Chou [25] recommend having 5 observations per estimated parameter, while Jackson [26] recommends having 10, and preferably 20 observations, for each parameter you want to estimate (e.g., for a one-factor model with 10 variables, one should aim for 30 (10 factor-loadings + 10 intercepts + 10 error-variances) * 10 = 300 cases. Remember that these recommendations are for the data the model is fitted to. Since you also need a holdout sample to assess the generalizability of your model, you need to have about twice the number of observations! The sample size can be assessed by asking for the number of rows in your dataset with the following command:\n\nnrow(dataset) # 876\n\n[1] 876\n\n\nFor the example data with 25 variables that are assumed to measure 5 latent constructs, you have to estimate 25 intercepts, 25 residual variances, and 5 * 25 = 125 factor loadings. This results in a total of 175 parameters. Looking at the output, you can conclude that the sample size is sufficiently large for both EFA and CFA according to the guidelines by Bentler and Chou [25] (5 * 175 = 875) but not those of Johnson [27]. Since this dataset does not have twice the recommended sample size, you should not set aside a holdout sample and save the validation of the model for a future study. However, for illustration purposes, it will nevertheless be shown how to create a holdout subset for evaluating the generalizability of the final factor model.\nNext, there need to be sufficiently large correlations between the variables. Otherwise, there is no point in looking at the factor structure. You can rule out that the variables in the dataset are uncorrelated with Bartlett’s test [28], which tests whether the correlation matrix is an identity matrix (a matrix with off-diagonal elements equal to zero) and, thus, whether the variables are uncorrelated. The null hypothesis of this test is that the correlation matrix is an identity matrix. If the null hypothesis is rejected, it can be concluded that the variables are correlated and, thus, that you can continue with the factor analysis. With the following command, you test whether the p-value for the Bartlett’s test is smaller than an alpha level of 0.05 and, thus, if the null hypothesis of “no correlation between variables” can be rejected:\n\n(cortest.bartlett(R = cor(dataset[, var_names]), n = nrow(dataset))$p.value) < 0.05 \n\n[1] TRUE\n\n\nWith the argument “R”, you provide the correlation matrix for the data (specifically for the variables that shall be part of the factor analysis), and with the argument “n” you determine the sample size, which is equal to the number of rows. The p-value is indeed smaller than 0.05. Thus, the variables correlate.\nIn addition to checking for correlations between variables, it is also relevant to determine if there is enough common variance among the variables. You can assess this with the Kaiser-Meyer-Olkin (KMO) test [29]. The KMO statistic measures what proportion of the total variance among variables might be common variance. The higher this proportion, the higher the KMO value, and the more suited the data are for factor analysis. Kaiser [29] indicated that the value should be at least .8 to have good data for factor analysis (and at least .9 to have excellent data). With the following command, you obtain the results:\n\nKMO(dataset)\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = dataset)\nOverall MSA =  0.94\nMSA for each item = \nTSC1 TSC2 TSC3 TSC4 TSC5  TE1  TE2  TE3  TE4  TE5  EE1  EE2  EE3  EE4  EE5  DE1 \n0.96 0.96 0.95 0.94 0.96 0.93 0.96 0.94 0.94 0.96 0.95 0.94 0.95 0.94 0.97 0.87 \n DE2  DE3 RPA1 RPA2 RPA3 RPA4 RPA5 \n0.86 0.92 0.91 0.91 0.95 0.94 0.96 \n\n\nThe overall KMO value equals 0.94. Thus, the data are excellent for data analysis.\nNext to the necessary data characteristics, you need to be aware of the non-normality of the variables and missing data. A robust estimation method is required if the variables are not normally distributed. If the data contain missing values for one or more variables, this must also be accounted for in the estimation. How to do this will be described below. Normality can be assessed by inspecting the variables’ histograms2:\n\ndataset |> pivot_longer(2:ncol(dataset), \n names_to = \"Variable\", values_to=\"Score\") |>\n   ggplot(aes(x=Score)) + geom_histogram(bins=6) +\n     scale_x_continuous(limits=c(0,6),breaks = c(1,2,3,4,5)) +\n     facet_wrap(\"Variable\",ncol = 4,scales = \"free\" ) + theme_minimal()\n\nWarning: Removed 44 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\nFigure 4. Histogram of the responses to each of the items\n\n\n\n\nLooking at Figure 20.4, you can see that the distribution of the variables is somewhat left-skewed. Therefore, an estimation method that is robust against non-normality should be used.\nNext, you can check for missing data with the following command:\n\ncolSums(is.na(dataset))\n\nTSC1 TSC2 TSC3 TSC4 TSC5  TE1  TE2  TE3  TE4  TE5  EE1  EE2  EE3  EE4  EE5  DE1 \n   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0 \n DE2  DE3 RPA1 RPA2 RPA3 RPA4 RPA5 \n   0    0    0    0    0    0    0 \n\n\nThere are no missing observations for any of the variables in this dataset.\n\n\n5.1.3 Setting a Holdout Sample Apart\nOnce you know that the data are suited for factor analysis, you can consider setting a holdout sample apart to assess the generalizability of your findings, as explained before. However, it is important to consider the sample size in your decision. As indicated above, the minimum required sample size should at least be 5 (and preferably 10 or 20) times the number of parameters you will estimate. Do not set a holdout sample apart unless your sample size is approximately twice as large as the minimum required sample size (or larger). Otherwise, you will not have enough data to build an appropriate model in the first place. The validation of the final model then needs to be done in future research. Note, however, that the number of parameters for a CFA model is generally smaller than that for an EFA model. Therefore, it is okay if the holdout sample is somewhat smaller than the model building sample.\nAs was already determined above, the sample size was not twice the minimum required sample size for a model with 25 variables and 5 latent factors, but for illustrative purposes, a holdout sample is set apart nevertheless. To this end, you can randomly assign 438 rows to a model-building and holdout dataset. For this, you can use the following commands:\n\nset.seed(19)\nind <- sample (c (rep(\"model.building\", 438), rep (\"holdout\", 438)))\ntmp <- split (dataset, ind)\nmodel.building <- tmp$model.building\nholdout <- tmp$holdout\n\nWith the first line of code, you set a seed. Setting a seed ensures that you get the same random sequence of elements every time you rerun the code, which is crucial for replicating your results. Then, you create a vector “ind” that contains 438 times the terms “model.building” and “holdout”, respectively, in random order. This gives you a total of 876 classifications, one for each participant (i.e., row) in your data. Subsequently, you create a temporal list termed “tmp”, which contains two datasets: for each row number, the function split() checks whether it is assigned the label “model.building” or “holdout” and assigns this row to the respective dataset accordingly. For example, suppose the vector “ind” has as the first three elements “model.building”, “model.building”, and “holdout”. In that case, the first two rows of the dataset are assigned to the model-building dataset, and the third observation is assigned to the holdout dataset. In the last step, the two new datasets are extracted from the list and stored in objects named “model.building” and “holdout”. You will use the model-building data for all the following analyses until the section “Assessing Generalizability”.\n\n\n\n5.2 Step 1: Exploring the Factor Structure\nThe first step in exploring the factor structure is to determine how many dimensions are likely underlying the construct of interest. In this tutorial, you will see how to determine this using a combination of two commonly used data-driven approaches: parallel analysis [18] and the Bayesian information criterion (BIC; [19]). These two methods complement each other greatly: Parallel analysis indicates a range for the number of underlying dimensions, and the BIC tells us which specific number of dimensions from this range fits the data best. Parallel analysis is a simulation-based method that chooses the number of factors by comparing the amount of variance explained by a certain number of factors in your data (with this amount of variance explained by each factor being called the factor’s eigenvalue) to the amount of information that the same number of factors would explain on average across many parallel simulated datasets (i.e., with the same number of variables and observations), but with zero correlations between all variables. This comparison allows for testing if the amount of explained variance in your data is larger than expected based on purely random sampling error alone. The number of factors chosen by parallel analysis is the number for which the explained variance in your data is larger than the explained variance for the simulated data. Details about how this method works are beyond the scope of this chapter but can be found in the help file3 of the fa.parallel() function to perform the analysis.\nWith the argument x, you specify that you want to use your model-building data and, more specifically, all the columns corresponding to your variables. With the second argument fa = \"fa\", you specify that you assess the best number of factors for factor analysis and not the best number of components for principal components analysis, which is a related yet different method for finding dimensions in data (e.g., [31]). The output consists of two parts: a message and a figure:\n\nfa.parallel(x = model.building[,var_names], fa = \"fa\")\n\nParallel analysis suggests that the number of factors =  5  and the number of components =  NA \n\n\n\n\n\nFigure 5. Parallel analysis\n\n\n\n\nThe message indicates that five factors are likely underlying the data. Figure 20.5 shows that with more than five factors (x-axis), the amount of information explained by the factors (on the y-axis) in your data is lower than for the simulated data.\nIt is important to note that parallel analysis is a purely data-driven method, and the solution depends on the specific sample. Therefore, you should see the 5 factors only as a starting point (such that the number of underlying factors is likely around five) and treat parallel analysis as giving you a plausible range for the number of underlying factors, equal to the solution plus and minus one factor (or more, if you want to). The final decision for the number of underlying factors is made by running several factor models—one for each plausible number of underlying factors as determined using the parallel analysis—and comparing these models in terms of interpretability (i.e., does the pattern of variable-factor relations make substantive sense and/or does it fit prior knowledge/assumptions) and fit (using the BIC). The BIC can be used for model selection for many analyses, including factor analysis, and the criterion balances the fit of the factor model to the data on the one hand and model parsimony (i.e., simplicity) on the other by penalizing models for each parameter they contain. The lower the BIC value, the better, so if the smallest BIC value corresponds to the model with five factors, you have stronger support for the five-factor solution. Still, the final decision should take interpretability into account. If the parallel analysis and BIC disagree, your final decision should consider interpretability even more.\nThe exploratory factor analysis can be performed with the following command:\n\nEFA <- efa (data = model.building[, var_names], nfactors = 4:6, \n            rotation = \"geomin\", estimator = \"MLR\", meanstructure = TRUE)\n\nWarning: lavaan->lav_model_vcov():  \n   The variance-covariance matrix of the estimated parameters (vcov) does not \n   appear to be positive definite! The smallest eigenvalue (= -1.833519e-20) \n   is smaller than zero. This may be a symptom that the model is not \n   identified.\n\n\nWarning: lavaan->lav_model_vcov():  \n   The variance-covariance matrix of the estimated parameters (vcov) does not \n   appear to be positive definite! The smallest eigenvalue (= -5.119020e-32) \n   is smaller than zero. This may be a symptom that the model is not \n   identified.\n\n\nWarning: lavaan->lav_model_vcov():  \n   The variance-covariance matrix of the estimated parameters (vcov) does not \n   appear to be positive definite! The smallest eigenvalue (= -6.404921e-32) \n   is smaller than zero. This may be a symptom that the model is not \n   identified.\n\n\nThe efa() function is part of lavaan [32], which is a very popular, open-source R package that can be used for estimating various latent variable models, including factor analysis and structural equation modeling (see Chapter 2 [21]). With the first argument data, you again specify the dataset on which you want to perform factor analysis, nfactors indicates the range of factors for which you want to retrieve the results. Next, the argument rotation pertains to identifying the model, which is only necessary for EFA and not CFA because, in the latter, you specify restrictions (i.e., restricting loadings to be equal to zero) that automatically lead to the identification of the model. In EFA, where you do not have restrictions, there are an infinite number of solutions identical in fit; that is, your factor matrix with loadings can be transformed or “rotated” in infinite ways. To clarify, consider the following: with (continuous) observed variables, you can visualize your data using scatterplots in which each point represents an individual’s combination of scores on two variables that are represented by the x-axis and the y-axis. With observed variables, you have individuals’ actual scores on the variables in your dataset. Therefore, the locations of the points relative to the x-axis and y-axis (and, therefore, the axes’ position) are fixed. With factors, the situation is different. The latent variables are unobserved, which means that the position of the axes (which now represent latent variables) is not “pinned down” by the data. Considering Figure 20.6, for example, both axis orientations are equally likely with latent variables. Note that in both plots, the data points are in the exact same location; only the orientations of the axes representing the (in this case, two) latent factors are different. Rotation is about choosing one out of all the many possible orientations of the axes.\nThe choice of the orientation of the axes is generally made based on how easy it makes the interpretation of the factors. Typically, the aim is to find a so-called simple structure [33] in which the orientation of the axes ensures that each observed variable is strongly related to one factor (i.e., has a large loading on one factor) and is as unrelated as possible to all others (i.e., has small cross-loadings, for example, smaller in absolute value than .3). This simple structure makes interpreting the factors (which is done by looking for a common theme in the variables that load on them) easier.\n\n\n\nFigure 6. The two possible axis orientations (out of an infinite number of plausible orientations) show that with rotation, the data points stay in exactly the same place, but the axes representing the latent factors (here, two factors) change.\n\n\nIn short, rotation is like taking a good selfie of yourself in front of the Eiffel Tower. You want both yourself and the tower to be clearly visible in the picture, so you move your camera until you get a clear shot of both. You and the Eiffel Tower both stay in exactly the same place! What changes is the vantage point, or angle, from which you look at both. Similarly, rotation moves your vantage point (the axes) to make the factors stand out clearly in your results, while the position of your observed data does not change.\nGoing back to the rotation argument in the code above, you can use geomin, a method of rotation that allows your factors to be correlated with each other, which is a reasonable assumption in educational settings. For other rotation options, see [23].\nThe argument estimator allows you to choose different estimation procedures. The default is standard maximum likelihood (“ML”) estimation. However, for the current data, a robust maximum likelihood (“MLR”) estimation was chosen to account for small violations of the normality assumption. If the data would contain missing values, you could add the argument “missing” and specify it to be equal to “fiml”, which corresponds to a full information maximum likelihood approach and is a sensible approach if you have at least missing at random (MAR) data (details about missing data mechanisms are beyond the scope of this tutorial but can be found in the lavaan tutorial [23]). The final argument meanstructure = TRUE is necessary if you want to estimate intercepts for the observed variables as well, as opposed to just estimating variances and covariances. Note that if you add the “missing” argument to your model, meanstructure will be set to TRUE automatically.\nWith the following command, you can extract and sort the BIC values in ascending order.\n\nsort(fitMeasures(EFA)[\"bic\",])\n\nnfactors = 5 nfactors = 4 nfactors = 6 \n    18142.38     18167.49     18189.29 \n\n\nThe output indicates that the model with five factors is the best according to the BIC. Thus, the two techniques to determine the number of factors agree. From the original article from which the data for this tutorial was obtained, it is known that the expected number of factors was also five. Therefore, continuing the model building with the five-factor solution for this tutorial makes the most sense.\nWith the following command, you obtain the factor loadings for the five factors. Note that, by default, lavaan gives you standardized loadings, which means that the loadings can be interpreted as correlations between variables and factors.\n\nEFA$nf5\n\n\n         f1      f2      f3      f4      f5 \nTSC1  0.584*                       *      .*\nTSC2  0.487*                       *      .*\nTSC3  0.637*                      .*       *\nTSC4  0.578*      .*              .*       *\nTSC5  0.547*                              . \nTE1           0.728*              .         \nTE2       .   0.672*                        \nTE3           0.708*      .                 \nTE4           0.651*              .*        \nTE5           0.337*      .*      .*        \nEE1               .   0.469*      .         \nEE2       .*          0.689*                \nEE3                   0.768*                \nEE4       .*          0.732*              . \nEE5               .   0.479*      .*        \nDE1                  -0.353*  0.744*      . \nDE2               .*          0.821*        \nDE3                       .*  0.755*        \nRPA1                                  0.851*\nRPA2                                  0.906*\nRPA3                                  0.624*\nRPA4                      .       .   0.350*\nRPA5                      .       .   0.338*\n\n\nIn the output, all loadings between the variables (rows) and factors (columns) larger than an absolute value of .3 are shown by default. Inspecting the results, you can clearly see a simple structure such that every variable loads on only one factor. An exception is variable DE1, which positively loads on factor 4 with the other DE variables and negatively on factor 3 with the EE variables. Besides this cross-loading, the results align with the theoretical model as all TSC variables, all TE variables, all EE variables, all DE variables, and all RPA variables load on the same factor, respectively. In the next step, the model can be further refined based on fit. Since the model without the cross-loading is entirely in line with theory, the loading of DE1 on factor 3 will be set equal to zero in the CFA in the next section. However, if the CFA model does not fit well, putting back this cross-loading would be the first logical step.\n\n\n5.3 Step 2: Building the Factor Model and Assessing Fit\nThe first step in building the model is to describe the model you want to estimate using special lavaan syntax. The arguments relevant for this tutorial are “=~”, which can be read as “is measured by”, and “~~”, which translates into “is correlated with”. In the following, a model is specified in which the 5 factors TSC, TE, EE, DE, and RPA are measured by different sets of variables (in line with theory and the EFA results from the previous step), separated by “+”. Moreover, it is explicitly stated that correlations between factors should be estimated. Intercepts are not explicitly included in the model, but these are included again by adding the argument meanstructure = TRUE to the command when estimating the CFA model.\n\nCFA_model <-'\n#  Regressing items on factors\nTSC =~ TSC1 + TSC2 + TSC3 + TSC5\nTE  =~ TE1  + TE2  + TE3  + TE5\nEE  =~ EE1  + EE2  + EE3  + EE4\nDE  =~ DE1  + DE2  + DE3\nRPA =~ RPA1 + RPA2 + RPA3 + RPA4\n\n# Correlations between factors\nTSC ~~ TE\nTSC ~~ EE\nTSC ~~ DE\nTSC ~~ RPA\n\nTE ~~ EE\nTE ~~ DE\nTE ~~ RPA\n\nEE ~~ DE\nEE ~~ RPA\n\nDE ~~ RPA\n'\n\nThe next step is to perform the CFA on the model-building data using the specified CFA model with the following command:\n\nCFA <- cfa(model = CFA_model, data = model.building[,var_names], \n           estimator = \"MLR\", std.lv = TRUE, meanstructure = TRUE)\n\nWhich model should be used is specified by the argument model. The arguments data and estimator are the same as in the code for the EFA. The argument std.lv is used to get results similar to the EFA. Since factors are not directly observed, their scale has to be set, which can be done in two different ways: i) by fixing one of the factor-loadings of each factor to 1 (which will set the scale of the factor equal to the scale of the observed variable which loadings was fixed), or ii) by setting the variance of the factor equal to 1. Here, the second option was chosen. As already mentioned, the final argument meanstructure is necessary if you also want to estimate the intercepts of the observed variables.\nAfter performing the CFA, you can assess how well the model fits to the data. There are two types of fit measures: global and local. You can start with the global fit measures that describe how well the model as a whole fits to the data. Many different global model fit indices exist in the literature. Kline [34] suggests that at least the following four indices should be considered: (1) the Chi-squared significance test, which tests whether the model has a perfect fit to the data, that is, if the model can perfectly recreate the observed relations between variables; (2) the comparative fit index (CFI), which compares the fit of the chosen model to the fit of a model assuming zero correlations between variables; (3) the root mean square error of approximation (RMSEA), which is closely related to the Chi-squared test, but does not test for perfect fit and instead quantifies (approximate) fit between the model and the data in a single number; and (4) the standardized root mean square residual (SRMR) which summarizes the difference between the sample covariance matrix of the variables and the model-implied covariance matrix into one number. Unlike the Chi-squared significance test, the CFI, RMSEA, and SRMR are no tests and assess approximate fit.\nEach fit measure is accompanied by rules of thumb to decide whether or not a model fits sufficiently to the data. The Chi-squared significance test should be nonsignificant because the null hypothesis is that the model fits to the data perfectly. It is important to note that with increasing sample size, the null hypothesis of perfect fit is easily rejected. Therefore, you should not base your decision on whether the model fits too much on this test.\nRegarding the other three measures, the CFI should be larger than 0.9 [35], the RMSEA point estimate and the upper bound of the 95 percent confidence interval should be smaller than 0.05 [36, 37], and the SRMR should be smaller than 0.08 [35]. The fit measures and their interpretation can be obtained with the following command:\n\nglobalFit(CFA)\n\nResults------------------------------------------------------------------------ \n \nChi-Square (142) = 318.8407 with p-value\n          = 1.332268e-15\n\nCFI = 0.9476614\n\nRMSEA = 0.05332242; lower bound = 0.04589762;\n      upper bound = 0.06076663\n\nSRMR = 0.04353874\n\nInterpretations--------------------------------------------------------------- \n \nThe hypothesis of perfect fit *is* rejected according to the Chi-\n          Square test statistics because the p-value is smaller than 0.05 \n \nThe hypothesis of approximate model fit *is not* rejected according\n          to the CFI because the value is larger than 0.9. \n \nThe hypothesis of approximate model fit *is* rejected according\n         to the RMSEA because the point estimate is larger or equal to\n         0.05. \n \nThe hypothesis of approximate model fit *is not* rejected according\n         to the SRMR because the value is smaller than 0.08. \n \n\n\nInspecting the output, you can see that the Chi-squared significance test rejected perfect fit, but that approximate fit holds according to the CFI and the SRMR. Ideally, at least three of the fit measures should indicate appropriate fit, but for the sake of this tutorial’s brevity, it was decided to continue with the model without further adjustments. In practice, you may further adjust the model, for example, by including the cross-loading between DE1 and factor 3 again, and re-evaluate fit by fitting the updated model and running the globalFit() function again.\nAll measures above inspect the global fit of the model, so if the model as a whole matches the data well. While informative and necessary, the above measures can miss local misfit between the model and the data. If, for example, the 5-factor model describes the relations between all but one of the observed variables well, then the global fit of the model will likely be sufficient even though the estimates of the relations between the ill-fitting observed variable and all others will be completely wrong. Because of this, you should also inspect the local fit of your model; that is, if every part of your model fits the data well. There are many local fit measures that you could use [38], but the most straightforward way of assessing local fit is to look at the absolute difference between the model-implied and the sample covariance matrix. These are the same two matrices that the SRMR is based on, but instead of quantifying the total difference between the two in one number, you obtain the difference between the two matrices for every variance and covariance separately. You can see how much the two matrices deviate for each pair of variables, as well as the maximum difference between the two matrices, by using the following command:\n\nlocalFit(CFA)\n\n$local_misfit\n      TSC1  TSC2  TSC3  TSC5   TE1   TE2   TE3   TE5   EE1   EE2   EE3   EE4\nTSC1 0.000                                                                  \nTSC2 0.012 0.000                                                            \nTSC3 0.007 0.012 0.000                                                      \nTSC5 0.007 0.002 0.010 0.000                                                \nTE1  0.019 0.000 0.009 0.010 0.000                                          \nTE2  0.025 0.014 0.031 0.021 0.011 0.000                                    \nTE3  0.013 0.010 0.048 0.005 0.003 0.008 0.000                              \nTE5  0.025 0.028 0.032 0.022 0.012 0.026 0.005 0.000                        \nEE1  0.013 0.010 0.004 0.016 0.042 0.044 0.001 0.072 0.000                  \nEE2  0.004 0.009 0.025 0.003 0.029 0.050 0.027 0.043 0.002 0.000            \nEE3  0.013 0.015 0.039 0.013 0.021 0.042 0.006 0.081 0.012 0.001 0.000      \nEE4  0.002 0.002 0.000 0.013 0.042 0.021 0.006 0.039 0.017 0.017 0.010 0.000\nDE1  0.011 0.019 0.015 0.002 0.010 0.026 0.011 0.036 0.010 0.048 0.042 0.040\nDE2  0.014 0.018 0.030 0.011 0.008 0.025 0.032 0.059 0.058 0.031 0.012 0.052\nDE3  0.000 0.008 0.041 0.021 0.023 0.006 0.012 0.019 0.048 0.015 0.022 0.012\nRPA1 0.008 0.015 0.034 0.011 0.013 0.022 0.001 0.012 0.011 0.018 0.019 0.041\nRPA2 0.006 0.008 0.044 0.007 0.021 0.004 0.009 0.008 0.015 0.016 0.002 0.053\nRPA3 0.041 0.016 0.012 0.003 0.006 0.010 0.017 0.034 0.035 0.008 0.022 0.009\nRPA4 0.020 0.000 0.003 0.031 0.001 0.027 0.031 0.039 0.042 0.035 0.031 0.053\n       DE1   DE2   DE3  RPA1  RPA2  RPA3  RPA4\nTSC1                                          \nTSC2                                          \nTSC3                                          \nTSC5                                          \nTE1                                           \nTE2                                           \nTE3                                           \nTE5                                           \nEE1                                           \nEE2                                           \nEE3                                           \nEE4                                           \nDE1  0.000                                    \nDE2  0.004 0.000                              \nDE3  0.002 0.006 0.000                        \nRPA1 0.008 0.006 0.002 0.000                  \nRPA2 0.010 0.025 0.024 0.024 0.000            \nRPA3 0.002 0.016 0.021 0.009 0.017 0.000      \nRPA4 0.006 0.056 0.074 0.046 0.011 0.052 0.000\n\n$max_misfit\n[1] 0.08051991\n\n\nBased on the local fit evaluation, you can conclude that no local misfit is present since the biggest difference between the two matrices is only .08, which is small compared to the scale of the observed variables. If local misfit is present, for example, if the correlation between two observed variables had been much larger than predicted by the model, further adjustments could be made to your model that specifically address the source of local misfit, like adding an additional covariance between these observed variables. However, these adjustments should always make sense according to theory! Never just add parameters to your model to improve its fit.\nThis concludes the assessment of fit. The last part of this model-building step is to look at the loadings of the final model with the following command:\n\ninspect(object = CFA, what = \"std\")$lambda\n\n       TSC    TE    EE    DE   RPA\nTSC1 0.657 0.000 0.000 0.000 0.000\nTSC2 0.692 0.000 0.000 0.000 0.000\nTSC3 0.628 0.000 0.000 0.000 0.000\nTSC5 0.726 0.000 0.000 0.000 0.000\nTE1  0.000 0.789 0.000 0.000 0.000\nTE2  0.000 0.745 0.000 0.000 0.000\nTE3  0.000 0.788 0.000 0.000 0.000\nTE5  0.000 0.649 0.000 0.000 0.000\nEE1  0.000 0.000 0.739 0.000 0.000\nEE2  0.000 0.000 0.802 0.000 0.000\nEE3  0.000 0.000 0.786 0.000 0.000\nEE4  0.000 0.000 0.760 0.000 0.000\nDE1  0.000 0.000 0.000 0.665 0.000\nDE2  0.000 0.000 0.000 0.640 0.000\nDE3  0.000 0.000 0.000 0.738 0.000\nRPA1 0.000 0.000 0.000 0.000 0.849\nRPA2 0.000 0.000 0.000 0.000 0.854\nRPA3 0.000 0.000 0.000 0.000 0.788\nRPA4 0.000 0.000 0.000 0.000 0.587\n\n\nThe loadings are very comparable to the ones of the EFA, which is not surprising given that only a single cross-loading (with absolute value > .3) was removed. Note that if you would want to extract other model parameters like the factor correlations, you could use the inspect() command without the “$lambda` at the end.\n\n\n5.4 Step 3: Assessing Generalizability\nThe final step is assessing the generalizability of the CFA model from Step 2 by fitting the same model to the holdout sample. If the model fits this alternative dataset, too, you can be more confident that your factor model applies more generally and can capture the underlying structure of your measurement instrument in future studies and samples as well. To assess the generalizability, you use the same code as in Step 2, but now specify your holdout sample under the data argument.\n\nCFA_holdout <- cfa(model = CFA_model, data = holdout[,var_names], \n                   estimator = \"MLR\", std.lv = TRUE, meanstructure = TRUE)\n\nAfter fitting your CFA model to the holdout sample, fit measures and their interpretation can again be obtained with the globalFit() command.\n\nglobalFit(CFA_holdout)\n\nResults------------------------------------------------------------------------ \n \nChi-Square (142) = 339.7732 with p-value\n          = 0\n\nCFI = 0.9429698\n\nRMSEA = 0.05639005; lower bound = 0.04898985;\n      upper bound = 0.06383685\n\nSRMR = 0.04161716\n\nInterpretations--------------------------------------------------------------- \n \nThe hypothesis of perfect fit *is* rejected according to the Chi-\n          Square test statistics because the p-value is smaller than 0.05 \n \nThe hypothesis of approximate model fit *is not* rejected according\n          to the CFI because the value is larger than 0.9. \n \nThe hypothesis of approximate model fit *is* rejected according\n         to the RMSEA because the point estimate is larger or equal to\n         0.05. \n \nThe hypothesis of approximate model fit *is not* rejected according\n         to the SRMR because the value is smaller than 0.08. \n \n\n\nInspecting the output, you can see that the fit of the model to the holdout sample is very comparable to its fit to the model-building data. Again, the Chi-squared significance test rejects perfect fit, but approximate fit holds according to the CFI and the SRMR.\nLocal fit is also tested with the same command as in Step 2 but again applied to your results on the holdout sample.\n\nlocalFit(CFA_holdout)\n\n$local_misfit\n      TSC1  TSC2  TSC3  TSC5   TE1   TE2   TE3   TE5   EE1   EE2   EE3   EE4\nTSC1 0.000                                                                  \nTSC2 0.010 0.000                                                            \nTSC3 0.012 0.015 0.000                                                      \nTSC5 0.007 0.007 0.005 0.000                                                \nTE1  0.023 0.023 0.003 0.014 0.000                                          \nTE2  0.027 0.012 0.019 0.008 0.008 0.000                                    \nTE3  0.012 0.010 0.024 0.008 0.012 0.002 0.000                              \nTE5  0.019 0.014 0.008 0.002 0.046 0.006 0.013 0.000                        \nEE1  0.037 0.023 0.009 0.005 0.035 0.009 0.002 0.011 0.000                  \nEE2  0.028 0.003 0.003 0.019 0.038 0.016 0.069 0.008 0.033 0.000            \nEE3  0.032 0.047 0.012 0.017 0.024 0.017 0.004 0.071 0.026 0.019 0.000      \nEE4  0.006 0.033 0.003 0.002 0.027 0.002 0.002 0.048 0.015 0.020 0.004 0.000\nDE1  0.056 0.005 0.007 0.007 0.005 0.020 0.003 0.032 0.037 0.072 0.020 0.036\nDE2  0.005 0.029 0.032 0.061 0.012 0.014 0.046 0.006 0.024 0.038 0.034 0.018\nDE3  0.012 0.019 0.022 0.002 0.034 0.016 0.014 0.005 0.057 0.032 0.050 0.020\nRPA1 0.019 0.009 0.012 0.028 0.018 0.001 0.003 0.004 0.030 0.031 0.037 0.003\nRPA2 0.009 0.020 0.023 0.001 0.017 0.016 0.018 0.004 0.003 0.045 0.008 0.051\nRPA3 0.000 0.007 0.004 0.009 0.000 0.006 0.000 0.028 0.011 0.021 0.025 0.002\nRPA4 0.018 0.015 0.006 0.014 0.021 0.019 0.040 0.036 0.049 0.023 0.046 0.023\n       DE1   DE2   DE3  RPA1  RPA2  RPA3  RPA4\nTSC1                                          \nTSC2                                          \nTSC3                                          \nTSC5                                          \nTE1                                           \nTE2                                           \nTE3                                           \nTE5                                           \nEE1                                           \nEE2                                           \nEE3                                           \nEE4                                           \nDE1  0.000                                    \nDE2  0.020 0.000                              \nDE3  0.019 0.006 0.000                        \nRPA1 0.014 0.029 0.004 0.000                  \nRPA2 0.006 0.010 0.005 0.020 0.000            \nRPA3 0.030 0.006 0.026 0.016 0.017 0.000      \nRPA4 0.008 0.028 0.046 0.057 0.005 0.093 0.000\n\n$max_misfit\n[1] 0.09310116\n\n\nResults show that the local fit is sufficient for the holdout sample as well (the biggest absolute difference between the two matrices is only .09) and that it is again comparable to the results on the model-building data.\nLastly, you can look at the loadings of the final model when fitted to the holdout sample.\n\ninspect(object = CFA_holdout, what = \"std\")$lambda\n\n       TSC    TE    EE    DE   RPA\nTSC1 0.679 0.000 0.000 0.000 0.000\nTSC2 0.689 0.000 0.000 0.000 0.000\nTSC3 0.691 0.000 0.000 0.000 0.000\nTSC5 0.702 0.000 0.000 0.000 0.000\nTE1  0.000 0.694 0.000 0.000 0.000\nTE2  0.000 0.772 0.000 0.000 0.000\nTE3  0.000 0.819 0.000 0.000 0.000\nTE5  0.000 0.677 0.000 0.000 0.000\nEE1  0.000 0.000 0.749 0.000 0.000\nEE2  0.000 0.000 0.794 0.000 0.000\nEE3  0.000 0.000 0.781 0.000 0.000\nEE4  0.000 0.000 0.801 0.000 0.000\nDE1  0.000 0.000 0.000 0.677 0.000\nDE2  0.000 0.000 0.000 0.659 0.000\nDE3  0.000 0.000 0.000 0.766 0.000\nRPA1 0.000 0.000 0.000 0.000 0.851\nRPA2 0.000 0.000 0.000 0.000 0.867\nRPA3 0.000 0.000 0.000 0.000 0.700\nRPA4 0.000 0.000 0.000 0.000 0.618\n\n\nAgain, you can see that results from the model-building data and holdout sample are very comparable, as the factor loadings are similar to before.\nSince the model fits the holdout sample sufficiently and equally well as the model-building data and parameter estimates are comparable between the two datasets, you can conclude that the model’s generalizability is okay. Had the model not fit the holdout sample sufficiently, you would have to conclude that while the CFA model from Step 2 fits the model-building data well, you cannot be certain that it reflects a generally applicable structure of your measure and that the factor structure needs to be further refined. Since you already used your holdout sample in this phase, however, this further refinement would require collecting a new set of data that can be split into a model-building and holdout sample and going through all three steps again."
  },
  {
    "objectID": "chapters/ch20-factor-analysis/ch20-factor.html#conclusion",
    "href": "chapters/ch20-factor-analysis/ch20-factor.html#conclusion",
    "title": "20  Factor Analysis in Education Research Using R",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nFactor analysis is a great way to study constructs that are not directly observable. Of course, factor analysis has vast applications across several fields that are usually interdisciplinary and has been extended in several ways (e.g., to multi-group factor analysis, which will play an important role in the next chapter, where you will also see a discussion of the important topic of measurement invariance). This chapter serves mainly as a primer to introduce and demonstrate the basics of the method and to get readers interested and confident in applying the method themselves."
  },
  {
    "objectID": "chapters/ch20-factor-analysis/ch20-factor.html#further-readings",
    "href": "chapters/ch20-factor-analysis/ch20-factor.html#further-readings",
    "title": "20  Factor Analysis in Education Research Using R",
    "section": "7 Further readings",
    "text": "7 Further readings\nIn this chapter, you have seen an introduction and tutorial on how to apply factor analysis in educational research. To learn more about factor analysis in general, you can consult:\n\nKline, R. B. (2015). Principles and Practice of Structural Equation Modeling (4 ed.). Guilford Press.\n\nTo learn more about the difference between EFA and CFA, you can consult:\n\nFlora, D. B., & Flake, J. K. (2017). The purpose and practice of exploratory and confirmatory factor analysis in psychological research: Decisions for scale development and validation. Canadian Journal of Behavioural Science, 49, 78–88.\n\nFinally, to learn more about the history of factor analysis, you can consult:\n\nBriggs, D. D. (2022). Historical and Conceptual Foundations of Measurement in the Human Sciences. Credos and Controversies. Routledge."
  },
  {
    "objectID": "chapters/ch21-sem/ch21-sem.html",
    "href": "chapters/ch21-sem/ch21-sem.html",
    "title": "21  Structural Equation Modeling with R for Education Scientists",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch21-sem/ch21-sem.html#introduction",
    "href": "chapters/ch21-sem/ch21-sem.html#introduction",
    "title": "21  Structural Equation Modeling with R for Education Scientists",
    "section": "1 Introduction",
    "text": "1 Introduction\nEducational research involves a variety of theoretical constructs that are not directly observable (e.g., motivation, engagement, cognitive development, and self-regulation) and that can only be indirectly studied by looking at participant’s responses to observable indicators of these constructs (e.g., participants’ responses to questionnaire items). The previous chapter about factor analysis [1] showed how to assess the factor structure of these unobserved, or latent, constructs and, thus, which items are good measurements of these constructs. This is crucial for developing valid instruments to measure (i.e., quantify) the latent constructs encountered in educational research. However, good measurement of latent constructs is typically not the researchers' ultimate goal. Usually, they subsequently want to answer questions about relationships or mean differences between (multiple) of these constructs, such as, “Is teacher motivation related to student engagement?” or “Does student engagement significantly differ for small and large-scale teaching styles?” If researchers only have (i) observed variables (e.g., years of teacher experience) for both predictor and outcome variables and (ii) want to investigate the effect of one or more predictor variables on a single outcome variable at a time (e.g., the effect of teacher experience and teacher gender on student GPA), these types of questions can be answered using familiar analysis methods such as multiple regression or ANOVA. However, as soon as questions involve latent variables or testing an entire system of (complex) interrelations between variables (like the ones found in most theoretical frameworks), researchers need an analysis technique with more flexibility that allows for modeling a multitude of relationships between (latent and/or observed) variables simultaneously [2]; that is, researchers need SEM.\nAt its core, SEM is a mix of factor analysis (discussed in the previous chapter) and a method called path analysis, which was invented by Wright [3]. One can think of path analysis as a type of multiple regression analysis because it also allows estimating and testing direct effects between variables. However, while multiple regression merely allows for testing the direct effects of predictor(s) on only a single outcome variable, path analysis can look at both direct and indirect effects between whole sets of predictor(s) on whole sets of other variables simultaneously. This implies that in path analysis, a variable can simultaneously be an outcome and a predictor: a variable might be predicted by one or more variables while also serving as a predictor for other variables. In other words, with path analysis, researchers can pretty much fit any model they can dream of as long as all variables are observed. Fitting a path model to latent variables requires going beyond path analysis. Fortunately, Jöreskog and Van Thillo [4] developed a statistical software called LISREL that allowed the use of latent variables in path analysis and thereby created SEM. Over the years, their work has been integrated into increasingly user-friendly software, contributing to the widespread use of the technique in social sciences today. This widespread use has also led to the definition of a SEM model becoming less concrete as the term is now also used for models with only observed variables, even though that would officially be a path model."
  },
  {
    "objectID": "chapters/ch21-sem/ch21-sem.html#literature-review",
    "href": "chapters/ch21-sem/ch21-sem.html#literature-review",
    "title": "21  Structural Equation Modeling with R for Education Scientists",
    "section": "2 Literature review",
    "text": "2 Literature review\nPerhaps the most well-known application of SEM in education research is the technology acceptance model (TAM), used to explain teachers’ adoption of education technology [5]. The model hypothesizes that a teacher's decision to adopt and use technology is influenced by their perceptions of its ease of use and usefulness. These perceptions shape their attitude toward technology use, which, in turn, affects their intention to use it and their actual usage. Such complex interplay in factors that influence technology acceptance requires using sophisticated methods such as SEM. The model has been extensively applied and validated in several contexts [6], investigating the adoption of multiple technological innovations such as serious games [7], virtual reality [8], or artificial intelligence [9]. The model has also been extended in several ways throughout the years to address the influence of other factors such as personality traits, result demonstrability, and risk, among many others [10].\nMany other applications of SEM exist in education research. For example Kusurkar et al. [11] used SEM to investigate how motivation affects academic performance. They hypothesized that greater levels of self-determined motivation are associated with the adoption of effective study strategies and increased study dedication, resulting in improved academic achievement. Their findings confirmed that the influence of motivation on academic performance is mediated by the use of effective study strategies. The work by Kucuk and Richardson [12] examined the interconnections between the three types of Community of Inquiry presence (teaching, social, and cognitive), four engagement components (agentic, behavioral, cognitive, and emotional), and satisfaction in online courses. The findings suggest that teaching presence, cognitive presence, emotional engagement, behavioral engagement, and cognitive engagement were significant predictors of satisfaction, explaining 88% of its variance.\nThe rise of digital learning and the opportunities for unobtrusive data collection have given rise to a new wave of studies that take advantage of the trace log data that students leave behind in online systems [13], instead of relying on self-reported data from questionnaires. For instance, Koç [14] proposed a model that explains the association between student participation and academic achievement. Student participation was measured through attendance to online lectures and discussion forum submissions. His results suggest that “student success in online learning [can] be promoted by increasing student participation in discussion forums and online lectures with more engaging learning activities” [14]. Fincham et al. [15] validated a theorized model of engagement in learning analytics using factor analysis and SEM. He found that affective engagement (measured by sentiment, sadness, and joy of students’ writing) and cognitive engagement (measured by syntactic simplicity, word concreteness, and referential cohesion) are not significantly associated with students’ final grades but academic and behavioral engagement (problem submissions, videos watched, and weeks active) are.\nThe rest of this chapter presents a recap of SEM, an integrated strategy for conducting a SEM analysis that is well suited for Educational Sciences, and an illustration of how to carry out a SEM analysis in R. Like the previous chapter, the presentation in this chapter will be kept applied and focus on how to conduct the analysis in R, instead of diving into technical details (for this, interested readers are referred to the readings listed at the end of this chapter)."
  },
  {
    "objectID": "chapters/ch21-sem/ch21-sem.html#recap-of-sem",
    "href": "chapters/ch21-sem/ch21-sem.html#recap-of-sem",
    "title": "21  Structural Equation Modeling with R for Education Scientists",
    "section": "3 Recap of SEM",
    "text": "3 Recap of SEM\nIn its most common form, SEM combines confirmatory factor analysis (CFA) with path analysis. Like CFA, SEM relates observed variables to latent variables that are measured by those observed variables, and, like path analysis, SEM allows for a wide range of relations between entire sets of variables (both latent and observed). As mentioned before, SEM is nowadays an umbrella term that also includes path analysis with only observed variables. However, this chapter focuses on SEM with latent variables to show its full potential. For the sake of brevity, it is assumed that the readers of this chapter have already read the factor analysis chapter, where a good description and recap of CFA (and how it differs from exploratory factor analysis) is provided.\nThe similarity between CFA and SEM is illustrated in Figure 1. It can be seen that both models consist of a part in which the observed and latent variables are related to each other (these relations are referred to as the measurement model) and a part in which the latent factors are related to each other (these relations are referred to as the structural model). The crucial difference between CFA and SEM is that factors in CFA can only correlate with each other (see Figure 1a). In contrast, SEM gives us more flexibility regarding (the set of) relationships between (observed and latent) variables. For example, Figure 1b shows the same factor model as Figure 1a. However, in Figure 1b, Factors 2 and 3 are both predicted by Factor 1a but are unrelated beyond that. Which model researchers specify, that is, what relations between (latent) variables they model, is entirely up to them. However, this flexibility is also a danger of SEM: the fact that researchers can fit any model they can think of does not mean they should. Ideally, the chosen relationships are driven by theory and substantive knowledge of the field.\n\n\n\n\n\n\n\n(a) A 3-factor CFA model with correlated factors, and Each factor is measured by three observed variables (X1 – X3, X4 – X6, X7 – X9).\n\n\n\n\n\n\n\n(b) A a SEM with one latent predictor variable and two latent outcome variables. The model is identical to the CFA model in a). However, now there are regression relationships between factors, as indicated by the straight arrows between them.\n\n\n\n\nFigure 1. CFA with covariances between factors vs. SEM with relationships between factors. The straight arrows from the factors to the observed variables indicate factor loadings (the regression weights obtained by regressing the observed variable on the factor). Double-headed arrows between factors indicate covariance. The \\(\\epsilon\\)’s represent errors. Curved single-headed arrows indicated variances. Each observed variable can also have an intercept, and each latent variable can also have a mean, but these are not explicitly indicated in the figure."
  },
  {
    "objectID": "chapters/ch21-sem/ch21-sem.html#integrated-strategy-for-structural-equation-modeling",
    "href": "chapters/ch21-sem/ch21-sem.html#integrated-strategy-for-structural-equation-modeling",
    "title": "21  Structural Equation Modeling with R for Education Scientists",
    "section": "4 Integrated Strategy for Structural Equation Modeling",
    "text": "4 Integrated Strategy for Structural Equation Modeling\nIn the following, we guide you, the researcher, through the different steps of conducting a SEM analysis. It is important to highlight that the most crucial part of a SEM analysis is checking its assumptions and checking measurement invariance (which was already briefly mentioned in the previous chapter). Measurement invariance means that the factor model underlying your instrument apply equally to relevant subgroups in your sample. In other words, measurement invariance implies that if you analyzed each of those subgroups with factor analysis separately (using the three steps described in the previous chapter), you would find the same model in each one. Measurement invariance is crucial: As discussed in the previous chapter, the interpretation of a latent variable depends on the factor model. Therefore, if the factor model of the latent variable(s) in your SEM model differs, for example, across biological sex, you effectively throw apples and oranges together if you analyze both males and females simultaneously in your analysis, which will result in uninterpretable outcomes. At the very least, you would want the same factor model for everyone in terms of the same number of factors and the same pattern of relationships between the factors and the observed variables. This scenario is called configural invariance. However, most research questions require higher levels of invariance, in which not only the number of factors and the pattern of factor loadings are the same across subgroups, but also the actual values of (most of the) parameters such as factor loadings and intercepts are equal across groups. More information about the different levels of invariance and how and when to test for it is presented below. After you have checked the assumptions and measurement invariance, you can safely apply and interpret a SEM analysis. The actual SEM analysis is just the final step of the modeling strategy presented here.\n\n4.1 Step 1: Steps from the previous chapter while assessing configural invariance\nSince SEM analyses typically look at the relationship of one or more latent variables with one or more other (latent or observed) variables, you first need to ensure that the latent variables represent what you think they represent. To this end, the first step is going through some or all the steps described in the previous chapter for every latent variable you include. If you use a new instrument to measure the latent variables, you should go through all the steps from (1) exploring the data structure, (2) building the factor model and assessing fit, and (3) assessing generalizability1. As explained in the previous chapter, step 3 should be conducted using a holdout sample or a new sample. However, you can use the same (holdout or new) sample for your SEM analysis. More on determining sufficient sample sizes is discussed below in the “SEM in R” section.\nOnce the factor model of your latent variables has been established or verified, you must ensure that this structure applies to all relevant subgroups in your sample. That is, you need to check for measurement invariance. The first level of invariance is configural invariance and, thus, whether the number of underlying factors and the pattern of relations between those factors and the observed variables is the same in each relevant subgroup. To test for configural invariance, you must go through the steps outlined in the previous chapter for each relevant group in your sample separately. Note that this also means you need sufficient observations for model building and generalizability in each subgroup you want to consider. Which subgroups are important to consider separately will always come down to theory.\nIf you use an existing instrument on a sample from a population for which the instrument had already been validated and configural invariance attained in previous research (i.e., you have strong a priori assumptions about the CFA model), you could skip the three steps from the factor analysis chapter and go straight to assessing the fit of the CFA model to your data. If it fits, you can continue with the SEM analysis. However, even when using existing validated instruments, it is strongly recommended to follow all the aforementioned steps for each relevant subgroup of your sample, as your sample might differ in important ways from the ones on which the instrument was validated, which could potentially introduce biases into your results.\n\n\n4.2 Step 2: Assessing Higher Levels of Invariance\nAfter establishing configural invariance between relevant groups, it is time to verify that the factor model parameters (specifically the loadings and intercepts) are also invariant across the factor models of each group. To illustrate why these “higher levels of invariance” are important, assume you want to investigate the relationship between children’s age and how engaged they are in class. You measure the latent variable “classroom engagement” using a factor model on the three observed variables engages in group activities, asks for assistance when needed, and prefers working alone. Of these three variables, asks for assistance might be more indicative of classroom engagement for girls than for boys due to cultural stereotypes. Boys could be way less likely to ask for assistance, not because they are less engaged, but because society taught them that boys should not ask for help and should instead figure things out themselves. In this example, the interpretation of the latent variable would be different between girls and boys: for girls, you truly measure classroom engagement; for boys, you measure an uninterpretable mix of classroom engagement and how much they internalized gender stereotypes. If you ignored this between-group difference in the meaning of the latent variable when including it in a SEM model, you would come to invalid conclusions about the structural relationships between (latent or latent and observed) variables. Fortunately, the type of invariance in this example would show up as a difference in the relation between the factor classroom engagement and the variable asks for assistance between boys and girls. In other words, it would show up as a difference in factor loadings between groups.\nTesting for higher levels of invariance in the factor model between categorical characteristics or “groups” (e.g., gender, nationality) can be done by testing for differences in model parameters between groups using a multi-group approach [16, 17] that compares the fit of a factor model in which all parameters are allowed to differ between groups to one in which one or more parameters are constrained to be equal across groups. If constraining parameters across groups does not cause a substantially worse model fit, invariance holds, and the constrained parameters can safely be considered and modeled as equal across groups. To assess for invariance across continuous variables (e.g., age) one could split the continuous variable into groups (e.g., changing the continuous variable age into the groups “young” and “old”), although one should ideally use more advanced methods like Moderated Non-Linear Factor Analysis [18] since categorizing continuous variables leads to loss of information. This chapter, however, will focus on the multi-group approach because it is the most common and widely available approach.\nThere are two important points about the invariance of model parameters that make the lives of researchers a little easier. First, not all model parameters have to be invariant. If you investigate relationships between (latent) constructs, as opposed to mean differences, only the factor loadings have to be invariant. For assessing differences in means, the factor loadings and the intercepts must be invariant. Note that the effects of categorical predictors on other variables also pertain to differences in means and that both loading- and intercept invariance are needed to interpret those effects. Note that a factor model comprises not only factor loadings and intercepts but also unique variances. However, unique variances do not have to be invariant across groups when looking at relationships or mean differences across latent variables. Therefore, we will not further consider them in this chapter. Second, the discussion of measurement invariance so far in this section has been about what is called full invariance, in which all parameters of a certain type (i.e., all factor loadings and/or all intercepts) are equivalent across groups (or across levels of a continuous variable). While this would represent an ideal situation, full invariance will rarely hold and is also not necessary. Partial invariance in which several, but not all, parameters of a certain type are invariant across groups is typically considered enough (e.g., the factor loadings of 4 out of 6 variables used to measure a factor are invariant across groups). Specifically, as long as at least two loadings are invariant across groups, one can look at relations between the corresponding latent variable and other (observed and/or latent) variables, and as long as at least two factor loadings and at least two intercepts are equal, one can meaningfully look at differences in means too ([19]; but also see [20]).\n\n\n4.3 Step 3: Building the Structural Equation Model and Assessing Fit\nAfter verifying the required level of measurement invariance for all latent variables that will be used in the SEM analysis, you are ready to investigate structural relations (e.g., regression relationships) between the factors (as well as observed variables if desired). Like with the factor models in the previous chapter, you first have to evaluate if the model fits your data sufficiently. You should only interpret the structural relations if the model fits the data. If the model does not fit, it is not a good description of the data and does not warrant further interpretation."
  },
  {
    "objectID": "chapters/ch21-sem/ch21-sem.html#sem-in-r",
    "href": "chapters/ch21-sem/ch21-sem.html#sem-in-r",
    "title": "21  Structural Equation Modeling with R for Education Scientists",
    "section": "5 SEM in R",
    "text": "5 SEM in R\nIn the following, you will be taken through the essential steps of performing SEM in the open-source software R. To this end, the same dataset [21] will be used as the one to which factor analysis was applied to in the previous chapter [1]. The dataset contains survey data about teacher burnout in Indonesia. In total, 876 respondents have answered questions on five domains: Teacher Self-Concept (TSC, 5 questions), Teacher Efficacy (TE, 5 questions), Emotional Exhaustion (EE, 5 questions), Depersonalization (DP, 3 questions), and Reduced Personal Accomplishment (RPA, 7 questions). Thus, the total number of variables equals 25. The questions were assessed on a 5-point Likert scale (ranging from 1 = “never” to 5 = “always”). For more information on the dataset, the interested reader is referred to the data chapter of the book [22].\nIn line with the seven hypotheses about the structural relationships in the original article, the SEM analysis presented below tests whether TE is predicted by TSC and whether EE, DE, and RPA are respectively predicted by TE and TSC. Note that, as SEM builds up on factor analysis, the code presented below also builds up on the factor analysis results and syntax from the previous chapter. Without reading the previous chapter, the steps in this chapter might therefore be harder to follow.\nTo provide an example of how to assess between-group invariance of the constructs, it will be tested whether the same factor structure holds across gender. More specifically, because the running example is only interested in relationships between constructs and not mean differences, only loading invariance will be tested for. However, code to also test for intercept invariance will also be provided.\n\n5.1 Preparation\nTo follow all the steps, you have to install the following packages with the function install.packages(). You only have to install them the first time you want to use them; therefore, the commands are commented below. Once you have the packages installed, you must load them with the library() function whenever you open the R script.\n\nlibrary(lavaan) # install.packages(\"lavaan\")\nlibrary(tidyverse) # install.packages(\"tidyverse\")\nlibrary(rio) # install.packages(\"rio\") \nlibrary(psych) # install.packages(\"psych\")\nlibrary(combinat) # install.packages(\"combinat\")\nlibrary(devtools) # install.packages(\"devtools\")\nlibrary(sleasy) # devtools::install_github(\"JoranTiU/sleasy\")\n\n\n5.1.1 Reading in the data\nThe data can be read in, and the variable names can be extracted with the following commands:\n\ndataset <- import(\"https://github.com/lamethods/data/raw/main/4_teachersBurnout/2.%20Response.xlsx\")\nvar_names <- colnames(dataset)\n\nIn the following, you will also find commands to add a gender variable to the dataset. This variable is only added to demonstrate how to perform invariance tests. Although the original data contains gender, it is unfortunately not part of the publicly shared data.2 The proportion of men (gender = 0) and women (gender = 1) in the created variable aligns with the reported values from the article corresponding to the dataset.\n\nset.seed(1611)\ndataset$gender <- as.factor((sample(c(rep(0, 618), rep(1, 258)))))\n\n\n\n5.1.2 Are the data suited for SEM?\nSeveral data characteristics are necessary for SEM. These largely overlap with the necessary data characteristics for factor analysis, as discussed in the previous chapter (i.e., are the variables continuous? Are the correlations between the variables sufficiently large? Is there enough common variance among the variables? Are the data normally distributed?). Therefore, the steps in R to check whether the data are suited will not be presented here, as you can look those up in the previous chapter. To summarize: The data were suited for analysis with factor analysis and SEM, but the distribution of the variables is somewhat left-skewed. Therefore, an estimation method that is robust against non-normality should be used.\nThe one characteristic that needs additional investigation is the sample size. As mentioned in the previous chapter on factor analysis, Bentler and Chou [23] recommend having 5 observations per estimated parameter, while Jackson [24] recommends having 10, and preferably 20 observations, for each parameter you want to estimate. Therefore, the total number of observations depends on the exact model fitted to the data. If you start your SEM analysis with going through all three recommended model-building steps from the previous chapter (see the “Integrated Strategy for Structural Equation Modeling” above), you need to base your required sample size on the number of parameters estimated by an exploratory factor analysis (e.g., for a two-factor model fitted to 10 variables, you would need about (20 factor loadings + 10 intercepts + 10 error-variances) * 10 = 400 cases) and multiply that by 2 to also have enough observations for a new or holdout sample to check generalizability of your factor-model and for fitting your SEM model. Note that the sample size calculated this way is what you need in each relevant subgroup for which you want to assess measurement invariance. As mentioned above, you could go straight to the SEM analysis if you use an existing instrument on a sample from a population for which the instrument has already been validated and shown to be invariant across relevant groups in previous research. In that case, you would need fewer observations because a SEM model (like a CFA model) will have more constraints than an exploratory factor analysis model and hence fewer parameters to estimate.\nYou will perform the SEM analysis using the holdout sample from the previous chapter. The sample size for that is 438, which is sufficiently large to estimate 20 intercepts, 10 loadings, 10 unique variances, and 10 relations between factors according to the more lenient rules by Bentler and Chou [23]. As you will see in Step 2, this number of parameters aligns well with the SEM model that will be fitted to the data.\n\n\n\n5.2 Step 1: Steps from the previous chapter\nAs explained in the strategy section, it is recommended to go through all the steps mentioned in the previous chapter (exploring the data structure, building the factor model and assessing fit, and assessing generalizability), and the R code for these steps can also be found there. For brevity, the steps and code will not be repeated here. Additionally, configural invariance (tested by following the steps from the previous chapter for each relevant subgroup separately) will be assumed to hold so that this section can focus on the code needed for a SEM analysis. We begin with picking up at the code for randomly assigning 438 rows of the data to a holdout dataset:\n\nset.seed(19)\nind <- sample(c(rep(\"model.building\", 438), rep(\"holdout\", 438)))\ntmp <- split(dataset, ind)\nmodel.building <- tmp$model.building\nholdout <- tmp$holdout\n\nFor an explanation of the code, please go back to the previous chapter.\n\n\n5.3 Step 2: Assessing Higher Levels of Invariance\nBefore looking into the structural relationships of interest, you have to ensure that the loading (and possibly intercept) invariance holds across groups for which this might be violated according to theory. To this end, you first have to specify the model, which is very similar to the model that was specified in the previous chapter for the CFA model. The model syntax looks as follows:\n\nSEM_model <- '\n# Regressing items on factors \nTSC =~ TSC1 + TSC2 + TSC3 + TSC4 + TSC5\nTE =~ TE1 + TE2 + TE3 + TE5\nEE =~ EE1 + EE2 + EE3 + EE4\nDE =~ DE1 + DE2 + DE3\nRPA =~ RPA1 + RPA2 + RPA3 + RPA4\n\n# Relations between factors\nTE ~ TSC\nEE ~ TE + TSC\nDE ~ TE + TSC\nRPA ~ TE + TSC\n'\n\nThe first part of this syntax is exactly the same as the syntax that defined the CFA model in the previous chapter, with the syntax elements “=~” indicating that a factor is measured by specific observed variables. The code above defines a model where the factors TSC, TE, EE, DE, and RPA are measured by different sets of variables, which are separated by “+”. In addition to the measurement part, this syntax includes the specification of relations between factors using the “~” operator. For example, TE is regressed onto TSC, indicating a directional relationship from TSC to TE. When comparing this syntax to the one from the previous chapter, you will see that the only thing that changes when you move from CFA to SEM is that you specify concrete structural relations (i.e., which latent factors are regressed on each other) instead of solely specifying correlations. Although intercepts are not explicitly mentioned in the model, they can be included by using the meanstructure = TRUE argument in the command when estimating the model.\nOnce the model syntax is specified, you can check whether invariance across groups (here, gender) holds. More specifically, you check this by comparing a model with and without equality constraints on the parameters of interest (here, as motivated before, only the loadings) and check whether adding constraints reduces model fit more than desired. To evaluate the degree of fit reduction, you can look at the changes in the global fit measures described in the previous chapter. These were (1) the Chi-squared significance test, (2) the comparative fit index (CFI), (3) the root mean square error of approximation (RMSEA), and (4) the standardized root mean square residual (SRMR). Recall that, unlike the Chi-squared significance test that assesses perfect fit, the CFI, RMSEA, and SRMR assess approximate fit. Similarly, only the Chi-squared significance test is a formal test for assessing invariance. As for assessing global fit, each fit measure is accompanied by rules of thumb when assessing invariance, allowing you to decide whether or not (approximate) invariance holds. The difference is that the rules of thumb now pertain to differences in the criteria between models with and without constraints. The Chi-significance test should be nonsignificant because otherwise, the model with constraints fits significantly worse. Keep in mind, however, that this test easily rejects the assumption of invariance with increasing sample size. Regarding the other three measures, the rules of thumb are that differences in CFI should be smaller than or equal to .01, differences in RMSEA should be smaller than or equal to .015, and differences in SRMR should be smaller than or equal to .030 [25].\nYou do not have to perform the invariance assessment by constraining parameters yourself. Instead, you can use the command below. The argument model asks you to specify the model syntax, data asks for the dataset that you want to use for the analysis, and group refers to the variable for which you want to assess invariance. The argument estimator indicates which estimation procedure is used. The default is standard maximum likelihood (“ML”) estimation. However, for the current data, a robust maximum likelihood (“MLR”) estimation is applied to account for small violations of the normality assumption. If the data contain missing values, you can add the argument missing and specify it as equal to “fiml”, corresponding to a full information maximum likelihood approach. As already discussed in the previous chapter, this is a sensible approach if you have at least missing at random (MAR) data details about missing data mechanisms can be found in the lavaan tutorial [26]. Next, the argument “intercept” indicates whether intercept invariance should be assessed in addition to loading invariance. If put to TRUE, the output contains information on both loading and intercept invariance unless two or more of the four fit criteria indicate that loading invariance is violated because then, assessing intercept invariance does not make sense. This information would be provided in the form of a message. Finally, the argument “display” indicates whether output about group-specific parameters and differences are provided (when set to TRUE) or not (when set to FALSE).\n\ninvarianceCheck(model = SEM_model, data = holdout, \n                group = \"gender\", estimator = \"MLR\", intercept = FALSE, \n                missing = FALSE, display = FALSE)\n\nNested Model Comparison------------------------------------------------------------------- \n \n\nScaled Chi-Squared Difference Test (method = \"satorra.bentler.2001\")\n\nlavaan->unknown():  \n   lavaan NOTE: The \"Chisq\" column contains standard test statistics, not the \n   robust test that should be reported per model. A robust difference test is \n   a function of two standard (not robust) statistics.\n                    Df   AIC   BIC  Chisq Chisq diff Df diff Pr(>Chisq)\nLoadings_Free      320 15711 16283 648.06                              \nLoadings_Invariant 335 15693 16203 660.17     9.8747      15     0.8275\n\nModel Fit Indices ------------------------------------------------------------------------ \n \n                   chisq.scaled df.scaled pvalue.scaled rmsea.robust cfi.robust\nLoadings_Free          632.823†       320          .000        .068       .923 \nLoadings_Invariant     638.987        335          .000        .065†      .925†\n                    srmr\nLoadings_Free      .050†\nLoadings_Invariant .053 \n\nDifferences in Fit Indices --------------------------------------------------------------- \n \n                                   df.scaled rmsea.robust cfi.robust  srmr\nLoadings_Invariant - Loadings_Free        15       -0.002      0.002 0.002\n\nLoading Invariance Interpretation -------------------------------------------------------- \n \nThe hypothesis of perfect loading invariance *is not* rejected according to the\n      Chi-Square difference test statistics because the p-value is larger or equal to 0.05. \n \nThe hypothesis of approximate loading invariance *is not* rejected\n        according to the CFI because the difference in CFI value is smaller than\n        or equal to 0.01. \n \nThe hypothesis of approximate loading invariance *is not* rejected\n       according to the RMSEA because the difference in RMSEA value is smaller\n       than or equal to 0.015. \n \nThe hypothesis of approximate loading invariance *is not* rejected\n       according to the SRMR because the difference in SRMR value is smaller than\n       or equal to 0.030. \n \n\n\nInspecting the output, you can see several sections. You may directly go to the last section “Loading Invariance Interpretation” because this provides you with information on whether or not invariance is rejected according to the four fit measures (following the cut-off values provided before). If you are interested in (reporting) details about the Chi-square significance test and the (differences in) model fit criteria, you can also inspect the beginning of the output.3 It can be seen that invariance is not rejected according to any of the criteria. Similar to assessing model fit in factor analysis, it is advised that no more than one criterion should reject invariance for concluding that invariance holds. If this is initially not the case, you may release one equality constraint at a time (i.e., moving towards partially invariant models) and run the invarianceCheck() function again until partial invariance holds. In order to decide which constraints to release, you can consult the information provided when the “display” argument is set to TRUE. The output will then show the loadings and intercepts per group as well as the differences in these parameters across groups. You will also get a message indicating which loading (and intercept) differs most between groups. These would be the first targets to estimate freely across groups (while making sure that the minimum requirements for partial invariance mentioned above hold). How to update the model syntax to freely estimate parameters between groups is explained on the lavaan package website [26].\n\n\n5.4 Step 3: Building the Structural Equation Model and Assessing Fit\nThe next step is to perform the SEM analysis on the holdout data using the specified SEM model with the following command:\n\nsem_robust <- sem(model = SEM_model, data = holdout, std.lv = TRUE,\n              estimator = \"MLR\", meanstructure = TRUE)\n\nThe arguments are the same as for the cfa() function discussed in the previous chapter. The relevant (standardized) output about the structural relations can be extracted from the SEM model with the command below. The first argument asks you to specify the lavaan object from which you want to extract the structural relations, nd lets you specify the number of decimals you want to display (with default 3).\n\nsem_structural_results(sem_robust, nd = 3)\n\n\n\n  \n\n\n\nLooking at the output from left to right, you can see the outcome variable of the structural relationship, the predictor of the structural relationship, the regression coefficient, the standard error of the regression coefficient, and the p-value of the effect. Here, you see that all structural relations are significantly different from zero and thus that TE, EE, DE, and RPA are all related to TSC and that EE, DE, and RPA are related to TE. As mentioned, the function sem_structural_results() displays the most relevant information about the structural relations. For more information and unstandardized estimates, you can use the summary() function on the lavaan object sem_robust also see the lavaan tutorial by [27]."
  },
  {
    "objectID": "chapters/ch21-sem/ch21-sem.html#conclusion",
    "href": "chapters/ch21-sem/ch21-sem.html#conclusion",
    "title": "21  Structural Equation Modeling with R for Education Scientists",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nSEM is a rich, powerful, and mature statistical framework that has more than a century of evolution and expansion. Several other extensions have been developed to include dynamic structural equation modelling for the analysis of temporal data, latent growth curve modelling for the analysis of systematic change over time, and multilevel SEM for the analysis of hierarchically clustered (or nested) data, to mention just a few. This chapter merely provided a primer that introduces the basics of the methods and an example application. Hopefully, it opens the door for interested readers to explore such a powerful framework that offers an excellent solution to many of the analytical tasks that researchers seek."
  },
  {
    "objectID": "chapters/ch21-sem/ch21-sem.html#further-readings",
    "href": "chapters/ch21-sem/ch21-sem.html#further-readings",
    "title": "21  Structural Equation Modeling with R for Education Scientists",
    "section": "7 Further readings",
    "text": "7 Further readings\nIn this chapter, you have seen an introduction and tutorial on how to apply SEM in educational research. To learn more about how SEM can be applied to this field, you can consult these resources:\n\nTeo, T., Ting Tsai, L., & Yang, C. 2013. \"Applying Structural Equation Modeling (SEM) in Educational Research: An Introduction\". In Application of Structural Equation Modeling in Educational Research and Practice.\nKhine, M. S., ed. 2013. “Application of Structural Equation Modeling in Educational Research and Practice”. Contemporary Approaches to Research in Learning Innovations.\n\nTo learn more about SEM in general, you can refer to the following:\n\nKline, R. B. 2015. “Principles and Practice of Structural Equation Modeling”. 4th Edition. Guilford Publications.\nHoyle, Rick H. 2012. “Handbook of Structural Equation Modeling”. Guilford Press."
  },
  {
    "objectID": "chapters/ch22-conclusion/ch22-conclusion.html",
    "href": "chapters/ch22-conclusion/ch22-conclusion.html",
    "title": "22  Why educational research needs a complex system revolution that embraces individual differences, heterogeneity, and uncertainty",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "chapters/ch22-conclusion/ch22-conclusion.html#introduction",
    "href": "chapters/ch22-conclusion/ch22-conclusion.html#introduction",
    "title": "22  Why educational research needs a complex system revolution that embraces individual differences, heterogeneity, and uncertainty",
    "section": "1 Introduction",
    "text": "1 Introduction\nLearning analytics (LA) has emerged to harness the opportunities created by the abundance of data and advanced machine learning methods to improve learning and teaching and offer the much-needed personalized support. The premise was that the availability of massive amounts of data would enable novel insights, improve inferences, and deliver real-life impact [1]. A wide array of learning analytics applications has been developed over the years to realize such aspirations. One of the initial applications of learning analytics focused on predictive modeling: that is, collecting data of online activities, such as clicks, access to educational resources, or forum discussions to create a predictive model that would early flag underachievers. The early identification of an underachieving student in a course paves the way for proactive intervention [2]. Several studies have reported the successful identification of underachievers in individual courses or limited samples. Yet, transferring such models across programs or courses has been a consistent disappointment. All the more so, very few have reported a successful proactive intervention [2].\nA recent massive study with data from 250,000 students tried to examine the effectiveness of a large-scale, evidence-based intervention, and reported small benefits. The researchers concluded that interventions are likely more effective when implemented for the right person, at the right moment in time. In fact, such a conclusion is far from new, Gordon Paul's stated in 1967 that the important question is “What treatment, by whom, is most effective for this individual with that specific problem, and under which set of circumstances?”[3]. This requires predictions at a dynamic (i.e., time-varying) and individual level [4]. Another recent large-scale study showed that a low proportion of variance in students’ performance was explained by the behavior-based indicators [5], and thus, students should best be identified on internal conditions (e.g., knowledge, self-regulation, and motivation). Recent reviews of intervention using LA methods further emphasize the difficulty of these promises [6]. As Du et al. [7] stated that “every course has different course requirements, it is impossible to identify generalisable thresholds of individual behaviors across courses” (p. 510). This holds even when courses are similar, homogenous, and use the same teaching [8].\nThe lackluster of predictive LA has led to a wide range of research threads aiming to tap into other methods that help explain and optimize students’ learning. Such methods have been used to analyze the relational temporal patterns of students’ learning processes. For instance, building on the importance of time, Process Mining (PM) and Sequence Analysis (SM) have gained wide popularity in the analysis of online learning activities to explain the time-ordered patterns of learning activities and to capture patterns of learning strategies [9]. Social network analysis (SNA) has also gained renewed interest and wider application in collaborative learning settings to understand students’ roles and interaction patterns and to find SNA measures that help predict performance [10, 11]. Nevertheless, most of such methods —which we covered with examples in the book— require an overarching framework or a theoretical underpinning to better ground the analysis. In this chapter, we discuss the importance and potential of complex systems in understanding learning, learners, and the educational milieu at large."
  },
  {
    "objectID": "chapters/ch22-conclusion/ch22-conclusion.html#complex-systems-and-education",
    "href": "chapters/ch22-conclusion/ch22-conclusion.html#complex-systems-and-education",
    "title": "22  Why educational research needs a complex system revolution that embraces individual differences, heterogeneity, and uncertainty",
    "section": "2 Complex systems and education",
    "text": "2 Complex systems and education\nMost learning theories and frameworks can be conceptualized as systems, that is, composed of multiple components, phases, or elements that interact with each other. Typically, such interactions are non-linear and vary across people, contexts, and time scales resulting in the emergence of a unique learning process [12–14]. For instance, engagement can be considered as a complex system. Engagement is then viewed as the result of interaction between different components, namely behavioral, cognitive, and emotional dimensions [15, 16]. Such interactions vary between tasks, times, and contexts which are often referred to as interaction-dominant systems (Figure 22.1 B) [17, 18]. In interaction-dominant systems, the relationships between components may change intensity and direction across times and situations. For instance, a student may enjoy school in the early days which drives their engagement and achievement and boosts their motivation. These dynamics may change over time, where achievement could be the driving force of future engagement but also results in anxiety rather than enjoyment. In turn, anxiety may negatively affect school enjoyment and engagement. This dynamic view is more realistic than the common box-and-arrow models where the components of the system are rigidly assembled in a stable manner and the relationships between the components are deemed to be linear (Figure 22.1 B). Viewing the previous example from a linear perspective would entail that we see that the student will always have a stable relationship: enjoyment always drives engagement and achievement with little changes in the future nor any new patterns emerge. A linear view of engagement is then far from realistic.\nEngagement follows Gestalt principles, meaning that engagement is considered more than just the sum of its parts (i.e., engagement ≠ emotions+cognition+behavior) [16], the interactions between these components are often multiplicative rather than simple linear sum and rely on the environment and contextual conditions (family, peers, teachers, school, etc.). Additionally, most engagement theories describe feedback loops (for instance, achievement drives further engagement and vice versa) [19]. Again, such feedback loops fit well with the salient features of a complex dynamic system [14].\n\n\n\nFigure 1. A. A complex dynamic system where the interactions vary across time, change direction and strength in a soft-assembled manner. B. A box and arrow framework where the interactions are fairly stable and linear.\n\n\nSelf-regulated learning (SRL), too, follows complex systems principles: the interaction between SRL phases across different learning scenarios and temporal scales results in unique learning strategies which are different mixtures of SRL phases. Such interactions may enhance, impede, or catalyze each other. For example, reflection on performance can lead to improved learning and better goal-setting in a student. In another student, reflection can lead to frustration, and low performance. We can expect vast amounts of variations and complex interactions in the same way. Such conceptualization of SRL as a state of a complex person-environment system is necessary to understand the interplay of the intricate SRL process [13, 17]. Indeed, Boekaerts and Cascallar [20] argue that it is impossible to understand learning and achievement “unless one adopts a systems approach to the study of self-regulated learning”. Similarly, other SRL theoreticians share the same conceptualization. For instance, [21] states that SRL is “a complex system of interdependent processes” [21], and so did Winne et al. when they described engagement as complex and dynamically changeable across contexts [22].\nIn fact, many learning concepts have been already described, operationalized, and framed in complex system terms including motivational [23, 24], achievement motivation theories [25], agency [26], and metacognition theories [27]. Also, the student has been described as a complex system [28], so have small collaborative groups [29], and the classroom as a whole to list a few. The merit of this complex systems view is that it not only accounts for many of the features of learning-related processes (e.g., being interaction-dominant, lacking central control) but also provides a framework for better understanding - and perhaps even predicting - such processes [13]. Nevertheless, endorsing that a system is interaction-dominant means that we also understand that forecasting and prediction of the system's future status may be more uncertain than the simple linear dynamics of component-dominant scenarios [30]. That is, the slightest change in initial conditions may lead to substantial differences in the end state (a phenomenon called “sensitive dependence on initial conditions”). It follows that in order to understand complex systems, one has to look into the dynamics of such systems.\n\n2.1 Dynamics in complex systems\nThe interactions that maintain a complex system tend to give rise to relatively stable configurations, which can be considered attractor states that emerge again and again [31]. Attractors can take many different forms, ranging from chaotic to cyclic to simple point attractors. In the context of learning analytics, for instance, a point attractor could resemble a state of “being relatively engaged”. Importantly, the attractors of complex systems may change over time. The aforementioned point attractor could for instance gradually lose its strength, up to a point where the attractor disappears. When reaching such a tipping point, the system switches to an alternate attractor [32]. Such a shift between different attractors is often labeled a transition. Transitions can be harmful - e.g., reflecting a shift from an adaptive state towards a maladaptive state - or beneficial - e.g., reflecting a reverse shift. Relatively well-investigated dynamics are “critical transitions”, which entail a shift from one stable regime (i.e., point attractor) towards another regime [31] (i.e., other point attractor). For instance, countries can shift between a state of peace towards a state of war and the climate can shift from a greenhouse to an icehouse state Similarly, a learning child may shift between a state of engagement towards a state of disengagement. An important premise of complex systems theory is that such transitions - albeit in very different systems - follow the same generic principles. Among these principles is the idea of critical slowing down [30, 33]. Critical slowing down describes that, prior to a critical transition, it becomes increasingly difficult to recover from perturbations [33, 34]. In the case of engagement, such perturbations can be school problems (e.g., problems with other pupils). When the student is in a stable, engaged state —and thus, unlikely to experience a transition towards a disengaged state— such perturbations only have a brief effect on the student’s attention. This means that, upon a perturbation, he/she quickly recovers his/her “baseline” engagement levels [35]. As the resilience of the engaged state declines, however, the student becomes increasingly affected by these perturbations. This means that recovering his/her normative engagement becomes more and more difficult. This in turn translates to altering system dynamics, meaning that the interactions between and within system elements changes. Monitoring such changes may then allow for anticipating otherwise unpredictable transitions in learning processes [30, 33, 34]. Ultimately, this could aid the prevention of harmful transitions or the fostering of beneficial transitions.\nAn important implication of viewing transitions in learning processes through a complex systems lens is that declining resilience may be detectable within systems, which in this case means that inferences are made on the level of the student. This approach contrasts with the common group-level inferences, which may allow for telling who is likely to undergo a transition. For instance, group-level approaches may lead to the notion that “individuals with this behavior, this personality, or this socio-economic background are more likely to drop out of school than others”. Within-individual approaches, in contrast, may allow for determining when a specific individual will drop out. For the purposes of targeted and timely intervention, such insight is invaluable. A related merit of complex systems principles is that they allow for personalization. For instance, it is likely that vulnerability to major changes (e.g., transitions in engagement, school drop-out) manifests in different variables for different individuals. Because declining resilience can be monitored within individuals, such heterogeneity does not pose a challenge. Rather, it can be accommodated by monitoring resilience in those variables that are considered most relevant for this particular student, in this specific context [36]. In conclusion, the possibility to monitor generic indicators of declining resilience may pave the way for deriving person-specific insights in predicting (and potentially, preventing or stimulating) changes in learning-related processes.\n\n\n2.2 From theory to practice: measurement and analyses\nIf we agree that the learning phenomena, process or construct can be conceptualized as states in a complex system, then it becomes essential that a complex system lens is used to map the structure and dynamics of the said phenomena [37]. This has profound consequences for both measurement and data analyses. With respect to measurement, a complex systems lens necessitates the collection of time series data. The reason is that systems —and the interactions between elements within those systems— are by definition time-varying, and it is precisely the changes over time that contain information about the system as a whole. Thus, instead of a single, cross-sectional measurement, a complex systems perspective requires collecting repeated measurements for each individual. With advancing technology, collecting such measurements has become increasingly feasible. Broadly speaking, we can distinguish between passively collected data - which includes mobile sensing data (e.g., typing speed, scrolling, app usage, and sometimes also location) and actigraphy data (e.g., movement, heart rate, skin conductance) —and self-reported data— which is gathered through repeatedly prompting students with a questionnaire on their mood, motivation, or other psychological variables. Both modalities have their pros and cons. The main benefit of passively collected data is the amount of data that can be collected without burdening participants. The other side of the coin is that this amount of data often needs aggregation and intensive cleaning, which is far from straightforward, and in that sense the data can be “hard to handle”. The main benefit of self-report data is that the content of measurement may be more closely related to the construct of interest. However, self-report data require considerable motivation from participants, and it is not inconceivable that such demanding research designs introduce sampling bias. Put differently, it is possible that the types of individuals who engage in studies involving long-term self-reports are not representative of the general population (e.g., in terms of conscientiousness [38]). At the same time, however, studies that investigated sampling bias in intensive longitudinal studies involving the collection of repeated self-reports did not find evidence for self-selection [39]. As intuitive as it may seem, scientific evidence for the self-selection of participants into intensive longitudinal studies is thus lacking. Besides these relatively practical considerations, the necessity of time series data also comes with more fundamental questions, for instance, related to the timescale of assessments. Ideally, this timescale should be informed by the timescale at which the system’s dynamics unfold. This in turn varies between constructs: engagement may shift over minutes, while student’s performance may shift over weeks.\nNaturally, the focus on time series data has consequences for the analyses that are useful. Not only do we require time series analyses —which can handle the temporal dependency in the data— but we also need methods that can capture nonlinear and person-specific trends. This is because the dynamics of complex systems are typically non-linear, as illustrated by the erratic behavior and sudden shifts that govern complex systems. Examples of such analytical methods include dynamic time-warp analyses, generalized additive models, recurrence quantification analysis, state space grids, and moving window analyses [17]. Despite that most learning theories and processes can be described in complex system terms and the long history of theoretical foundations of complex systems in learning sciences, learners’ and learning environments, the uptake of suitable methods and approaches is lagging behind [13, 17]. Furthermore, applications, framing, and operationalization of learning theories as complex systems are rare in educational research [13, 17]. In this book, we therefore provide some theoretical underpinnings of a complex systems perspective on learning and education, and we further included several chapters that deal with methods and analyses that accommodate a complex systems lens e.g., psychological networks, Markovian models, and model-based clustering. In other fields, the adoption of such methods has resulted in the renewal of theories, understanding of human behavior, and the emergence of new solutions to real-life problems [40, 41]. Our aim was to help interested researchers to embrace such methods in their analysis.\n\n\n2.3 Complex systems and individual differences\nComplex systems —as a paradigm— facilitates a better understanding of the heterogeneity and individualized nature of human behavior and psychological phenomena. In fact, many complex system methods, some of which described earlier, have a strong emphasis on person-specific fine-grained dynamics. The next section will offer a more in-depth discussion of the individual mechanisms and how they relate to the general average assumptions.\n\n2.3.1 The Individual\nThe “individual”, or the “self” is a central construct in several learning theories, methodologies, and approaches. For instance, self-regulated learning, self-concept, self-control, and self-directedness to mention a few [42]. Further, the literature is awash with the notion of personalization, student-centeredness, and adaptive learning. Nonetheless, research is commonly conducted using methods that essentially ignore the “individual” process. In that, research is performed using what is known as variable-centered methods where data is collected from a “group of others” to derive generalizable laws. In variable-centered methods, researchers compute standard tendency measures (mean or median) from a sample of individuals (often referred to as group-level analysis) to derive “norms” or “standard recommendations”. The average is considered a “norm” where everyone is assumed to fit. What is more, the outcome of such analysis is deemed representative and therefore, generalizable to the population at large. Given that such an average is derived from a sample of others, it rarely represents any single student [43, 44]. An accumulating body of evidence is mounting that humans are heterogeneous with diverse behaviors, attitudes, cognition, and learning approaches. Thereupon, using insights based on group-level analysis has so far resulted in recommendations that don’t work, assumptions that fail to hold, and replications that are hard to obtain. Furthermore, intervention programs or procedures based on such samples offered no more than negligible effects, e.g., [4].\nThe fact that group-level analysis is less representative of the person is far from new and has been recognized for decades. Yet, the methods that are more suited for person-specific analysis may have not progressed fast enough. The last two decades have witnessed a revolution in data collection methods, statistical approaches, and procedures that allow such analysis, collectively known as person-specific analysis. In many ways, person-specific methods are a paradigm shift in research which according to [45] represent a “brink of a major reorientation” that is “no longer an option, but a necessity”. Endorsing a person-specific approach may change how research is performed and how findings are applied [46, 47]. The person-specific methods —being individualistic— have low potential for generating generalizable recommendations [46]. Therefore, a combination of group-level and person-specific methods may be the best way forward. Such a combination may augment our understanding and provide precise interventions at the high resolution of the single student and sharpen our insights of the group level that are generalizable to the wider population [48].\nThere is an abundance of digital tools and data collection methods that allow the gathering of fine-grained intensive data about students. Such data -where several measurements from the same person are gathered- can allow the analysis of more person-specific insights. In doing so, it can help obtain an accurate view of a student's learning processes and offer more precise personalized support [45–47].\n\n\n2.3.2 Heterogeneity\nAs discussed in the previous section, a central assumption of group-level analysis is that “the average individual” represents every individual. Yet, the average individual very often does not exist [49]. To illustrate this problem, let us consider the story of Gilbert Daniels. Daniels was given the task to measure the physical dimensions of more than 4,000 pilots who were part of the American Air Force around 1950. The goal was to find the average pilot size, so that cockpits could be re-designed accordingly. However, a remarkable finding of Daniels was that not a single pilot (out of all pilots who were measured) was approximately equal to the average of the 10 most relevant dimensions. Further, for any given combination of three dimensions, only 4% of pilots would match the average. Hence, he concluded that “The tendency to think in terms of the average man is a pitfall into which many people blunder [...]. Actually, it is virtually impossible to find an average man”. The consequence of this discovery was that most cockpit material became adjustable so that it would suit everyone [50].\nIt is not difficult to translate Daniels’ findings to the field of LA. Here, too, students are measured in many dimensions. It is often implicitly assumed that the average of those dimensions will illustrate “a representative student”, but this is not the case. To accommodate this lack of “average students”, we should embrace person-centered methods, similar to how the American Airforce embraced adjustable furniture and clothing. In contrast to group-level analyses, person-centered methods attempt to find patterns where differences are minimal, assumptions are likely to hold and apply to wider groups of people. Recently, the range of available person-centered methods has vastly increased, coupled with improving rigor and potential. Therefore, person-centered methods are increasingly endorsed to model heterogeneity and individual differences across a vast range of empirical designs. In the current book, we have introduced several methods for capturing the heterogeneity of multivariate and longitudinal data, and we encourage researchers to take advantage of such data to capture the diversity and individual differences of learners [51–54]."
  },
  {
    "objectID": "chapters/ch22-conclusion/ch22-conclusion.html#conclusion",
    "href": "chapters/ch22-conclusion/ch22-conclusion.html#conclusion",
    "title": "22  Why educational research needs a complex system revolution that embraces individual differences, heterogeneity, and uncertainty",
    "section": "3 Conclusion",
    "text": "3 Conclusion\nThe birth of learning analytics signaled a new wave of educational research that embraced modern computational methods. Whereas the field has matured, several methodological and theoretical issues remain unresolved. In this chapter, we discussed the potentials of complexity theory and individual differences in advancing the field bringing a much-needed theoretical perspective that could help offer answers to some of our pressing issues. In fact, a complex systems view on learning processes can address some of the major barriers that have hampered progress in the field of education and possibly offer a venue for the renewal of knowledge."
  }
]