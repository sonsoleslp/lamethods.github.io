[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning Analytics Methods",
    "section": "",
    "text": "Learning Analytics Methods\n                Comprehensive guides for understanding and implementing learning analytics using R and advanced AI techniques.\n                \n                    Explore Books\n                    Learn More\n                \n            \n        \n\n        \n            \n                Our Books\n                \n                    \n                        \n                            \n                                \n                                \n                                    \n                                    \n                                    \n                                \n                                Learning Analytics Methods and Tutorials\n                                A Practical Guide Using R\n                                 This book includes all the basics of R as a programming language along with data cleaning, statistics, and data manipulation. It provides an accessible entry point for newcomers to learning analytics while covering major methodologies comprehensively. Each method begins with fundamental concepts, essential techniques, and basic functions before progressing to advanced innovations.\n                                \n                                    Step-by-step tutorials with practical examples\n                                    Comprehensive R code implementations\n                                    Real-world educational datasets\n                                    Authored by world-renowned researchers\n                                \n                                \n                                    Read\n                                \n                            \n                        \n                    \n                    \n                        \n                            \n                                \n                                    \n                                      \n                                    NEW!\n                                    \n                                    \n\n                                \n                                Advanced Learning Analytics Methods\n                                AI, Precision and Complexity\n                                Our new book builds upon the foundational methods introduced in the previous volume, adapting to the rapid advancements in artificial intelligence and complexity science. It provides researchers with cutting-edge methodologies at the intersection of learning analytics, AI, and complexity science, presenting novel techniques and their applications in education research.\n                                \n                                    Advanced AI applications in education\n                                    Complex systems analysis techniques\n                                    Precision learning analytics\n                                    Cutting-edge research methods\n                                \n                                \n                                    Read\n                                \n                            \n                        \n                    \n                \n            \n        \n\n        \n            \n                Motivation\n\n                The lack of resources and methodological guidance on learning analytics has been a problem ever since the field started, and continues to be a problem today. We thought that the arduous journey in learning analytics should not be endured by everyone and we decided to make that resource with the help of the learning analytics community as well as our collaborators.\n            \n        \n\n        \n            \n                Meet the Editors\n                \n                    \n                        \n                            \n                                 \n                                Mohammed Saqr\n                                \n                                Dr. Mohammed Saqr is an Associate Professor of Learning Analytics and an Academy of Finland Research Council Researcher leading the Unit of Learning Analytics at the University of Eastern Finland (UEF), School of Computing, recognized as Europe’s most productive learning analytics lab (2019–2024, Scopus/Web of Science). He earned his PhD in Learning Analytics from Stockholm University, Sweden, completed a postdoc at Université Paris Cité, France, and holds the title of Docent in Learning Analytics from the University of Oulu, Finland. \n\nMohammed has published over 200 peer-reviewed studies spanning learning analytics, AI, big data, network science, and interdisciplinary applications. His work has earned him numerous accolades, including Best Full Paper Awards at LAK24 (2024), TEEM Conferences (2023, 2024), and ICCE (2020). He was awarded Europe’s Emerging Scholar by SoLAR (2023) for advancing learning analytics research and practice, the Outstanding Paper Award at SITE (2022), and the Best Thesis Award (2018) from Stockholm University. He has also been nominated for Best Paper and Poster Awards at IEEE ICALT and EC-TEL (2024). Additionally, he has received prestigious grants, such as funding from the Academy of Finland (as PI) for idiographic learning analytics and from the Swedish Research Council (as Co-PI). \n\nMohammed serves as an Associate Editor for IEEE Transactions on Learning Technologies and Frontiers in Education, an Editorial Board Member of the British Journal of Educational Technology, and an Academic Editor for PLOS One. He has guest-edited special issues on AI and education for major journals and serves as Editor-in-Chief for three books. \n\nMohammed has delivered over 30 invited keynotes and talks, chaired tracks at major conferences such as ICCE and TEEM, contributed to senior program committees for LAK, AIED, and EC-TEL, and edited proceedings for leading conferences. His international research network includes collaborations with over 200 researchers from Europe, North America, Asia, and Australia and Africa.\n                                \n                                \n                                \n                                \n                                Visit\n                                \n                            \n                        \n                    \n                    \n                        \n                            \n                                 \n                                Sonsoles López-Pernas\n                                \n                                Dr. Sonsoles López-Pernas is an Academy Fellow at University of Eastern Finland (UEF), where she works since 2022 and holds the title of Docent (adjunct professor) in educational data mining. She obtained her Masters and PhD in Engineering from Universidad Politécnica de Madrid (Spain). During her career, she has developed several open-source software projects related to educational technology and big data analysis. She is skilled in quantitative methods that include learning analytics, machine learning, process and sequence mining, network analysis and data visualization.  \n                                Sonsoles has more than 150 publications in learning analytics, artificial intelligence, and game-based learning among other topics. She has obtained several prestigious awards and recognitions that include a research award from the Royal Academy of Doctors of Spain (RADE), the IEEE TCLT early career researcher award, the extraordinary PhD thesis award from Universidad Politécnica de Madrid, and two open source project awards in the university open source software contest (CUSL), as well as best paper awards and nominations in learning analytics and game-based learning. She has contributed to the organization of several scientific conferences and workshops (e.g., Finnish Learning Analytics and Artificial Intelligence in Education conference and Network Analysis workshop at LAK), and has been part of the program committee of LAK, AIED, ICCE, and Koli Calling among others. Sonsoles sits on the editorial board of IEEE Transactions on Education, Frontiers in Education, and PloS One. \n\n                                \n                                Sonsoles is currently funded as a fellow by the Research Council of Finland through the project CRETIC (Optimizing Clinical Reasoning in Time-Critical Scenarios: A data-driven multimodal approach). She is also the Principal Investigator of the EU-funded projects ISILA (Improving the quality and sustainability of learning using early intervention methods based on learning analytics) and ENDGAME (Escaping New Disinformation through GAmified cross-border Media literacy Education).\n                                \n                                \n                                \n                                Visit\n                            \n                        \n                    \n                \n                \n                 \n                 Together, Mohammed and Sonsoles have extensively published in top journals in the field such as Computers & Education, International Journal of Computer Supported Collaborative Learning, Computers in Human Behavior, Journal of Learning Analytics and Educational Research Review as well as top conferences e.g., LAK, ECTEL, ICALT and TEEM."
  },
  {
    "objectID": "book2/chapters/ch16-ftna/ch16-ftna.html",
    "href": "book2/chapters/ch16-ftna/ch16-ftna.html",
    "title": "Capturing The Breadth and Dynamics of the Temporal Processes with Frequency Transition Network Analysis: A Primer and Tutorial",
    "section": "",
    "text": "Capturing the dynamics of learning has become an increasingly important goal for educators and researchers due the value it brings to understanding the learning process (Saqr and Peeters 2022). This chapter introduces a novel method for capturing such dynamics based on how events are dynamically interrelated (Saqr, López-Pernas, and Tikka 2025). Frequency-Based Transition Network Analysis (FTNA) is a type of transition network analysis (TNA) where the network structure is based on modelling the frequency of transitions. FTNA models the count or number of occurrence of each transition between states and use them as weights reflecting the magnitude of dynamic relationships Zou et al. (2019). This approach is a straightforward representation of the relational dynamics within the learning process, capturing how often one state follows another without conditioning on dependencies or making strong assumptions about the process.\nIn the previous chapter, we have studied TNA, which assumes that the learning process follows a Markov process where the transitions are probabilities and the current states depends on the previous state (Saqr, López-Pernas, and Tikka 2025; Saqr, López-Pernas, Törmänen, et al. 2024). In comparison, FTNA is well-suited when the research focus is on describing, summarizing, or visually analyzing the observed data without the probabilistic assumptions —and constraints— of Markov modeling. Additionally, there are contexts where FTNA is more suitable (see below). As a method, FTNA has a rich tool set of techniques and large array of analytical functions to harness the dynamics of the learning process and its temporality. These tools enable researchers to identify the dominant events, the notable patterns, as well as to compare different processes (López-Pernas, Saqr, and Tikka 2024; Saqr, López-Pernas, Törmänen, et al. 2024). More importantly, given that FTNA inherits most of the capabilities of TNA, it can be used to draw inferences with its statistical methods, e.g., permutation and bootstrapping (Saqr, López-Pernas, and Tikka 2025)."
  },
  {
    "objectID": "book2/chapters/ch16-ftna/ch16-ftna.html#the-basic-principles-of-ftna",
    "href": "book2/chapters/ch16-ftna/ch16-ftna.html#the-basic-principles-of-ftna",
    "title": "Capturing The Breadth and Dynamics of the Temporal Processes with Frequency Transition Network Analysis: A Primer and Tutorial",
    "section": "The basic principles of FTNA",
    "text": "The basic principles of FTNA\nFTNA captures the dynamic relationship between events, i.e., how an event leads to another, follows another, or is dependent on a previous event. These events and the transitions between them are used to construct a transition (edge) between events. The aggregation of edges gives rise to the network of transitions (Schnakenberg 1976; Nicolis, Cantú, and Nicolis 2005; Zou et al. 2019). This is similar to the well-known post-reply networks - yet in reverse direction- when a reply follows a post in forum discussions or chat messages (Poquet, Saqr, and Chen 2021). However, no reply is required here, and the post-reply networks are not strictly sequential and only address conversations. FTNA extends such formulation to modeling transitions between any temporally related events, e.g., learning actions, strategies, roles, states, emotions to mention a few (Saqr, López-Pernas, and Tikka 2025). Also, FTNA models the transitions, or the succession of events rather than replies.\nWhile FTNA allows the full potential of traditional network analysis, it extends these functions with several modeling and statistical techniques (e.g., permutation, scaling, bootstrapping etc.) that enhance the analysis and provides tools for understanding how the events unfold and draw inferences using statistical rigor (Saqr, López-Pernas, Törmänen, et al. 2024; López-Pernas, Saqr, and Tikka 2024). Similarly, FTNA enables the use of most TNA functions but with a different interpretation and modeling purpose. That said, it is important here to draw a comparison with TNA which is based on Markov modeling and explain when and why FTNA may be needed (Saqr, López-Pernas, and Tikka 2025).\nMarkov-based models assume that the transition probability between events depends only on the current state (the “Markov property”) and not on prior states, meaning that the process has no memory nor is it influenced by any events beyond the immediate events (Helske et al. 2024). This assumption may be unrealistic in some processes, where transitions might depend on a broader sequence of previous states (e.g., reasoning over multiple steps). FTNA avoids this limitation, capturing sequences of behaviors without requiring this independence assumption which makes it more flexible for processes where this independence assumption may not hold. In some analyses, the goal may not be to predict future behavior based on current states (as in a Markov models) but rather to identify patterns and structural relationships among observed behaviors without imposing a model-based structure. FTNA models observed patterns of any length or breadth. This is because FTNA is flexible in representing longer-range dependencies or multi-step behaviors by aggregating transitions without conditioning on prior states. Lastly, FTNA is more appropriate for handling small datasets or processes where some transitions are rare. This is because Markov models often require substantial data to produce reliable probability estimates. In contrast, frequency-based TNA is often robust with smaller datasets, as it only tallies transitions rather than estimating conditional probabilities and therefore, does not inflate or conflate the modeled process (Saqr, López-Pernas, Törmänen, et al. 2024; López-Pernas, Saqr, and Tikka 2024).\nIt should also be mentioned that FTNA holds resemblance with frequency based process mining (López-Pernas and Saqr 2024), since both techniques can be used to represent the frequency of transitions between events or states. However, while process mining is a valuable exploration and visualization tool, it has been heavily criticized for its lack of statistical rigor when it comes to identifying (statistically) meaningful transitions and comparing networks, which are two limitations that FTNA overcomes. Moreover, current implementations in R do not allow to filter out infrequent transitions (only infrequent nodes), which limit the opportunities for meaningful analysis and visualization (López-Pernas and Saqr 2024; Helske et al. 2024)."
  },
  {
    "objectID": "book2/chapters/ch16-ftna/ch16-ftna.html#the-building-blocks-of-ftna",
    "href": "book2/chapters/ch16-ftna/ch16-ftna.html#the-building-blocks-of-ftna",
    "title": "Capturing The Breadth and Dynamics of the Temporal Processes with Frequency Transition Network Analysis: A Primer and Tutorial",
    "section": "The building blocks of FTNA",
    "text": "The building blocks of FTNA\nThe building blocks of FTNA are transitions between events where each event is a node, and the weight of the edge is the transition frequency between the edges. Rather than calculating probabilities, FTNA tracks the count of these transitions to reveal a straightforward picture of patterns in sequential data. Let us consider a hypothetical example where we model the learning transitions. Imagine we have five types of activities: Watch Video, Quiz, Read Materials, Submit Assignment, and Feedback. The FTNA model captures the frequency of students moving from one activity to another: 60 transitions from “Watch Video” to “Quiz” mean a frequency of 60, while “Quiz” leading to “Submit Assignment” shows a frequency of 100. This raw count provides a direct representation of how students move through the learning steps with no transformation or scaling (Table 1).\n\n\nTable 1. Frequency of transitions in the network\n\n\nFrom\nTo\nFrequency (weight)\n\n\n\n\nWatch Video\nQuiz\n60\n\n\nWatch Video\nRead Materials\n10\n\n\nQuiz\nSubmit Assignment\n100\n\n\nRead Materials\nSubmit Assignment\n20\n\n\nRead Materials\nFeedback\n30\n\n\nRead Materials\nQuiz\n50\n\n\nSubmit Assignment\nFeedback\n100\n\n\nWatch Video\nFeedback\n30"
  },
  {
    "objectID": "book2/chapters/ch16-ftna/ch16-ftna.html#the-mathematical-basis-of-ftna",
    "href": "book2/chapters/ch16-ftna/ch16-ftna.html#the-mathematical-basis-of-ftna",
    "title": "Capturing The Breadth and Dynamics of the Temporal Processes with Frequency Transition Network Analysis: A Primer and Tutorial",
    "section": "The mathematical basis of FTNA",
    "text": "The mathematical basis of FTNA\nThe FTNA network can be represented as \\(G = (V, E, W)\\) with three basic elements:nodes (\\(V\\)), directed edges (\\(E\\)), and weights (\\(W\\)). Nodes serve as the the elements, states or events while edges represent transitions between them. For example, if \\(v_i\\) is Watch Video and \\(v_j\\) is Quiz, the edge \\((v_i, v_j)\\) captures the transition from watching a video to attempting a quiz. The weight function \\(W(v_i, v_j)\\) represents the transition frequency—how often students move from activity \\(v_i\\) to \\(v_j\\). A weight of \\(W(v_i, v_j) = 60\\) signifies that students frequently transition from Watch Video to Quiz (60 times). From Quiz to Submit Assignment (\\(v_2 \\to v_4\\), \\(W(v_2, v_4) = 100\\)): showing that student transitioned from a quiz directly to submitting an assignment 100 times which is more than the previous edge.\nMathematically, this can be represented as follows: let \\(T_{ij}\\) denote the absolute frequency of transitions from state \\(v_i\\) to state \\(v_j\\). If we consider a state space \\(V = \\{v_1, v_2, \\ldots, v_n\\}\\), the transition matrix \\(T\\) has entries defined as \\[\nT_{ij} = \\text{number of transitions from } v_i \\text{ to } v_j.\n\\] This approach does not normalize the values, allowing the raw frequencies to capture the total count of each transition observed in the data. However, researchers can build networks in different ways and scaling that makes it easier to read or interpret namely normalized or rank-based FTNA which are discussed below.\n\nFTNA Networks based on normalized values\nCounts and raw frequencies may be conflated by several factors and are not directly comparable. That is why many network measures are best understood as ranks or scales. For instance, comparing across two classrooms where classroom A has 32 students and classroom B has 48. It becomes obvious that this is an unfair comparison, given that the larger classroom is expected to generate more data. The same can be applied about tasks which differ in duration, workload or design. Therefore, scaling allows a better way of comparison not only in the same dataset but also for comparison when data comes from different studies. In TNA, the scaling transform raw counts into values between zero and one, which as we mentioned help to compare transitions more uniformly by accounting for different sample sizes making it directly comparable regardless of absolute occurrence. One option in FTNA is min-max normalization. In our example, given that the minimum frequency is 10 and the maximum frequency 100, the formula for normalization is:\n\\[\n\\text{Normalized Weight} = \\frac{\\text{Frequency} - \\text{Minimum Frequency}}{\\text{Maximum Frequency} - \\text{Minimum Frequency}} = \\frac{\\text{Frequency} - 10}{100 - 10}\n\\]\nFor example, given that the transition from Quiz to Submit Assignment has the highest frequency, it has a normalized weight of 1.00, while Watch Video to Read Materials has a normalized weight of 0.00, because it is the least frequent transition in the network (Table 2) (Figure 1).\n\n\nTable 2. Raw counts and normalized values\n\n\n\n\n\n\n\n\nFrom\nTo\nFrequency\nNormalized Weight\n\n\n\n\nWatch Video\nQuiz\n60\n0.56\n\n\nWatch Video\nRead Materials\n10\n0.00\n\n\nWatch Video\nFeedback\n30\n0.22\n\n\nQuiz\nSubmit Assignment\n100\n1.00\n\n\nRead Materials\nSubmit Assignment\n20\n0.11\n\n\nRead Materials\nFeedback\n30\n0.22\n\n\nRead Materials\nQuiz\n50\n0.44\n\n\nSubmit Assignment\nFeedback\n100\n1.00\n\n\n\n\nAnother option is to simply use max normalization, i.e., dividing the weights by the largest weight. This option can be useful when the smallest weight is positive and we would like keep it so after normalization, whearas the min-max normalization would make such weights zero.\n\n\n\n\n\n\n\n(a) Raw frequencies\n\n\n\n\n\n\n\n(b) Normalized frequencies\n\n\n\n\nFigure 1. Network visualizations for raw frequencies and normalized values.\n\n\n\n\nFTNA Networks based on ranked Frequencies\nThe “ranked” option in TNA orders transitions in terms of their frequency and scales these ranks to fit within 0 to 1 for easier readability. Instead of counting or scaling the frequencies, this option assigns a rank based on how frequently each transition occurs, with the most common transition assigned the highest rank, and then scales these ranks. In case of ties, the average rank is used. Finally, the ranks are min-max normalized. Ranking can be particularly insightful in contexts where the order of transitions —rather than their absolute counts— offers meaningful insights. Equidistant ranking is also useful when comparing two processes generated with different mechanisms, i.e., to transform the data to comparable scales.\nThe equidistant normalized weight can be calculated for the previous table based on the rank as follows (Table 3):\n\nTop 1 and 2 (100): Normalized Weight \\(\\,= 1.0\\)\nTop 3 (60): Normalized Weight \\(\\, \\approx 0.769\\)\nTop 4 (50): Normalized Weight \\(\\,\\approx 0.615\\)\nTop 5 and 6 (30): Normalized Weight \\(\\,\\approx 0.385\\)\nTop 7 (20): Normalized Weight \\(\\,\\approx 0.154\\)\nTop 8 (10): Normalized Weight \\(\\, = 0.0\\)\n\n\n\nTable 3. Final Table of Raw Frequencies, Ranks, and Normalized Weights\n\n\n\n\n\n\n\n\n\nFrom\nTo\nFrequency\nRank\nScaled rank\n\n\n\n\nQuiz\nSubmit Assignment\n100\n7.5\n1.0\n\n\nSubmit Assignment\nFeedback\n100\n7.5\n1.0\n\n\nWatch Video\nQuiz\n60\n6\n0.769\n\n\nRead Materials\nQuiz\n50\n5\n0.615\n\n\nWatch Video\nFeedback\n30\n3.5\n0.385\n\n\nRead Materials\nFeedback\n30\n3.5\n0.385\n\n\nRead Materials\nSubmit Assignment\n20\n2\n0.154\n\n\nWatch Video\nRead Materials\n10\n1\n0.0\n\n\n\n\nWe can visualize the differences between the two networks in Figure 2.\n\n\n\n\n\n\n\n(a) Raw frequencies\n\n\n\n\n\n\n\n(b) Rank\n\n\n\n\n\n\n\n(c) Scaled rank\n\n\n\n\nFigure 2. Network visualizations for raw Frequencies, Ranks, and Normalized Weights."
  },
  {
    "objectID": "book2/chapters/ch16-ftna/ch16-ftna.html#ftna-features",
    "href": "book2/chapters/ch16-ftna/ch16-ftna.html#ftna-features",
    "title": "Capturing The Breadth and Dynamics of the Temporal Processes with Frequency Transition Network Analysis: A Primer and Tutorial",
    "section": "FTNA features",
    "text": "FTNA features\nFTNA inherits most of the capabilities of TNA that were discussed in the TNA chapter which will be mentioned here briefly. However, some features may not be transferable or applicable to FTNA and some features are only available in FTNA which will be discussed here (Saqr, López-Pernas, and Tikka 2025).\n\nVisualization\nFTNA visualization helps researchers get a bird’s eye view of the modeled process and capture patterns, central learning events, and how they are temporally related to each other as well as the strength of these connections. FTNA plotting relies on the qgraph framework which offers a rich and modern framework for network visualization with wide array of features (Epskamp et al. 2012). These visualization features include custom and fixed layouts to enable comparison across processes as well as several customization options for node, edge shapes, forms and sizes. Comparative plots can be enhanced by having fixed layouts to allow easy comparisons. Furthermore, most FTNA output can be plotted using the plot function which includes networks, centrality objects and communities.\n\n\nNetwork measures\nSimilar to TNA, FTNA offers network measures at the edge, node and network levels. Network level measures provide insights about the whole network structure, e.g., network connectivity, density, and dominance of events. Node level measures —e.g., centrality measures— allow researchers to quantify or rank the importance of a state or event in the network. Researchers can find which behaviors or cognitive states are central to the learning process, which events are more important (receive more transitions), or bridges other connections (betweenness centrality) (Newman 2018; Saqr et al. 2022; Saqr, López-Pernas, Conde-González, et al. 2024).\nGiven that FTNA allows multi-step modeling, centralities that capture the downstream diffusion of events or propagation of a certain state are more plausible here. For instance, diffusion centrality can help understand which events are more likely to diffuse or result in more interactions or long threads of related events, e.g., longer “chains of thoughts” or multiple steps in a learning process (Saqr and López-Pernas 2021). Given that FTNA is built with emphasis on edges, edge centralities are rather important here. Edge-level measures —e.g., edge centralities— can tell which transitions are central in the learning process and are therefore critical to the models (Csardi and Nepusz 2006).\n\n\nNetwork structure and patterns\nPatterns are special configurations or constellations that manifest in the data as strong connections between network nodes. They are the hallmark of the structure and the building blocks of the dynamics of the learning process. What makes patterns unique is that they tell how the data was generated, how the underlying process shaped learning or led to the emergence of certain patterns (Winne 2010; Saqr and Peeters 2022; Saqr, López-Pernas, and Tikka 2025). For instance, we expect a strong connection in a network of self-regulation between planning and task execution in well-performing students. We also expect to see strong connections between dialogue moves of argument and agreement in cohesive groups. Finding such patterns allows us to, e.g., track students, offer them support, verify existing theories about the learning processes and also test existing ones. Like TNA, patterns may include cliques: dyads (two strongly connected nodes), triads (three strongly connected nodes), or communities (of any size) which reflect underlying patterns of connections inferred from the network (Saqr, López-Pernas, and Tikka 2025).\n\n\nNetwork validation\nLike TNA, FTNA offer several statistical methods for verifying and establishing the significance of identified edges through bootstrapping. Bootstrapping is a re-sampling technique that in our context helps filter out small, negligible or unstable edges resulting in a rigorous model. In doing so, researchers can understand which parts of the model are weak and therefore, cannot be reliable in future analyses. Significant edges here represent transitions that are likely to appear in future iterations, and more importantly cannot be used for inference about future processes López-Pernas, Saqr, and Tikka (2024). Bootstrapping is a unique feature in TNA and is not offered by other methods, e.g., process mining (López-Pernas and Saqr 2024) or social network analysis (Saqr, López-Pernas, Conde-González, et al. 2024). The inclusion of bootstrapping in FTNA along with other statistical inference methods (permutation and case-dropping for centrality) are in fact what differentiates FTNA from traditional network analysis. This is of course in addition to the distinct data sources, the modeling choices as well as the contextual differences (Saqr, López-Pernas, and Tikka 2025; López-Pernas, Tikka, and Saqr 2025).\n\n\nComparison between groups\nResearchers and educators encounter conditions where groups may differ based on contexts, such as students who engage in collaborative versus non-collaborative learning, high versus low levels of achievement, or learning formats like problem versus project based learning. When researchers like to compare the learning process resulting from either conditions, they can do this using counts, e.g., comparison of frequencies or other techniques like process mining. Typically, such process comparisons relies on visual inspection, such as contrasting process maps or network models, which can reveal the descriptive differences but fall short of demonstrating the statistical significance or pinpointing where these differences matter the most (Saqr, López-Pernas, and Tikka 2025; Borkulo et al. 2023).\nFTNA enhances this comparison by using a robust permutation-based approach to process comparison. Permutation allows not only a clear visual representation of differences but also enables researchers to estimate the statistical significance of each transition in the network and automatically quantifying the magnitude of differences in transition frequencies. This level of statistical rigor supports more meaningful inferences and contributes to theoretical development."
  },
  {
    "objectID": "book2/chapters/ch16-ftna/ch16-ftna.html#ftna-tutorial",
    "href": "book2/chapters/ch16-ftna/ch16-ftna.html#ftna-tutorial",
    "title": "Capturing The Breadth and Dynamics of the Temporal Processes with Frequency Transition Network Analysis: A Primer and Tutorial",
    "section": "FTNA tutorial",
    "text": "FTNA tutorial\nFTNA is capable of analyzing any data that can be represented as as a temporal sequence with transitions between events or states occurring over time. FTNA is particularly well-suited for categorically ordered event data, such as sequences of learning events, phases, roles, dialogue moves, or interactions, to name a few. The source of this data can vary, including time-stamped learning management system data, coded interactions, event-log data, or ordered event data.\nIn this tutorial, we will use the tna R package (López-Pernas, Saqr, and Tikka 2024), alongside other essential packages that facilitate data manipulation and visualization. It is assumed that the reader is familiar with the R programming language. If that is not the case, it is recommended to refer to previous tutorials on the basics of R (Tikka et al. 2024a), data cleaning (Kopra et al. 2024), basic statistics (Tikka et al. 2024b), and visualization (López-Pernas et al. 2024). It is also recommended to have prior knowledge about Markov models (Helske et al. 2024) and to have read the previous chapter on TNA (Saqr, López-Pernas, and Tikka 2025).\nInitially, the process involves loading the necessary packages to set the groundwork for analysis. The tna R package is the main package that we will rely on for analysis which will build the FTNA models. The tna package provides a wide range of functions for estimating, visualizing, and manipulating FTNA models. In addition, it offers various functions for calculating network metrics, such as centrality measures, and discerning patterns like dyads and triads. The package also includes verification functions, such as bootstrapping and permutation tests. We will also use the tidyverse package for data manipulation, wrangling and visualization (Wickham et al. 2019).\nThe build_model function from the tna package accept several types of data, namely, sequence data, data frames in wide format and square matrices. Sequence data must be in stslist format (sequence object), which is typically created using the seqdef() function from the TraMineR package (Gabadinho et al. 2011; Saqr, López-Pernas, Helske, et al. 2024). This sequence defines the data and its order (for guidance on creating a sequence object, please refer to (Saqr, López-Pernas, Helske, et al. 2024)). Also, tna can accept wide data format where each row represents sequential data and each column is a time point with no extra columns. Finally, the tna package can also process event data with the highly flexible prepare_data() function. Like in Table 4, each row represents a sequence by a person, group or a in a task, and column represent the order of such events.\n\n\nTable 4. Wide format data which can be used in FTNA\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n\n\n\n\nSubmit Assignment\nWatch Video\nRead Materials\nWatch Video\nRead Materials\n\n\nWatch Video\nRead Materials\nSubmit Assignment\nRead Materials\nWatch Video\n\n\nRead Materials\nWatch Video\nWatch Video\nSubmit Assignment\nRead Materials\n\n\nRead Materials\nSubmit Assignment\nWatch Video\nRead Materials\nWatch Video\n\n\nWatch Video\nRead Materials\nWatch Video\nRead Materials\nSubmit Assignment\n\n\n\n\nThe FTNA analysis starts by constructing the model which is simply performed by passing the data to the ftna() function or to the build_model() function specifying the type like build_model(data, type = \"frequency\"), or simply ftna(data). The FTNA model contains all the information necessary for subsequent analyses, such as plotting, centrality estimation, or model comparison. We can also scale the frequencies using the scaling argument, which can be:\n\n“minmax”: This option scales the frequencies between 0 and 1 via min-max normalization such that the lowest weight becomes 0 and the largest becomes 1. Note that if all frequencies are positive, then the lowest will be converted to a weight of zero.\n“max”: The frequencies are divided by the largest frequency. This option also scales the frequencies such that the largest weight is 1, but the lowest will only be 0 if some transition never occurred in the original data.\n“rank”: This involves ranking all the frequencies from highest to lowest with the average used for ties. Ranking highlights the most significant transitions and provides a standardized way to compare processes across different contexts.\n\nWe can also perform multiple types of scaling sequentially, for example scaling = c(\"rank\", \"minmax\") would first rank the frequencies and them scale them between 0 and 1 with the min-max normalization.\nThe (F)TNA model is organized as a list that has four elements to facilitate the analysis and can be accessed by the users if needed. These are:\n\nWeights (weights): These are the transition weights (frequencies or scaled frequencies) of moving from one state or event to another, serving as the primary element for various network processes, analyses, and visualizations.\nInitial States (inits): These are the probabilities of starting in each state at the first time point. Understanding these initial states helps show how events evolve in relation to their starting conditions.\nLabels (labels): The descriptive names for each node in the network and is included to enhance the interpretability of the analysis. The labels are automatically retrieved from the alphabet of the sequence object or the categories in the data frame.\nData (data): This is a version of the sequence data (or data frame) that contains all the necessary sequence data, converted into an internal format used by the tna package for further analysis (permutation, bootstrapping etc.).\n\nThe code below loads the tna package. Next, it retrieves the built-in group_regulation dataset included in the package, which has data on group behaviors related to regulation during a collaborative project based on the paper by (Saqr, López-Pernas, Törmänen, et al. 2024). Then we call the ftna() function with the group_regulation dataset as the data argument. The resulting model is an FTNA model with frequencies as edge weights and stored in an R object called model, which can be further utilized for analysis. We also use call print(model) to view the model. The model output shows the labels, the transition frequency matrix and the initial probabilities.\n\n# Install 'tna' package from CRAN if needed (uncomment if required).\n# install.packages(\"tna\")\n\n# Load packages\nlibrary(\"tna\")\nlibrary(\"tidyverse\")\n\n# Load example data provided within the 'tna' package, \n# representing group regulatory interactions\ndata(group_regulation)\n\n# Run FTNA on 'group_regulation' data using raw counts of \n# transitions (\"absolute\" type) and print the result\nmodel <- ftna(group_regulation)\n\n# Print the output to inspect the model\nprint(model)\n\n\n\nState Labels\n\nadapt, cohesion, consensus, coregulate, discuss, emotion, monitor, plan, synthesis \n\nTransition Frequency Matrix\n\n           adapt cohesion consensus coregulate discuss emotion monitor plan synthesis\nadapt          0      139       243         11      30      61      17    8         0\ncohesion       5       46       844        202     101     196      56  239         6\nconsensus     30       94       519       1188    1190     460     295 2505        48\n [ reached getOption(\"max.print\") -- omitted 6 rows ]\n\nInitial Probabilities\n\n     adapt   cohesion  consensus coregulate    discuss    emotion    monitor       plan  synthesis \n     0.011      0.060      0.214      0.019      0.175      0.151      0.144      0.204      0.019 \n\n\nAs we mentioned above, FTNA can be estimated with a scaled transition matrix where the highest transition frequency has the weight of 1 and the lowest has the weight of 0. The resulting transition matrix is easier to read and interpret and more importantly can be useful when comparing across datasets with different samples sizes. The following code estimates a FTNA model using the argument scaling = \"minmax\". All of the subsequent analysis will be performed using the scaled version given its ease of use, interpretability and ease of comparison across datasets. \n\n# Calculate the Transition Network Analysis (TNA) on the group_regulation \n# data with scaled weights between 0 and 1\nmodel_scaled <- ftna(group_regulation, scaling = \"minmax\")\nprint(model_scaled) # Print the FTNA model with scaled weights\n\n\n\nState Labels\n\nadapt, cohesion, consensus, coregulate, discuss, emotion, monitor, plan, synthesis \n\nTransition Frequency Matrix\n\n             adapt cohesion consensus coregulate discuss emotion monitor    plan synthesis\nadapt      0.00000  0.05549     0.097    0.00439  0.0120  0.0244 0.00679 0.00319   0.00000\ncohesion   0.00200  0.01836     0.337    0.08064  0.0403  0.0782 0.02236 0.09541   0.00240\nconsensus  0.01198  0.03752     0.207    0.47425  0.4750  0.1836 0.11776 1.00000   0.01916\n [ reached getOption(\"max.print\") -- omitted 6 rows ]\n\nInitial Probabilities\n\n     adapt   cohesion  consensus coregulate    discuss    emotion    monitor       plan  synthesis \n    0.0115     0.0605     0.2140     0.0190     0.1755     0.1515     0.1440     0.2045     0.0195 \n\n\n\n\n\nThe last type of model is the ranked model which can be estimated using the argument scaling = \"rank\". \n\n# Calculate the Transition Network Analysis (TNA) on the `group_regulation` \n# data with ranked weights\nmodel_ranked <- ftna(group_regulation, scaling = \"rank\")\nprint(model_ranked) # Print the FTNA model with ranked weights\n\n\n\nState Labels\n\nadapt, cohesion, consensus, coregulate, discuss, emotion, monitor, plan, synthesis \n\nTransition Frequency Matrix\n\n           adapt cohesion consensus coregulate discuss emotion monitor plan synthesis\nadapt        2.0     42.0        52       11.5    19.5    31.0      14  9.0       2.0\ncohesion     4.0     25.5        72       48.0    38.0    47.0      30 51.0       5.5\nconsensus   19.5     36.0        67       76.0    77.0    64.0      57 81.0      28.0\n [ reached getOption(\"max.print\") -- omitted 6 rows ]\n\nInitial Probabilities\n\n     adapt   cohesion  consensus coregulate    discuss    emotion    monitor       plan  synthesis \n    0.0115     0.0605     0.2140     0.0190     0.1755     0.1515     0.1440     0.2045     0.0195 \n\n\n\nPlotting\nPlotting the FTNA model is a simple process. We simply use the plot() function with the model as the argument i.e., the model_scaled object we estimated earlier. In fact, most FTNA objects can be visualized using the plot() function directly without the need to any arguments. This include centralities, communities, permutation.\nAs the code below shows, the plot is overly dense and hard to read. This is because even rare and small edges are plotted. In our model, we have 9 nodes, the model includes up to 81 edges and this is far from informative due to the multidimensionality and the difficulty in picking the patterns that matter. The easiest way it to set a minimum value which will hide (but not delete) the edges below that threshold. In the example below, we set the minimum to 0.01 just to show all the model but you can set it to 0.05 or more if you want.\n\n\n\n\n\nInterpretation of the model\nThe model shows strong transitions from “consensus” to “plan,” “discuss” to “consensus,” and “emotion” to “cohesion” highlighting a network structure where agreement, dialogue, and emotional regulation play central roles in influencing and organizing the self-regulated collaboration. The transition between “consensus” and “plan” shows a high transition frequency of 1000 (simply means that it is the highest transition frequency in the network). This indicates a strong direct link where achieving consensus is usually followed by planning further steps. Another prominent transition exists from “plan” to “consensus” with a transition strength of 0.71, showing a feedback loop reinforcing a cyclic pattern of planning and consensus building within SRL.\n“Discuss” also appears as a central node with strong outward transitions to “consensus” (0.51) and “emotion” (0.31), highlighting that discussions often yield mutual understanding or emotional bonding. Similarly, “emotion” shows a strong connection to “cohesion” with a transition frequency of 0.368, and to “consensus” (0.362), reflecting how emotional states may influence group cohesion and shared agreement. Other important transitions include “coregulate” to “consensus” (0.474) and “monitor” to “consensus” (0.091), revealing that collaborative regulation and monitoring are necessary for building agreement and cohesion. We also see, “adapt” and “synthesis” show lower transition frequencies overall, suggesting that these may either serve more limited roles in the learning process where synthesis or adaptation occurs with fewer preceding dependencies. In general, we see strong influence on planning and task enactment where emotions and discussion play an important role with less emphasis on adaptation or monitoring.\n\n\nPruning\nIn many instances, you may want to prune the model, i.e., remove small edges and continue to work with a trimmed model that contain only strong-enough edges. The prune() function is designed to do that by removing —pruning— edges based on specified criteria. The options of pruning are designed to remove small edges (either based on threshold or percentile) or using an algorithm that can retrieve the backbone of the network. The three options are:\n\nThreshold-based pruning allows researchers to specify a fixed numeric value, which deletes all edge weights that are less than or equal to the defined value while keeping the network fully connected (or at least weakly so).\nPercentile-based pruning allows to define a numeric percentile where edges with weights that fall below this specified percentile will be deleted. For instance, if a user sets a percentile of 0.20, this will eliminate the lowest 20% of edges based on their weights and retain the rest of strong edges within the network. Both threshold-based and percentile-based pruning make sure that pruning does not disconnect the networks.\nDisparity filter uses a disparity filter algorithm to prune negligible edges and retain the backbone of the network. The disparity filter algorithm works by randomizing the network connections, allowing researchers to determine which edges are significantly stronger than expected by chance. Edges are retained if they are significant relative to a null model. However, disparity filter is strict and we recommend using an alpha level of around 0.5 to retain edges that are less likely to have resulted from chance. Please note that the three methods delete edges from the model.\n\nThe code below demonstrates the three approaches to pruning we just described, followed by plotting each pruned network for visual comparison (Figure 3). First, we use the prune() function with the arguments method = \"threshold\" and threshold = 0.1 to keep only the edges with weights above 0.1. The second approach, using method = \"lowest\" and lowest = 0.15, filters the network by retaining the top 85% of edges to remove the weakest 15%. The third method, method = \"disparity\" with level = 0.5, applies a disparity filter to keep only edges deemed statistically significant within the network structure, thereby helping to reveal meaningful links based on a significance level threshold of 0.5 or edges that are more likely than by chance.\n\n# Pruning with different methods\npruned_threshold <- prune(model_scaled, method = \"threshold\", threshold = 0.1)\npruned_lowest <- prune(model_scaled, method = \"lowest\", lowest = 0.15)\npruned_disparity <- prune(model_scaled, method = \"disparity\", alpha = 0.5)\n\n# Plotting for comparison\nplot(pruned_threshold)\nplot(pruned_lowest)\nplot(pruned_disparity)\nplot(model_scaled, minimum = 0.05, cut = 0.1)\n\n\n\n\n\n\n\n(a) Threshold = 0.15\n\n\n\n\n\n\n\n(b) Lowest 15%\n\n\n\n\n\n\n\n\n\n(c) Disparity filter\n\n\n\n\n\n\n\n(d) Not pruned (minimum threshold)\n\n\n\n\nFigure 3. FTNA scaled pruned with different methods\n\n\n\nAlternatively, to plot the model without deleting any edges, you can hide small edges while retaining them for further computations by setting the minimum argument to a value e.g., 0.05. This will hide the edges below this value (0.05) but will retain the full model for further analysis (Figure 4).\n\nplot(model_scaled, minimum = 0.1, cut = 0.01)\n\n\n\n\nFigure 4. FTNA scaled\n\n\n\n\n\n\nPatterns\nPatterns provide insights into the the underlying mechanisms (e.g., behavior) that generated the data. These mechanisms give rise to recurring structures and connections between the learning processes. The study of such structures can help researchers interpret the underlying process, build new hypotheses or verify existing ones and identify areas that may need support (e.g., patterns indicating lack of regulation). FTNA allows users to detect and visualize n-cliques or groups of nodes that are all interconnected with transitions meeting specific criteria.\nCliques are strong, recurring patterns where nodes (representing behaviors or states) are strongly interconnected. While one can get several types of cliques, in FTNA we focus on dyads and triads. Dyads are two strongly linked nodes and triads are three well-connected nodes that reflect more complex dependencies. These strong structures are in a way what form or shape the entire process.\nThe cliques() function in FTNA allows users identify different cliques of different sizes by specifying the number of nodes in each clique. For instance, setting size = 2 will identify dyads with strong pairwise mutual relationships. The function also has a threshold argument to get only dyads which are strongly connected above that threshold to ensure that only meaningful connections are visualized. Triads can be also identified in the same way by setting the size argument as size = 3 which will capture patterns of three interconnected nodes above the stated threshold, in our case it is 0.05.\nThe clique function has a sum_weights argument that lets users decide if the sum of edge weights should be considered in forming the cliques. For example, setting the threshold to 0.10 and sum_weights = FALSE means each of the two edges should have a weight of at least 0.10 to be considered as part of the same clique. While sum_weights = TRUE means that only the sum of the weights of the two edges needs to be 0.1, i.e., an edge can be 0.04 and the other can be 0.06.\nThe code below is straightforward as it identifies and plots the cliques and shows examples of dyads, triads and 4-cliques and 5-cliques. In the first example, we set the size = 2 and threshold = 0.1 to get the dyads with strong transitions, while size = 3 with a lower threshold = 0.05 captures triads, identifying three-node interdependencies. Larger cliques, such as structures with 4 or 5 nodes (or more) can be obtained in the same way. We can also print and plot the cliques. For brevity, we only show the plot for the dyads (Figure 5).\n\n# Identify 2-cliques (dyads) from the FTNA model with a weight threshold, \n# excluding loops in visualization.\n# A 2-clique represents a pair of nodes that are strongly connected based on \n# the specified weight threshold.\ncliques_of_two <- cliques(\n  model_scaled,      # The FTNA model with scaled edge weights\n  size = 2,          # Looking for pairs of connected nodes (dyads)\n  threshold = 0.1    # Only include edges with weights greater than 0.1\n)\n\n# Print and visualize the identified 2-cliques (dyads)\nprint(cliques_of_two)  # Display details of 2-cliques\n\nNumber of 2-cliques: 8 (weight threshold = 0.1)\nShowing 6 cliques starting from clique number 1\n\nClique 1:\n          consensus plan\nconsensus     0.207 1.00\nplan          0.714 0.92\n\nClique 2:\n          consensus discuss\nconsensus     0.207   0.475\ndiscuss       0.507   0.307\n\nClique 3:\n        discuss emotion\ndiscuss   0.307   0.167\nemotion   0.115   0.087\n\nClique 4:\n        emotion  plan\nemotion   0.087 0.113\nplan      0.361 0.920\n\nClique 5:\n          consensus emotion\nconsensus     0.207   0.184\nemotion       0.363   0.087\n\nClique 6:\n        monitor  plan\nmonitor  0.0104 0.123\nplan     0.1856 0.920\n\n\n\nplot(cliques_of_two)   # Visualize 2-cliques in the network\n\n\n\n\n\n\n\n\n(a) Dyad 1\n\n\n\n\n\n\n\n(b) Dyad 2\n\n\n\n\n\n\n\n(c) Dyad 3\n\n\n\n\n\n\n\n\n\n(d) Dyad 4\n\n\n\n\n\n\n\n(e) Dyad 5\n\n\n\n\n\n\n\n(f) Dyad 6\n\n\n\n\nFigure 5. Identified dyads\n\n\n\n# Identify 3-cliques (triads) from the FTNA model.\n# A 3-clique is a fully connected set of three nodes, indicating a strong \n# triplet structure.\ncliques_of_three <- cliques(\n  model_scaled,      # The FTNA model with scaled edge weights\n  size = 3,          # Looking for triplets of fully connected nodes (triads)\n  threshold = 0.05   # Only include edges with weights greater than 0.05\n)\n\n# Print and visualize the identified 3-cliques (triads)\n# Uncomment the code below to view the results\n# print(cliques_of_three) # Display details of 3-cliques\n# plot(cliques_of_three)  # Visualize 3-cliques in the network\n\n# Identify 4-cliques (quadruples) from the FTNA model.\n# A 4-clique includes four nodes where each node is connected to every other \n# node in the group.\n# Uncomment the code below to view the results\ncliques_of_four <- cliques(\n  model_scaled,      # The FTNA model with scaled edge weights\n  size = 4,          # Looking for quadruples of fully connected nodes (4-cliques)\n  threshold = 0.03   # Only include edges with weights greater than 0.03\n)\n\n# Print and visualize the identified 4-cliques (quadruples) \n# Uncomment the code below to view the results\n# print(cliques_of_four)  # Display details of 4-cliques\n# plot(cliques_of_four)   # Visualize 4-cliques in the network\n\n# Identify 5-cliques (quintuples) from the FTNA model, summing edge weights.\n# Here, the sum of edge weights in both directions must meet the specified \n# threshold for inclusion.\n# Uncomment the code below to view the results\ncliques_of_five <- cliques(\n  model_scaled,      # The FTNA model with scaled edge weights\n  size = 5,          # Looking for quintuples of fully connected nodes (5-cliques)\n  threshold = 0.1,   # Only edges with total bidirectional weights greater than 0.1\n  sum_weights = TRUE # Sum edge weight in both directions when computing  threshold\n)\n\n# Print and visualize the identified 5-cliques (quintuples)\n# print(cliques_of_five)  # Display details of 5-cliques\n# plot(cliques_of_five)   # Visualize 5-cliques in the network\n\n\n\nGraph level measures\nFTNA provides provide an easy access to all graph level measures through the summary() function. The summary function gives insights about nodes, the frequency and strength of transitions, and the overall connectedness of the network. The network metrics include the number of unique states (nodes) and the number of transitions (edges) which form the network’s basic structure. Measures of density, mean distance, and reciprocity and other metrics which that help interpret the dynamic structure of the transition network:\n\nNode Count: The number of unique states or events being analyzed in the network.\nEdge Count: Total number of transitions recorded between states.\nNetwork Density*: The proportion of possible transitions that are actually observed, where a density of 1 implies every possible transition between states has occurred at least once. Please note that in most cases of networks that are not pruned, density metric does not offer much value.\nMean Distance: The average number of transitions (or steps) needed to move between any two states in the network reflecting the network’s overall “connectivity” or flow.\nMean Out-Strength*: The total strength (sum of transition weights) of transitions that each state initiates.\nMean In-Strength*: The mean of total strength (sum of transition weights) directed towards each state.\nMean Out-Degree*: The mean of number of distinct states to which a particular state transitions, representing its transition “breadth.”\nMean In-Degree*: The number of distinct states transitioning into a specific state, indicating its tendency to be a transition destination.\nCentralization*: Measures how much the network is dominated by a central “hub” state, often a highly frequent transition point or common intermediary.\nReciprocity*: Reflects the tendency of transitions to be bidirectional, meaning if a transition exists from State A to B, it is likely B also transitions back to A, highlighting cyclic or reversible behavior within the network.\n\nPlease note that the measures marked with * do not offer much information in fully connected networks which is the case in most cases of FTNA and therefore should be interpreted with caution in these situations. However, the metrics are particularly useful in pruned models. See and contrast below two examples: the original model on the left side and the pruned model on the right side.\n\n\n\n\n\nsummary(model_scaled)\n\n                        metric   value\n1                   Node Count  9.0000\n2                   Edge Count 78.0000\n3              Network Density  1.0000\n4                Mean Distance  0.0240\n5            Mean Out-Strength  1.1325\n6              SD Out-Strength  0.8855\n7             Mean In-Strength  1.1325\n8               SD In-Strength  0.8778\n9              Mean Out-Degree  8.6667\n10               SD Out-Degree  0.7071\n11 Centralization (Out-Degree)  0.0156\n12  Centralization (In-Degree)  0.0156\n13                 Reciprocity  0.9859\n\n\n\n\n\n\n\nsummary(pruned_disparity)\n\n                        metric  value\n1                   Node Count  9.000\n2                   Edge Count 39.000\n3              Network Density  0.542\n4                Mean Distance  0.188\n5            Mean Out-Strength  0.900\n6              SD Out-Strength  0.700\n7             Mean In-Strength  0.900\n8               SD In-Strength  0.673\n9              Mean Out-Degree  4.333\n10               SD Out-Degree  1.225\n11 Centralization (Out-Degree)  0.234\n12  Centralization (In-Degree)  0.516\n13                 Reciprocity  0.615\n\n\n\n\n\n\nThe results of the summary() function of our examined networks show the network consists of 9 nodes and 78 edges, resulting in a high density of 1.0, indicating that all possible connections between nodes are present. The mean distance between nodes is minimal at 0.024, reflecting a tightly connected network where nodes are generally very close to each other in terms of path length. The average out-strength and in-strength for nodes are both approximately 1.1325, with standard deviations of 0.886 and 0.878, respectively. These values indicate some variability in the strength of connections. Similarly, the mean out-degree (the average number of connections each node has outward) is 8.67, with a low standard deviation of 0.707, showing that most nodes have nearly equal connectivity. Network centralization for both out-degree and in-degree is low 0.016. Reciprocity is 0.986 indicating that nearly all directed connections in this network are mutual, or reciprocated, which is typical of networks where bidirectional interaction is common. As mentioned above these numbers are expected in a fully connected network before pruning. Compare these values to the pruned network.\n\n\nNode level measures\nCentrality measures can help identify the influential events or states and rank their importance within the learning process Saqr, López-Pernas, Conde-González, et al. (2024). The tna package provides the centralities() function designed for calculating various centralities in directed weighted networks. This function requires only the FTNA model object as input and it automatically computes a range of centralities, namely, out-strength, in-strength, closeness, betweenness, fiffusion and clustering measures. By default, the function does not consider self-transitions (loops) as they do not represent changes of a state. Removing loops helps focus the analysis on transitions between distinct states rather than within-state repetitions. Of course, users can set loops = TRUE to count loops which may be useful in cases where researchers are estimating stability of states, e.g., remaining in the same state or other contextual reasons.\nOut-strength centrality indicates how frequently a state transitions to other states which captures the change and stability within that state or that event. A higher out-strength suggests that the state often leads to different actions, while a lower out-strength implies stability, repetition or being stuck in the same state. In a learning context, a state like “plan” might have a high out-strength, indicating that students quickly transition from planning to other steps. In-strength centrality, by contrast, represents how frequently a state receives transitions and being a common destination. For instance, if “consensus” has high in-strength centrality it signifies that other learning states frequently result in consensus, reflecting group dynamics of agreement and cohesion.\nBetweenness centrality reflects which states act as a bridge or mediates between other transitions. For instance, a state with high betweenness, like “synthesis”, might connect various learning actions signifying its role as a juncture in the process. Given the flexible multi-step conceptualization of the network, several other centralities can be useful here —depending on the context of course— like diffusion centrality which is a good indication of diffusion of ideas or actions across the network.\nThe code below begins by calculating centrality measures using the centralities() function on model_scaled. After these measures are computed, we print them out to inspect the results (Table 5). Next, we visualize these centrality measures with plot(centrality_measures) (Figure 6).\n\n# Compute centrality measures for the FTNA model\ncentrality_measures <- centralities(model_scaled)\n\n# Print the calculated centrality measures in the FTNA model\nprint(centrality_measures)\n\n\n\n\n\nTable 5. Centrality measures plot for the FTNA model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nState\nOutStrength\nInStrength\nClosenessIn\nClosenessOut\nCloseness\nBetweenness\nBetweennessRSP\nDiffusion\nClustering\n\n\n\n\nadapt\n0.20\n0.21\n14.56\n5.83\n21.59\n20\n91\n9.14\n0.39\n\n\ncohesion\n0.66\n0.67\n6.61\n6.54\n18.69\n0\n86\n32.55\n0.50\n\n\nconsensus\n2.32\n2.34\n1.25\n5.89\n7.68\n0\n2\n87.21\n0.22\n\n\ncoregulate\n0.77\n0.82\n11.28\n4.02\n12.98\n0\n83\n30.27\n0.63\n\n\ndiscuss\n1.27\n1.26\n7.22\n3.30\n7.68\n0\n82\n46.98\n0.37\n\n\nemotion\n1.05\n1.02\n4.46\n6.68\n16.93\n0\n75\n43.56\n0.53\n\n\nmonitor\n0.56\n0.48\n10.84\n5.24\n13.05\n7\n86\n22.90\n0.58\n\n\nplan\n1.54\n1.56\n11.60\n5.88\n16.81\n9\n1\n68.86\n0.43\n\n\nsynthesis\n0.26\n0.28\n11.99\n5.72\n16.93\n21\n91\n10.90\n0.52\n\n\n\n\n\n\n\n# Visualize the calculated centrality measures in the FTNA model\nplot(centrality_measures)\n\n\n\n\nFigure 6. Centrality measures plot for the FTNA model\n\n\n\n\nWe can also compute other centralities using the function as.igraph(model_scaled) which converts the FTNA model into an igraph object and thus enables all possible centralities and measures that can be computed using the igraph R package. For instance, the below code calculates the hub and authority scores for the FTNA network model using the hits_scores() function from the igraph package, it first converts the FTNA model (model_scaled) into an igraph object. Hub scores measure how often a node (event or state) points to other important nodes, reflecting its influence as a source. Authority scores measure how frequently a node is targeted by others, indicating its importance as a destination or key reference point. The resulting scores provide insight into which states act as influential hubs or authoritative endpoints within the learning process.\nThe code below exemplifies how to compute extra measures. We first convert our scaled FTNA model (model_scaled) into an igraph object. The igraph package supports a large array of graph analyses, including the HITS (Hyperlink-Induced Topic Search) algorithm, which calculates “hub” and “authority” scores for each node.\n\n# Convert the FTNA model to an igraph object and \n# calculate HITS (Hub and Authority) scores\nhits_results <- igraph::hits_scores(as.igraph(model_scaled))\n\n# Extract the hub and authority scores from the HITS results for further analysis\nhub_scores <- hits_results$hub\nauthority_scores <- hits_results$authority\n\n\n# Print the hub and authority scores to view influential nodes\nprint(hub_scores)\nprint(authority_scores)\n\n\n\n     adapt   cohesion  consensus coregulate    discuss    emotion    monitor       plan  synthesis \n    0.0556     0.2404     0.9551     0.2557     0.3768     0.3053     0.1895     1.0000     0.0738 \n\n\n     adapt   cohesion  consensus coregulate    discuss    emotion    monitor       plan  synthesis \n    0.0331     0.1292     0.6724     0.2925     0.4368     0.3437     0.1735     1.0000     0.0564 \n\n\n\n\nEdge level measures\nFTNA can also compute edge level measures which would show which edges are important in the transition model and act as a bridge. Namely, FTNA can compute the edge betweenness.\n\nedge_between <- betweenness_network(model_scaled)\nplot(edge_between)\n\n\n\n\nFigure 7. Edge betweenness network\n\n\n\n\n\n\nCommunity detection\nCommunity detection can help identify groups of nodes —such as states, events, or actions— that exhibit strong interconnections between the nodes. Unlike cliques, which are defined by criteria of mutual connections among a fixed set of nodes and thresholds, communities are identified algorithmically and can vary in size and connection strength. This flexibility allows for a more nuanced and realistic approach to grouping transitions that share similar connectivity patterns regardless of their size, be it two nodes or five, as long as they exhibit shared interconnections.\nFinding communities has been a central theme in social science for finding groups of constructs that are related (e.g., meta-cognition related nodes) to understand the structure of these constructs and their tight-knit relationships. Other examples include finding latent variables, propose new constructs and understand the semantics of discourse or behavior.\nTNA includes several community detection algorithms that are designed for transition networks (which are typically small, weighted, and directed). Each algorithm provides a distinct perspective on the grouping process. Some algorithms may focus on maximizing modularity, while others might emphasize flow or path length. As such the researcher can implement whatever suites their context. The code below performs community detection with the function communities() which takes the model argument and computes the communities, e.g., communities(model_scaled). The results are stored in the detected_communities object which contains the results of the algorithms: Walktrap, Label Propagation, Infomap, Edge Betweenness, Leading Eigen, and Spinglass. To visualize the communities assignment, you can simply use the function plot with the name of the community that you want to use (Figure 8). The detected_communities object can also be printed.\n\ndetected_communities <- communities(model_scaled)\nplot(detected_communities, minimum = 0.05)\n\n\n\n\nFigure 8. Community Detection using Fast Greedy Method\n\n\n\n\n\nprint(detected_communities)\n\nNumber of communities found by each algorithm:\n        walktrap      fast_greedy       label_prop          infomap edge_betweenness \n               1                3                1                1                1 \n   leading_eigen        spinglass \n               3                3 \n\nCommunity assignments:\n       node walktrap fast_greedy label_prop infomap edge_betweenness leading_eigen spinglass\n1     adapt        1           1          1       1                1             1         1\n2  cohesion        1           2          1       1                1             3         2\n3 consensus        1           3          1       1                1             2         3\n [ reached 'max' / getOption(\"max.print\") -- omitted 6 rows ]\n\n\n\n\nBootstrapping\nBootstrapping is a technique for assessing the accuracy and stability of edge weights in the network. With bootstrapping, we can estimate the likelihood that each transition would replicate or appear with similar strength in repeated sampling. In fact, bootstrapping is one of the strengths of FTNA as it allows us to verify the strength of each edge in the network and make sure that it is strong enough to be interpretable. Bootstrapping entails re-sampling the data repeatedly. Each of these sub-samples is created by randomly selecting data points from the original dataset with replacement. Then we reconstruct an FTNA network, recalculate the edge weights each time (i.e., build a new FTNA model). The process is commonly performed a large number of times (usually 1,000 iterations) to generate a distribution of edge weights for each transition in the network. Then we compare the original edge weight against the range of edge weights obtained from the 1,000 bootstrap samples and calculate confidence intervals for each edge. We can also compute p-values for each edge to estimate how different the edge is given a certain threshold value (e.g., 0.05) across the bootstrap samples. When an edge consistently exceeds this threshold, we consider it statistically significant. Bootstrapping also offers an idea about the stability and the robustness of the entire FTNA network as a whole.\nTo perform bootstrapping we use the bootstrap() function, which requires the model argument as an argument. The function typically uses a default of 1,000 bootstrap iterations (iter = 1000), though increasing the number of iterations may improve reliability. Additionally, by setting a threshold (e.g., threshold = 0.05), we can directly test whether an edge’s weight is consistently above this value, indicating statistical significance across samples. To view the edge data, confidence intervals and their p-values, we can print a summary output which contains the full bootstrap results. \n\n# Perform bootstrapping on the FTNA model with a fixed seed for reproducibility\nset.seed(265)\nboot <- bootstrap(model_scaled, threshold = 0.05)\n\n# Print the combined results data frame containing\nprint(summary(boot))\n\n   from         to weight p_value   sig ci_lower ci_upper\n2 adapt   cohesion 0.0020       1 FALSE 0.000397  0.00364\n3 adapt  consensus 0.0120       1 FALSE 0.007822  0.01631\n4 adapt coregulate 0.0128       1 FALSE 0.008502  0.01767\n5 adapt    discuss 0.1126       0  TRUE 0.099843  0.12688\n [ reached 'max' / getOption(\"max.print\") -- omitted 74 rows ]\n\n# View non-significant edges  which are less likely to be stable across bootstrap samples\nprint(boot, type = \"nonsig\")\n\nNon-significant Edges\n\n   from         to  weight p_value ci_lower ci_upper\n2 adapt   cohesion 0.00200       1 0.000397  0.00364\n3 adapt  consensus 0.01198       1 0.007822  0.01631\n4 adapt coregulate 0.01277       1 0.008502  0.01767\n6 adapt    emotion 0.00279       1 0.000795  0.00489\n7 adapt    monitor 0.00639       1 0.003546  0.00992\n [ reached 'max' / getOption(\"max.print\") -- omitted 34 rows ]\n\n\n\n\nComparing Models\nTwo methods can be used to compare models. The first is to compare the raw difference between the weights of the models. The second is to use a statistical technique that shows to what extent each edge differ statistically. FTNA has both methods, the first method offers a descriptive view of the differences with an intuitive visualization that shows the differences. To perform such comparison, we use the plot_compare() function to visualize the differences. In the plot, green edges are higher in the first model, red means lower edges and so is the case in the pie.\nHowever, to rigorously compare models, FTNA uses a permutation test which estimates which edges are statistically significantly different and produces a p-value for each edge. The code below uses the group_regulation dataset which has two subsets. The first subset, group_regulation[1:1000,], represents the high achievers model, while the second subset group_regulation[1001:2000,] represents the low achievers. For each of the subsets we create a model by applying the tna() function to its respective subset. Next, we perform a permutation test with permutation_test() to assess whether the observed differences between the “Hi” and “Lo” models are statistically significant. Setting iter = 1000 instructs the function to run 1000 iterations, creating a distribution of differences by repeatedly shuffling and comparing the groups. Finally, we plot the significant differences identified through the permutation test using plot(Permutation, minimum = 0.001) (Figure 9). This final plot visually highlights edges that have a statistically significant difference in transition weight between the “Hi” and “Lo” models.\n\n# Create FTNA for the high-achievers subset (rows 1 to 1000)\nHi <- ftna(group_regulation[1:1000, ], scaling = \"minmax\")\n\n# Create FTNA for the low-achievers subset (rows 1001 to 2000)\nLo <- ftna(group_regulation[1001:2000, ], scaling = \"minmax\")\n\n# Plot a comparison of the \"Hi\" and \"Lo\" models\n# The 'minimum' parameter is set to 0.001, so edges with weights >= 0.001 are shown\nplot_compare(Hi, Lo, minimum = 0.0001)\n\n# Run a permutation test to determine statistical significance of \n# differences between \"Hi\" and \"Lo\"\n# The 'it' parameter is set to 1000, meaning 1000 permutations are performed\nPermutation <- permutation_test(Hi, Lo, it = 1000)\n\n# Plot the significant differences identified in the permutation test\nplot(Permutation, minimum = 0.01)\n\n\n\n\n\n\n\n(a) Subtraction of high achievers minus low achievers\n\n\n\n\n\n\n\n(b) Significant differences identified in the permutation test\n\n\n\n\nFigure 9. Network comparison\n\n\n\n\n\nCentrality stability\nIn contrast to most network models. FTNA has a robust mechanism to test the stability and the significance of centrality measures through case-dropping. Case-dropping involves sequentially removing data points and re-calculating correlation coefficients that compare the centrality measures in the original network with those derived from networks missing the data points. A correlation coefficient closer to 1 suggests that the centrality rankings of nodes remain stable even when data is removed which indicates that the centrality value is resilient to sample variability and more likely to generalize. A correlation coefficient above 0.7, for instance, would indicate that the centrality measure is reliable; the most central nodes in the original FTNA network remain central even as data points are omitted. An average coefficient between 0.5 and 0.7 reflects moderate stability. However, an average coefficient below 0.5 would indicate low stability.\nThe function estimate_centrality_stability() (abbreviated as estimate_cs()) can be used to assess the stability of centrality measures in FTNA models. The function generates bootstrap results that can then be visualized (Figure 10), offering a clear representation of centrality stability within the FTNA framework.\n\nCentrality_stability <- estimate_centrality_stability(model_scaled, detailed = FALSE)\nplot(Centrality_stability)\n\n\n\n\nFigure 10. Centrality stability"
  },
  {
    "objectID": "book2/chapters/ch16-ftna/ch16-ftna.html#conclusion",
    "href": "book2/chapters/ch16-ftna/ch16-ftna.html#conclusion",
    "title": "Capturing The Breadth and Dynamics of the Temporal Processes with Frequency Transition Network Analysis: A Primer and Tutorial",
    "section": "Conclusion",
    "text": "Conclusion\nThis chapter presented FTNA as a flexible and robust approach to analyzing temporal data of learning processes. FTNA is a novel tool for analyzing complex learning processes. Its flexibility, scalability, and straightforward modeling of transition frequencies make it well-suited for exploring and comparing learning dynamics across varied educational contexts. Unlike traditional Transition Network Analysis (TNA) Saqr, López-Pernas, and Tikka (2025), which models transitions probabilistically based on Markov assumptions, FTNA focuses on the absolute frequency of transitions between states, offering a straightforward representation of learning dynamics without conditioning on prior states. This method is particularly advantageous where dependencies span multiple steps, such as collaborative learning or complex problem-solving tasks or for contexts with rare transitions, small sample sizes, or processes.\nWhat makes FTNA stand out in comparison to existing techniques is its statistical rigor. In that, FTNA allows meaningful comparisons across different datasets or conditions with permutation. Statistical validation methods, such as bootstrapping increase the rigor of FTNA and provide a validation method for the resulting models. Additionally, FTNA inherits the network capabilities of TNA, such as centrality measures and pattern detection (López-Pernas, Tikka, and Saqr 2025) which offer a deeper understanding of structural relationships within the learning process.\n\n\nBorkulo, Claudia D. van, Riet van Bork, Lynn Boschloo, Jolanda J. Kossakowski, Pia Tio, Robert A. Schoevers, Denny Borsboom, and Lourens J. Waldorp. 2023. “Comparing Network Structures on Three Aspects: A Permutation Test.” Psychological Methods 28 (6): 1273–85. https://doi.org/10.1037/met0000476.\n\n\nCsardi, Gabor, and Tamas Nepusz. 2006. “The Igraph Software Package for Complex Network Research.” InterJournal Complex Sy: 1695. http://igraph.org.\n\n\nEpskamp, Sacha, Angélique O J Cramer, Lourens J Waldorp, Verena D Schmittmann, and Denny Borsboom. 2012. “Qgraph: Network Visualizations of Relationships in Psychometric Data.” Journal of Statistical Software 48. https://doi.org/10.18637/jss.v048.i04.\n\n\nGabadinho, Alexis, Gilbert Ritschard, Nicolas S. Müller, and Matthias Studer. 2011. “Analyzing and Visualizing State Sequences in R with TraMineR” 40. https://doi.org/10.18637/jss.v040.i04.\n\n\nHelske, Jouni, Satu Helske, Mohammed Saqr, Sonsoles López-Pernas, and Keefe Murphy. 2024. “A Modern Approach to Transition Analysis and Process Mining with Markov Models in Education.” In Springer Nature Switzerland, 381–427. https://doi.org/10.1007/978-3-031-54464-4_12.\n\n\nKopra, Juho, Santtu Tikka, Merja Heinäniemi, Sonsoles López-Pernas, and Mohammed Saqr. 2024. “An R Approach to Data Cleaning and Wrangling for Education Research.” In Learning Analytics Methods and Tutorials: A Practical Guide Using r, edited by Mohammed Saqr and Sonsoles López-Pernas, in–press. Springer.\n\n\nLópez-Pernas, Sonsoles, Kamila Misiejuk, Santtu Tikka, Juho Kopra, Merja Heinäniemi, and Mohammed Saqr. 2024. “Visualizing and Reporting Educational Data with R.” In Learning Analytics Methods and Tutorials, 151–94. Cham: Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-54464-4\\_6.\n\n\nLópez-Pernas, Sonsoles, and Mohammed Saqr. 2024. “The Why, the How and the When of Educational Process Mining in R.” In Learning Analytics Methods and Tutorials, 467–88. Cham: Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-54464-4\\_14.\n\n\nLópez-Pernas, Sonsoles, Mohammed Saqr, and Santtu Tikka. 2024. “tna: Transition Network Analysis (TNA).” CRAN: Contributed Packages. The R Foundation. https://doi.org/10.32614/cran.package.tna.\n\n\nLópez-Pernas, Sonsoles, Santtu Tikka, and Mohammed Saqr. 2025. “Mining Patterns and Clusters with Transition Network Analysis: A Heterogeneity Approach.” In Advanced Learning Analytics Methods: AI, Precision and Complexity, edited by Mohammed Saqr and Sonsoles López-Pernas. Cham: Springer Nature Switzerland.\n\n\nNewman, Mark. 2018. Networks. 2nd ed. London, England: Oxford University Press. https://doi.org/10.1093/oso/9780198805090.001.0001.\n\n\nNicolis, G, A García Cantú, and C Nicolis. 2005. “Dynamical Aspects of Interaction Networks.” International Journal of Bifurcation and Chaos in Applied Sciences and Engineering 15: 3467–80. https://doi.org/10.1142/s0218127405014167.\n\n\nPoquet, Oleksandra, Mohammed Saqr, and Bodong Chen. 2021. “Recommendations for Network Research in Learning Analytics: To Open a Conversation.” In Proceedings of the NetSciLA2021 Workshop \"Using Network Science in Learning Analytics: Building Bridges Towards a Common Agenda\" (NetSciLA2021), edited by Oleksandra Poquet, Bodong Chen, Mohammed Saqr, and Tobias Hecking, 34–41. CEUR Workshop Proceedings 2868. Aachen. http://ceur-ws.org/Vol-2868/.\n\n\nSaqr, Mohammed, Ramy Elmoazen, Matti Tedre, Sonsoles López-Pernas, and Laura Hirsto. 2022. “How Well Centrality Measures Capture Student Achievement in Computer-Supported Collaborative Learning?  A Systematic Review and Meta-Analysis.” Educational Research Review 35 (February): 100437. https://doi.org/10.1016/j.edurev.2022.100437.\n\n\nSaqr, Mohammed, and Sonsoles López-Pernas. 2021. “Modelling Diffusion in Computer-Supported Collaborative Learning: A Large Scale Learning Analytics Study.” International Journal of Computer-Supported Collaborative Learning 16 (4): 441–83. https://doi.org/10.1007/s11412-021-09356-4.\n\n\nSaqr, Mohammed, Sonsoles López-Pernas, Miguel Ángel Conde-González, and Ángel Hernández-García. 2024. “Social Network Analysis: A Primer, a Guide and a Tutorial in R.” In, 491–518. Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-54464-4_15.\n\n\nSaqr, Mohammed, Sonsoles López-Pernas, Satu Helske, Marion Durand, Keefe Murphy, Matthias Studer, and Gilbert Ritschard. 2024. “Sequence Analysis in Education: Principles, Technique, and Tutorial with R.” In, 321–54. Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-54464-4_10.\n\n\nSaqr, Mohammed, Sonsoles López-Pernas, and Santtu Tikka. 2025. “Mapping Relational Dynamics with Transition Network Analysis: A Primer and Tutorial.” In Advanced Learning Analytics Methods: AI, Precision and Complexity, edited by Mohammed Saqr and Sonsoles López-Pernas. Cham: Springer Nature Switzerland.\n\n\nSaqr, Mohammed, Sonsoles López-Pernas, Tiina Törmänen, Rogers Kaliisa, Kamila Misiejuk, and Santtu Tikka. 2024. “Transition Network Analysis: A Novel Framework for Modeling, Visualizing, and Identifying the Temporal Patterns of Learners and Learning Processes.” https://doi.org/10.48550/ARXIV.2411.15486.\n\n\nSaqr, Mohammed, and Ward Peeters. 2022. “Temporal Networks in Collaborative Learning: A Case Study.” British Journal of Educational Technology 53 (5): 1283–303. https://doi.org/10.1111/bjet.13187.\n\n\nSchnakenberg, J. 1976. “Network Theory of Microscopic and Macroscopic Behavior of Master Equation Systems.” Reviews of Modern Physics 48 (4): 571–85. https://doi.org/10.1103/revmodphys.48.571.\n\n\nTikka, Santtu, Juho Kopra, Merja Heinäniemi, Sonsoles López-Pernas, and Mohammed Saqr. 2024a. “Getting Started with R for Education Research.” In Learning Analytics Methods and Tutorials: A Practical Guide Using r, edited by Mohammed Saqr and Sonsoles López-Pernas, in–press. Springer.\n\n\n———. 2024b. “Introductory Statistics with R for Educational Researchers.” In Learning Analytics Methods and Tutorials: A Practical Guide Using r, edited by Mohammed Saqr and Sonsoles López-Pernas, in–press. Springer.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the Tidyverse” 4: 1686. https://doi.org/10.21105/joss.01686.\n\n\nWinne, Philip H. 2010. “Improving Measurements of Self-Regulated Learning.” Educational Psychologist 45 (4): 267–76. https://doi.org/10.1080/00461520.2010.517150.\n\n\nZou, Yong, Reik V Donner, Norbert Marwan, Jonathan F Donges, and Jürgen Kurths. 2019. “Complex Network Approaches to Nonlinear Time Series Analysis.” Physics Reports 787: 1–97. https://doi.org/10.1016/j.physrep.2018.10.005."
  },
  {
    "objectID": "book2/chapters/ch02-AIxAI/ch02-aixai.html",
    "href": "book2/chapters/ch02-AIxAI/ch02-aixai.html",
    "title": "AI, Explainable AI and Evaluative AI: Informed Data-Driven Decision-Making in Education",
    "section": "",
    "text": "AI has a long and rich history in education. In fact, artificial intelligence in education predates several other technologies and disciplines e.g., online learning. Among the first examples of artificial intelligence applications in education is the creation of the PLATO system at the University of Illinois around the 1960s which allowed for interactive learning and automated feedback (Cope and Kalantzis 2023). While PLATO was limited in scope, it laid the foundation for future developments to follow and encouraged other researchers to explore the nascent technology in education. Most importantly, interest started to grow in developing intelligent tutoring systems (ITS) and AI powered educational solutions (Cope and Kalantzis 2023).\nHowever, it was not until the early 1980s when the field of artificial intelligence in education saw its major milestone with the launch of the first International Conference on Artificial Intelligence in Education (AIED) which was held in Montreal (Chaudhry and Kazim 2022). AIED conference brought together researchers from both AI and educational psychology, marking the formal recognition of AI as a significant discipline. Furthermore, AIED set the stage for collaboration between the fields of AI and education, which led to several significant developments in intelligent tutoring systems, adaptive learning technologies, and educational software (Guan, Mou, and Jiang 2020).\nThe last two decades have witnessed several remarkable and fast paced developments in the field of AIED fueled by the rapid adoption of the internet, the increase in computation power and advances in methodology (Mustafa et al. 2024). Such advances led to the birth of learning analytics as a field —along with educational data mining— which all contributed to wide use of machine learning and data analytics, alongside other uses of AIED that extends to almost all fields of research and practice (Wang et al. 2024).\nAs AI continues to play an increasingly central role in education, concerns around transparency, and fairness have become more prominent (O. Ali et al. 2024). These issues stem from the inherently complex and often opaque nature of AI systems, particularly those powered by machine learning and deep learning (Gillani et al. 2023). For instance, while AI-driven systems can offer personalized recommendations and adaptive learning experiences, their underlying decision-making processes are not always clear to educators or learners. This lack of transparency can prevent users from trusting AI systems and hinder their adoption in educational contexts (Bedué and Fritzsche 2022). Worse even, they might follow AI decisions without knowing if they are wrong or right (Miller 2019).\nOne of the reasons for the mistrust in AI is that its performance is commonly affected by bias, as AI models are only as unbiased as the data they are trained on (O. Ali et al. 2024). Historical inequities or imbalances in training datasets can perpetuate and even amplify existing disparities, leading to outcomes that disproportionately disadvantage certain groups of students (Bogina et al. 2022). For example, adaptive systems might inadvertently favor learners whose behavior aligns with the dominant patterns in the training data, leaving those with less conventional learning trajectories underserved. Such risks highlight the importance of rigorous evaluation, diverse datasets, and ethical oversight in the design and deployment of AI systems in education.\nAddressing these challenges requires the integration of explainable AI (xAI) techniques, which aim to make AI outputs interpretable and accountable to users. xAI tools can provide insights into how AI systems reach their conclusions, enabling educators to critically evaluate and trust these technologies (Khosravi et al. 2022). Moreover, the adoption of fairness-aware machine learning practices and inclusive design principles can help mitigate biases, ensuring that AI-driven solutions support equitable outcomes for all learners. These considerations are particularly relevant when it comes to personalized education, where the promise of tailored interventions must be balanced with a commitment to fairness and transparency.\nThis chapter explores the current state of artificial intelligence in education, as well as explainable AI (xAI). AI and xAI techniques and tools are illustrated in the subsequent tutorials in the book and this chapter provides an introduction for the reader to have an overarching view on their main characteristics and potential. We further introduce the concept of evaluative AI (Miller 2023), which aims to address some of the limitations of xAI."
  },
  {
    "objectID": "book2/chapters/ch02-AIxAI/ch02-aixai.html#an-overview-of-ai-applications-in-education",
    "href": "book2/chapters/ch02-AIxAI/ch02-aixai.html#an-overview-of-ai-applications-in-education",
    "title": "AI, Explainable AI and Evaluative AI: Informed Data-Driven Decision-Making in Education",
    "section": "An overview of AI applications in education",
    "text": "An overview of AI applications in education\nAI is an extremely broad field that permeates many disciplines, encompassing a wide range of techniques and applications that aim to replicate or enhance human intelligence. In spite of —or maybe because of— its widespread influence across fields such as healthcare, finance and, of course, education, there is no universally agreed-upon definition of AI (Monett and Lewis 2018), nor a standardized taxonomy to categorize its many methods and applications. In the context of education, this lack of clarity is furthered by the diverse range of technologies that fall —sometimes wrongly— under the label of AI —from rule-based systems to complex neural networks— each serving diverse purposes such as personalization, prediction, and automation. In this section, we aim to provide an overview of the key AI applications that have influenced education, presenting relevant use cases in the field. We follow the structure of the recent meta-review by Bond et al. (Bond et al. 2024), which is based on the original typology by (Zawacki-Richter et al. 2019), and add an additional section on AI in education research.\n\nAdaptive systems and personalization\nOne of the main promises of AI in education is personalization. Personalized learning focuses on tailoring educational experiences to individual learners’ needs, preferences, and readiness, and it can be implemented through various methods and technologies (Taylor, Yeung, and Bashet 2021). For example, adaptive systems implement personalized learning by constructing a model of each learner using artificial intelligence and machine learning as well as continuous data collection of students’ activities and performance (Taylor, Yeung, and Bashet 2021). According to Zhong (Zhong 2023), personalized learning has been mainly implemented through learning content structuring and sequencing (organizing and ordering learning materials according to individual needs) as well as readiness support (adapting to students’ performance).\nAnother form of personalization —closely related to adaptive systems— is the use of recommender systems. Recommender systems are algorithms designed to suggest learning resources, activities, or courses to students based on their individual profiles, past performance, and expressed preferences. According to the review by Urdaneta-Ponte et al. (Urdaneta-Ponte, Mendez-Zorrilla, and Oleagordia-Ruiz 2021), most recommender systems in the education literature use collaborative filtering, which entails recommending resources based on similarity to other students.\nIn recent years, personalization has expanded beyond adaptivity and recommendation through the provision of truly personalized learning experiences, made possible thanks to the latest AI breakthroughs. Chatbots and virtual assistants currently represent one of the most prominent applications of personalization of education using AI. These tools take advantage of the advances in natural language processing —and most recently Large language Models (LLMs)— to interact with students in a conversational manner. A recent review by Labadze et al. (Labadze, Grigolia, and Machaidze 2023) discovered that students benefit most from AI-powered chatbots in three main areas: support with homework and studying, personalized learning experiences, and skill development (Fichten et al. 2022).\nOther AI-powered technologies like facial recognition or mood detection are emerging as relevant tools for tailoring educational experiences (Imani and Montazer 2019). These technologies can complement traditional adaptive systems through the incorporation of real-time emotional and behavioral information on students’ facial expressions, tone of voice, and body language. Despite their potential, these technologies raise concerns regarding privacy, consent, and data security (Andrejevic and Selwyn 2020). Furthermore, the accuracy of emotion recognition algorithms and the potential for cultural or individual biases remain critical challenges to address.\n\n\nIntelligent tutoring systems ITS)\nClosely related to adaptive systems are Intelligent Tutoring Systems (ITSs). ITSs use AI techniques to provide personalized tutoring based on a model of the learner’s knowledge, behavior, and learning progress (Lin, Huang, and Lu 2023). ITSs aim to emulate one-on-one human tutoring through the adaptation of instructional strategies, delivering tailored feedback, and offering customized learning experiences to help students achieve specific educational goals. ITSs often include features such as error diagnosis, hint generation, and real-time adaptation to individual learner needs, typically in well-defined domains. For example, virtual patients are a form of ITS used in healthcare to practice clinical reasoning (Lamti et al. 2024). ITSs are also extensively used in programming education as a step-by-step tutor to master a programming language (Crow, Luxton-Reilly, and Wuensche 2018).\n\n\nProfiling and prediction\nPredictive modeling has been a primary focus of AI in education. Early attempts dealt with the use of demographic and administrative data from learners to predict performance and dropouts (Shafiq et al. 2022). With the birth of the educational data mining and learning analytics fields, utilizing intensive log data about learners’ online activities became the norm in the field, which came accompanied to increased predictive accuracy thanks to advances in ML algorithms (Hellas et al. 2018).\nThe goals of performing predictive modeling have been diverse. A central goal has been the early identification of students who may be at risk of failing or dropping out (Liz-Domínguez et al. 2019). Based on data from previous iterations of a course, researchers attempt to predict students’ performance early on in the current course to be able to conduct remedial interventions. Other less frequent goals of predictive modeling are related to career placement (Pinto et al. 2023), admission (Zawacki-Richter et al. 2019), or student satisfaction (Hellas et al. 2018).\nResearch suggests that predictive modeling is hardly generalizable and lacks portability between courses (Gašević et al. 2016), or even between course iterations (Saqr et al. 2022). Thus, recent research has shifted the focus from predicting performance to understanding learners based on their behavioral data. Clustering algorithms have been operationalized to detect distinct patterns or profiles of engagement (López-Pernas and Saqr 2024), collaboration roles (Saqr and López-Pernas 2022), self-regulated learning (Dörrenbächer and Perels 2016), or learning strategies (Saqr et al. 2023).\nLastly, recent research strands that are gathering increased attention focus on identifying and mitigating biases in AI algorithms (Baker and Hawn 2022), ensuring equitable treatment of all learners, and enhancing the interpretability of AI systems through eXplainable Artificial Intelligence (xAI) (Khosravi et al. 2022).\n\n\nAssessment and evaluation\nStudents often long for formative feedback that offers actionable points to improve their learning and performance. This has traditionally come at the expense of overburdening teachers with a higher workload. AI offers a lot of promise to alleviate this challenge by providing automated, personalized feedback to students (Zhai et al. 2021). Although automated assessment has been present for decades, e.g., through multiple-choice questionnaires that can be automatically graded or autograders for programming assignments, advancements in artificial intelligence, particularly in NLP, have led to the development of systems capable of analyzing student work and generating constructive feedback.\nAI assessment and feedback have been especially relevant in the evaluation of artifacts that do not have a deterministic correct solution. In fact, the most widely used application of AI-powered feedback in education is automated essay scoring (Ramesh and Sanampudi 2022). Another important focus has been the evaluation of programming code. Although non-AI automated assessment tools have existed for decades (Ala-Mutka 2005), the recent advances in AI have allowed the provision of more relevant and detailed feedback to students (Jukiewicz 2024). The main advantage of AI-powered assessment systems is that they can provide immediate feedback, allowing students to understand their mistakes and learn from them in real-time. In addition, such systems enable educators to support a larger number of students without a proportional increase in workload, making personalized education more accessible.\nBeyond evaluating students’ assignments and artifacts, AI has also opened new avenues for evaluating students’ learning process. For instance, automated discourse coding allows educators and researchers to analyze conversations or discussion forums in order to identify patterns of collaboration, critical thinking, and engagement (Siiman et al. 2023).\n\n\nAI in Education Research and Learning Analytics\nLastly, AI has not only brought opportunities to education practice but also to education research and learning analytics. Advances in AI, such as deep learning, natural language processing (NLP), and more recently, LLMs, have significantly expanded the possibilities for analyzing unstructured data such as text, speech, and video, and facilitating new approaches to personalization and feedback in education (Cukurova 2024; Williams 2023).\nMoreover, the increasing role of AI as an educational tool has further expanded the possibilities of education and learning analytics researchers for understanding learners’ behaviors in the presence of these new learning technologies (Yan, Martinez-Maldonado, and Gasevic 2024). For example, researchers can examine the types of prompts students pose to AI systems, providing fine-grained information about common misconceptions, knowledge gaps, or areas where additional support may be needed. These analyses can also showcase how students respond to feedback or recommendations generated by AI, and hence whether AI’s guidance promotes better learning outcomes or contributes to over-reliance on AI-generated solutions (Chen et al. 2025)."
  },
  {
    "objectID": "book2/chapters/ch02-AIxAI/ch02-aixai.html#explainable-ai",
    "href": "book2/chapters/ch02-AIxAI/ch02-aixai.html#explainable-ai",
    "title": "AI, Explainable AI and Evaluative AI: Informed Data-Driven Decision-Making in Education",
    "section": "Explainable AI",
    "text": "Explainable AI\nHeadlines around the globe are increasingly spotlighting instances of AI bias, from skewed hiring algorithms (Lytton 2024) to discriminatory facial recognition systems (McCallum 2024), exposing the urgent need to address the unintended consequences of AI in our daily lives. These incidents highlight not only the limitations of AI but also the opaque nature of many of these systems, which makes identifying and mitigating bias a significant challenge. Explainable AI (xAI) emerges as a promising solution to shed light on the ‘black box’ of AI decision-making. In a nutshell, xAI can be defined as a set of methods and tools that enables humans to understand the outcomes of AI models (S. Ali et al. 2023). Making the processes behind AI decisions transparent and interpretable through xAI allows all involved stakeholders—developers, users, and policymakers —to detect bias and assess fairness, and hence build trust in AI systems (Leichtmann et al. 2023). xAI aims to answer questions such as: why did the AI make this particular decision?; how would changes in the input data influence the decision?, and in what changes are needed in AI’s behavior to align with ethical and societal values? This is achieved through techniques such as feature importance rankings, model-agnostic interpretability methods, and visualization tools that reveal the relationships between inputs and outputs (Saeed and Omlin 2023).\nAs we have seen in the previous section, the adoption of AI tools and systems in the field of education has opened new opportunities for personalized learning, automated assessments, and early intervention for at-risk students (Mustafa et al. 2024). However, as these systems increasingly influence critical decisions—such as grading, admissions, and resource allocation—they also bring the risk of embedding and perpetuating biases present in training data or algorithms. As such, xAI is of particular relevance in education, where fairness, transparency, and trust should be the basis of an equitable learning environment (Khosravi et al. 2022). For instance, xAI techniques can reveal which factors are most influential in predicting a student’s performance (Saqr and López-Pernas 2024), enabling interventions that are data-informed yet account for individual differences. Similarly, transparency in automated grading systems ensures that assessments are objective (Kumar and Boulanger 2020), and therefore they can be trusted by students and educators. Moreover, xAI can provide students with actionable feedback that is both personalized and understandable (Afzaal et al. 2021), helping them detect areas for improvement.\nIn the remainder of this section, we provide an overview of the main concepts, techniques, and tools associated with xAI. We describe the main AI methodologies, including model-agnostic interpretability, feature importance analysis, and counterfactual reasoning, with an emphasis on how they enable transparency and accountability in AI systems.\n\nThe trade-off between explainability and performance\nWe start our journey into xAI by talking about explainability. We must differentiate between two broad categories of AI models: intrinsically explainable algorithms and black-box models. This distinction is critical for contextualizing the need for explainability tools and techniques discussed in later sections.\n\nIntrinsically Explainable Algorithms\nIntrinsically explainable (or white-box) algorithms are designed to be interpretable by their very nature (Leichtmann et al. 2023). These algorithms have transparent structures and straightforward mechanisms for decision-making, making it easy to understand how input features contribute to output predictions. Examples include linear regression, logistic regression, decision trees, and rule-based systems (Yang, Ye, and Xia 2022). Stakeholders can directly observe relationships, such as the weight of a variable in a regression model (Figure 1 (a)) or the sequence of decisions in a tree structure (Figure 1 (b)).\n\n\n\n\n\n\n\n(a) Explanation of a Linear Regression model.\n\n\n\n\n\n\n\n(b) Explanation of a Decision Tree model\n\n\n\n\nFigure 1. Example of using intrinsically explainable algorithms to predict students’ grades.\n\n\nIn education, intrinsically explainable algorithms are particularly valuable when transparency is a priority. For instance, a linear or logistic regression model predicting a student’s grade based on attendance and participation provides a clear and intuitive explanation for its outcomes. Educators and other stakeholders can readily interpret the model and take actionable steps, such as encouraging students to engage in self-assessment activities.\nHowever, intrinsically explainable models often come with limitations. Their simplicity can result in lower predictive accuracy when dealing with complex, non-linear relationships or high-dimensional data (Yang, Ye, and Xia 2022), which are common in educational contexts and a common problem in learning analytics research and practice. As a result, their applicability is sometimes constrained when achieving high accuracy is critical.\n\n\nBlack-Box Models\nBlack-box models, such as deep neural networks, gradient boosting machines, and ensemble methods (Yang, Ye, and Xia 2022), are characterized by their complex internal structures, which are not inherently interpretable. These models are often chosen for their ability to capture highly complex patterns and relationships in data, hence yielding higher predictive performance in many applications (Leichtmann et al. 2023). However, their complexity creates a significant barrier to understanding how decisions are made.\nIn education, black-box models might be needed when dealing with more complex unstructured data such as students’ free-form text or images (Shafiq et al. 2022). Although these models may achieve higher accuracy than intrinsically explainable algorithms, their opacity raises concerns. Without insights into how predictions are generated, educators and administrators may struggle to trust or act on these outputs, especially when high-stakes decisions that could be consequential for students are involved.\n\n\nBalancing Explainability and Predictive Power\nThe choice between intrinsically explainable algorithms and black-box models often involves a trade-off between interpretability and performance (Yang, Ye, and Xia 2022). Simpler models provide clarity and ease of use, at the expense of failing to capture the complexity of educational data. On the other hand, black-box models excel in handling such complexity but require additional methods to make their decisions interpretable (Saeed and Omlin 2023). xAI aims to bridge this gap by providing explainability techniques for black-box models. These tools enable stakeholders to take advantage of the high performance of black box models while maintaining transparency and accountability such as in the case of white-box models. For example, black-box models predicting student dropout can be paired with interpretability methods to reveal the main factors driving each prediction, hence allowing educators to make informed, data-driven decisions.\n\n\n\nGlobal vs. Local Explainability: xAI tools\nAs we have seen in the previous section, we need xAI tools to be able to explain the outcomes of black-box AI models. We can approach explainability from two complementary perspectives: global and local. Both play a crucial role in understanding and effectively applying AI systems, since both macro-level patterns and individual-level insights are essential for achieving trust and fairness. In this section, we describe each of these perspectives and the xAI tools used in each of them.\n\nGlobal Explainability\nGlobal explainability focuses on understanding the overall behavior of a model. It provides insights into how the model makes decisions across the entire dataset, pointing to the features that are most influential and the patterns that drive predictions. For instance, in a model designed to predict student performance (such as the ones shown in Figure 1) global explainability might reveal that the average quiz score, attendance, and assignment submission are consistently the most important factors that predict performance. This “big picture” explanation enables educators and administrators to evaluate the model’s logic and ensure it aligns with their expectations. Moreover, global explainability is instrumental in assessing whether the model behaves fairly across different demographic groups, helping to identify and address potential biases. However, its high-level nature means that it cannot explain why specific predictions are made, which limits its usefulness in scenarios requiring personalized action. It is, though, useful to take general actions that would benefit “the majority”, for example changing the course design or learning materials. Chapter 6 (Saqr and López-Pernas 2025a) in this book offers a tutorial on global explainability using the R programming language. Below, we overview the main techniques for global explainability\n\nVariable/Feature Importance\nFeature importance measures how much each input variable contributes to a model’s predictions, helping to identify the most influential factors driving its decisions (Wei, Lu, and Song 2015). In educational applications, feature importance provides actionable insights into what matters most for student outcomes. For example, a model predicting grades (see Figure 2) might show that the frequency of forum contributions is the most important factor. This insight helps educators prioritize interventions where they are most needed, such as encouraging participation .\n\n\n\nFigure 2. Variable importance plot when predicting students’ grades\n\n\nIn models like decision trees or random forests, feature importance is calculated based on how often a variable is used in splits and how much it reduces prediction error (e.g., impurity). In linear regression, the magnitude of coefficients represents the relative weight of each variable, directly showing how much they influence the target outcome. These metrics are intrinsic to these models, making them straightforward to compute and interpret.\nFor black-box models, such as neural networks, gradient boosting, or ensemble methods, feature importance is not inherently available and must be derived through additional techniques. Unlike intrinsically interpretable models, black-box models process data in ways that obscure the direct contribution of individual features. To estimate feature importance, methods like permutation importance are often used (Wei, Lu, and Song 2015). This technique evaluates how model performance changes when the values of a feature are randomly shuffled, effectively breaking its relationship with the target variable. Features that cause a significant drop in performance when permuted are deemed more important. The derived feature importance remains an approximation, as black-box models can capture nonlinear interactions that are difficult to fully interpret without additional tools like SHAP or LIME. Thus, while feature importance provides valuable insights into black-box models, it often requires advanced methods to ensure reliability and transparency.\nIt is important to bear in mind that, while feature importance highlights what influences predictions, it does not fully explain how features influence prediction and how features interact with one another, underlining its value as a foundational but limited interpretability tool.\n\n\nPartial Dependence Plots\nPartial Dependence Plots (PDPs) depict how changes in a feature influence predictions while accounting for the average behavior of all other features. They isolate the effect of one feature at a time, making it easier to understand the relationship between input variables and model outcomes. For instance, Figure Figure 3 shows how increasing the number of reads of the forum —while the rest of the features remain the same— has barely any effect in the prediction of students’ grades, while increasing the number of contributions results in a higher grade prediction. This insight allows educators to focus interventions in the variables that are most effective.\n\n\n\nFigure 3. PDP of frequency of forum consume and contribute\n\n\nAlthough PDPs are useful for understanding non-linear relationships and interactions, they rely on the assumption that features are independent. In reality features like reading and writing forum contributions may be correlated, so this assumption can lead to misleading interpretations. Additionally, PDPs represent average effects, which may not fully capture individual variations in how a feature influences predictions. Despite these limitations, PDPs remain a valuable tool for interpreting complex models, especially in contexts like education where understanding feature contributions can inform better decisions.\n\n\nSHAP Summary Plots\nA SHAP summary plot combines two critical aspects of global feature importance: the magnitude of a feature’s impact (ordered from top to bottom in the figure) on the predictions and the range of each feature’s effects (the horizontal axis). For each feature, the plot shows the SHAP values across all instances in the dataset, where each SHAP value represents the contribution of the feature to a single prediction (i.e. individual dots) (Lundberg and Lee 2017). In the figure, top features such as the frequency of forum contributions have greater importance, its wide horizontal dispersion shows that the feature’s contribution varies greatly across instances. Additionally, the color of each point typically reflects the feature value (e.g., high values in red and low values in blue), which helps identify whether higher or lower feature values drive predictions.\nIn educational contexts, SHAP summary plots can provide insights not only into which factors are most critical in predicting outcomes like grades, engagement, or dropout risk, but also about the direction towards which each feature pushes the prediction. For instance, a summary plot (Figure 4) might reveal that frequency of forum contributions has the highest average SHAP values, indicating it is the most influential feature. The plot might also show that a higher number of active days consistently contributes to lower predictions, while regularity in viewing lectures has a more variable effect depending on other factors. This level of detail helps educators and administrators not only understand which features matter most but also how their impact differs across students.\n\n\n\nFigure 4. SHAP summary plot\n\n\nIt is worth mentioning that SHAP plots can also be used for explainability in other ML tasks such as unsupervised learning (e.g., clustering). For instance, (Tiukhova et al. 2024) used clustering to detect study patterns, and leveraged xAI to discover unexpected patterns that are not apparent from the clustering alone.\nLastly, other variants of SHAP exist, such as Kernel SHAP (Roshan and Zafar 2023), which extends SHAP by using kernel methods to efficiently approximate feature attributions, making it suitable for complex models. Permutation SHAP, another variant, evaluates the importance of features by analyzing the change in model performance when feature values are randomly permuted, providing an intuitive measure of feature impact.\n\n\n\nLocal Explainability\nContrary to global explainability, local explainability provides a detailed understanding of individual predictions, explaining why the model arrived at a specific outcome for a single student or instance. For example, if a student is flagged as being at risk of failing or dropping out, local explainability can identify that the prediction was driven by a drop in attendance and low quiz scores, for example. This level of granularity is of the utmost importance in education, where interventions are often targeted at individuals. Teachers, parents, counselors, and administrators can use these explanations to issue tailored interventions, such as reaching out to the student about attendance or offering extra tutoring for quizzes. Furthermore, local explainability builds trust in AI systems by making their decisions transparent and actionable, particularly in high-stakes contexts like grading or admission. Chapter 7 in this book (Saqr and López-Pernas 2025b) offers a tutorial on local explainability using the R programming language.\n\nSHAP Values for Individual Predictions\nSHAP values for individual predictions provide a breakdown of how each feature contributes to a specific prediction made by a machine learning model. Unlike SHAP summary plots, which summarize the overall behavior of the model across all predictions, individual SHAP values focus on explaining why the model made a particular prediction for a single instance. These explanations highlight the direction and magnitude of each feature’s impact on the prediction, making them invaluable for personalized decision-making.\nFor an individual prediction, SHAP values calculate the contribution of each feature by comparing the model’s output when the feature is included versus when it is excluded, averaged across all possible subsets of features. The SHAP values sum up to the difference between the model’s baseline prediction (the average prediction for all instances) and the specific prediction for the instance. For example, in a model predicting a student’s grade (see Figure 5), if the baseline prediction is 68.35 and the model predicts 55.22 for the particular student under examination, the SHAP values will explain the 13-point difference by assigning contributions to features such as frequency and regularity of forum contribute, lecture view, etc. In the plot, we can see that almost all features have a negative value, thus contributing to a lower grade, except for frequency of viewing lectures.\n\n\n\nFigure 5. SHAP instance explanations\n\n\n\n\nLocal Interpretable Model-Agnostic Explanations\nLIME (Local Interpretable Model-Agnostic Explanations) is a method used to explain individual predictions by approximating the behavior of a complex machine learning model with a simpler, interpretable model within a localized region around the specific instance being explained (Ribeiro, Singh, and Guestrin 2016). Unlike SHAP, which relies on theoretical foundations from cooperative game theory, LIME focuses on practical, local interpretability by creating a surrogate model, such as linear regression, to approximate the original model’s decisions in the vicinity of a single instance.\nLIME works by perturbing the input data around the instance to generate a synthetic dataset. The model’s predictions for these perturbed data points are then used to train a simple, interpretable model that mimics the original model locally. For instance, in a model predicting a student’s grade, LIME might generate slightly varied versions of the student’s features (e.g., tweaking the frequency of forum contributions and session count) and observe how the predictions change (see Figure 6). The surrogate model identifies the main features driving the prediction, providing an interpretable explanation of the outcome.\n\n\n\nFigure 6. LIME explanations\n\n\n\n\nCounterfactual Explanations\nCounterfactual explanations offer a way to understand model predictions by identifying the minimal changes in input features needed to achieve a different outcome (Guidotti 2024). Instead of explaining why a particular prediction was made, counterfactual explanations answer the question: What could have been different to achieve the desired result? This makes them distinctly actionable, as they focus on what is required to alter the prediction. In other words, they offer “prescriptive analytics” in which a specific course of action is recommended (Susnjak 2024).\nA counterfactual explanation involves generating a hypothetical instance similar to the original input but modified to produce the desired prediction. For example, in an educational context, if a student is predicted to score below a passing grade, a counterfactual explanation might suggest that increasing forum participation by 20% and improving the regularity of viewing the lectures by 10% would lead to a passing grade. These explanations are of particular value when the focus is often on understanding how to improve outcomes for individual students. Counterfactual explanations are generated using optimization techniques that find the smallest or most plausible changes to input features to alter the prediction.\nLLMs can play a significant role in enhancing the usability and accessibility of counterfactual explanations (Susnjak 2024), particularly in education. Although counterfactual explanations are inherently actionable, their technical nature can make them difficult for educators or learners to interpret and apply without additional support. LLMs, with their natural language generation capabilities, can translate counterfactual outputs into easily understandable, context-specific recommendations. For instance, rather than presenting raw numerical changes in input features, an LLM can reframe the explanation as a conversational suggestion: “To improve your chances of passing, consider increasing your forum participation by 20% and attending lectures more regularly by 10%.”\nA related alternative to counterfactual explanations is the Contrastive Explanation Method (CEM) (Jacovi et al. 2021), which complements counterfactual explanations by focusing on contrastive reasoning. Rather than identifying minimal changes to alter a prediction, CEM emphasizes what features must be present (pertinent positives) and what features must be absent (pertinent negatives) for the current prediction to occur. For instance, in an educational setting, if a student is predicted to excel in a course, CEM might highlight pertinent positives such as consistent lecture engagement and forum participation, while pertinent negatives could include the absence of frequent late submissions. A question CEM aims to answer is: “Why this prediction instead of another?”. Thus, CEM provides a broader understanding of model behavior, offering both prescriptive insights and a deeper contextual explanation.\n\n\nSaliency Maps and Grad-CAM\nSaliency maps and Grad-CAM (Gradient-weighted Class Activation Mapping) are visualization techniques primarily used to explain predictions made by deep learning models (Selvaraju et al. 2017). These methods highlight the parts of the input data (e.g., pixels in an image or words in a text) that are most influential in the model’s decision-making process, providing localized and intuitive explanations of how the model interprets its inputs.\nSaliency maps visualize how sensitive a model’s prediction is to small changes in the input features. They compute the gradient of the model’s output with respect to the input, capturing how each input feature contributes to the prediction. The resulting visualization highlights areas of the input data that, if altered, would most significantly change the prediction. In text data, a saliency map might underline specific words or phrases in a reflective essay that contributed most to predicting a student’s understanding of a concept. Grad-CAM extends saliency maps by providing class-specific explanations. It focuses on the deeper convolutional layers of a neural network, computing gradients with respect to specific target classes. Grad-CAM generates a heatmap that overlays the input data, indicating regions that contributed most strongly to the prediction for a given class (Selvaraju et al. 2017). For instance, in an image classification model predicting whether a student is engaged in a video lecture, Grad-CAM might highlight that the model focused on the student’s face and eyes when assessing engagement. This is extremely relevant to assess bias in AI models. For example, in the automated engagement detection system depicted in Figure 7, it is clear that the fact that the student is wearing an eye patch is affecting the AI decision-making.\n\n\n\nFigure 7. Illustration of how a saliency map can assist in explaining an image classification model for student lecture engagement. The student image has been AI-generated."
  },
  {
    "objectID": "book2/chapters/ch02-AIxAI/ch02-aixai.html#why-xai-may-not-be-enough",
    "href": "book2/chapters/ch02-AIxAI/ch02-aixai.html#why-xai-may-not-be-enough",
    "title": "AI, Explainable AI and Evaluative AI: Informed Data-Driven Decision-Making in Education",
    "section": "Why xAI may not be enough",
    "text": "Why xAI may not be enough\nRecent research on AI in education has shown that algorithms alone cannot fully capture the full gamut of complexity of learners, learning and the way they learn. While AI can identify important variables that might affect a student’s performance, it may —and often does— miss the meaningful connections to educational theories. For instance, recent work has shown that while xAI may offer insights into the factors that predict student performance, it failed to account for the heterogeneity of learners and their learning approaches. Such limitation is rather consequential given that educational interventions are often effective on average but may not be suitable for every student and such, may harm some (Saqr et al. 2024; Saqr and López-Pernas 2024). The researchers also demonstrated that when instance-level explainability was considered, it showed that AI made decisions based on incorrect predictors, leading to mis-predictions. These incorrect predictions show the inherent risk in a fully data-driven approach and the necessity for human oversight or judgment (Saqr and López-Pernas 2024). As such, a hybrid human-AI collaboration, where the expertise of educators complements —and critically evaluates— the explainability of AI might be needed. In doing so, hybrid human-AI collaboration could lead to more effective educational strategies that are responsive to the diverse needs of each learner and less prone to problematic conclusions. Such an approach would improve the reliability of AI predictions and also ensure that AI serves as an aid rather than a substitute for human expertise. It also further emphasizes the central role of critical AI literacy where AI decisions are understood and cortically evaluated.\n\nEvaluative AI: A New Perspective on xAI\nTraditional xAI techniques, such as SHAP and LIME, provide explanations for the single most likely prediction or recommendation done by a system. For example, if an AI system predicts that a student is likely to drop out of a course, the xAI techniques that we have reviewed before can help understand which engagement indicators caused the AI to make that prediction. This can lead to overtrust or misinterpretation by users, especially in complex decision-making environments, since alternative hypotheses are not inspected. A recent approach by Miller (Miller 2023) highlights the limitations of single outputs and introduces the concept of evaluative AI.\nInstead of providing definitive explanations, evaluative AI presents evidence for and against multiple hypotheses, allowing users to critically assess plausible outcomes. This approach could address trust issues by ensuring that AI-driven insights, such as student performance predictions, do not dictate decisions but inform them. For example, the same AI system described earlier could present arguments supporting and refuting the influence of forum participation, encouraging educators and learners to engage in critical analysis rather than passively accepting the AI’s output. This also aligns with Bearman et al. (Bearman et al. 2024), who highlight the importance of developing students’ evaluative judgment —the ability to appraise AI outputs and processes critically. Evaluative AI can not only enhance transparency but also support the critical engagement required for students and educators to maintain autonomy over AI-augmented decisions.Advocates for evaluative AI recognize, however, that this novel paradigm poses an additional cognitive load on the decision-makers compared with usual xAI recommendation-driven outputs, although not as much as having no explanations whatsoever (Miller 2023).\nLastly, it should be noted that evaluative AI does not compete with but rather builds on xAI methods (Miller 2023) and techniques such as feature importance, SHAP, and LIME. Nonetheless, new tools are needed to apply these tools in the framework of evaluative AI. Most likely, these will be more interactive tools that enable testing specific hypotheses in a similar way as local explainability is assessed. Though a system that supports inquiry-based decision-making based on evaluative AI is easy to envision for classic regression or classification models in which the outcome is heavily constrained, it is not as straightforward to picture in the context of generative AI where the possibilities are endless and therefore the need for users’ evaluative judgmentis greater (Bearman et al. 2024)."
  },
  {
    "objectID": "book2/chapters/ch02-AIxAI/ch02-aixai.html#conclusion",
    "href": "book2/chapters/ch02-AIxAI/ch02-aixai.html#conclusion",
    "title": "AI, Explainable AI and Evaluative AI: Informed Data-Driven Decision-Making in Education",
    "section": "Conclusion",
    "text": "Conclusion\nThis chapter has provided an overview of the main applications of AI in education as well as an introduction to xAI as a set of tools and methods for providing transparency and interpretability to otherwise opaque AI-driven systems, which are critical requirements for enabling trust, mitigating bias, and ensuring an ethical use of AI in educational contexts.\nThe recent criticism about AI with regard to bias, fairness and transparency has also raised concerns in the education field (Khalil, Prinsloo, and Slade 2023). Data used in AI models often reflects existing societal inequities, such as socioeconomic, racial, or gender disparities, which can inadvertently lead to models amplifying these biases. For example, patterns in historical educational data may disadvantage underrepresented groups, further perpetuating systemic inequities in learning environments (Madaio et al. 2022). Additionally, the aggregation of data across diverse populations can obscure the unique needs of specific learner subgroups, resulting in interventions that lack contextual relevance or efficacy (Saqr, Vogelsmeier, and López-Pernas 2024). A further challenge lies in the opacity of data processing and decision-making pipelines, which often leave educators and learners unaware of how their data influences predictions and recommendations.\nThe interplay between xAI and LLMs offers a promising synergy that enhances the transparency, usability, and adaptability of AI systems in education. On the one hand, LLMs can improve the accessibility of xAI outputs by converting abstract visualizations, such as feature importance plots or decision attribution maps, into clear textual explanations. This ability ensures that the rationale behind AI-driven decisions is not only available but also comprehensible to non-technical stakeholders. In this way, LLMs serve as an interface that bridges the gap between the technical workings of xAI tools and their practical applications in educational contexts. On the other hand, xAI enhances the application of LLMs by addressing challenges related to their opacity and potential biases. It can clarify why a specific response or recommendation was generated, identify patterns of bias or unfairness in outputs, and explain the logic behind personalization or content adaptations. As these technologies continue to evolve, their integration into education, research, and other fields will further redefine workflows and learning experiences, opening new opportunities for innovation and personalization.\nAs AI systems increasingly influence decision-making in education, it is critical to ensure that xAI tools promote autonomy rather than replace human judgment (Saqr and López-Pernas 2024). Developing evaluative judgment, as Bearman et al. (Bearman et al. 2024) argue, is essential for enabling students and educators to critically engage with AI outputs. Emerging approaches, such as evaluative AI, suggest a shift from single-point explanations to evidence-based decision support, fostering critical thinking among educators and learners. Additionally, tools like LLMs can act as interpreters of xAI outputs, bridging data literacy gaps and encouraging users to challenge AI-driven recommendations. Fostering a culture of inquiry —where explanations are evaluated, not merely accepted— is necessary for xAI to truly empower stakeholders to make informed, ethical decisions in education.\n\n\nAfzaal, Muhammad, Jalal Nouri, Aayesha Zia, Panagiotis Papapetrou, Uno Fors, Yongchao Wu, Xiu Li, and Rebecka Weegar. 2021. “Explainable AI for Data-Driven Feedback and Intelligent Action Recommendations to Support Students Self-Regulation.” Frontiers in Artificial Intelligence 4 (November): 723447. https://doi.org/10.3389/frai.2021.723447.\n\n\nAla-Mutka, Kirsti M. 2005. “A Survey of Automated Assessment Approaches for Programming Assignments.” Computer Science Education 15 (2): 83–102. https://doi.org/10.1080/08993400500150747.\n\n\nAli, Omar, Peter A. Murray, Mujtaba Momin, Yogesh K. Dwivedi, and Tegwen Malik. 2024. “The Effects of Artificial Intelligence Applications in Educational Settings: Challenges and Strategies.” Technological Forecasting and Social Change 199 (123076): 123076. https://doi.org/10.1016/j.techfore.2023.123076.\n\n\nAli, Sajid, Tamer Abuhmed, Shaker El-Sappagh, Khan Muhammad, Jose M. Alonso-Moral, Roberto Confalonieri, Riccardo Guidotti, Javier Del Ser, Natalia Díaz-Rodríguez, and Francisco Herrera. 2023. “Explainable Artificial Intelligence (XAI): What We Know and What Is Left to Attain Trustworthy Artificial Intelligence.” An International Journal on Information Fusion 99 (101805): 101805. https://doi.org/10.1016/j.inffus.2023.101805.\n\n\nAndrejevic, Mark, and Neil Selwyn. 2020. “Facial Recognition Technology in Schools: Critical Questions and Concerns.” Learning, Media and Technology 45 (2): 115–28. https://doi.org/10.1080/17439884.2020.1686014.\n\n\nBaker, Ryan S., and Aaron Hawn. 2022. “Algorithmic Bias in Education.” International Journal of Artificial Intelligence in Education 32 (4): 1052–92. https://doi.org/10.1007/s40593-021-00285-9.\n\n\nBearman, Margaret, Joanna Tai, Phillip Dawson, David Boud, and Rola Ajjawi. 2024. “Developing Evaluative Judgement for a Time of Generative Artificial Intelligence.” Assessment and Evaluation in Higher Education 49 (6): 893–905. https://doi.org/10.1080/02602938.2024.2335321.\n\n\nBedué, Patrick, and Albrecht Fritzsche. 2022. “Can We Trust AI? An Empirical Investigation of Trust Requirements and Guide to Successful AI Adoption.” Journal of Enterprise Information Management 35 (2): 530–49. https://doi.org/10.1108/jeim-06-2020-0233.\n\n\nBogina, Veronika, Alan Hartman, Tsvi Kuflik, and Avital Shulner-Tal. 2022. “Educating Software and AI Stakeholders about Algorithmic Fairness, Accountability, Transparency and Ethics.” International Journal of Artificial Intelligence in Education 32 (3): 808–33. https://doi.org/10.1007/s40593-021-00248-0.\n\n\nBond, Melissa, Hassan Khosravi, Maarten De Laat, Nina Bergdahl, Violeta Negrea, Emily Oxley, Phuong Pham, Sin Wang Chong, and George Siemens. 2024. “A Meta Systematic Review of Artificial Intelligence in Higher Education: A Call for Increased Ethics, Collaboration, and Rigour.” International Journal of Educational Technology in Higher Education 21 (1). https://doi.org/10.1186/s41239-023-00436-z.\n\n\nChaudhry, Muhammad Ali, and Emre Kazim. 2022. “Artificial Intelligence in Education (AIEd): A High-Level Academic and Industry Note 2021.” AI and Ethics 2 (1): 157–65. https://doi.org/10.1007/s43681-021-00074-z.\n\n\nChen, Angxuan, Mengtong Xiang, Junyi Zhou, Jiyou Jia, Junjie Shang, Xinyu Li, Dragan Gašević, and Yizhou Fan. 2025. “Unpacking Help-Seeking Process Through Multimodal Learning Analytics: A Comparative Study of ChatGPT Vs Human Expert.” Computers & Education 226 (105198): 105198. https://doi.org/10.1016/j.compedu.2024.105198.\n\n\nCope, Bill, and Mary Kalantzis. 2023. “A Little History of e-Learning: Finding New Ways to Learn in the PLATO Computer Education System, 1959–1976.” History of Education, January, 1–32. https://doi.org/10.1080/0046760x.2022.2141353.\n\n\nCrow, Tyne, Andrew Luxton-Reilly, and Burkhard Wuensche. 2018. “Intelligent Tutoring Systems for Programming Education: A Systematic Review.” In Proceedings of the 20th Australasian Computing Education Conference, 53–62. ACE ’18. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3160489.3160492.\n\n\nCukurova, Mutlu. 2024. “The Interplay of Learning, Analytics and Artificial Intelligence in Education: A Vision for Hybrid Intelligence.” British Journal of Educational Technology: Journal of the Council for Educational Technology, August. https://doi.org/10.1111/bjet.13514.\n\n\nDörrenbächer, Laura, and Franziska Perels. 2016. “Self-Regulated Learning Profiles in College Students: Their Relationship to Achievement, Personality, and the Effectiveness of an Intervention to Foster Self-Regulated Learning.” Learning and Individual Differences 51 (October): 229–41. https://doi.org/10.1016/j.lindif.2016.09.015.\n\n\nFichten, Catherine, David Pickup, Jennison Asunsion, Mary Jorgensen, Christine Vo, Anick Legault, and Eva Libman. 2022. “State of the Research on Artificial Intelligence Based Apps for Post-Secondary Students with Disabilities.” Exceptionality Education International 31 (1): 62–76. https://doi.org/10.5206/eei.v31i1.14089.\n\n\nGašević, Dragan, Shane Dawson, Tim Rogers, and Danijela Gasevic. 2016. “Learning Analytics Should Not Promote One Size Fits All: The Effects of Instructional Conditions in Predicting Academic Success.” The Internet and Higher Education 28 (January): 68–84. https://doi.org/10.1016/j.iheduc.2015.10.002.\n\n\nGillani, Nabeel, Rebecca Eynon, Catherine Chiabaut, and Kelsey Finkel. 2023. “Unpacking the ‘Black Box’ of AI in Education.” Journal of Educational Technology & Society 26 (1): 99–111. https://www.jstor.org/stable/48707970.\n\n\nGuan, Chong, Jian Mou, and Zhiying Jiang. 2020. “Artificial Intelligence Innovation in Education: A Twenty-Year Data-Driven Historical Analysis.” International Journal of Innovation Studies 4 (4): 134–47. https://doi.org/10.1016/j.ijis.2020.09.001.\n\n\nGuidotti, Riccardo. 2024. “Counterfactual Explanations and How to Find Them: Literature Review and Benchmarking.” Data Mining and Knowledge Discovery 38 (5): 2770–2824. https://doi.org/10.1007/s10618-022-00831-6.\n\n\nHellas, Arto, Petri Ihantola, Andrew Petersen, Vangel V. Ajanovski, Mirela Gutica, Timo Hynninen, Antti Knutas, Juho Leinonen, Chris Messom, and Soohyun Nam Liao. 2018. “Predicting Academic Performance: A Systematic Literature Review.” In Proceedings Companion of the 23rd Annual ACM Conference on Innovation and Technology in Computer Science Education. New York, NY, USA: ACM. https://doi.org/10.1145/3293881.3295783.\n\n\nImani, Maryam, and Gholam Ali Montazer. 2019. “A Survey of Emotion Recognition Methods with Emphasis on e-Learning Environments.” Journal of Network and Computer Applications 147 (102423): 102423. https://doi.org/10.1016/j.jnca.2019.102423.\n\n\nJacovi, Alon, Swabha Swayamdipta, Shauli Ravfogel, Yanai Elazar, Yejin Choi, and Yoav Goldberg. 2021. “Contrastive Explanations for Model Interpretability.” http://arxiv.org/abs/2103.01378.\n\n\nJukiewicz, Marcin. 2024. “The Future of Grading Programming Assignments in Education: The Role of ChatGPT in Automating the Assessment and Feedback Process.” Thinking Skills and Creativity 52 (101522): 101522. https://doi.org/10.1016/j.tsc.2024.101522.\n\n\nKhalil, Mohammad, Paul Prinsloo, and Sharon Slade. 2023. “Fairness, Trust, Transparency, Equity, and Responsibility in Learning Analytics.” Journal of Learning Analytics 10 (1): 1–7. https://doi.org/10.18608/jla.2023.7983.\n\n\nKhosravi, Hassan, Simon Buckingham Shum, Guanliang Chen, Cristina Conati, Yi-Shan Tsai, Judy Kay, Simon Knight, Roberto Martinez-Maldonado, Shazia Sadiq, and Dragan Gašević. 2022. “Explainable Artificial Intelligence in Education.” Computers and Education: Artificial Intelligence 3 (100074): 100074. https://doi.org/10.1016/j.caeai.2022.100074.\n\n\nKumar, Vivekanandan, and David Boulanger. 2020. “Explainable Automated Essay Scoring: Deep Learning Really Has Pedagogical Value.” Frontiers in Education 5 (October): 572367. https://doi.org/10.3389/feduc.2020.572367.\n\n\nLabadze, Lasha, Maya Grigolia, and Lela Machaidze. 2023. “Role of AI Chatbots in Education: Systematic Literature Review.” International Journal of Educational Technology in Higher Education 20 (1). https://doi.org/10.1186/s41239-023-00426-1.\n\n\nLamti, Sanae, Marouane El Malhi, Rachid Sekhsoukh, and Noureddine Kerzazi. 2024. “Intelligent Tutoring System for Medical Students: Opportunities and Challenges.” In Information Systems Engineering and Management, 242–51. Cham: Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-66850-0_27.\n\n\nLeichtmann, Benedikt, Christina Humer, Andreas Hinterreiter, Marc Streit, and Martina Mara. 2023. “Effects of Explainable Artificial Intelligence on Trust and Human Behavior in a High-Risk Decision Task.” Computers in Human Behavior 139 (107539): 107539. https://doi.org/10.1016/j.chb.2022.107539.\n\n\nLin, Chien-Chang, Anna Y. Q. Huang, and Owen H. T. Lu. 2023. “Artificial Intelligence in Intelligent Tutoring Systems Toward Sustainable Education: A Systematic Review.” Smart Learning Environments 10 (1): 1–22. https://doi.org/10.1186/s40561-023-00260-y.\n\n\nLiz-Domínguez, Martín, Manuel Caeiro-Rodríguez, Martín Llamas-Nistal, and Fernando A. Mikic-Fonte. 2019. “Systematic Literature Review of Predictive Analysis Tools in Higher Education.” Applied Sciences (Basel, Switzerland) 9 (24): 5569. https://doi.org/10.3390/app9245569.\n\n\nLópez-Pernas, Sonsoles, and Mohammed Saqr. 2024. “How the Dynamics of Engagement Explain the Momentum of Achievement and the Inertia of Disengagement: A Complex Systems Theory Approach.” Computers in Human Behavior 153 (April): 108126. https://doi.org/10.1016/j.chb.2023.108126.\n\n\nLundberg, Scott M., and Su-In Lee. 2017. “A Unified Approach to Interpreting Model Predictions.” In Advances in Neural Information Processing Systems 30, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 4765–74. Curran Associates, Inc. http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf.\n\n\nLytton, Charlotte. 2024. “AI Hiring Tools May Be Filtering Out the Best Job Applicants.” BBC. https://www.bbc.com/worklife/article/20240214-ai-recruiting-hiring-software-bias-discrimination.\n\n\nMadaio, Michael, Su Lin Blodgett, Elijah Mayfield, and Ezekiel Dixon-Román. 2022. “Beyond ‘Fairness’.” In The Ethics of Artificial Intelligence in Education, 1st Edition, 203–39. New York: Routledge. https://doi.org/10.4324/9780429329067-11.\n\n\nMcCallum, Shiona. 2024. “Payout for Uber Eats Driver over Face Scan Bias Case.” BBC News, March. https://www.bbc.com/news/technology-68655429.\n\n\nMiller, Tim. 2019. “Explanation in Artificial Intelligence: Insights from the Social Sciences.” Artificial Intelligence 267 (February): 1–38. https://doi.org/10.1016/j.artint.2018.07.007.\n\n\n———. 2023. “Explainable AI Is Dead, Long Live Explainable AI!: Hypothesis-Driven Decision Support Using Evaluative AI.” In 2023 ACM Conference on Fairness, Accountability, and Transparency, 333–42. New York, NY, USA: ACM. https://doi.org/10.1145/3593013.3594001.\n\n\nMonett, Dagmar, and Colin W. P. Lewis. 2018. “Getting Clarity by Defining Artificial Intelligence—a Survey.” In Studies in Applied Philosophy, Epistemology and Rational Ethics, 212–14. Studies in Applied Philosophy, Epistemology and Rational Ethics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-96448-5_21.\n\n\nMustafa, Muhammad Yasir, Ahmed Tlili, Georgios Lampropoulos, Ronghuai Huang, Petar Jandrić, Jialu Zhao, Soheil Salha, et al. 2024. “A Systematic Review of Literature Reviews on Artificial Intelligence in Education (AIED): A Roadmap to a Future Research Agenda.” Smart Learning Environments 11 (1): 1–33. https://doi.org/10.1186/s40561-024-00350-5.\n\n\nPinto, Agostinho Sousa, António Abreu, Eusébio Costa, and Jerónimo Paiva. 2023. “How Machine Learning (ML) Is Transforming Higher Education: A Systematic Literature Review.” Journal of Information Systems Engineering & Management 8 (2): 21168. https://doi.org/10.55267/iadt.07.13227.\n\n\nRamesh, Dadi, and Suresh Kumar Sanampudi. 2022. “An Automated Essay Scoring Systems: A Systematic Literature Review.” Artificial Intelligence Review 55 (3): 2495–2527. https://doi.org/10.1007/s10462-021-10068-2.\n\n\nRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. “Why Should i Trust You?: Explaining the Predictions of Any Classifier.” In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. New York, NY, USA: ACM. https://doi.org/10.1145/2939672.2939778.\n\n\nRoshan, Khushnaseeb, and Aasim Zafar. 2023. “Using Kernel SHAP XAI Method to Optimize the Network Anomaly Detection Model.” https://ieeexplore.ieee.org/abstract/document/9763241/.\n\n\nSaeed, Waddah, and Christian Omlin. 2023. “Explainable AI (XAI): A Systematic Meta-Survey of Current Challenges and Future Opportunities.” Knowledge-Based Systems 263 (110273): 110273. https://doi.org/10.1016/j.knosys.2023.110273.\n\n\nSaqr, Mohammed, Rongxin Cheng, Sonsoles López-Pernas, and Emorie D. Beck. 2024. “Idiographic Artificial Intelligence to Explain Students’ Self-Regulation: Toward Precision Education.” Learning and Individual Differences 114 (102499): 102499. https://doi.org/10.1016/j.lindif.2024.102499.\n\n\nSaqr, Mohammed, Jelena Jovanović, Olga Viberg, and Dragan Gašević. 2022. “Is There Order in the Mess? A Single Paper Meta-Analysis Approach to Identification of Predictors of Success in Learning Analytics.” Studies in Higher Education 47 (12): 2370–91. https://doi.org/10.1080/03075079.2022.2061450.\n\n\nSaqr, Mohammed, and Sonsoles López-Pernas. 2022. “How CSCL Roles Emerge, Persist, Transition, and Evolve over Time: A Four-Year Longitudinal Study.” Computers & Education, 104581. https://doi.org/10.1016/j.compedu.2022.104581.\n\n\n———. 2024. “Why Explainable AI May Not Be Enough: Predictions and Mispredictions in Decision Making in Education.” Smart Learning Environments, no. in–press. https://doi.org/10.1186/s40561-024-00343-4.\n\n\n———. 2025a. “Explainable Artificial Intelligence in Education: A Tutorial for Identifying the Variables That Matter.” In Advanced Learning Analytics Methods: AI, Precision and Complexity. Springer.\n\n\n———. 2025b. “Explainable Artificial Intelligence: Evaluating Individual and Local Machine Learning Predictions in r.” In Advanced Learning Analytics Methods: AI, Precision and Complexity. Springer.\n\n\nSaqr, Mohammed, Sonsoles López-Pernas, Jelena Jovanović, and Dragan Gašević. 2023. “Intense, Turbulent, or Wallowing in the Mire: A Longitudinal Study of Cross-Course Online Tactics, Strategies, and Trajectories.” The Internet and Higher Education 57: 100902. https://doi.org/10.1016/j.iheduc.2022.100902.\n\n\nSaqr, Mohammed, Leonie V. D. E. Vogelsmeier, and Sonsoles López-Pernas. 2024. “Capturing Where the Learning Process Takes Place: A Person-Specific and Person-Centered Primer.” Learning and Individual Differences 113 (102492): 102492. https://doi.org/10.1016/j.lindif.2024.102492.\n\n\nSelvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. “Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization.” In Proceedings of the IEEE International Conference on Computer Vision, 618–26. http://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf.\n\n\nShafiq, Dalia Abdulkareem, Mohsen Marjani, Riyaz Ahamed Ariyaluran Habeeb, and David Asirvatham. 2022. “Student Retention Using Educational Data Mining and Predictive Analytics: A Systematic Literature Review.” IEEE Access 10: 72480–503. https://doi.org/10.1109/ACCESS.2022.3188767.\n\n\nSiiman, Leo A., Meeli Rannastu-Avalos, Johanna Pöysä-Tarhonen, Päivi Häkkinen, and Margus Pedaste. 2023. “Opportunities and Challenges for AI-Assisted Qualitative Data Analysis: An Example from Collaborative Problem-Solving Discourse Data.” In Lecture Notes in Computer Science, 87–96. Lecture Notes in Computer Science. Cham: Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-40113-8_9.\n\n\nSusnjak, Teo. 2024. “Beyond Predictive Learning Analytics Modelling and onto eXplainable Artificial Intelligence with Prescriptive Analytics and ChatGPT.” International Journal of Artificial Intelligence in Education 34 (2): 452–82. https://doi.org/10.1007/s40593-023-00336-3.\n\n\nTaylor, Deborah L., Michelle Yeung, and A. Z. Bashet. 2021. “Personalized and Adaptive Learning.” In Innovative Learning Environments in STEM Higher Education, 17–34. SpringerBriefs in Statistics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-58948-6_2.\n\n\nTiukhova, Elena, Pavani Vemuri, María Óskarsdóttir, Stephan Poelmans, Bart Baesens, and Monique Snoeck. 2024. “Discovering Unusual Study Patterns Using Anomaly Detection and XAI.” In Hawaii International Conference on System Sciences 2024 (HICSS-57). https://aisel.aisnet.org/hicss-57/da/learning_analytics/3.\n\n\nUrdaneta-Ponte, María Cora, Amaia Mendez-Zorrilla, and Ibon Oleagordia-Ruiz. 2021. “Recommendation Systems for Education: Systematic Review.” Electronics 10 (14): 1611. https://doi.org/10.3390/electronics10141611.\n\n\nWang, Shan, Fang Wang, Zhen Zhu, Jingxuan Wang, Tam Tran, and Zhao Du. 2024. “Artificial Intelligence in Education: A Systematic Literature Review.” Expert Systems with Applications 252 (124167): 124167. https://doi.org/10.1016/j.eswa.2024.124167.\n\n\nWei, Pengfei, Zhenzhou Lu, and Jingwen Song. 2015. “Variable Importance Analysis: A Comprehensive Review.” Reliability Engineering & System Safety 142 (October): 399–432. https://doi.org/10.1016/j.ress.2015.05.018.\n\n\nWilliams, Peter. 2023. “AI, Analytics and a New Assessment Model for Universities.” Education Sciences 13 (10): 1040. https://doi.org/10.3390/educsci13101040.\n\n\nYan, Lixiang, Roberto Martinez-Maldonado, and Dragan Gasevic. 2024. “Generative Artificial Intelligence in Learning Analytics: Contextualising Opportunities and Challenges Through the Learning Analytics Cycle.” In Proceedings of the 14th Learning Analytics and Knowledge Conference. New York, NY, USA: ACM. https://doi.org/10.1145/3636555.3636856.\n\n\nYang, Guang, Qinghao Ye, and Jun Xia. 2022. “Unbox the Black-Box for the Medical Explainable AI via Multi-Modal and Multi-Centre Data Fusion: A Mini-Review, Two Showcases and Beyond.” An International Journal on Information Fusion 77 (January): 29–52. https://doi.org/10.1016/j.inffus.2021.07.016.\n\n\nZawacki-Richter, Olaf, Victoria I. Marín, Melissa Bond, and Franziska Gouverneur. 2019. “Systematic Review of Research on Artificial Intelligence Applications in Higher Education – Where Are the Educators?” International Journal of Educational Technology in Higher Education 16 (1). https://doi.org/10.1186/s41239-019-0171-0.\n\n\nZhai, Xuesong, Xiaoyan Chu, Ching Sing Chai, Morris Siu Yung Jong, Andreja Istenic, Michael Spector, Jia-Bao Liu, Jing Yuan, and Yan Li. 2021. “A Review of Artificial Intelligence (AI) in Education from 2010 to 2020.” Complexity 2021 (1): 1–18. https://doi.org/10.1155/2021/8812542.\n\n\nZhong, Lin. 2023. “A Systematic Review of Personalized Learning in Higher Education: Learning Content Structure, Learning Materials Sequence, and Learning Readiness Support.” Interactive Learning Environments 31 (10): 7053–73. https://doi.org/10.1080/10494820.2022.2061006."
  },
  {
    "objectID": "book1/preface.html",
    "href": "book1/preface.html",
    "title": "Learning Analytics Methods",
    "section": "",
    "text": "Preface\nA foreword"
  },
  {
    "objectID": "book1/chapters/ch22-conclusion/meta.html",
    "href": "book1/chapters/ch22-conclusion/meta.html",
    "title": "Learning Analytics Methods",
    "section": "",
    "text": "Abstract\nWhereas the field of learning analytics has matured, several methodological and theoretical issues remain unresolved. In this chapter, we discuss the potentials of complex systems as an overarching paradigm for understanding the learning process, learners and the learning environments and how they influence learning. We show how using complex system methodologies open doors for new possibilities that may contribute new knowledge and solve some of the unresolved problems in learning analytics. Furthermore, we unpack the importance of individual differences in advancing the field bringing a much-needed theoretical perspective that could help offer answers to some of our pressing issues."
  },
  {
    "objectID": "book1/chapters/ch22-conclusion/meta.html#dynamics-in-complex-systems",
    "href": "book1/chapters/ch22-conclusion/meta.html#dynamics-in-complex-systems",
    "title": "Learning Analytics Methods",
    "section": "Dynamics in complex systems",
    "text": "Dynamics in complex systems\nThe interactions that maintain a complex system tend to give rise to relatively stable configurations, which can be considered attractor states that emerge again and again Scheffer_2009 Attractors can take many different forms, ranging from chaotic to cyclic to simple point attractors. In the context of learning analytics, for instance, a point attractor could resemble a state of “being relatively engaged”. Importantly, the attractors of complex systems may change over time. The aforementioned point attractor could for instance gradually lose its strength, up to a point where the attractor disappears. When reaching such a tipping point, the system switches to an alternate attractor Scheffer_Carpenter_Dakos_van Nes_2015. Such a shift between different attractors is often labeled a transition. Transitions can be harmful - e.g., reflecting a shift from an adaptive state towards a maladaptive state - or beneficial - e.g., reflecting a reverse shift. Relatively well-investigated dynamics are “critical transitions”, which entail a shift from one stable regime (i.e., point attractor) towards another regime Scheffer_2009 (i.e., other point attractor). For instance, countries can shift between a state of peace towards a state of war and the climate can shift from a greenhouse to an icehouse state Similarly, a learning child may shift between a state of engagement towards a state of disengagement. An important premise of complex systems theory is that such transitions - albeit in very different systems - follow the same generic principles. Among these principles is the idea of critical slowing down Scheffer_Bascompte_Brock_Brovkin_Carpenter_Dakos_Held_van Nes_Rietkerk_Sugihara_2009_Wichers_Schreuder_Goekoop_Groen_2019. Critical slowing down describes that, prior to a critical transition, it becomes increasingly difficult to recover from perturbations Scheffer_Bascompte_Brock_Brovkin_Carpenter_Dakos_Held_van Nes_Rietkerk_Sugihara_2009_Scholz_Kelso_Schöner_1987. In the case of engagement, such perturbations can be school problems (e.g., problems with other pupils). When the student is in a stable, engaged state - and thus, unlikely to experience a transition towards a disengaged state - such perturbations only have a brief effect on the student’s attention. This means that, upon a perturbation, he/she quickly recovers his/her “baseline” engagement levels Masten_Nelson_Gillespie_2022. As the resilience of the engaged state declines, however, the student becomes increasingly affected by these perturbations. This means that recovering his/her normative engagement becomes more and more difficult. This in turn translates to altering system dynamics, meaning that the interactions between and within system elements changes. Monitoring such changes may then allow for anticipating otherwise unpredictable transitions in learning processes Scheffer_Bascompte_Brock_Brovkin_Carpenter_Dakos_Held_van Nes_Rietkerk_Sugihara_2009_Scholz_Kelso_Schöner_1987_Wichers_Schreuder_Goekoop_Groen_2019. Ultimately, this could aid the prevention of harmful transitions or the fostering of beneficial transitions.\nAn important implication of viewing transitions in learning processes through a complex systems lens is that declining resilience may be detectable within systems, which in this case means that inferences are made on the level of the student. This approach contrasts with the common group-level inferences, which may allow for telling who is likely to undergo a transition. For instance, group-level approaches may lead to the notion that “individuals with this behavior, this personality, or this socio-economic background are more likely to drop out of school than others”. Within-individual approaches, in contrast, may allow for determining when a specific individual will drop out. For the purposes of targeted and timely intervention, such insight is invaluable. A related merit of complex systems principles is that they allow for personalization. For instance, it is likely that vulnerability to major changes (e.g., transitions in engagement, school drop-out) manifests in different variables for different individuals. Because declining resilience can be monitored within individuals, such heterogeneity does not pose a challenge. Rather, it can be accommodated by monitoring resilience in those variables that are considered most relevant for this particular student, in this specific context Olthof_Hasselman_Aas_Lamoth_Scholz_Daniels-Wredenhagen_Goldbeck_Weinans_Strunk_Schiepek_et al._2023. In conclusion, the possibility to monitor generic indicators of declining resilience may pave the way for deriving person-specific insights in predicting (and potentially, preventing or stimulating) changes in learning-related processes."
  },
  {
    "objectID": "book1/chapters/ch22-conclusion/meta.html#from-theory-to-practice-measurement-and-analyses",
    "href": "book1/chapters/ch22-conclusion/meta.html#from-theory-to-practice-measurement-and-analyses",
    "title": "Learning Analytics Methods",
    "section": "From theory to practice: measurement and analyses",
    "text": "From theory to practice: measurement and analyses\nIf we agree that the learning phenomena, process or construct can be conceptualized as states in a complex system, then it becomes essential that a complex system lens is used to map the structure and dynamics of the said phenomena Olthof_Hasselman_Oude Maatman_Bosman_Lichtwarck-Aschoff_2023. This has profound consequences for both measurement and data analyses. With respect to measurement, a complex systems lens necessitates the collection of time series data. The reason is that systems - and the interactions between elements within those systems - are by definition time-varying, and it is precisely the changes over time that contain information about the system as a whole. Thus, instead of a single, cross-sectional measurement, a complex systems perspective requires collecting repeated measurements for each individual. With advancing technology, collecting such measurements has become increasingly feasible. Broadly speaking, we can distinguish between passively collected data - which includes mobile sensing data (e.g., typing speed, scrolling, app usage, and sometimes also location) and actigraphy data (e.g., movement, heart rate, skin conductance) - and self-reported data - which is gathered through repeatedly prompting students with a questionnaire on their mood, motivation, or other psychological variables. Both modalities have their pros and cons. The main benefit of passively collected data is the amount of data that can be collected without burdening participants. The other side of the coin is that this amount of data often needs aggregation and intensive cleaning, which is far from straightforward, and in that sense the data can be “hard to handle”. The main benefit of self-report data is that the content of measurement may be more closely related to the construct of interest. However, self-report data require considerable motivation from participants, and it is not inconceivable that such demanding research designs introduce sampling bias. Put differently, it is possible that the types of individuals who engage in studies involving long-term self-reports are not representative of the general population (e.g., in terms of conscientiousness Scollon_Kim-Prieto_Diener_2003). At the same time, however, studies that investigated sampling bias in intensive longitudinal studies involving the collection of repeated self-reports did not find evidence for self-selection Schreuder_Groen_Wigman_Wichers_Hartman_2023. As intuitive as it may seem, scientific evidence for the self-selection of participants into intensive longitudinal studies is thus lacking. Besides these relatively practical considerations, the necessity of time series data also comes with more fundamental questions, for instance, related to the timescale of assessments. Ideally, this timescale should be informed by the timescale at which the system’s dynamics unfold. This in turn varies between constructs: engagement may shift over minutes, while student’s performance may shift over weeks.\nNaturally, the focus on time series data has consequences for the analyses that are useful. Not only do we require time series analyses —which can handle the temporal dependency in the data— but we also need methods that can capture nonlinear and person-specific trends. This is because the dynamics of complex systems are typically non-linear, as illustrated by the erratic behavior and sudden shifts that govern complex systems. Examples of such analytical methods include dynamic time-warp analyses, generalized additive models, recurrence quantification analysis, state space grids, and moving window analyses Hilpert_Marchand_2018. Despite that most learning theories and processes can be described in complex system terms and the long history of theoretical foundations of complex systems in learning sciences, learners’ and learning environments, the uptake of suitable methods and approaches is lagging behind Hilpert_Marchand_2018_Koopmans_2020. Furthermore, applications, framing, and operationalization of learning theories as complex systems are rare in educational research Hilpert_Marchand_2018_Koopmans_2020. In this book, we therefore provide some theoretical underpinnings of a complex systems perspective on learning and education, and we further included several chapters that deal with methods and analyses that accommodate a complex systems lens e.g., psychological networks, Markovian models, and model-based clustering. In other fields, the adoption of such methods has resulted in the renewal of theories, understanding of human behavior, and the emergence of new solutions to real-life problems Borsboom_Haslbeck_Robinaugh_2022_Quintana_2023. Our aim was to help interested researchers to embrace such methods in their analysis."
  },
  {
    "objectID": "book1/chapters/ch22-conclusion/meta.html#complex-systems-and-individual-differences",
    "href": "book1/chapters/ch22-conclusion/meta.html#complex-systems-and-individual-differences",
    "title": "Learning Analytics Methods",
    "section": "Complex systems and individual differences",
    "text": "Complex systems and individual differences\nComplex systems —as a paradigm— facilitates a better understanding of the heterogeneity and individualized nature of human behavior and psychological phenomena. In fact, many complex system methods, some of which described earlier, have a strong emphasis on person-specific fine-grained dynamics. The next section will offer a more in-depth discussion of the individual mechanisms and how they relate to the general average assumptions.\n\nThe Individual\nThe “individual”, or the “self” is a central construct in several learning theories, methodologies, and approaches. For instance, self-regulated learning, self-concept, self-control, and self-directedness to mention a few Panadero_2017. Further, the literature is awash with the notion of personalization, student-centeredness, and adaptive learning. Nonetheless, research is commonly conducted using methods that essentially ignore the “individual” process. In that, research is performed using what is known as variable-centered methods where data is collected from a “group of others'' to derive generalizable laws. In variable-centered methods, researchers compute standard tendency measures (mean or median) from a sample of individuals (often referred to as group-level analysis) to derive”norms” or “standard recommendations”. The average is considered a “norm” where everyone is assumed to fit. What is more, the outcome of such analysis is deemed representative and therefore, generalizable to the population at large. Given that such an average is derived from a sample of others, it rarely represents any single student Fisher_Medaglia_Jeronimus_2018_Winne_2017. An accumulating body of evidence is mounting that humans are heterogeneous with diverse behaviors, attitudes, cognition, and learning approaches. Thereupon, using insights based on group-level analysis has so far resulted in recommendations that don’t work, assumptions that fail to hold, and replications that are hard to obtain. Furthermore, intervention programs or procedures based on such samples offered no more than negligible effects, e.g., Kizilcec_Reich_Yeomans_Dann_Brunskill_Lopez_Turkay_Williams_Tingley_2020.\nThe fact that group-level analysis is less representative of the person is far from new and has been recognized for decades. Yet, the methods that are more suited for person-specific analysis may have not progressed fast enough. The last two decades have witnessed a revolution in data collection methods, statistical approaches, and procedures that allow such analysis, collectively known as person-specific analysis. In many ways, person-specific methods are a paradigm shift in research which according to Molenaar_Campbell_2009 represent a “brink of a major reorientation” that is “no longer an option, but a necessity”. Endorsing a person-specific approach may change how research is performed and how findings are applied Saqr_2023a_Saqr_Lopez-Pernas_2021. The person-specific methods —being individualistic— have low potential for generating generalizable recommendations Saqr_2023a. Therefore, a combination of group-level and person-specific methods may be the best way forward. Such a combination may augment our understanding and provide precise interventions at the high resolution of the single student and sharpen our insights of the group level that are generalizable to the wider population Epskamp_Waldorp_Mõttus_Borsboom_2018.\nThere is an abundance of digital tools and data collection methods that allow the gathering of fine-grained intensive data about students. Such data -where several measurements from the same person are gathered- can allow the analysis of more person-specific insights. In doing so, it can help obtain an accurate view of a student's learning processes and offer more precise personalized support Molenaar_Campbell_2009_Saqr_2023a_Saqr_Lopez-Pernas_2021.\n\n\nHeterogeneity\nAs discussed in the previous section, a central assumption of group-level analysis is that “the average individual” represents every individual. Yet, the average individual very often does not exist Molenaar_2004. To illustrate this problem, let us consider the story of Gilbert Daniels. Daniels was given the task to measure the physical dimensions of more than 4,000 pilots who were part of the American Air Force around 1950. The goal was to find the average pilot size, so that cockpits could be re-designed accordingly. However, a remarkable finding of Daniels was that not a single pilot (out of all pilots who were measured) was approximately equal to the average of the 10 most relevant dimensions. Further, for any given combination of three dimensions, only 4% of pilots would match the average. Hence, he concluded that “The tendency to think in terms of the average man is a pitfall into which many people blunder [...]. Actually, it is virtually impossible to find an average man”. The consequence of this discovery was that most cockpit material became adjustable so that it would suit everyone Rose_2016.\nIt is not difficult to translate Daniels’ findings to the field of LA. Here, too, students are measured in many dimensions. It is often implicitly assumed that the average of those dimensions will illustrate “a representative student”, but this is not the case. To accommodate this lack of “average students”, we should embrace person-centered methods, similar to how the American Airforce embraced adjustable furniture and clothing. In contrast to group-level analyses, person-centered methods attempt to find patterns where differences are minimal, assumptions are likely to hold and apply to wider groups of people. Recently, the range of available person-centered methods has vastly increased, coupled with improving rigor and potential. Therefore, person-centered methods are increasingly endorsed to model heterogeneity and individual differences across a vast range of empirical designs. In the current book, we have introduced several methods for capturing the heterogeneity of multivariate and longitudinal data, and we encourage researchers to take advantage of such data to capture the diversity and individual differences of learners Helske_Helske_Saqr_López-Pernas_Murphy_2024_López-Pernas_Saqr_2024_Saqr_2023b_Scrucca_Saqr_López-Pernas_Murphy_2023."
  },
  {
    "objectID": "book1/chapters/ch01-intro/intro-pdf.html",
    "href": "book1/chapters/ch01-intro/intro-pdf.html",
    "title": "Capturing the Wealth and Diversity of Learning Processes with Learning Analytics Methods",
    "section": "",
    "text": "Keywords: learning analytics, educational data mining, tutorial, methods, learning analytics, curriculum"
  },
  {
    "objectID": "book1/chapters/ch01-intro/intro-pdf.html#introductory-chapters",
    "href": "book1/chapters/ch01-intro/intro-pdf.html#introductory-chapters",
    "title": "Capturing the Wealth and Diversity of Learning Processes with Learning Analytics Methods",
    "section": "Introductory chapters",
    "text": "Introductory chapters\nThe first section of the book provides the basis for getting up to speed with the R programming language and the data that will be analyzed throughout the book. This section covers the fundamental steps of the data analysis process, such as data preprocessing and exploratory analysis. During data preprocessing, educational data is cleaned and prepared for further analysis. Many crucial decisions about building and conceptualizing learning indicators from raw data are made in this essential step of the learning analytics process. Exploratory analysis enables an early detection of interesting phenomena that can be discovered in the data using visualizations or simple statistics. Using these techniques helps to guide the direction of the in-depth analysis and the selection of more advanced analytical methods.\nChapter 2: A Broad Collection of Datasets for Educational Research Training and Application [8]\nSonsoles López-Pernas, Mohammed Saqr, Javier Conde, Laura Del-Río-Carazo\nSince the goal of this book is to provide a guide and tutorial on how to implement learning analytics methods, the use of relevant data is a key aspect of the contextualization of these methods within learning analytics research. Chapter 2 kicks off the book with an introduction to the most relevant types of data in learning analytics and provides a diverse collection of curated datasets that we will use throughout the book to illustrate the different methods. Understanding the data under examination is a crucial step for the interpretability of the analyses that we will learn to perform in this book, and therefore, readers should familiarize themselves with the datasets described in this chapter to facilitate following the tutorials presented in subsequent ones.\nChapter 3: Getting started with R for Education Research [9]\nSanttu Tikka, Juho Kopra, Merja Heinäniemi, Sonsoles López-Pernas, Mohammed Saqr\nThe first tutorial-like chapter of the book provides an introduction to the basics of R programming, with a focus on the Rstudio integrated development environment and the tidyverse programming paradigm. The R programming language has become a popular tool for conducting data analysis in the field of learning analytics. The chapter covers topics such as data types and structures, control structures, pipes, functions, loops, and input/output operations. By the end of the chapter, readers should have a solid understanding of the basics of R programming and have the necessary tools to learn more in-depth topics such as data wrangling and basic statistics using R.\nChapter 4: An R Approach to Data Cleaning and Wrangling for Education [10]\nJuho Kopra, Santtu Tikka, Merja Heinäniemi, Sonsoles López-Pernas, Mohammed Saqr\nAfter learning the basics of the R programming language, Chapter 4 goes one step further by introducing the reader to data wrangling, also known as data cleaning and preprocessing. Data wrangling is a critical step in the data analysis process, particularly in the context of learning analytics. This chapter provides an introduction to data wrangling using R and covers topics such as data importing, cleaning, manipulation, and reshaping with a focus on tidy data. Specifically, readers will learn how to read data from different file formats (e.g., CSV, Excel), how to manipulate data using the dplyr package, and how to reshape data using the tidyr package. Additionally, the chapter covers techniques for combining multiple data sources.\nChapter 5: Introductory Statistics with R for Educational Researchers [11]\nSanttu Tikka, Juho Kopra, Merja Heinäniemi, Sonsoles López-Pernas, Mohammed Saqr\nStatistics play a fundamental role in learning analytics, providing a means to analyze and make sense of the vast amounts of data generated by learning environments, visualize relationships, test hypotheses, and make comparisons. Chapter 5 in this book provides an introduction to basic statistical concepts using R and covers topics such as measures of central tendency, variability, correlation, and regression analysis. Specifically, readers will learn how to compute descriptive statistics, test hypotheses, and perform simple linear regression analysis. The chapter also includes practical examples using realistic data sets from the field of learning analytics. By the end of the chapter, readers should have a solid understanding of the basic statistical concepts and methods commonly used in learning analytics, as well as a practical understanding of how to use R to conduct statistical analysis of learning data.\nChapter 6: Visualizing and Reporting Educational Data with R [12]\nSonsoles López-Pernas, Kamila Misiejuk, Santtu Tikka, Mohammed Saqr, Juho Kopra, Merja Heinäniemi\nVisualizing data is central in learning analytics research, underpins learning dashboards, and is a prime method for reporting results and insights to stakeholders. Chapter 6 guides the reader through the process of generating meaningful and aesthetically pleasing visualizations of different types of datasets using well-known R packages. The main visualization types will be demonstrated with an explanation of their usage and use cases. Furthermore, learning-related examples will be discussed in detail. For instance, readers will learn how to visualize learners’ logs extracted from learning management systems to show how trace data can be used to track students’ learning activities. Readers will also be able to generate professional-looking tables with summary statistics."
  },
  {
    "objectID": "book1/chapters/ch01-intro/intro-pdf.html#machine-learning-methods",
    "href": "book1/chapters/ch01-intro/intro-pdf.html#machine-learning-methods",
    "title": "Capturing the Wealth and Diversity of Learning Processes with Learning Analytics Methods",
    "section": "Machine learning methods",
    "text": "Machine learning methods\nThe next section follows with some of the classic machine learning methods of learning analytics, which date to the early beginnings of the field: predictive modelling and cluster analysis. Predictive modelling is a supervised learning method used widely in learning analytics research, where past data patterns are analysed to predict students’ future outcomes. Clustering is an unsupervised learning method that detects similar patterns in the data and is typically used to group students based on their personal characteristics, observed behavior, or learning outcomes. Both methods are applied to address various challenges in education, such as preventing student drop-out, comparing strategies to improve academic performance, or identifying disengaged students. In addition, the results are often used to trigger specific interventions to help students succeed or to raise awareness about students’ performance based on specific indicators.\nChapter 7: Predictive Modelling in Learning Analytics using R [13]\nJelena Jovanovic, Sonsoles López Pernas, Mohamed Saqr\nPrediction of learners’ course performance has been a central theme in learning analytics since the inception of the field. The main motivation behind it has been to identify learners who are at risk of low achievement so that they could be offered timely support based on intervention strategies derived from analysis of learners’ data. To predict student success, numerous indicators, from varying data sources, have been examined and reported in the literature, as well as various predictive algorithms. Chapter 7 introduces the reader to predictive modeling, through a review of the main objectives, indicators, and algorithms that have been operationalized in previous works as well as a step-by-step tutorial on how to perform predictive modeling in learning analytics using R. The tutorial demonstrates how to predict student success using learning traces originating from a learning management system, guiding the reader through all the required steps from the data preparation to the evaluation of the built models.\nChapter 8: Dissimilarity-based Clustering Educational Data using R [14]\nKeefe Murphy, Sonsoles López-Pernas, Mohammed Saqr\nChapter 8 presents another central method in learning analytics research: clustering. Clustering is a collective term that refers to techniques aimed at uncovering patterns and subgroups within the data. Finding patterns or differences among students enables teachers and researchers to improve their understanding of the diversity of students —and their learning processes— and tailor their support to different needs. This chapter introduces the theory underpinning the dissimilarity-based clustering methods. Then, we focus on some of the most widely-used heuristic dissimilarity-based clustering algorithms; namely, \\(K\\)-means, \\(K\\)-medoids, and agglomerative hierarchical clustering. Methods for choosing the optimal number of clusters are provided, particularly the criteria that can guide the choice of clustering solution among multiple competing methodologies, and not only the choice of the number of clusters \\(K\\) for a given method. All of these are demonstrated in detail with a tutorial in R using a real-life educational dataset.\nChapter 9: An Introduction and R Tutorial to Model-based Clustering in Education via Latent Profile Analysis [15]\nLuca Scrucca, Mohammed Saqr, Sonsoles López-Pernas, Keefe Murphy\nChapter 9 presents an alternative approach for capturing different patterns or subgroups within students’ behavior or functioning. Assuming that there is an average pattern that represents the entirety of student populations requires the measured construct to have the same causal mechanism, the same development pattern, and affect students in exactly the same way. Using a person-centered method (Finite Gaussian mixture model or latent profile analysis), this chapter offers an introduction to model-based clustering that includes the principles of the methods, a guide to the choice of number of clusters, an evaluation of clustering results and a detailed guide with code and a real-life dataset. The tutorial part shows how to uncover the heterogeneity within engagement data by identifying latent or unobserved clusters. The discussion elaborates on the interpretation of the results, the advantages of model-based clustering as well as how this method compares with others."
  },
  {
    "objectID": "book1/chapters/ch01-intro/intro-pdf.html#temporal-methods",
    "href": "book1/chapters/ch01-intro/intro-pdf.html#temporal-methods",
    "title": "Capturing the Wealth and Diversity of Learning Processes with Learning Analytics Methods",
    "section": "Temporal methods",
    "text": "Temporal methods\nWe continue our journey with an introduction to temporal methods in learning analytics. Unlike the methods based on mere counts of events or activities, temporal methods acknowledge the order and temporality of events, as well as the transitions thereof, which are key aspects of learning. Temporal methods have garnered increasing attention since they allow researchers to take advantage of the trace log data that students leave behind when using educational technology and also to study longitudinal processes (e.g., a whole study program). Such methods originate in social sciences and have been imported and adapted into the learning analytics field. We provide three chapters focused on sequence analysis, and two on transition analysis through Markovian modeling and process mining.\nChapter 10: Sequence Analysis in Education: Principles, Technique, and Tutorial with R [16]\nMohammed Saqr, Sonsoles López-Pernas, Satu Helske, Marion Durand, Keefe Murphy, Matthias Studer, Gilbert Ritschard\nSequence analysis is a data mining technique that is increasingly gaining ground in learning analytics. Sequence analysis enables researchers to extract meaningful insights from sequential data, i.e., to summarize the sequential patterns of learning data and classify those patterns into homogeneous groups. Chapter 10 introduces readers to sequence analysis techniques and tools through real-life step-by-step examples of sequential trace log data of students’ online activities. Readers are guided on how to visualize the common sequence plots and interpret such visualizations. An essential part of sequence analysis is the discovery of patterns within sequences through clustering techniques. Therefore, this chapter demonstrates the various sequence clustering methods, calculation of cluster indices, and evaluation of clustering results.\nChapter 11: Modeling the Dynamics of Longitudinal Processes in Education. A tutorial with R for The VaSSTra Method [17]\nSonsoles López-Pernas, Mohammed Saqr\nBuilding upon the knowledge acquired in the previous chapter, Chapter 11 covers VaSSTra, a method for analyzing multiple variables across multiple time points. The idea behind this method is to summarize multiple variables at each time point into a single state using person-based methods. Then, sequence analysis can be used to analyze the sequences of such states for each person, and clustering techniques can be implemented to detect similar trajectories of the evolution of such states. The method is illustrated in a case study about engagement. Several engagement-related variables are derived from students’ online activities (frequency of each activity, regularity, etc.). These variables are used for clustering students into three states (active, moderate, and disengaged) at each course. Then, sequence analysis is used to map the sequence of engagement states across a whole program. Lastly, clustering mechanisms are used to detect distinct trajectories of engagement.\nChapter 12: A Modern Approach to Transition Analysis and Process Mining with Markov Models in Education [18]\nJouni Helske, Satu Helske, Mohammed Saqr, Sonsoles López-Pernas, Keefe Murphy\nChapter 12 presents Markov models, a widely used technique to model temporal processes. Contrary to the deterministic approach seen in the previous sequence analysis chapters, Markovian models are probabilistic models, focusing on the transitions between states instead of studying sequences as a whole. The chapter provides an introduction to Markov models and differentiates between its most common variations: first-order Markov models, hidden Markov models, mixture Markov models, and hidden mixture Markov models. All implementations are illustrated with a step-by-step tutorial using the R package seqHMM using students’ longitudinal data. The chapter also provides a complete guide to performing stochastic process mining with Markovian models as well as plotting, comparing, and clustering different process models\nChapter 13: Multichannel Sequence Analysis in Educational Research Using R [19]\nSonsoles López-Pernas, Satu Helske, Mohammed Saqr, Keefe Murphy\nWhen dealing with learners’ data, sometimes one single source of information is not enough to capture all of the dimensions of the learning process. Fortunately, sequence analysis as a method supports the examination of multiple sequences (termed channels) at the same time as long as they follow the same time scheme. Chapter 13 covers multi-channel sequence analysis, allowing the reader to study and visualize synchronized sequences together, and cluster them into distinct trajectories based on the values of the various channels. We present two methods for clustering: one distance-based (see Chapter 10) and one based on Markovian models (see Chapter 12). We illustrate the method by studying the longitudinal association between student engagement and achievement across a study program.\nChapter 14: The Why, the How, and the When of Educational Process Mining in R [20]\nSonsoles López-Pernas, Mohammed Saqr\nProcess mining is a recent analytical method that enables the extraction of meaningful insights from time-ordered event logs. The goal of process mining is to discover processes from the data, evaluate process efficiency, and help or enhance processes. Since its introduction in education, process mining has been used to map students’ learning processes, visualize learners’ strategies, as well as demonstrate differences in approach to learning across different learning groups. Chapter 14 illustrates how to prepare learners’ data for process mining and how to visualize the process data using the bupaverse framework. Moreover, readers will learn how to examine the transitions between phases or activities within a learning process."
  },
  {
    "objectID": "book1/chapters/ch01-intro/intro-pdf.html#network-analysis",
    "href": "book1/chapters/ch01-intro/intro-pdf.html#network-analysis",
    "title": "Capturing the Wealth and Diversity of Learning Processes with Learning Analytics Methods",
    "section": "Network Analysis",
    "text": "Network Analysis\nThe next section of the book deals with the relational aspects of analyzing educational data such as relationships between students, teachers, and topics. Network analysis is the underlying method used to study such relational aspects. Social network analysis allows researchers to study collaboration and discussion between peers and understand the role each student occupies in the network. Moreover, community finding allows the detection of distinct groups of peers in the network that interact with each other more than with the rest. We can even combine network analysis with the temporal methods presented in the last section through temporal networks or use epistemic or ordered network analysis to explore topic or construct co-occurrence.\nChapter 15: Social Network Analysis: A Primer, a Guide and a Tutorial in R [21]\nMohammed Saqr, Sonsoles López-Pernas, Miguel Ángel Conde, Ángel Hernández-García\nFor five decades, learning networks have been used to map collaboration networks among students, study the influence of peers, and capture the relational dimension of collaborative learning. Additionally, networks have been used to study the semantics of discourse, relations between behaviors, and patterns of relations among teachers. Networks offer a powerful framework with vast potential for data analysis. Chapter 15 introduces the concept and methods of social network analysis and a detailed guide on how researchers can use network analysis using real-world data. The chapter demonstrates network analysis and visualisation with an emphasis on learners’ roles and relevance to the educational context. The chapter further provides a mathematical analysis and interpretation of the different social network metrics such as centrality and betweenness measures with several examples of how they can be used in practice.\nChapter 16: Community Detection in Learning Networks Using R [22]\nÁngel Hernández-García, Carlos Cuenca-Enrique, Adrienne Traxler, Sonsoles López-Pernas, Miguel Ángel Conde, Mohammed Saqr\nIn learning situations, communities can be groups of students within a whole cohort who collaborate with each to a larger extent than with other students in a learning situation. Finding these communities is integral to understanding the interaction process, the structure and behaviour of the formed groups and how they contribute to the overall learning process. Chapter 16 builds on the principles of social networks from Chapter 15 and introduces the topic of community detection. The main aim of community detection is to identify different groups or clusters of nodes within the network that share some similar characteristics. One way of understanding communities in social networks is as subnetworks where the number of internal connections is larger than the number of external connections, and therefore members of a community have a higher probability of being connected to each other than to members of other communities. The chapter focuses on detecting communities (groups of highly connected nodes) within a wider network and shows how to visualize them using R.\nChapter 17: Temporal Network Analysis: Introduction, Methods, and Analysis with R [23]\nMohammed Saqr\nLearning can be viewed as relational, interdependent, and temporal and therefore, methods that account for such multifaceted dynamic processes that unfold overtime are required. Chapter 17 combines the temporal and relational aspects in a single analytics framework: temporal networks. Temporal networks allow modeling of the temporal learning processes i.e., the emergence and flow of activities, communities, and social processes through fine-grained dynamic analysis. This can provide insights into phenomena like knowledge co-construction, information flow, and relationship building. This chapter introduces the basic concepts of temporal networks, their types (i.e, contact and interval), and techniques. The chapter further provides a detailed guide to temporal network analysis, which involves network building, visualization, and statistical analysis at the graph and node level.\nChapter 18: Epistemic Network Analysis and Ordered Network Analysis in Learning Analytics [24]\nYuanru Tan, Zachari Swiecki, Andrew Ruis, David Williamson Shaffer\nThe increasing use of technology in many areas of society and life has led to an increasing amount of Big Data about human behavior and interaction. However, this volume of data is usually too large and strains the capabilities of human interpretation and the traditional social science research approaches. Chapter 18 presents two quantitative ethnographic approaches that links the power of statistics and in-depth ethnographic approaches to understand learning behaviour through large-scale qualitative data. Epistemic Network Analysis (ENA) and Ordered Network Analysis (ONA), are two methods for quantifying, visualizing, and interpreting network data. Taking coded data as input, ENA and ONA represent associations between codes (e.g., topics or categories) in undirected or directed weighted network models, respectively. Both techniques measure the strength of association among codes and illustrate the structure of connections in network graphs, quantify changes in the composition and strength of those connections over time, and enable comparison of networks. The chapter presents a thorough description of the methods and a step-by-step guide on how to implement them with R."
  },
  {
    "objectID": "book1/chapters/ch01-intro/intro-pdf.html#psychometrics",
    "href": "book1/chapters/ch01-intro/intro-pdf.html#psychometrics",
    "title": "Capturing the Wealth and Diversity of Learning Processes with Learning Analytics Methods",
    "section": "Psychometrics",
    "text": "Psychometrics\nWe finalize the book with a section on psychometrics. In the field of educational psychology, psychometrics aims to study how psychological constructs (e.g., intelligence or aptitude) are related to observable variables (e.g., test scores). Traditionally, psychometric methods in educational psychology have relied on self-reported data from validated questionnaire-like instruments, although nowadays researchers have begun to make use of digital data. We present several techniques to investigate the relationship between measured variables and to test hypotheses and theories: psychological networks, factor analysis, and structural equation modeling (SEM).\nChapter 19: Psychological Networks: A Modern Approach to Analysis of Learning and Complex Learning Processes [25]\nMohammed Saqr, Emorie Beck, Sonsoles López-Pernas\nWhen analyzing psychological phenomena that take place in educational settings, a multitude of variables are at play that may interact with, trigger, and influence each other. To understand such dependency between variables, it is not enough to analyze the linear relationships between each pair of variables, but rather such complexity calls for using more sophisticated methods that capture the full breadth of the interplay between variables: psychological networks. ​As opposed to social networks where nodes often represent people and edges represent the interactions or relations between them, the nodes in psychological networks represent observed psychological variables and edges represent a statistical relationship between them. Chapter 19 opens the section on psychometric methods by presenting the concept of psychological networks as well as a tutorial for their estimation, visualization, and interpretation with R.\nChapter 20: Factor Analysis in Education Research using R [26]\nLeonie V. D. E. Vogelsmeier, Mohammed Saqr, Sonsoles López-Pernas, Joran Jongerling\nChapter 20 presents factor analysis, a method employed to reduce a large number of variables into fewer numbers of factors. The method is commonly used to identify which observable indicators are representative of latent, not directly-observed constructs. This is a key step in developing valid instruments to assess latent constructs such as student engagement in educational research. The chapter describes the two main approaches for conducting factor analysis in detail and provides a tutorial on how to implement both techniques. The first is confirmatory factor analysis (CFA), a more theory-driven approach, in which a researcher actively specifies the number of underlying constructs as well as the pattern of relations between these dimensions and observed variables. The second is exploratory factor analysis (EFA), a more data-driven approach, in which the number of underlying constructs is inferred from the data, and all underlying constructs are assumed to influence all observed variables (at least to some degree).\nChapter 21: Structural Equation Modeling with R for Education Scientists [27]\nJoran Jongerling, Sonsoles López-Pernas, Mohammed Saqr, Leonie V. D .E. Vogelsmeier\nChapter 21 presents the last method in our book: Structural Equation Modeling (SEM). SEM is a suitable and useful method for modeling the multitude of relationships between latent variables and the observable indicators, as well as the relationship between the latent variables themselves to test theories. In its most common form, SEM combines CFA (covered in Chapter 20) with another method named path analysis. Just like CFA, SEM relates observed variables to latent variables that are measured by those observed variables and, as path analysis does, SEM allows for a wide range of regression-type relations between sets of variables (both latent and observed). This chapter presents an introduction to SEM, an integrated strategy for conducting SEM analysis that is well-suited for educational sciences, and a tutorial on how to carry out an SEM analysis in R."
  },
  {
    "objectID": "book1/index.html",
    "href": "book1/index.html",
    "title": "Learning Analytics Methods",
    "section": "",
    "text": "Learning Analytics Unit, University of Eastern Finland\n\n\n\n\nThis is the website for the book “Learning analytics methods and tutorials: A practical guide using R” published by Springer in 2024.\n\n\nThe lack of resources and methodological guidance on learning analytics has been a problem ever since the field started, and continues to be a problem today. We thought that the arduous journey in learning analytics should not be endured by everyone and we decided to make that resource with the help of the learning analytics community as well as our collaborators.\n\n\nIn this book, we tried to include all the basics of R as a programming language as well as the basics of data cleaning, statistics, and data manipulation. In doing so, we wanted the newcomers to find an easy entry to the field. We also tried to be as comprehensive as we could and included almost all major methodologies. For every method, we started with the basics, explaining the main concepts, the essential techniques, and basic functions. In subsequent chapters, we went deeper into advanced methods that are at the forefront of novel methodological innovations. We also used real-life learning analytics data and made it readily available to researchers. To do so, we collaborated with world-renowned researchers, package developers, and methodological experts from other fields to offer an unprecedented resource on novel topics that are hard to find any resource for.\n\n\nWe hope the readers find this book useful as a guide through learning analytics methods, highlighting the ways in which data-driven insights can benefit educators, learners, and researchers.\n\n\nThe editors\n\n\nMohammed Saqr and Sonsoles López-Pernas"
  },
  {
    "objectID": "book2/index.html",
    "href": "book2/index.html",
    "title": "Learning Analytics Methods",
    "section": "",
    "text": "Learning Analytics Unit, University of Eastern Finland\n\n\n\n\nThis is the website for the book “Advanced learning analytics methods: AI, precision and complexity” which will be published by Springer in 2025.\n\n\nLearning analytics (LA) has grown over the years, both in methods and practices. As an applied field, LA embraced a wide range of research methodologies and traditions, including those from computer science, education, statistics, and psychology. While such growth has produced a vast body of research and brought significant progress, the past few years have raised several critical questions: What has LA achieved? What is the current status of the field? And, more importantly, what is LA as a field?\n\n\nThe meteoric rise of artificial intelligence (AI), and generative AI in particular, has sent shockwaves through many disciplines, and LA is no exception. As such the field has further expanded to encompass the new technology as it has always done. In doing so, LA has grown even larger and extended to the new areas which raised more questions about what LA is after all. In fact, the borders between LA, AI, educational data mining, and quantitative methods in general have always been unclear.\n\n\nNotwithstanding the LA identity debate, this book addresses several novel issues in LA and education, as well as the overlapping technologies and fields, from a methodological perspective. The book covers a range of topics related to advanced LA methods, including AI, precision education, and complex systems. While we do not claim to define the future of the field, our aim is to guide researchers in exploring some of these barely untrodden territories. We refrain from emphasizing or asserting that any topic lies within the definitive borders of LA. Instead, the book presents these advanced topics of significant interest to educational researchers, without placing undue focus on taxonomy or rigid classifications. Such borders have always been —at least to us— more artificial than real. In fact, almost every methodology we covered in the previous LA methods book came years before LA included them under its veneer. As such, the current book you are reading comprises four overarching methodological themes that reflect its title. While the said title says “advanced LA” we make no stress on LA in its narrow definition, and in fact, many chapters and discussions address the educational communities at large without being restrictive to LA.\n\n\nThe first theme of the book addresses AI and machine learning (ML) which both have become increasingly important nowadays. Including AI and ML topics fills a gap in our previous book and introduces several current topics that are of interest to all researchers and practitioners. We also made sure to include introductory chapters that present these topics in considerable detail to the readers so they can equip themselves with the basic knowledge required to read the dense methodological chapters. This enhancement over the previous book makes it easier for early researchers to have a swift introduction.\n\n\nThe book then introduces the “talk of the town”: Large Language Models (LLMs) and Natural Language Processing (NLP) topics. The second section introduces the foundational concepts of Large Language Models and their potential applications in education in an introductory chapter that discusses the main concepts. Then, several tutorials cover areas like using LLMs to explain predictive models, code text and offer insights to researchers. Relevant to this theme is the use of NLP to understand students’ text which is presented with several advanced techniques.\n\n\nThe editors\n\n\nMohammed Saqr and Sonsoles López-Pernas"
  }
]